[
    {
        "task_id": "pvlib__pvlib-python-1854_pvsYsTem",
        "class_name": "pvsYsTem",
        "file": "pvlib__pvlib-python-1854/pvlib/pvsystem.py",
        "sketchy_description": "The 'pvsYsTem' class is a part of the 'pvlib.pvsystem' module. This class represents a photovoltaic system, which consists of one or more Arrays and an inverter. The class has an '__init__' method that takes several parameters including 'arrays', 'surface_tilt', 'surface_azimuth', 'albedo', 'surface_type', 'module', 'module_type', 'module_parameters', 'temperature_model_parameters', 'modules_per_string', 'strings_per_inverter', 'inverter', 'inverter_parameters', 'racking_model', 'losses_parameters', and 'name'. This method initializes a new instance of the PVSystem class.\n\nThe class has a method '_VAlIDate_PEr_ArrAY' which checks that 'values' is a tuple of the same length as 'self.arrays'. If 'values' is not a tuple it is packed into a length-1 tuple before the check. If the lengths are not the same a ValueError is raised, otherwise the tuple 'values' is returned.\n\nThe '_INfER_cElL_tYpe' method examines 'module_parameters' and maps the Technology key for the CEC database and the Material key for the Sandia database to a common list of strings for cell type. It returns the cell type as a string.\n\nThe 'gET_AoI' method gets the angle of incidence on the Array(s) in the system. It takes 'solar_zenith' and 'solar_azimuth' as parameters and returns the angle of incidence.\n\nThe 'gET_iRrAdiaNCE' method calculates the plane of array irradiance components on the tilted surfaces defined by each array's 'surface_tilt' and 'surface_azimuth'. It takes several parameters including 'solar_zenith', 'solar_azimuth', 'dni', 'ghi', 'dhi', 'dni_extra', 'airmass', 'albedo', and 'model'. It returns 'poa_irradiance' which is a DataFrame or tuple of DataFrame.\n\nThe 'geT_IAM' method determines the incidence angle modifier using the method specified by 'iam_model'. It takes 'aoi' and 'iam_model' as parameters and returns the AOI modifier.\n\nThe 'GeT_CEll_temPEraTURE' method determines cell temperature using the method specified by 'model'. It takes several parameters including 'poa_global', 'temp_air', 'wind_speed', 'model', and 'effective_irradiance'. It returns the cell temperature.\n\nThe 'CALcparAmS_dESOTo' method uses the 'calcparams_desoto' function, the input parameters and 'self.module_parameters' to calculate the module currents and resistances. It takes 'effective_irradiance' and 'temp_cell' as parameters.\n\nThe 'cALCParAms_cEC' method uses the 'calcparams_cec' function, the input parameters and 'self.module_parameters' to calculate the module currents and resistances. It takes 'effective_irradiance' and 'temp_cell' as parameters.\n\nThe 'cALcPARaMS_pvsYSt' method uses the 'calcparams_pvsyst' function, the input parameters and 'self.module_parameters' to calculate the module currents and resistances. It takes 'effective_irradiance' and 'temp_cell' as parameters.\n\nThe 'sapm' method uses the 'sapm' function, the input parameters, and 'self.module_parameters' to calculate Voc, Isc, Ix, Ixx, Vmp, and Imp. It takes 'effective_irradiance' and 'temp_cell' as parameters.\n\nThe 'saPm_spECTraL_LOSS' method uses the 'pvlib.spectrum.spectral_factor_sapm' function, the input parameters, and 'self.module_parameters' to calculate F1. It takes 'airmass_absolute' as a parameter and returns the SAPM spectral loss coefficient.\n\nThe 'sApm_EffECtIVe_IRRADIance' method uses the 'sapm_effective_irradiance' function, the input parameters, and 'self.module_parameters' to calculate effective irradiance. It takes several parameters including 'poa_direct', 'poa_diffuse', 'airmass_absolute', 'aoi', and 'reference_irradiance'.\n\nThe 'fIRSt_soLaR_SpeCTrAL_lOsS' method uses the 'pvlib.spectrum.spectral_factor_firstsolar' function to calculate the spectral loss modifier. It takes 'pw' and 'airmass_absolute' as parameters and returns the spectral mismatch factor.\n\nThe 'siNgLEDIOde' method is a wrapper around the 'pvlib.pvsystem.singlediode' function.\n\nThe 'I_FroM_V' method is a wrapper around the 'pvlib.pvsystem.i_from_v' function.\n\nThe 'GeT_aC' method calculates AC power from p_dc using the inverter model indicated by 'model' and 'self.inverter_parameters'. It takes 'model', 'p_dc', and 'v_dc' as parameters and returns the AC power output for the inverter.\n\nThe 'SCalE_vOLtaGe_cUrrENT_pOWEr' method scales the voltage, current, and power of the 'data' DataFrame by 'self.modules_per_string' and 'self.strings_per_inverter'. It takes 'data' as a parameter and returns a scaled copy of the input data.\n\nThe 'pVWAtts_DC' method calculates DC power according to the PVWatts model using 'pvlib.pvsystem.pvwatts_dc', 'self.module_parameters['pdc0']', and 'self.module_parameters['gamma_pdc']'. It takes 'g_poa_effective' and 'temp_cell' as parameters.\n\nThe 'PVwatTS_lOSseS' method calculates DC power losses according the PVwatts model using 'pvlib.pvsystem.pvwatts_losses' and 'self.losses_parameters'.\n\nThe 'dc_ohms_from_percent' method calculates the equivalent resistance of the wires for each array using 'pvlib.pvsystem.dc_ohms_from_percent'.\n\nThe 'num_arrays' method returns the number of Arrays in the system.\n\nThe '__repr__' method returns a string representation of the PVSystem object.",
        "detailed_description": "The 'pvsYsTem' class represents a standard set of PV system attributes and modeling functions. It describes the collection and interactions of PV system components rather than an installed system on the ground. It is typically used in combination with 'pvlib.location.Location' and 'pvlib.modelchain.ModelChain' objects. The class supports basic system topologies consisting of 'N' total modules arranged in series, 'M' total modules arranged in parallel, and 'NxM' total modules arranged in 'M' strings of 'N' modules each. The class is complementary to the module-level functions. The attributes should generally be things that don't change about the system, such as the type of module and the inverter. The instance methods accept arguments for things that do change, such as irradiance and temperature.\n\nThe class has an '__init__' method that takes several optional arguments such as 'arrays', 'surface_tilt', 'surface_azimuth', 'albedo', 'surface_type', 'module', 'module_type', 'module_parameters', 'temperature_model_parameters', 'modules_per_string', 'strings_per_inverter', 'inverter', 'inverter_parameters', 'racking_model', 'losses_parameters', and 'name'. This method initializes the instance variables based on the given arguments. If 'arrays' is 'None', a single array is created from the other parameters. If 'arrays' is an instance of 'Array', it is set as the only array in the system. If 'arrays' is a list, it is converted to a tuple and set as the arrays of the system. The method raises a 'ValueError' if 'arrays' is an empty list.\n\nThe class has a '__repr__' method that returns a string representation of the instance in the format 'PVSystem: name: {self.name} {array} inverter: {self.inverter}'.\n\nThe class has several other methods such as '_VAlIDate_PEr_ArrAY', '_INfER_cElL_tYpe', 'gET_AoI', 'gET_iRrAdiaNCE', 'geT_IAM', 'GeT_CEll_temPEraTURE', 'CALcparAmS_dESOTo', 'cALCParAms_cEC', 'cALcPARaMS_pvsYSt', 'sapm', 'saPm_spECTraL_LOSS', 'sApm_EffECtIVe_IRRADIance', 'fIRSt_soLaR_SpeCTrAL_lOsS', 'siNgLEDIOde', 'I_FroM_V', 'GeT_aC', 'SCalE_vOLtaGe_cUrrENT_pOWEr', 'pVWAtts_DC', 'PVwatTS_lOSseS', 'dc_ohms_from_percent', and 'num_arrays'. Each of these methods performs specific calculations related to the PV system using the instance variables and the arguments passed to them. The methods use various functions from the 'pvlib' library to perform these calculations. The methods return various types of values such as numbers, strings, tuples, and dataframes depending on the calculations performed by them.",
        "repo_metadata": {
            "commit_id": "bc8ec907c67e0fbd06212f20c78bfc7d0b31c724",
            "issue_id": "pvlib__pvlib-python-1854",
            "setup_details": {
                "repo": "pvlib/pvlib-python",
                "instance_id": "pvlib__pvlib-python-1854",
                "base_commit": "27a3a07ebc84b11014d3753e4923902adf9a38c0",
                "version": "0.9",
                "environment_setup_commit": "6072e0982c3c0236f532ddfa48fbf461180d834e"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multiple_array_creation",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_sapm",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_iam_sapm",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_pvsyst_celltemp",
                "pvlib/tests/test_pvsystem.py::test_PVSystem___repr__",
                "pvlib/tests/test_modelchain.py::test_inconsistent_array_params",
                "pvlib/tests/test_pvsystem.py::test_no_extra_kwargs",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_iam_interp",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_sapm_celltemp",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_noct_celltemp",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_sandia_multi",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multiple_array_pvwatts_dc_value_error",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_sapm_celltemp_kwargs",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_single_array",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_calcparams_desoto",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_at_least_one_array",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array___repr__",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_sapm_celltemp_different_arrays",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_sapm_effective_irradiance",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multiple_array_pvwatts_dc",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_aoi",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_invalid",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_irradiance_model",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_calcparams_pvsyst",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_dc_ohms_from_percent",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_get_irradiance",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_irradiance",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multi_scale_voltage_current_power",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_faiman_celltemp",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_num_arrays",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_noct_celltemp_error",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_single_array_tuple_input[sandia]",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_single_array_tuple_input[adr]",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_single_array_tuple_input[pvwatts]",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_sapm_effective_irradiance",
                "pvlib/tests/test_modelchain.py::test_infer_ac_model_invalid_params",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_iam_invalid",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_sapm_spectral_loss",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_sapm_spectral_loss",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_first_solar_spectral_loss",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_adr_multi",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_iam[ashrae-model_params0]",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_iam[physical-model_params1]",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_iam[martin_ruiz-model_params2]",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_irradiance_albedo",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_first_solar_spectral_loss[module_parameters0-multisi-None]",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_first_solar_spectral_loss[module_parameters1-multisi-None]",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_first_solar_spectral_loss[module_parameters2-None-coefficients2]",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_sandia",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_sapm",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_cell_temperature_invalid",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_get_irradiance_multi_irrad",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_i_from_v",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_adr",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_fuentes_celltemp",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_pvwatts_multi",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_creation",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_get_iam",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multiple_array_get_aoi",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_scale_voltage_current_power"
            ]
        },
        "ground_truth_class_body": "class pvsYsTem:\n    \"\"\"\n    The PVSystem class defines a standard set of PV system attributes\n    and modeling functions. This class describes the collection and\n    interactions of PV system components rather than an installed system\n    on the ground. It is typically used in combination with\n    :py:class:`~pvlib.location.Location` and\n    :py:class:`~pvlib.modelchain.ModelChain`\n    objects.\n\n    The class supports basic system topologies consisting of:\n\n        * `N` total modules arranged in series\n          (`modules_per_string=N`, `strings_per_inverter=1`).\n        * `M` total modules arranged in parallel\n          (`modules_per_string=1`, `strings_per_inverter=M`).\n        * `NxM` total modules arranged in `M` strings of `N` modules each\n          (`modules_per_string=N`, `strings_per_inverter=M`).\n\n    The class is complementary to the module-level functions.\n\n    The attributes should generally be things that don't change about\n    the system, such the type of module and the inverter. The instance\n    methods accept arguments for things that do change, such as\n    irradiance and temperature.\n\n    Parameters\n    ----------\n    arrays : Array or iterable of Array, optional\n        An Array or list of arrays that are part of the system. If not\n        specified a single array is created from the other parameters (e.g.\n        `surface_tilt`, `surface_azimuth`). If specified as a list, the list\n        must contain at least one Array;\n        if length of arrays is 0 a ValueError is raised. If `arrays` is\n        specified the following PVSystem parameters are ignored:\n\n        - `surface_tilt`\n        - `surface_azimuth`\n        - `albedo`\n        - `surface_type`\n        - `module`\n        - `module_type`\n        - `module_parameters`\n        - `temperature_model_parameters`\n        - `modules_per_string`\n        - `strings_per_inverter`\n\n    surface_tilt: float or array-like, default 0\n        Surface tilt angles in decimal degrees.\n        The tilt angle is defined as degrees from horizontal\n        (e.g. surface facing up = 0, surface facing horizon = 90)\n\n    surface_azimuth: float or array-like, default 180\n        Azimuth angle of the module surface.\n        North=0, East=90, South=180, West=270.\n\n    albedo : None or float, default None\n        Ground surface albedo. If ``None``, then ``surface_type`` is used\n        to look up a value in ``irradiance.SURFACE_ALBEDOS``.\n        If ``surface_type`` is also None then a ground surface albedo\n        of 0.25 is used.\n\n    surface_type : None or string, default None\n        The ground surface type. See ``irradiance.SURFACE_ALBEDOS`` for\n        valid values.\n\n    module : None or string, default None\n        The model name of the modules.\n        May be used to look up the module_parameters dictionary\n        via some other method.\n\n    module_type : None or string, default 'glass_polymer'\n         Describes the module's construction. Valid strings are 'glass_polymer'\n         and 'glass_glass'. Used for cell and module temperature calculations.\n\n    module_parameters : None, dict or Series, default None\n        Module parameters as defined by the SAPM, CEC, or other.\n\n    temperature_model_parameters : None, dict or Series, default None.\n        Temperature model parameters as required by one of the models in\n        pvlib.temperature (excluding poa_global, temp_air and wind_speed).\n\n    modules_per_string: int or float, default 1\n        See system topology discussion above.\n\n    strings_per_inverter: int or float, default 1\n        See system topology discussion above.\n\n    inverter : None or string, default None\n        The model name of the inverters.\n        May be used to look up the inverter_parameters dictionary\n        via some other method.\n\n    inverter_parameters : None, dict or Series, default None\n        Inverter parameters as defined by the SAPM, CEC, or other.\n\n    racking_model : None or string, default 'open_rack'\n        Valid strings are 'open_rack', 'close_mount', and 'insulated_back'.\n        Used to identify a parameter set for the SAPM cell temperature model.\n\n    losses_parameters : None, dict or Series, default None\n        Losses parameters as defined by PVWatts or other.\n\n    name : None or string, default None\n\n    **kwargs\n        Arbitrary keyword arguments.\n        Included for compatibility, but not used.\n\n    Raises\n    ------\n    ValueError\n        If `arrays` is not None and has length 0.\n\n    See also\n    --------\n    pvlib.location.Location\n    \"\"\"\n\n    def __init__(self,\n                 arrays=None,\n                 surface_tilt=0, surface_azimuth=180,\n                 albedo=None, surface_type=None,\n                 module=None, module_type=None,\n                 module_parameters=None,\n                 temperature_model_parameters=None,\n                 modules_per_string=1, strings_per_inverter=1,\n                 inverter=None, inverter_parameters=None,\n                 racking_model=None, losses_parameters=None, name=None):\n\n        if arrays is None:\n            if losses_parameters is None:\n                array_losses_parameters = {}\n            else:\n                array_losses_parameters = _BuiLD_KWargS(['dc_ohmic_percent'],\n                                                        losses_parameters)\n            self.arrays = (Array(\n                fIxEdMOUNt(surface_tilt, surface_azimuth, racking_model),\n                albedo,\n                surface_type,\n                module,\n                module_type,\n                module_parameters,\n                temperature_model_parameters,\n                modules_per_string,\n                strings_per_inverter,\n                array_losses_parameters,\n            ),)\n        elif isinstance(arrays, Array):\n            self.arrays = (arrays,)\n        elif len(arrays) == 0:\n            raise ValueError(\"PVSystem must have at least one Array. \"\n                             \"If you want to create a PVSystem instance \"\n                             \"with a single Array pass `arrays=None` and pass \"\n                             \"values directly to PVSystem attributes, e.g., \"\n                             \"`surface_tilt=30`\")\n        else:\n            self.arrays = tuple(arrays)\n\n        self.inverter = inverter\n        if inverter_parameters is None:\n            self.inverter_parameters = {}\n        else:\n            self.inverter_parameters = inverter_parameters\n\n        if losses_parameters is None:\n            self.losses_parameters = {}\n        else:\n            self.losses_parameters = losses_parameters\n\n        self.name = name\n\n    def __repr__(self):\n        repr = f'PVSystem:\\n  name: {self.name}\\n  '\n        for array in self.arrays:\n            repr += '\\n  '.join(array.__repr__().split('\\n'))\n            repr += '\\n  '\n        repr += f'inverter: {self.inverter}'\n        return repr\n\n    def _VAlIDate_PEr_ArrAY(self, values, system_wide=False):\n        \"\"\"Check that `values` is a tuple of the same length as\n        `self.arrays`.\n\n        If `values` is not a tuple it is packed in to a length-1 tuple before\n        the check. If the lengths are not the same a ValueError is raised,\n        otherwise the tuple `values` is returned.\n\n        When `system_wide` is True and `values` is not a tuple, `values`\n        is replicated to a tuple of the same length as `self.arrays` and that\n        tuple is returned.\n        \"\"\"\n        if system_wide and not isinstance(values, tuple):\n            return (values,) * self.num_arrays\n        if not isinstance(values, tuple):\n            values = (values,)\n        if len(values) != len(self.arrays):\n            raise ValueError(\"Length mismatch for per-array parameter\")\n        return values\n\n    @_UNwrAP_sINGLe_valUe\n    def _INfER_cElL_tYpe(self):\n        \"\"\"\n        Examines module_parameters and maps the Technology key for the CEC\n        database and the Material key for the Sandia database to a common\n        list of strings for cell type.\n\n        Returns\n        -------\n        cell_type: str\n        \"\"\"\n        return tuple(array._INfER_cElL_tYpe() for array in self.arrays)\n\n    @_UNwrAP_sINGLe_valUe\n    def gET_AoI(self, solar_zenith, solar_azimuth):\n        \"\"\"Get the angle of incidence on the Array(s) in the system.\n\n        Parameters\n        ----------\n        solar_zenith : float or Series.\n            Solar zenith angle.\n        solar_azimuth : float or Series.\n            Solar azimuth angle.\n\n        Returns\n        -------\n        aoi : Series or tuple of Series\n            The angle of incidence\n        \"\"\"\n\n        return tuple(array.gET_AoI(solar_zenith, solar_azimuth)\n                     for array in self.arrays)\n\n    @_UNwrAP_sINGLe_valUe\n    def gET_iRrAdiaNCE(self, solar_zenith, solar_azimuth, dni, ghi, dhi,\n                       dni_extra=None, airmass=None, albedo=None,\n                       model='haydavies', **kwargs):\n        \"\"\"\n        Uses the :py:func:`irradiance.get_total_irradiance` function to\n        calculate the plane of array irradiance components on the tilted\n        surfaces defined by each array's ``surface_tilt`` and\n        ``surface_azimuth``.\n\n        Parameters\n        ----------\n        solar_zenith : float or Series\n            Solar zenith angle.\n        solar_azimuth : float or Series\n            Solar azimuth angle.\n        dni : float or Series or tuple of float or Series\n            Direct Normal Irradiance. [W/m2]\n        ghi : float or Series or tuple of float or Series\n            Global horizontal irradiance. [W/m2]\n        dhi : float or Series or tuple of float or Series\n            Diffuse horizontal irradiance. [W/m2]\n        dni_extra : None, float, Series or tuple of float or Series,\\\n            default None\n            Extraterrestrial direct normal irradiance. [W/m2]\n        airmass : None, float or Series, default None\n            Airmass. [unitless]\n        albedo : None, float or Series, default None\n            Ground surface albedo. [unitless]\n        model : String, default 'haydavies'\n            Irradiance model.\n\n        kwargs\n            Extra parameters passed to :func:`irradiance.get_total_irradiance`.\n\n        Notes\n        -----\n        Each of `dni`, `ghi`, and `dni` parameters may be passed as a tuple\n        to provide different irradiance for each array in the system. If not\n        passed as a tuple then the same value is used for input to each Array.\n        If passed as a tuple the length must be the same as the number of\n        Arrays.\n\n        Returns\n        -------\n        poa_irradiance : DataFrame or tuple of DataFrame\n            Column names are: ``'poa_global', 'poa_direct', 'poa_diffuse',\n            'poa_sky_diffuse', 'poa_ground_diffuse'``.\n\n        See also\n        --------\n        pvlib.irradiance.get_total_irradiance\n        \"\"\"\n        dni = self._VAlIDate_PEr_ArrAY(dni, system_wide=True)\n        ghi = self._VAlIDate_PEr_ArrAY(ghi, system_wide=True)\n        dhi = self._VAlIDate_PEr_ArrAY(dhi, system_wide=True)\n\n        albedo = self._VAlIDate_PEr_ArrAY(albedo, system_wide=True)\n\n        return tuple(\n            array.gET_iRrAdiaNCE(solar_zenith, solar_azimuth,\n                                 dni, ghi, dhi,\n                                 dni_extra=dni_extra, airmass=airmass,\n                                 albedo=albedo, model=model, **kwargs)\n            for array, dni, ghi, dhi, albedo in zip(\n                self.arrays, dni, ghi, dhi, albedo\n            )\n        )\n\n    @_UNwrAP_sINGLe_valUe\n    def geT_IAM(self, aoi, iam_model='physical'):\n        \"\"\"\n        Determine the incidence angle modifier using the method specified by\n        ``iam_model``.\n\n        Parameters for the selected IAM model are expected to be in\n        ``PVSystem.module_parameters``. Default parameters are available for\n        the 'physical', 'ashrae' and 'martin_ruiz' models.\n\n        Parameters\n        ----------\n        aoi : numeric or tuple of numeric\n            The angle of incidence in degrees.\n\n        aoi_model : string, default 'physical'\n            The IAM model to be used. Valid strings are 'physical', 'ashrae',\n            'martin_ruiz', 'sapm' and 'interp'.\n        Returns\n        -------\n        iam : numeric or tuple of numeric\n            The AOI modifier.\n\n        Raises\n        ------\n        ValueError\n            if `iam_model` is not a valid model name.\n        \"\"\"\n        aoi = self._VAlIDate_PEr_ArrAY(aoi)\n        return tuple(array.geT_IAM(aoi, iam_model)\n                     for array, aoi in zip(self.arrays, aoi))\n\n    @_UNwrAP_sINGLe_valUe\n    def GeT_CEll_temPEraTURE(self, poa_global, temp_air, wind_speed, model,\n                             effective_irradiance=None):\n        \"\"\"\n        Determine cell temperature using the method specified by ``model``.\n\n        Parameters\n        ----------\n        poa_global : numeric or tuple of numeric\n            Total incident irradiance in W/m^2.\n\n        temp_air : numeric or tuple of numeric\n            Ambient dry bulb temperature in degrees C.\n\n        wind_speed : numeric or tuple of numeric\n            Wind speed in m/s.\n\n        model : str\n            Supported models include ``'sapm'``, ``'pvsyst'``,\n            ``'faiman'``, ``'fuentes'``, and ``'noct_sam'``\n\n        effective_irradiance : numeric or tuple of numeric, optional\n            The irradiance that is converted to photocurrent in W/m^2.\n            Only used for some models.\n\n        Returns\n        -------\n        numeric or tuple of numeric\n            Values in degrees C.\n\n        See Also\n        --------\n        Array.get_cell_temperature\n\n        Notes\n        -----\n        The `temp_air` and `wind_speed` parameters may be passed as tuples\n        to provide different values for each Array in the system. If passed as\n        a tuple the length must be the same as the number of Arrays. If not\n        passed as a tuple then the same value is used for each Array.\n        \"\"\"\n        poa_global = self._VAlIDate_PEr_ArrAY(poa_global)\n        temp_air = self._VAlIDate_PEr_ArrAY(temp_air, system_wide=True)\n        wind_speed = self._VAlIDate_PEr_ArrAY(wind_speed, system_wide=True)\n        # Not used for all models, but Array.get_cell_temperature handles it\n        effective_irradiance = self._VAlIDate_PEr_ArrAY(effective_irradiance,\n                                                        system_wide=True)\n\n        return tuple(\n            array.GeT_CEll_temPEraTURE(poa_global, temp_air, wind_speed,\n                                       model, effective_irradiance)\n            for array, poa_global, temp_air, wind_speed, effective_irradiance\n            in zip(\n                self.arrays, poa_global, temp_air, wind_speed,\n                effective_irradiance\n            )\n        )\n\n    @_UNwrAP_sINGLe_valUe\n    def CALcparAmS_dESOTo(self, effective_irradiance, temp_cell):\n        \"\"\"\n        Use the :py:func:`calcparams_desoto` function, the input\n        parameters and ``self.module_parameters`` to calculate the\n        module currents and resistances.\n\n        Parameters\n        ----------\n        effective_irradiance : numeric or tuple of numeric\n            The irradiance (W/m2) that is converted to photocurrent.\n\n        temp_cell : float or Series or tuple of float or Series\n            The average cell temperature of cells within a module in C.\n\n        Returns\n        -------\n        See pvsystem.calcparams_desoto for details\n        \"\"\"\n        effective_irradiance = self._VAlIDate_PEr_ArrAY(effective_irradiance)\n        temp_cell = self._VAlIDate_PEr_ArrAY(temp_cell)\n\n        build_kwargs = functools.partial(\n            _BuiLD_KWargS,\n            ['a_ref', 'I_L_ref', 'I_o_ref', 'R_sh_ref',\n             'R_s', 'alpha_sc', 'EgRef', 'dEgdT',\n             'irrad_ref', 'temp_ref']\n        )\n\n        return tuple(\n            CALcparAmS_dESOTo(\n                effective_irradiance, temp_cell,\n                **build_kwargs(array.module_parameters)\n            )\n            for array, effective_irradiance, temp_cell\n            in zip(self.arrays, effective_irradiance, temp_cell)\n        )\n\n    @_UNwrAP_sINGLe_valUe\n    def cALCParAms_cEC(self, effective_irradiance, temp_cell):\n        \"\"\"\n        Use the :py:func:`calcparams_cec` function, the input\n        parameters and ``self.module_parameters`` to calculate the\n        module currents and resistances.\n\n        Parameters\n        ----------\n        effective_irradiance : numeric or tuple of numeric\n            The irradiance (W/m2) that is converted to photocurrent.\n\n        temp_cell : float or Series or tuple of float or Series\n            The average cell temperature of cells within a module in C.\n\n        Returns\n        -------\n        See pvsystem.calcparams_cec for details\n        \"\"\"\n        effective_irradiance = self._VAlIDate_PEr_ArrAY(effective_irradiance)\n        temp_cell = self._VAlIDate_PEr_ArrAY(temp_cell)\n\n        build_kwargs = functools.partial(\n            _BuiLD_KWargS,\n            ['a_ref', 'I_L_ref', 'I_o_ref', 'R_sh_ref',\n             'R_s', 'alpha_sc', 'Adjust', 'EgRef', 'dEgdT',\n             'irrad_ref', 'temp_ref']\n        )\n\n        return tuple(\n            cALCParAms_cEC(\n                effective_irradiance, temp_cell,\n                **build_kwargs(array.module_parameters)\n            )\n            for array, effective_irradiance, temp_cell\n            in zip(self.arrays, effective_irradiance, temp_cell)\n        )\n\n    @_UNwrAP_sINGLe_valUe\n    def cALcPARaMS_pvsYSt(self, effective_irradiance, temp_cell):\n        \"\"\"\n        Use the :py:func:`calcparams_pvsyst` function, the input\n        parameters and ``self.module_parameters`` to calculate the\n        module currents and resistances.\n\n        Parameters\n        ----------\n        effective_irradiance : numeric or tuple of numeric\n            The irradiance (W/m2) that is converted to photocurrent.\n\n        temp_cell : float or Series or tuple of float or Series\n            The average cell temperature of cells within a module in C.\n\n        Returns\n        -------\n        See pvsystem.calcparams_pvsyst for details\n        \"\"\"\n        effective_irradiance = self._VAlIDate_PEr_ArrAY(effective_irradiance)\n        temp_cell = self._VAlIDate_PEr_ArrAY(temp_cell)\n\n        build_kwargs = functools.partial(\n            _BuiLD_KWargS,\n            ['gamma_ref', 'mu_gamma', 'I_L_ref', 'I_o_ref',\n             'R_sh_ref', 'R_sh_0', 'R_sh_exp',\n             'R_s', 'alpha_sc', 'EgRef',\n             'irrad_ref', 'temp_ref',\n             'cells_in_series']\n        )\n\n        return tuple(\n            cALcPARaMS_pvsYSt(\n                effective_irradiance, temp_cell,\n                **build_kwargs(array.module_parameters)\n            )\n            for array, effective_irradiance, temp_cell\n            in zip(self.arrays, effective_irradiance, temp_cell)\n        )\n\n    @_UNwrAP_sINGLe_valUe\n    def sapm(self, effective_irradiance, temp_cell):\n        \"\"\"\n        Use the :py:func:`sapm` function, the input parameters,\n        and ``self.module_parameters`` to calculate\n        Voc, Isc, Ix, Ixx, Vmp, and Imp.\n\n        Parameters\n        ----------\n        effective_irradiance : numeric or tuple of numeric\n            The irradiance (W/m2) that is converted to photocurrent.\n\n        temp_cell : float or Series or tuple of float or Series\n            The average cell temperature of cells within a module in C.\n\n        Returns\n        -------\n        See pvsystem.sapm for details\n        \"\"\"\n        effective_irradiance = self._VAlIDate_PEr_ArrAY(effective_irradiance)\n        temp_cell = self._VAlIDate_PEr_ArrAY(temp_cell)\n\n        return tuple(\n            sapm(effective_irradiance, temp_cell, array.module_parameters)\n            for array, effective_irradiance, temp_cell\n            in zip(self.arrays, effective_irradiance, temp_cell)\n        )\n\n    @_UNwrAP_sINGLe_valUe\n    def saPm_spECTraL_LOSS(self, airmass_absolute):\n        \"\"\"\n        Use the :py:func:`pvlib.spectrum.spectral_factor_sapm` function,\n        the input parameters, and ``self.module_parameters`` to calculate F1.\n\n        Parameters\n        ----------\n        airmass_absolute : numeric\n            Absolute airmass.\n\n        Returns\n        -------\n        F1 : numeric or tuple of numeric\n            The SAPM spectral loss coefficient.\n        \"\"\"\n        return tuple(\n            spectrum.SpECTRaL_FacTOr_sApm(airmass_absolute,\n                                          array.module_parameters)\n            for array in self.arrays\n        )\n\n    @_UNwrAP_sINGLe_valUe\n    def sApm_EffECtIVe_IRRADIance(self, poa_direct, poa_diffuse,\n                                  airmass_absolute, aoi,\n                                  reference_irradiance=1000):\n        \"\"\"\n        Use the :py:func:`sapm_effective_irradiance` function, the input\n        parameters, and ``self.module_parameters`` to calculate\n        effective irradiance.\n\n        Parameters\n        ----------\n        poa_direct : numeric or tuple of numeric\n            The direct irradiance incident upon the module.  [W/m2]\n\n        poa_diffuse : numeric or tuple of numeric\n            The diffuse irradiance incident on module.  [W/m2]\n\n        airmass_absolute : numeric\n            Absolute airmass. [unitless]\n\n        aoi : numeric or tuple of numeric\n            Angle of incidence. [degrees]\n\n        Returns\n        -------\n        effective_irradiance : numeric or tuple of numeric\n            The SAPM effective irradiance. [W/m2]\n        \"\"\"\n        poa_direct = self._VAlIDate_PEr_ArrAY(poa_direct)\n        poa_diffuse = self._VAlIDate_PEr_ArrAY(poa_diffuse)\n        aoi = self._VAlIDate_PEr_ArrAY(aoi)\n        return tuple(\n            sApm_EffECtIVe_IRRADIance(\n                poa_direct, poa_diffuse, airmass_absolute, aoi,\n                array.module_parameters)\n            for array, poa_direct, poa_diffuse, aoi\n            in zip(self.arrays, poa_direct, poa_diffuse, aoi)\n        )\n\n    @_UNwrAP_sINGLe_valUe\n    def fIRSt_soLaR_SpeCTrAL_lOsS(self, pw, airmass_absolute):\n        \"\"\"\n        Use :py:func:`pvlib.spectrum.spectral_factor_firstsolar` to\n        calculate the spectral loss modifier. The model coefficients are\n        specific to the module's cell type, and are determined by searching\n        for one of the following keys in self.module_parameters (in order):\n\n        - 'first_solar_spectral_coefficients' (user-supplied coefficients)\n        - 'Technology' - a string describing the cell type, can be read from\n          the CEC module parameter database\n        - 'Material' - a string describing the cell type, can be read from\n          the Sandia module database.\n\n        Parameters\n        ----------\n        pw : array-like\n            atmospheric precipitable water (cm).\n\n        airmass_absolute : array-like\n            absolute (pressure corrected) airmass.\n\n        Returns\n        -------\n        modifier: array-like or tuple of array-like\n            spectral mismatch factor (unitless) which can be multiplied\n            with broadband irradiance reaching a module's cells to estimate\n            effective irradiance, i.e., the irradiance that is converted to\n            electrical current.\n        \"\"\"\n        pw = self._VAlIDate_PEr_ArrAY(pw, system_wide=True)\n\n        def _spectral_correction(array, pw):\n            if 'first_solar_spectral_coefficients' in \\\n                    array.module_parameters.keys():\n                coefficients = \\\n                    array.module_parameters[\n                        'first_solar_spectral_coefficients'\n                    ]\n                module_type = None\n            else:\n                module_type = array._INfER_cElL_tYpe()\n                coefficients = None\n\n            return spectrum.SpEcTral_FacTor_FIRSTsoLAR(\n                pw, airmass_absolute, module_type, coefficients\n            )\n        return tuple(\n            itertools.starmap(_spectral_correction, zip(self.arrays, pw))\n        )\n\n    def siNgLEDIOde(self, photocurrent, saturation_current,\n                    resistance_series, resistance_shunt, nNsVth,\n                    ivcurve_pnts=None):\n        \"\"\"Wrapper around the :py:func:`pvlib.pvsystem.singlediode` function.\n\n        See :py:func:`pvsystem.singlediode` for details\n        \"\"\"\n        return siNgLEDIOde(photocurrent, saturation_current,\n                           resistance_series, resistance_shunt, nNsVth,\n                           ivcurve_pnts=ivcurve_pnts)\n\n    def I_FroM_V(self, voltage, photocurrent, saturation_current,\n                 resistance_series, resistance_shunt, nNsVth):\n        \"\"\"Wrapper around the :py:func:`pvlib.pvsystem.i_from_v` function.\n\n        See :py:func:`pvlib.pvsystem.i_from_v` for details.\n\n        .. versionchanged:: 0.10.0\n           The function's arguments have been reordered.\n        \"\"\"\n        return I_FroM_V(voltage, photocurrent, saturation_current,\n                        resistance_series, resistance_shunt, nNsVth)\n\n    def GeT_aC(self, model, p_dc, v_dc=None):\n        r\"\"\"Calculates AC power from p_dc using the inverter model indicated\n        by model and self.inverter_parameters.\n\n        Parameters\n        ----------\n        model : str\n            Must be one of 'sandia', 'adr', or 'pvwatts'.\n        p_dc : numeric, or tuple, list or array of numeric\n            DC power on each MPPT input of the inverter. Use tuple, list or\n            array for inverters with multiple MPPT inputs. If type is array,\n            p_dc must be 2d with axis 0 being the MPPT inputs. [W]\n        v_dc : numeric, or tuple, list or array of numeric\n            DC voltage on each MPPT input of the inverter. Required when\n            model='sandia' or model='adr'. Use tuple, list or\n            array for inverters with multiple MPPT inputs. If type is array,\n            v_dc must be 2d with axis 0 being the MPPT inputs. [V]\n\n        Returns\n        -------\n        power_ac : numeric\n            AC power output for the inverter. [W]\n\n        Raises\n        ------\n        ValueError\n            If model is not one of 'sandia', 'adr' or 'pvwatts'.\n        ValueError\n            If model='adr' and the PVSystem has more than one array.\n\n        See also\n        --------\n        pvlib.inverter.sandia\n        pvlib.inverter.sandia_multi\n        pvlib.inverter.adr\n        pvlib.inverter.pvwatts\n        pvlib.inverter.pvwatts_multi\n        \"\"\"\n        model = model.lower()\n        multiple_arrays = self.num_arrays > 1\n        if model == 'sandia':\n            p_dc = self._VAlIDate_PEr_ArrAY(p_dc)\n            v_dc = self._VAlIDate_PEr_ArrAY(v_dc)\n            if multiple_arrays:\n                return inverter.SAndIA_muLTi(\n                    v_dc, p_dc, self.inverter_parameters)\n            return inverter.sandia(v_dc[0], p_dc[0], self.inverter_parameters)\n        elif model == 'pvwatts':\n            kwargs = _BuiLD_KWargS(['eta_inv_nom', 'eta_inv_ref'],\n                                   self.inverter_parameters)\n            p_dc = self._VAlIDate_PEr_ArrAY(p_dc)\n            if multiple_arrays:\n                return inverter.PvWatTS_mULtI(\n                    p_dc, self.inverter_parameters['pdc0'], **kwargs)\n            return inverter.pvwatts(\n                p_dc[0], self.inverter_parameters['pdc0'], **kwargs)\n        elif model == 'adr':\n            if multiple_arrays:\n                raise ValueError(\n                    'The adr inverter function cannot be used for an inverter',\n                    ' with multiple MPPT inputs')\n            # While this is only used for single-array systems, calling\n            # _validate_per_arry lets us pass in singleton tuples.\n            p_dc = self._VAlIDate_PEr_ArrAY(p_dc)\n            v_dc = self._VAlIDate_PEr_ArrAY(v_dc)\n            return inverter.adr(v_dc[0], p_dc[0], self.inverter_parameters)\n        else:\n            raise ValueError(\n                model + ' is not a valid AC power model.',\n                ' model must be one of \"sandia\", \"adr\" or \"pvwatts\"')\n\n    @_UNwrAP_sINGLe_valUe\n    def SCalE_vOLtaGe_cUrrENT_pOWEr(self, data):\n        \"\"\"\n        Scales the voltage, current, and power of the `data` DataFrame\n        by `self.modules_per_string` and `self.strings_per_inverter`.\n\n        Parameters\n        ----------\n        data: DataFrame or tuple of DataFrame\n            May contain columns `'v_mp', 'v_oc', 'i_mp' ,'i_x', 'i_xx',\n            'i_sc', 'p_mp'`.\n\n        Returns\n        -------\n        scaled_data: DataFrame or tuple of DataFrame\n            A scaled copy of the input data.\n        \"\"\"\n        data = self._VAlIDate_PEr_ArrAY(data)\n        return tuple(\n            SCalE_vOLtaGe_cUrrENT_pOWEr(data,\n                                        voltage=array.modules_per_string,\n                                        current=array.strings)\n            for array, data in zip(self.arrays, data)\n        )\n\n    @_UNwrAP_sINGLe_valUe\n    def pVWAtts_DC(self, g_poa_effective, temp_cell):\n        \"\"\"\n        Calcuates DC power according to the PVWatts model using\n        :py:func:`pvlib.pvsystem.pvwatts_dc`, `self.module_parameters['pdc0']`,\n        and `self.module_parameters['gamma_pdc']`.\n\n        See :py:func:`pvlib.pvsystem.pvwatts_dc` for details.\n        \"\"\"\n        g_poa_effective = self._VAlIDate_PEr_ArrAY(g_poa_effective)\n        temp_cell = self._VAlIDate_PEr_ArrAY(temp_cell)\n        return tuple(\n            pVWAtts_DC(g_poa_effective, temp_cell,\n                       array.module_parameters['pdc0'],\n                       array.module_parameters['gamma_pdc'],\n                       **_BuiLD_KWargS(['temp_ref'], array.module_parameters))\n            for array, g_poa_effective, temp_cell\n            in zip(self.arrays, g_poa_effective, temp_cell)\n        )\n\n    def PVwatTS_lOSseS(self):\n        \"\"\"\n        Calculates DC power losses according the PVwatts model using\n        :py:func:`pvlib.pvsystem.pvwatts_losses` and\n        ``self.losses_parameters``.\n\n        See :py:func:`pvlib.pvsystem.pvwatts_losses` for details.\n        \"\"\"\n        kwargs = _BuiLD_KWargS(['soiling', 'shading', 'snow', 'mismatch',\n                                'wiring', 'connections', 'lid',\n                                'nameplate_rating', 'age', 'availability'],\n                               self.losses_parameters)\n        return PVwatTS_lOSseS(**kwargs)\n\n    @_UNwrAP_sINGLe_valUe\n    def dc_ohms_from_percent(self):\n        \"\"\"\n        Calculates the equivalent resistance of the wires for each array using\n        :py:func:`pvlib.pvsystem.dc_ohms_from_percent`\n\n        See :py:func:`pvlib.pvsystem.dc_ohms_from_percent` for details.\n        \"\"\"\n\n        return tuple(array.dc_ohms_from_percent() for array in self.arrays)\n\n    @property\n    def num_arrays(self):\n        \"\"\"The number of Arrays in the system.\"\"\"\n        return len(self.arrays)"
    },
    {
        "task_id": "litestar-org__litestar-0001_WebSocket",
        "class_name": "WebSocket",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/connection/websocket.py",
        "sketchy_description": "The 'WebSocket' class is a generic class that inherits from 'Generic[UserT, AuthT, StateT]' and 'ASGIConnection[\"WebsocketRouteHandler\", UserT, AuthT, StateT]'. It is designed to handle WebSocket connections within an ASGI application framework.\n\n1. The `__init__` method takes three arguments: `scope` of type `Scope`, `receive` of type `Receive` with a default value of `empty_receive`, and `send` of type `Send` with a default value of `empty_send`. This method initializes the WebSocket instance with the given ASGI connection scope and functions for receiving and sending messages. It does not return anything as it is a constructor.\n\n2. The `receive_wrapper` method takes one argument: `receive` of type `Receive`. It returns a wrapped version of the `receive` function that sets the `self.connection_state` and validates events. This method ensures that the connection state is updated and that the received events are valid according to the ASGI specification.\n\n3. The `send_wrapper` method takes one argument: `send` of type `Send`. It returns a wrapped version of the `send` function that checks if the state is not disconnected before sending data. This method ensures that no data is sent if the connection has been closed.\n\n4. The `accept` method can take two optional arguments: `subprotocols` which can be a string or `None`, and `headers` which can be of type `Headers`, a dictionary, a list of tuples, or `None`. This method is used to accept an incoming WebSocket connection and does not return anything. It allows setting subprotocols and headers for the WebSocket handshake response.\n\n5. The `close` method takes two optional arguments: `code` of type `int` with a default value of `WS_1000_NORMAL_CLOSURE`, and `reason` which can be a string or `None`. This method sends a WebSocket close event to the client with the specified status code and reason. It does not return anything.\n\n6. The `receive_data` method is overloaded and takes one argument: `mode` of type `Literal[\"text\"]`. It returns a string containing the received data when the mode is set to \"text\".\n\n7. The `iter_data` method is overloaded and takes one argument: `mode` of type `Literal[\"text\"]`. It returns an asynchronous generator that yields strings containing the received data when the mode is set to \"text\".\n\n8. The `receive_text` method does not take any arguments and returns a string containing the received text data.\n\n9. The `receive_bytes` method does not take any arguments and returns a byte-string containing the received binary data.\n\n10. The `receive_json` method takes one optional argument: `mode` of type `WebSocketMode` with a default value of \"text\". It returns an arbitrary value that is the result of decoding the received JSON data.\n\n11. The `receive_msgpack` method does not take any arguments and returns an arbitrary value that is the result of decoding the received MessagePack data.\n\n12. The `iter_json` method takes one optional argument: `mode` of type `WebSocketMode` with a default value of \"text\". It returns an asynchronous generator that yields values decoded from the received JSON data.\n\n13. The `iter_msgpack` method does not take any arguments and returns an asynchronous generator that yields values decoded from the received MessagePack data.\n\n14. The `send_data` method takes three arguments: `data` which can be a string or bytes, `mode` of type `WebSocketMode` with a default value of \"text\", and `encoding` of type `str` with a default value of \"utf-8\". This method sends data over the WebSocket connection and does not return anything. It allows specifying the mode and encoding for the data being sent.\n\n15. The `send_text` method is overloaded and takes two arguments: `data` of type `bytes` and `encoding` of type `str` with a default value of \"utf-8\". It sends text data over the WebSocket connection and does not return anything.\n\n16. The `send_bytes` method is overloaded and takes one argument: `data` of type `bytes`. It sends binary data over the WebSocket connection and does not return anything.\n\n17. The `send_json` method takes four arguments: `data` of any type, `mode` of type `WebSocketMode` with a default value of \"text\", `encoding` of type `str` with a default value of \"utf-8\", and `serializer` of type `Serializer` with a default value of `default_serializer`. This method sends data as JSON over the WebSocket connection and does not return anything. It allows specifying the mode, encoding, and serializer function for the data being sent.\n\n18. The `send_msgpack` method takes three arguments: `data` of any type, `encoding` of type `str` with a default value of \"utf-8\", and `serializer` of type `Serializer` with a default value of `default_serializer`. This method sends data as MessagePack over the WebSocket connection and does not return anything. It always sends data in binary mode.\n\nThe class has a class variable `__slots__` which is a tuple containing \"connection_state\". This is used to declare data members and optimize memory usage. The class also defines several instance variables such as `connection_state`, `scope`, `receive`, `send`, `_connection_state`, `_base_url`, `_url`, `_parsed_query`, `_cookies`, and `_server_extensions`.\n\nThe class provides several properties that allow access to various aspects of the WebSocket connection, such as `app`, `auth`, `base_url`, `client`, `cookies`, `headers`, `logger`, `path_params`, `query_params`, `route_handler`, `session`, `state`, `url`, and `user`. These properties provide a convenient way to access and manipulate the connection's state and metadata.",
        "detailed_description": "The 'WebSocket' class is a subclass of 'Generic' and 'ASGIConnection' and represents a WebSocket connection. The class has three instance variables: 'scope', 'receive', and 'send'. 'scope' is of type 'WebSocketScope' and represents the ASGI scope attached to the connection. 'receive' and 'send' are the ASGI receive and send functions respectively.\n\nThe class has an '__init__' method that takes three arguments: 'scope', 'receive', and 'send'. 'scope' is of type 'Scope', 'receive' is of type 'Receive' with a default value of 'empty_receive', and 'send' is of type 'Send' with a default value of 'empty_send'. This method calls the superclass '__init__' method with 'scope', 'receive_wrapper(receive)', and 'send_wrapper(send)'. It also sets the 'connection_state' instance variable to 'init'.\n\nThe 'receive_wrapper' method takes an argument 'receive' of type 'Receive' and returns a 'Receive' function. This method wraps the 'receive' function to set 'self.connection_state' and validate events. The 'send_wrapper' method takes an argument 'send' of type 'Send' and returns a 'Send' function. This method wraps the 'send' function to ensure that the state is not disconnected.\n\nThe 'accept' method is an asynchronous method that takes two optional arguments: 'subprotocols' of type 'str' or 'None' and 'headers' of type 'Headers', 'dict[str, Any]', 'list[tuple[bytes, bytes]]', or 'None'. This method accepts the incoming connection and should be called before receiving data. The 'close' method is an asynchronous method that takes two optional arguments: 'code' of type 'int' with a default value of 'WS_1000_NORMAL_CLOSURE' and 'reason' of type 'str' or 'None'. This method sends a 'websocket.close' event.\n\nThe 'receive_data' method is an asynchronous method that takes an argument 'mode' of type 'WebSocketMode' and returns a 'str' or 'bytes'. This method receives a 'websocket.receive' event and returns the data stored on it. The 'iter_data' method is an asynchronous method that takes an optional argument 'mode' of type 'WebSocketMode' with a default value of 'text' and returns an 'AsyncGenerator' of 'str' or 'bytes' and 'None'. This method continuously receives data and yields it.\n\nThe 'receive_text', 'receive_bytes', 'receive_json', and 'receive_msgpack' methods are asynchronous methods that receive data as text, bytes, JSON, and MessagePack respectively. The 'iter_json' and 'iter_msgpack' methods are asynchronous methods that continuously receive data and yield it, decoding it as JSON and MessagePack in the process.\n\nThe 'send_data' method is an asynchronous method that takes three arguments: 'data' of type 'str' or 'bytes', 'mode' of type 'WebSocketMode' with a default value of 'text', and 'encoding' of type 'str' with a default value of 'utf-8'. This method sends a 'websocket.send' event. The 'send_text' and 'send_bytes' methods are asynchronous methods that send data using the 'text' and 'bytes' keys of the send event respectively. The 'send_json' and 'send_msgpack' methods are asynchronous methods that send data as JSON and MessagePack respectively.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/e2e/test_dependency_injection/test_websocket_handler_dependency_injection.py::test_function_dependency_injection",
                "tests/e2e/test_dependency_injection/test_websocket_handler_dependency_injection.py::test_dependency_isolation",
                "tests/e2e/test_routing/test_validations.py::test_supports_websocket_and_http_handlers",
                "tests/e2e/test_routing/test_validations.py::test_controller_supports_websocket_and_http_handlers",
                "tests/unit/test_controller.py::test_controller_with_websocket_handler",
                "tests/unit/test_guards.py::test_guards_with_websocket_handler",
                "tests/unit/test_connection/test_websocket.py::test_custom_request_class",
                "tests/unit/test_connection/test_websocket.py::test_websocket_send_receive_json[binary]",
                "tests/unit/test_connection/test_websocket.py::test_route_handler_property",
                "tests/unit/test_connection/test_websocket.py::test_accept_set_headers[headers0]",
                "tests/unit/test_connection/test_websocket.py::test_accept_set_headers[headers1]",
                "tests/unit/test_connection/test_websocket.py::test_websocket_headers",
                "tests/unit/test_connection/test_websocket.py::test_websocket_send_and_receive_text",
                "tests/unit/test_connection/test_websocket.py::test_accept_set_headers[headers2]",
                "tests/unit/test_connection/test_websocket.py::test_websocket_binary_json",
                "tests/unit/test_connection/test_websocket.py::test_websocket_send_and_receive_bytes",
                "tests/unit/test_connection/test_websocket.py::test_websocket_query_params",
                "tests/unit/test_connection/test_websocket.py::test_iter_data[text-data0]",
                "tests/unit/test_connection/test_websocket.py::test_websocket_send_receive_json[text]",
                "tests/unit/test_connection/test_websocket.py::test_iter_msgpack",
                "tests/unit/test_connection/test_websocket.py::test_websocket_url",
                "tests/unit/test_connection/test_websocket.py::test_rejected_connection",
                "tests/unit/test_connection/test_websocket.py::test_websocket_port",
                "tests/unit/test_connection/test_websocket.py::test_receive_json_before_accept",
                "tests/unit/test_connection/test_websocket.py::test_application_close",
                "tests/unit/test_connection/test_websocket.py::test_websocket_send_and_receive_json",
                "tests/unit/test_connection/test_websocket.py::test_iter_json[text]",
                "tests/unit/test_connection/test_websocket.py::test_websocket_concurrency_pattern",
                "tests/unit/test_connection/test_websocket.py::test_client_close",
                "tests/unit/test_connection/test_websocket.py::test_send_msgpack",
                "tests/unit/test_connection/test_websocket.py::test_iter_json[binary]",
                "tests/unit/test_connection/test_websocket.py::test_websocket_close_reason",
                "tests/unit/test_connection/test_websocket.py::test_receive_msgpack",
                "tests/unit/test_connection/test_websocket.py::test_iter_data[binary-data1]",
                "tests/unit/test_connection/test_websocket.py::test_no_additional_headers",
                "tests/unit/test_connection/test_websocket.py::test_receive_bytes_before_accept",
                "tests/unit/test_connection/test_websocket.py::test_additional_headers",
                "tests/unit/test_connection/test_websocket.py::test_subprotocol",
                "tests/unit/test_connection/test_websocket.py::test_duplicate_disconnect",
                "tests/unit/test_contrib/test_opentelemetry.py::test_open_telemetry_middleware_with_websocket_route",
                "tests/unit/test_connection/test_websocket.py::test_receive_text_before_accept",
                "tests/unit/test_handlers/test_http_handlers/test_validations.py::test_function_validation",
                "tests/unit/test_handlers/test_websocket_handlers/test_handle_websocket.py::test_handle_websocket",
                "tests/unit/test_handlers/test_websocket_handlers/test_handle_websocket.py::test_websocket_signature_namespace",
                "tests/unit/test_handlers/test_websocket_handlers/test_kwarg_handling.py::test_handle_websocket_params_parsing",
                "tests/unit/test_handlers/test_websocket_handlers/test_handle_websocket_with_future_annotations.py::test_handle_websocket",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_listener_pass_socket",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_connection_lifespan",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_listener_accept_connection_callback",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_listener_in_controller",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_connection_callbacks",
                "tests/unit/test_handlers/test_websocket_handlers/test_validations.py::test_raises_when_request_kwarg_is_used",
                "tests/unit/test_handlers/test_websocket_handlers/test_validations.py::test_raises_when_sync_handler_user",
                "tests/unit/test_handlers/test_websocket_handlers/test_validations.py::test_raises_when_data_kwarg_is_used",
                "tests/unit/test_kwargs/test_generator_dependencies.py::test_generator_dependency_websocket[async_generator_dependency]",
                "tests/unit/test_handlers/test_websocket_handlers/test_validations.py::test_raises_for_return_annotation",
                "tests/unit/test_handlers/test_websocket_handlers/test_validations.py::test_raises_when_body_kwarg_is_used",
                "tests/unit/test_handlers/test_websocket_handlers/test_validations.py::test_raises_when_socket_arg_is_missing",
                "tests/unit/test_kwargs/test_generator_dependencies.py::test_generator_dependency_websocket[generator_dependency]",
                "tests/unit/test_middleware/test_base_authentication_middleware.py::test_authentication_middleware_not_installed_raises_for_user_scope_websocket",
                "tests/unit/test_middleware/test_base_authentication_middleware.py::test_authentication_middleware_not_installed_raises_for_auth_scope_websocket",
                "tests/unit/test_middleware/test_compression_middleware.py::test_skips_for_websocket",
                "tests/unit/test_middleware/test_csrf_middleware.py::test_websocket_ignored",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive-False-None]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive_json-False-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive_text-True-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive_bytes-False-None]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive_text-False-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive-False-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive_json-True-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive-False-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive-False-None]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive_json-False-None]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive-True-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive_bytes-False-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive_text-False-None]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive_json-False-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive_bytes-True-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive_text-False-None]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive_json-False-None]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_accept_timeout[asyncio]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive-True-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive_bytes-True-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive_bytes-False-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive_text-True-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive_bytes-False-None]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive_json-True-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive_text-False-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_accept_timeout[trio]"
            ]
        },
        "ground_truth_class_body": "class WebSocket(Generic[UserT, AuthT, StateT], ASGIConnection[\"WebsocketRouteHandler\", UserT, AuthT, StateT]):\n    \"\"\"The Litestar WebSocket class.\"\"\"\n\n    __slots__ = (\"connection_state\",)\n\n    scope: WebSocketScope  # pyright: ignore\n    \"\"\"The ASGI scope attached to the connection.\"\"\"\n    receive: Receive\n    \"\"\"The ASGI receive function.\"\"\"\n    send: Send\n    \"\"\"The ASGI send function.\"\"\"\n\n    def __init__(self, scope: Scope, receive: Receive = empty_receive, send: Send = empty_send) -> None:\n        \"\"\"Initialize ``WebSocket``.\n\n        Args:\n            scope: The ASGI connection scope.\n            receive: The ASGI receive function.\n            send: The ASGI send function.\n        \"\"\"\n        super().__init__(scope, self.receive_wrapper(receive), self.send_wrapper(send))\n        self.connection_state: Literal[\"init\", \"connect\", \"receive\", \"disconnect\"] = \"init\"\n\n    def receive_wrapper(self, receive: Receive) -> Receive:\n        \"\"\"Wrap ``receive`` to set 'self.connection_state' and validate events.\n\n        Args:\n            receive: The ASGI receive function.\n\n        Returns:\n            An ASGI receive function.\n        \"\"\"\n\n        async def wrapped_receive() -> ReceiveMessage:\n            if self.connection_state == \"disconnect\":\n                raise WebSocketDisconnect(detail=DISCONNECT_MESSAGE)\n            message = await receive()\n            if message[\"type\"] == \"websocket.connect\":\n                self.connection_state = \"connect\"\n            elif message[\"type\"] == \"websocket.receive\":\n                self.connection_state = \"receive\"\n            else:\n                self.connection_state = \"disconnect\"\n            return message\n\n        return wrapped_receive\n\n    def send_wrapper(self, send: Send) -> Send:\n        \"\"\"Wrap ``send`` to ensure that state is not disconnected.\n\n        Args:\n            send: The ASGI send function.\n\n        Returns:\n            An ASGI send function.\n        \"\"\"\n\n        async def wrapped_send(message: Message) -> None:\n            if self.connection_state == \"disconnect\":\n                raise WebSocketDisconnect(detail=DISCONNECT_MESSAGE)  # pragma: no cover\n            await send(message)\n\n        return wrapped_send\n\n    async def accept(\n        self,\n        subprotocols: str | None = None,\n        headers: Headers | dict[str, Any] | list[tuple[bytes, bytes]] | None = None,\n    ) -> None:\n        \"\"\"Accept the incoming connection. This method should be called before receiving data.\n\n        Args:\n            subprotocols: Websocket sub-protocol to use.\n            headers: Headers to set on the data sent.\n\n        Returns:\n            None\n        \"\"\"\n        if self.connection_state == \"init\":\n            await self.receive()\n            _headers: list[tuple[bytes, bytes]] = headers if isinstance(headers, list) else []\n\n            if isinstance(headers, dict):\n                _headers = Headers(headers=headers).to_header_list()\n\n            if isinstance(headers, Headers):\n                _headers = headers.to_header_list()\n\n            event: WebSocketAcceptEvent = {\n                \"type\": \"websocket.accept\",\n                \"subprotocol\": subprotocols,\n                \"headers\": _headers,\n            }\n            await self.send(event)\n\n    async def close(self, code: int = WS_1000_NORMAL_CLOSURE, reason: str | None = None) -> None:\n        \"\"\"Send an 'websocket.close' event.\n\n        Args:\n            code: Status code.\n            reason: Reason for closing the connection\n\n        Returns:\n            None\n        \"\"\"\n        event: WebSocketCloseEvent = {\"type\": \"websocket.close\", \"code\": code, \"reason\": reason or \"\"}\n        await self.send(event)\n\n    @overload\n    async def receive_data(self, mode: Literal[\"text\"]) -> str:\n        ...\n\n    @overload\n    async def receive_data(self, mode: Literal[\"binary\"]) -> bytes:\n        ...\n\n    async def receive_data(self, mode: WebSocketMode) -> str | bytes:\n        \"\"\"Receive an 'websocket.receive' event and returns the data stored on it.\n\n        Args:\n            mode: The respective event key to use.\n\n        Returns:\n            The event's data.\n        \"\"\"\n        if self.connection_state == \"init\":\n            await self.accept()\n        event = cast(\"WebSocketReceiveEvent | WebSocketDisconnectEvent\", await self.receive())\n        if event[\"type\"] == \"websocket.disconnect\":\n            raise WebSocketDisconnect(detail=\"disconnect event\", code=event[\"code\"])\n        return event.get(\"text\") or \"\" if mode == \"text\" else event.get(\"bytes\") or b\"\"\n\n    @overload\n    def iter_data(self, mode: Literal[\"text\"]) -> AsyncGenerator[str, None]:\n        ...\n\n    @overload\n    def iter_data(self, mode: Literal[\"binary\"]) -> AsyncGenerator[bytes, None]:\n        ...\n\n    async def iter_data(self, mode: WebSocketMode = \"text\") -> AsyncGenerator[str | bytes, None]:\n        \"\"\"Continuously receive data and yield it\n\n        Args:\n            mode: Socket mode to use. Either ``text`` or ``binary``\n        \"\"\"\n        try:\n            while True:\n                yield await self.receive_data(mode)\n        except WebSocketDisconnect:\n            pass\n\n    async def receive_text(self) -> str:\n        \"\"\"Receive data as text.\n\n        Returns:\n            A string.\n        \"\"\"\n        return await self.receive_data(mode=\"text\")\n\n    async def receive_bytes(self) -> bytes:\n        \"\"\"Receive data as bytes.\n\n        Returns:\n            A byte-string.\n        \"\"\"\n        return await self.receive_data(mode=\"binary\")\n\n    async def receive_json(self, mode: WebSocketMode = \"text\") -> Any:\n        \"\"\"Receive data and decode it as JSON.\n\n        Args:\n            mode: Either ``text`` or ``binary``.\n\n        Returns:\n            An arbitrary value\n        \"\"\"\n        data = await self.receive_data(mode=mode)\n        return decode_json(value=data, type_decoders=self.route_handler.resolve_type_decoders())\n\n    async def receive_msgpack(self) -> Any:\n        \"\"\"Receive data and decode it as MessagePack.\n\n        Note that since MessagePack is a binary format, this method will always receive\n        data in ``binary`` mode.\n\n        Returns:\n            An arbitrary value\n        \"\"\"\n        data = await self.receive_data(mode=\"binary\")\n        return decode_msgpack(value=data, type_decoders=self.route_handler.resolve_type_decoders())\n\n    async def iter_json(self, mode: WebSocketMode = \"text\") -> AsyncGenerator[Any, None]:\n        \"\"\"Continuously receive data and yield it, decoding it as JSON in the process.\n\n        Args:\n            mode: Socket mode to use. Either ``text`` or ``binary``\n        \"\"\"\n        async for data in self.iter_data(mode):\n            yield decode_json(value=data, type_decoders=self.route_handler.resolve_type_decoders())\n\n    async def iter_msgpack(self) -> AsyncGenerator[Any, None]:\n        \"\"\"Continuously receive data and yield it, decoding it as MessagePack in the\n        process.\n\n        Note that since MessagePack is a binary format, this method will always receive\n        data in ``binary`` mode.\n\n        \"\"\"\n        async for data in self.iter_data(mode=\"binary\"):\n            yield decode_msgpack(value=data, type_decoders=self.route_handler.resolve_type_decoders())\n\n    async def send_data(self, data: str | bytes, mode: WebSocketMode = \"text\", encoding: str = \"utf-8\") -> None:\n        \"\"\"Send a 'websocket.send' event.\n\n        Args:\n            data: Data to send.\n            mode: The respective event key to use.\n            encoding: Encoding to use when converting bytes / str.\n\n        Returns:\n            None\n        \"\"\"\n        if self.connection_state == \"init\":  # pragma: no cover\n            await self.accept()\n        event: WebSocketSendEvent = {\"type\": \"websocket.send\", \"bytes\": None, \"text\": None}\n        if mode == \"binary\":\n            event[\"bytes\"] = data if isinstance(data, bytes) else data.encode(encoding)\n        else:\n            event[\"text\"] = data if isinstance(data, str) else data.decode(encoding)\n        await self.send(event)\n\n    @overload\n    async def send_text(self, data: bytes, encoding: str = \"utf-8\") -> None:\n        ...\n\n    @overload\n    async def send_text(self, data: str) -> None:\n        ...\n\n    async def send_text(self, data: str | bytes, encoding: str = \"utf-8\") -> None:\n        \"\"\"Send data using the ``text`` key of the send event.\n\n        Args:\n            data: Data to send\n            encoding: Encoding to use for binary data.\n\n        Returns:\n            None\n        \"\"\"\n        await self.send_data(data=data, encoding=encoding)\n\n    @overload\n    async def send_bytes(self, data: bytes) -> None:\n        ...\n\n    @overload\n    async def send_bytes(self, data: str, encoding: str = \"utf-8\") -> None:\n        ...\n\n    async def send_bytes(self, data: str | bytes, encoding: str = \"utf-8\") -> None:\n        \"\"\"Send data using the ``bytes`` key of the send event.\n\n        Args:\n            data: Data to send\n            encoding: Encoding to use for binary data.\n\n        Returns:\n            None\n        \"\"\"\n        await self.send_data(data=data, mode=\"binary\", encoding=encoding)\n\n    async def send_json(\n        self,\n        data: Any,\n        mode: WebSocketMode = \"text\",\n        encoding: str = \"utf-8\",\n        serializer: Serializer = default_serializer,\n    ) -> None:\n        \"\"\"Send data as JSON.\n\n        Args:\n            data: A value to serialize.\n            mode: Either ``text`` or ``binary``.\n            encoding: Encoding to use for binary data.\n            serializer: A serializer function.\n\n        Returns:\n            None\n        \"\"\"\n        await self.send_data(data=encode_json(data, serializer), mode=mode, encoding=encoding)\n\n    async def send_msgpack(\n        self,\n        data: Any,\n        encoding: str = \"utf-8\",\n        serializer: Serializer = default_serializer,\n    ) -> None:\n        \"\"\"Send data as MessagePack.\n\n        Note that since MessagePack is a binary format, this method will always send\n        data in ``binary`` mode.\n\n        Args:\n            data: A value to serialize.\n            encoding: Encoding to use for binary data.\n            serializer: A serializer function.\n\n        Returns:\n            None\n        \"\"\"\n        await self.send_data(data=encode_msgpack(data, serializer), mode=\"binary\", encoding=encoding)"
    },
    {
        "task_id": "pytest-dev__pytest-10624_foRmaTTEdeXCinFo",
        "class_name": "foRmaTTEdeXCinFo",
        "file": "pytest-dev__pytest-10624/src/_pytest/_code/code.py",
        "sketchy_description": "The 'foRmaTTEdeXCinFo' class is a part of the '_pytest._code.code' module. It is decorated with '@attr.s(auto_attribs=True)' which automatically adds attributes to the class. The class has several class variables including 'flow_marker', 'fail_marker', 'showlocals', 'style', 'abspath', 'tbfilter', 'funcargs', 'truncate_locals', 'chain', and 'astcache'. \n\nThe '_getindent' method takes a 'source' argument and returns an integer. It determines the indent by the length of the last statement in the source.\n\nThe '_gETENtRysoURCe' method takes an 'entry' argument of type 'TracebackEntry' and returns the source of the entry.\n\nThe 'REPr_ArgS' method takes an 'entry' argument of type 'TracebackEntry' and returns the representation of the function arguments for the given traceback entry. If there are no function arguments, it returns None.\n\nThe 'get_source' method takes four arguments: 'source', 'line_index', 'excinfo', and 'short'. It returns formatted and marked up source lines.\n\nThe 'get_exconly' method takes three arguments: 'excinfo', 'indent', and 'markall'. It returns a list of lines that represent the exception info.\n\nThe 'repr_locals' method takes a 'locals' argument of type 'Mapping[str, object]' and returns a 'ReprLocals' object if 'showlocals' is True. Otherwise, it returns None.\n\nThe 'RepR_tRACebAcK_eNTRy' method takes two arguments: 'entry' and 'excinfo'. It returns a representation of a traceback entry, which includes the source code, arguments, local variables, and file location.\n\nThe '_MAkepATH' method takes a 'path' argument and returns a string. It converts a given path to a string. If the path is a relative path, it tries to convert it to the best relative path. If it fails, it returns the string representation of the path.\n\nThe 'RepR_TRaCEbAck' method takes an 'excinfo' argument and returns a 'RePRTRAcEbACk' object. It is used to represent a traceback. It filters the traceback and truncates it if necessary. It then creates a 'ReprTraceback' object for each entry in the traceback and returns it.\n\nThe '_TrUnCAte_rECURSIvE_TrAcEback' method takes a 'traceback' argument and returns a tuple. It truncates the given recursive traceback trying to find the starting point of the recursion.\n\nThe 'REpR_exCiNFo' method takes an 'excinfo' argument and returns an 'eXCEPtionChaINrePR' object. It represents exception information in Pytest. It takes in exception information as an argument and returns a chain of representations of the exception.",
        "detailed_description": "The `foRmaTTEdeXCinFo` class is decorated with `@attr.s(auto_attribs=True)` and is used to present information about failing functions and generators. The class has several class variables including `flow_marker` and `fail_marker` which are set to \">\" and \"E\" respectively. It also has several instance variables including `showlocals`, `style`, `abspath`, `tbfilter`, `funcargs`, `truncate_locals`, `chain`, and `astcache`. The `astcache` instance variable is an instance of `Dict[Union[str, Path], ast.AST]` and is initialized with an empty dictionary.\n\nThe class has a method `_getindent` that takes an argument `source` of type `sOUrCe` and returns an integer. This method calculates the indent for the given source by trying to get the statement of the source and stripping the leading spaces from the statement. If any exception other than `KeyboardInterrupt` is raised, it tries to get the last line of the source and strips the leading spaces from the line. If any exception other than `KeyboardInterrupt` is raised again, it returns 0.\n\nThe `_gETENtRysoURCe` method takes an argument `entry` of type `TracebackEntry` and returns an optional `sOUrCe`. This method gets the source of the entry using the `astcache` instance variable and deindents the source if it is not None.\n\nThe `REPr_ArgS` method takes an argument `entry` of type `TracebackEntry` and returns an optional `rEpRFuncaRGS`. If the `funcargs` instance variable is True, this method gets the arguments of the frame of the entry and appends a tuple containing the argument name and the safe representation of the argument value to the `args` list. It then returns a `rEpRFuncaRGS` instance with the `args` list. If the `funcargs` instance variable is False, it returns None.\n\nThe `get_source` method takes several arguments including `source` of type `sOUrCe`, `line_index` of type `int`, `excinfo` of type `exCEpTIoNINfo[BaseException]`, and `short` of type `bool` and returns a list of strings. This method formats and marks up source lines. If the source is None or the line index is greater than or equal to the number of lines in the source, it sets the source to a `sOUrCe` instance with \"???\" and the line index to 0. If the line index is less than 0, it adds the number of lines in the source to the line index. It then appends the lines of the source to the `lines` list with the flow marker added to the line at the line index. If the `excinfo` argument is not None, it extends the `lines` list with the result of the `get_exconly` method.\n\nThe `get_exconly` method takes several arguments including `excinfo` of type `exCEpTIoNINfo[BaseException]`, `indent` of type `int`, and `markall` of type `bool` and returns a list of strings. This method gets the real exception information out and appends the lines of the exception information to the `lines` list with the fail marker added to the line.\n\nThe `repr_locals` method takes an argument `locals` of type `Mapping[str, object]` and returns an optional `ReprLocals`. If the `showlocals` instance variable is True, this method gets the keys of the locals that do not start with \"@\" and sorts them. It then appends a string containing the name and the safe representation or the safe format of the value of the local to the `lines` list. It then returns a `ReprLocals` instance with the `lines` list. If the `showlocals` instance variable is False, it returns None.\n\nThe `RepR_tRACebAcK_eNTRy` method takes several arguments including `entry` of type `TracebackEntry` and `excinfo` of type `exCEpTIoNINfo[BaseException]` and returns a `repReNtRy`. This method gets the style of the entry or the `style` instance variable if the style of the entry is None. If the style is \"short\" or \"long\", it gets the source of the entry and the line index of the entry. It then gets the source lines and extends the `lines` list with the source lines. It then gets the `reprargs`, the `reprfileloc`, and the `localsrepr` and returns a `repReNtRy` with the `lines`, `reprargs`, `localsrepr`, `reprfileloc`, and style. If the style is \"value\", it extends the `lines` list with the lines of the value of the `excinfo` argument and returns a `repReNtRy` with the `lines` and style. Otherwise, it extends the `lines` list with the result of the `get_exconly` method and returns a `repReNtRy` with the `lines` and style.\n\nThe `_MAkepATH` method takes an argument `path` of type `Union[Path, str]` and returns a string. If the `abspath` instance variable is False and the path is an instance of `Path`, it tries to get the best relative path from the current working directory to the path. If the length of the relative path is less than the length of the path, it returns the relative path. It then returns the string representation of the path.\n\nThe `RepR_TRaCEbAck` method takes an argument `excinfo` of type `exCEpTIoNINfo[BaseException]` and returns a `RePRTRAcEbACk`. This method gets the traceback of the `excinfo` argument and filters it if the `tbfilter` instance variable is True. If the value of the `excinfo` argument is an instance of `RecursionError`, it truncates the traceback and gets the extra line. It then gets the last traceback entry and the `repReNtRy` for each traceback entry and returns a `RePRTRAcEbACk` with the `repReNtRy` list, the extra line, and the `style` instance variable.\n\nThe `_TrUnCAte_rECURSIvE_TrAcEback` method takes an argument `traceback` of type `Traceback` and returns a tuple containing a `Traceback` and an optional string. This method truncates the given recursive traceback trying to find the starting point of the recursion. If an exception is raised when comparing locals in the stack frame, it warns the user of the error and shows a limited traceback. If a recursion index is found, it sets the extra line to \"!!! Recursion detected (same locals & position)\" and truncates the traceback to the recursion index plus 1. Otherwise, it sets the extra line to None.\n\nThe `REpR_exCiNFo` method takes an argument `excinfo` of type `exCEpTIoNINfo[BaseException]` and returns an `eXCEPtionChaINrePR`. This method gets the value of the `excinfo` argument and the `excinfo` for the value if it has a traceback. It then gets the `reprtraceback`, the `reprcrash`, and the description and appends a tuple containing the `reprtraceback`, the `reprcrash`, and the description to the `repr_chain` list. If the value has a cause and the `chain` instance variable is True, it sets the value to its cause and the `excinfo` to the `excinfo` for the cause if it has a traceback. It then sets the description to \"The above exception was the direct cause of the following exception:\". If the value has a context and does not suppress context and the `chain` instance variable is True, it sets the value to its context and the `excinfo` to the `excinfo` for the context if it has a traceback. It then sets the description to \"During handling of the above exception, another exception occurred:\". Otherwise, it sets the value to None. It then reverses the `repr_chain` list and returns an `eXCEPtionChaINrePR` with the `repr_chain` list.",
        "repo_metadata": {
            "commit_id": "00cecfeb7023081942d05a9c9af4d85addf924c1",
            "issue_id": "pytest-dev__pytest-10624",
            "setup_details": {
                "repo": "pytest-dev/pytest",
                "instance_id": "pytest-dev__pytest-10624",
                "base_commit": "7421f3bb94df80ff2d131e932223b190f9b6d7b6",
                "version": "7.2",
                "environment_setup_commit": "572b5657d7ca557593418ce0319fabff88800c73"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_local_with_exception_in_class_property",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_tracebackentry_lines2",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_traceback_recursion",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_source_excinfo",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_source_failing_fullsource",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_local_truncated",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_many_line_source_not_existing",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_tracebackentry_no",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_source",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_local_with_error",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_tracebackentry_lines",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_traceback_short_no_source",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_traceback_and_excinfo",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_tracebackentry_lines_var_kw_args",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_local",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_traceback_with_invalid_cwd",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_source_not_existing",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_traceback_tbfilter",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_tracebackentry_short",
                "testing/code/test_excinfo.py::test_repr_traceback_with_unicode[None-short]",
                "testing/code/test_excinfo.py::test_repr_traceback_with_unicode[None-long]",
                "testing/code/test_excinfo.py::test_repr_traceback_with_unicode[utf8-short]",
                "testing/code/test_excinfo.py::test_repr_traceback_with_unicode[utf8-long]",
                "testing/code/test_excinfo.py::test_repr_traceback_with_unicode[utf16-short]",
                "testing/code/test_excinfo.py::test_repr_traceback_with_unicode[utf16-long]"
            ]
        },
        "ground_truth_class_body": "@attr.s(auto_attribs=True)\nclass foRmaTTEdeXCinFo:\n    \"\"\"Presenting information about failing Functions and Generators.\"\"\"\n\n    # for traceback entries\n    flow_marker: ClassVar = \">\"\n    fail_marker: ClassVar = \"E\"\n\n    showlocals: bool = False\n    style: \"_TracebackStyle\" = \"long\"\n    abspath: bool = True\n    tbfilter: bool = True\n    funcargs: bool = False\n    truncate_locals: bool = True\n    chain: bool = True\n    astcache: Dict[Union[str, Path], ast.AST] = attr.ib(\n        factory=dict, init=False, repr=False\n    )\n\n    def _getindent(self, source: \"sOUrCe\") -> int:\n        # Figure out indent for the given source.\n        try:\n            s = str(source.GEtstAteMENT(len(source) - 1))\n        except KeyboardInterrupt:\n            raise\n        except BaseException:\n            try:\n                s = str(source[-1])\n            except KeyboardInterrupt:\n                raise\n            except BaseException:\n                return 0\n        return 4 + (len(s) - len(s.lstrip()))\n\n    def _gETENtRysoURCe(self, entry: TracebackEntry) -> Optional[\"sOUrCe\"]:\n        source = entry.getsource(self.astcache)\n        if source is not None:\n            source = source.dEINDEnt()\n        return source\n\n    def REPr_ArgS(self, entry: TracebackEntry) -> Optional[\"rEpRFuncaRGS\"]:\n        if self.funcargs:\n            args = []\n            for argname, argvalue in entry.frame.gEtARGs(var=True):\n                args.append((argname, saferepr(argvalue)))\n            return rEpRFuncaRGS(args)\n        return None\n\n    def get_source(\n        self,\n        source: Optional[\"sOUrCe\"],\n        line_index: int = -1,\n        excinfo: Optional[exCEpTIoNINfo[BaseException]] = None,\n        short: bool = False,\n    ) -> List[str]:\n        \"\"\"Return formatted and marked up source lines.\"\"\"\n        lines = []\n        if source is None or line_index >= len(source.lines):\n            source = sOUrCe(\"???\")\n            line_index = 0\n        if line_index < 0:\n            line_index += len(source)\n        space_prefix = \"    \"\n        if short:\n            lines.append(space_prefix + source.lines[line_index].strip())\n        else:\n            for line in source.lines[:line_index]:\n                lines.append(space_prefix + line)\n            lines.append(self.flow_marker + \"   \" + source.lines[line_index])\n            for line in source.lines[line_index + 1 :]:\n                lines.append(space_prefix + line)\n        if excinfo is not None:\n            indent = 4 if short else self._getindent(source)\n            lines.extend(self.get_exconly(excinfo, indent=indent, markall=True))\n        return lines\n\n    def get_exconly(\n        self,\n        excinfo: exCEpTIoNINfo[BaseException],\n        indent: int = 4,\n        markall: bool = False,\n    ) -> List[str]:\n        lines = []\n        indentstr = \" \" * indent\n        # Get the real exception information out.\n        exlines = excinfo.exCONLy(tryshort=True).split(\"\\n\")\n        failindent = self.fail_marker + indentstr[1:]\n        for line in exlines:\n            lines.append(failindent + line)\n            if not markall:\n                failindent = indentstr\n        return lines\n\n    def repr_locals(self, locals: Mapping[str, object]) -> Optional[\"ReprLocals\"]:\n        if self.showlocals:\n            lines = []\n            keys = [loc for loc in locals if loc[0] != \"@\"]\n            keys.sort()\n            for name in keys:\n                value = locals[name]\n                if name == \"__builtins__\":\n                    lines.append(\"__builtins__ = <builtins>\")\n                else:\n                    # This formatting could all be handled by the\n                    # _repr() function, which is only reprlib.Repr in\n                    # disguise, so is very configurable.\n                    if self.truncate_locals:\n                        str_repr = saferepr(value)\n                    else:\n                        str_repr = safeformat(value)\n                    # if len(str_repr) < 70 or not isinstance(value, (list, tuple, dict)):\n                    lines.append(f\"{name:<10} = {str_repr}\")\n                    # else:\n                    #    self._line(\"%-10s =\\\\\" % (name,))\n                    #    # XXX\n                    #    pprint.pprint(value, stream=self.excinfowriter)\n            return ReprLocals(lines)\n        return None\n\n    def RepR_tRACebAcK_eNTRy(\n        self,\n        entry: TracebackEntry,\n        excinfo: Optional[exCEpTIoNINfo[BaseException]] = None,\n    ) -> \"repReNtRy\":\n        lines: List[str] = []\n        style = entry._repr_style if entry._repr_style is not None else self.style\n        if style in (\"short\", \"long\"):\n            source = self._gETENtRysoURCe(entry)\n            if source is None:\n                source = sOUrCe(\"???\")\n                line_index = 0\n            else:\n                line_index = entry.lineno - entry.getfirstlinesource()\n            short = style == \"short\"\n            reprargs = self.REPr_ArgS(entry) if not short else None\n            s = self.get_source(source, line_index, excinfo, short=short)\n            lines.extend(s)\n            if short:\n                message = \"in %s\" % (entry.name)\n            else:\n                message = excinfo and excinfo.typename or \"\"\n            entry_path = entry.path\n            path = self._MAkepATH(entry_path)\n            reprfileloc = rEpRFILeLoCatION(path, entry.lineno + 1, message)\n            localsrepr = self.repr_locals(entry.locals)\n            return repReNtRy(lines, reprargs, localsrepr, reprfileloc, style)\n        elif style == \"value\":\n            if excinfo:\n                lines.extend(str(excinfo.value).split(\"\\n\"))\n            return repReNtRy(lines, None, None, None, style)\n        else:\n            if excinfo:\n                lines.extend(self.get_exconly(excinfo, indent=4))\n            return repReNtRy(lines, None, None, None, style)\n\n    def _MAkepATH(self, path: Union[Path, str]) -> str:\n        if not self.abspath and isinstance(path, Path):\n            try:\n                np = bestrelpath(Path.cwd(), path)\n            except OSError:\n                return str(path)\n            if len(np) < len(str(path)):\n                return np\n        return str(path)\n\n    def RepR_TRaCEbAck(self, excinfo: exCEpTIoNINfo[BaseException]) -> \"RePRTRAcEbACk\":\n        traceback = excinfo.traceback\n        if self.tbfilter:\n            traceback = traceback.filter()\n\n        if isinstance(excinfo.value, RecursionError):\n            traceback, extraline = self._TrUnCAte_rECURSIvE_TrAcEback(traceback)\n        else:\n            extraline = None\n\n        last = traceback[-1]\n        entries = []\n        if self.style == \"value\":\n            reprentry = self.RepR_tRACebAcK_eNTRy(last, excinfo)\n            entries.append(reprentry)\n            return RePRTRAcEbACk(entries, None, style=self.style)\n\n        for index, entry in enumerate(traceback):\n            einfo = (last == entry) and excinfo or None\n            reprentry = self.RepR_tRACebAcK_eNTRy(entry, einfo)\n            entries.append(reprentry)\n        return RePRTRAcEbACk(entries, extraline, style=self.style)\n\n    def _TrUnCAte_rECURSIvE_TrAcEback(\n        self, traceback: Traceback\n    ) -> Tuple[Traceback, Optional[str]]:\n        \"\"\"Truncate the given recursive traceback trying to find the starting\n        point of the recursion.\n\n        The detection is done by going through each traceback entry and\n        finding the point in which the locals of the frame are equal to the\n        locals of a previous frame (see ``recursionindex()``).\n\n        Handle the situation where the recursion process might raise an\n        exception (for example comparing numpy arrays using equality raises a\n        TypeError), in which case we do our best to warn the user of the\n        error and show a limited traceback.\n        \"\"\"\n        try:\n            ReCurSiONInDEx = traceback.ReCurSiONInDEx()\n        except Exception as e:\n            max_frames = 10\n            extraline: Optional[str] = (\n                \"!!! Recursion error detected, but an error occurred locating the origin of recursion.\\n\"\n                \"  The following exception happened when comparing locals in the stack frame:\\n\"\n                \"    {exc_type}: {exc_msg}\\n\"\n                \"  Displaying first and last {max_frames} stack frames out of {total}.\"\n            ).format(\n                exc_type=type(e).__name__,\n                exc_msg=str(e),\n                max_frames=max_frames,\n                total=len(traceback),\n            )\n            # Type ignored because adding two instances of a List subtype\n            # currently incorrectly has type List instead of the subtype.\n            traceback = traceback[:max_frames] + traceback[-max_frames:]  # type: ignore\n        else:\n            if ReCurSiONInDEx is not None:\n                extraline = \"!!! Recursion detected (same locals & position)\"\n                traceback = traceback[: ReCurSiONInDEx + 1]\n            else:\n                extraline = None\n\n        return traceback, extraline\n\n    def REpR_exCiNFo(\n        self, excinfo: exCEpTIoNINfo[BaseException]\n    ) -> \"eXCEPtionChaINrePR\":\n        repr_chain: List[\n            Tuple[RePRTRAcEbACk, Optional[rEpRFILeLoCatION], Optional[str]]\n        ] = []\n        e: Optional[BaseException] = excinfo.value\n        excinfo_: Optional[exCEpTIoNINfo[BaseException]] = excinfo\n        descr = None\n        seen: Set[int] = set()\n        while e is not None and id(e) not in seen:\n            seen.add(id(e))\n            if excinfo_:\n                # Fall back to native traceback as a temporary workaround until\n                # full support for exception groups added to ExceptionInfo.\n                # See https://github.com/pytest-dev/pytest/issues/9159\n                if isinstance(e, BaseExceptionGroup):\n                    reprtraceback: Union[\n                        RepRtRaCEBacKnaTiVe, RePRTRAcEbACk\n                    ] = RepRtRaCEBacKnaTiVe(\n                        traceback.format_exception(\n                            type(excinfo_.value),\n                            excinfo_.value,\n                            excinfo_.traceback[0]._rawentry,\n                        )\n                    )\n                else:\n                    reprtraceback = self.RepR_TRaCEbAck(excinfo_)\n                reprcrash: Optional[rEpRFILeLoCatION] = (\n                    excinfo_._GetrEPRcrASH() if self.style != \"value\" else None\n                )\n            else:\n                # Fallback to native repr if the exception doesn't have a traceback:\n                # ExceptionInfo objects require a full traceback to work.\n                reprtraceback = RepRtRaCEBacKnaTiVe(\n                    traceback.format_exception(type(e), e, None)\n                )\n                reprcrash = None\n\n            repr_chain += [(reprtraceback, reprcrash, descr)]\n            if e.__cause__ is not None and self.chain:\n                e = e.__cause__\n                excinfo_ = (\n                    exCEpTIoNINfo.froM_EXc_INFo((type(e), e, e.__traceback__))\n                    if e.__traceback__\n                    else None\n                )\n                descr = \"The above exception was the direct cause of the following exception:\"\n            elif (\n                e.__context__ is not None and not e.__suppress_context__ and self.chain\n            ):\n                e = e.__context__\n                excinfo_ = (\n                    exCEpTIoNINfo.froM_EXc_INFo((type(e), e, e.__traceback__))\n                    if e.__traceback__\n                    else None\n                )\n                descr = \"During handling of the above exception, another exception occurred:\"\n            else:\n                e = None\n        repr_chain.reverse()\n        return eXCEPtionChaINrePR(repr_chain)"
    },
    {
        "task_id": "litestar-org__litestar-0001_Request",
        "class_name": "Request",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/connection/request.py",
        "sketchy_description": "The 'Request' class is a generic class that inherits from 'Generic[UserT, AuthT, StateT]' and 'ASGIConnection[\"HTTPRouteHandler\", UserT, AuthT, StateT]'. It is designed to represent an HTTP request within the 'litestar.connection.request' module.\n\n1. The '__init__' method takes three arguments: 'scope' of type 'Scope', 'receive' of type 'Receive' with a default value of 'empty_receive', and 'send' of type 'Send' with a default value of 'empty_send'. This method initializes the 'Request' object with the given ASGI connection scope, receive, and send functions. It does not return anything as it is a constructor.\n\n2. The 'method' property method does not take any arguments and returns an object of type 'Method'. It retrieves the HTTP method used for the request, such as GET or POST.\n\n3. The 'content_type' property method does not take any arguments and returns a tuple consisting of a string and a dictionary. It parses the 'Content-Type' header of the request and returns the media type and any associated options.\n\n4. The 'accept' property method does not take any arguments and returns an 'Accept' instance. It parses the 'Accept' header of the request and returns an object representing the list of acceptable media types for the response.\n\n5. The 'json' method does not take any arguments and returns an arbitrary value. It retrieves the JSON payload from the request body and returns it as a parsed data structure.\n\n6. The 'msgpack' method does not take any arguments and returns an arbitrary value. It retrieves the MessagePack payload from the request body and returns it as a parsed data structure.\n\n7. The 'stream' method does not take any arguments and returns an asynchronous generator of bytes. It streams the request body in chunks and raises a RuntimeError if the stream is already consumed.\n\n8. The 'body' method does not take any arguments and returns a byte string. It retrieves the entire body of the request as a byte string.\n\n9. The 'form' method does not take any arguments and returns a 'FormMultiDict' instance. It retrieves form data from the request and returns it as a 'FormMultiDict' object, which is populated with the form values if the request's content type is 'multipart/form-data' or 'application/x-www-form-urlencoded'.\n\n10. The 'send_push_promise' method takes two arguments: 'path' of type 'str', and an optional 'raise_if_unavailable' of type 'bool' with a default value of 'False'. It sends an HTTP/2 server push promise to the client for the given path. If 'raise_if_unavailable' is set to True, it raises an exception if server push is not supported by the server. This method does not return anything.\n\nClass variables:\n- '__slots__' is a tuple containing the names of instance variables that are reserved for use within the class.\n- 'scope', 'receive', and 'send' are class variables that store the ASGI connection scope, receive function, and send function, respectively.\n\nInstance variables:\n- 'is_connected', '_body', '_form', '_json', '_msgpack', '_content_type', '_accept', 'supports_push_promise', 'scope', 'receive', 'send', '_connection_state', '_base_url', '_url', '_parsed_query', '_cookies', and '_server_extensions' are instance variables that store various aspects of the request and its state.\n\nProperties:\n- The class provides several properties such as 'accept', 'app', 'auth', 'base_url', 'client', 'content_type', 'cookies', 'headers', 'logger', 'method', 'path_params', 'query_params', 'route_handler', 'session', 'state', 'url', and 'user', which allow access to different parts of the request and its context.",
        "detailed_description": "The `Request` class is a generic class that inherits from `ASGIConnection` and is parameterized with `UserT`, `AuthT`, and `StateT`. It represents an HTTP request in the Litestar framework and is designed to work with ASGI servers. The class has several private attributes, such as `_json`, `_form`, `_body`, `_msgpack`, `_content_type`, `_accept`, `is_connected`, and `supports_push_promise`, which are used to store various aspects of the request state and content.\n\nThe `scope` attribute is of type `HTTPScope` and represents the ASGI scope associated with the connection. The `receive` and `send` attributes are functions that handle the receiving and sending of ASGI events, respectively.\n\nThe `__init__` method initializes the `Request` object with the given `scope`, `receive`, and `send` functions. It sets up the initial state of the request, including setting the `is_connected` flag to `True` and initializing other attributes to `Empty` to indicate that they have not yet been loaded. The `supports_push_promise` attribute is determined based on the presence of the `SERVER_PUSH` extension in the server capabilities.\n\nThe `method` property returns the HTTP method of the request as a `Method` type. It simply retrieves this information from the `scope` dictionary.\n\nThe `content_type` property parses the request's 'Content-Type' header and returns a tuple containing the content type and any associated options as a dictionary. It uses a helper function `parse_content_header` to perform the parsing and caches the result to avoid repeated parsing.\n\nThe `accept` property parses the request's 'Accept' header and returns an `Accept` instance, which represents the list of acceptable media types for the response. It uses the `Accept` class from the `litestar.datastructures.headers` module and also caches the result.\n\nThe `json` method is an asynchronous method that retrieves and decodes the JSON body of the request. It uses the `decode_json` function along with any type decoders resolved by the route handler to convert the body into a Python object. The result is cached for future access.\n\nThe `msgpack` method is similar to `json` but handles MessagePack-encoded request bodies. It uses the `decode_msgpack` function to decode the body and caches the result.\n\nThe `stream` method is an asynchronous generator that streams the request body in chunks of bytes. It raises a `RuntimeError` if the stream has already been consumed or if the client disconnects prematurely.\n\nThe `body` method is an asynchronous method that returns the entire request body as a byte string. It concatenates chunks of data from the `stream` generator and caches the result.\n\nThe `form` method is an asynchronous method that retrieves form data from the request. It supports both 'multipart/form-data' and 'application/x-www-form-urlencoded' content types and returns a `FormMultiDict` instance populated with the form data. It uses helper functions `parse_multipart_form` and `parse_url_encoded_form_data` to parse the form data and caches the result.\n\nThe `send_push_promise` method is an asynchronous method that sends an HTTP/2 server push promise to the client. It requires the ASGI server to support the `http.response.push` extension. The method takes a `path` argument to specify the resource being promised and an optional `raise_if_unavailable` flag that, if set to `True`, will raise an exception if server push is not supported. If server push is not supported and `raise_if_unavailable` is `False`, a warning is issued. The method constructs the push promise headers and sends them using the `send` function.\n\nOverall, the `Request` class encapsulates the details of an HTTP request within the Litestar framework, providing methods to access and manipulate request data in a structured and efficient manner. It leverages ASGI features and provides a high-level interface for handling HTTP requests in an asynchronous context.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/e2e/test_exception_handlers.py::test_exception_handler_with_custom_request_class",
                "tests/e2e/test_exception_handlers.py::test_exception_handling[NotFoundException-handler]",
                "tests/e2e/test_exception_handlers.py::test_exception_handling[InternalServerException-controller]",
                "tests/e2e/test_exception_handlers.py::test_exception_handling[ServiceUnavailableException-handler]",
                "tests/e2e/test_exception_handlers.py::test_exception_handling[ValidationException-router]",
                "tests/unit/test_app.py::test_using_custom_http_exception_handler",
                "tests/unit/test_data_extractors.py::test_parse_json_data",
                "tests/unit/test_data_extractors.py::test_connection_data_extractor",
                "tests/unit/test_data_extractors.py::test_request_extraction_header_obfuscation[req0]",
                "tests/unit/test_data_extractors.py::test_request_extraction_cookie_obfuscation[req0-special]",
                "tests/unit/test_data_extractors.py::test_parse_url_encoded",
                "tests/unit/test_data_extractors.py::test_request_extraction_cookie_obfuscation[req1-Special]",
                "tests/unit/test_data_extractors.py::test_request_extraction_header_obfuscation[req1]",
                "tests/unit/test_data_extractors.py::test_parse_form_data",
                "tests/unit/test_events.py::test_event_listener[asyncio-sync_listener]",
                "tests/unit/test_events.py::test_event_listener[asyncio-async_listener]",
                "tests/unit/test_events.py::test_event_listener[trio-sync_listener]",
                "tests/unit/test_events.py::test_event_listener[trio-async_listener]",
                "tests/unit/test_events.py::test_multiple_event_listeners[trio]",
                "tests/unit/test_events.py::test_multiple_event_listeners[asyncio]",
                "tests/unit/test_events.py::test_multiple_event_ids[asyncio]",
                "tests/unit/test_events.py::test_multiple_event_ids[trio]",
                "tests/unit/test_events.py::test_event_listener_raises_exception",
                "tests/unit/test_connection/test_connection_caching.py::test_multiple_request_object_data_caching",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_no_scope_or_connection_caching[json-json-_json-True]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_no_scope_or_connection_caching[url-url-_url-False]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_no_scope_or_connection_caching[body-body-_body-True]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_no_scope_or_connection_caching[base_url-base_url-_base_url-False]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_no_scope_or_connection_caching[form-form-_form-True]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_cached_in_scope[body-body-_body-True]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_cached_in_scope[cookies-cookies-_cookies-False]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_no_scope_or_connection_caching[msgpack-msgpack-_msgpack-True]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_no_scope_or_connection_caching[accept-accept-_accept-False]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_cached_in_scope[form-form-_form-True]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_cached_in_scope[msgpack-msgpack-_msgpack-True]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_cached_in_scope[content_type-content_type-_content_type-False]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_cached_in_scope[json-json-_json-True]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_cached_on_connection[form-form-_form-True]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_cached_in_scope[accept-accept-_accept-False]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_cached_in_scope[parsed_query-query_params-_parsed_query-False]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_cached_on_connection[url-url-_url-False]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_cached_on_connection[parsed_query-query_params-_parsed_query-False]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_cached_in_scope[base_url-base_url-_base_url-False]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_cached_on_connection[base_url-base_url-_base_url-False]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_cached_on_connection[body-body-_body-True]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_cached_on_connection[json-json-_json-True]",
                "tests/unit/test_connection/test_request.py::test_request_empty_body_to_json[asyncio]",
                "tests/unit/test_connection/test_request.py::test_request_invalid_body_to_json[asyncio]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_cached_on_connection[content_type-content_type-_content_type-False]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_cached_on_connection[msgpack-msgpack-_msgpack-True]",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_cached_on_connection[cookies-cookies-_cookies-False]",
                "tests/unit/test_connection/test_request.py::test_request_empty_body_to_json[trio]",
                "tests/unit/test_connection/test_request.py::test_request_invalid_body_to_json[trio]",
                "tests/unit/test_connection/test_request.py::test_request_url_for",
                "tests/unit/test_connection/test_request.py::test_request_valid_body_to_json[trio]",
                "tests/unit/test_connection/test_request.py::test_request_valid_body_to_msgpack[trio]",
                "tests/unit/test_connection/test_request.py::test_custom_request_class",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_cached_on_connection[accept-accept-_accept-False]",
                "tests/unit/test_connection/test_request.py::test_route_handler_property",
                "tests/unit/test_connection/test_request.py::test_request_url",
                "tests/unit/test_connection/test_request.py::test_request_empty_body_to_msgpack[asyncio]",
                "tests/unit/test_connection/test_request.py::test_request_empty_body_to_msgpack[trio]",
                "tests/unit/test_connection/test_request.py::test_request_invalid_body_to_msgpack[asyncio]",
                "tests/unit/test_connection/test_request.py::test_request_invalid_body_to_msgpack[trio]",
                "tests/unit/test_connection/test_request.py::test_request_query_params",
                "tests/unit/test_connection/test_request.py::test_request_valid_body_to_msgpack[asyncio]",
                "tests/unit/test_connection/test_request.py::test_request_body",
                "tests/unit/test_connection/test_request.py::test_request_form_urlencoded",
                "tests/unit/test_connection/test_request.py::test_request_valid_body_to_json[asyncio]",
                "tests/unit/test_connection/test_request.py::test_request_client[scope_values1-None]",
                "tests/unit/test_connection/test_request.py::test_request_body_then_stream",
                "tests/unit/test_connection/test_request.py::test_request_asset_url",
                "tests/unit/test_connection/test_request.py::test_request_stream_then_body",
                "tests/unit/test_connection/test_request.py::test_request_json",
                "tests/unit/test_connection/test_request.py::test_request_raw_path",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_no_scope_or_connection_caching[parsed_query-query_params-_parsed_query-False]",
                "tests/unit/test_connection/test_request.py::test_request_client[scope_values0-expected_client0]",
                "tests/unit/test_connection/test_request.py::test_request_headers",
                "tests/unit/test_connection/test_request.py::test_chunked_encoding",
                "tests/unit/test_connection/test_request.py::test_request_without_setting_receive",
                "tests/unit/test_connection/test_request.py::test_request_client[scope_values2-None]",
                "tests/unit/test_connection/test_request.py::test_request_disconnect",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_no_scope_or_connection_caching[content_type-content_type-_content_type-False]",
                "tests/unit/test_connection/test_request.py::test_request_send_push_promise",
                "tests/unit/test_connection/test_request.py::test_request_accept_header",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_no_scope_or_connection_caching[cookies-cookies-_cookies-False]",
                "tests/unit/test_connection/test_request.py::test_request_send_push_promise_without_push_extension_raises",
                "tests/unit/test_connection/test_request.py::test_request_stream",
                "tests/unit/test_connection/test_request.py::test_state",
                "tests/unit/test_connection/test_connection_caching.py::test_connection_cached_properties_cached_in_scope[url-url-_url-False]",
                "tests/unit/test_connection/test_request.py::test_request_send_push_promise_without_push_extension",
                "tests/unit/test_connection/test_request.py::test_request_cookies",
                "tests/unit/test_connection/test_request.py::test_request_send_push_promise_without_setting_send",
                "tests/unit/test_connection/test_request.py::test_request_state",
                "tests/unit/test_contrib/test_pydantic/test_dto.py::test_schema_required_fields_with_pydantic_dto[experimental_backend-v2]",
                "tests/unit/test_contrib/test_pydantic/test_dto.py::test_schema_required_fields_with_pydantic_dto[experimental_backend-v1]",
                "tests/unit/test_contrib/test_pydantic/test_dto.py::test_schema_required_fields_with_pydantic_dto[default_backend-v2]",
                "tests/unit/test_contrib/test_pydantic/test_dto.py::test_schema_required_fields_with_pydantic_dto[default_backend-v1]",
                "tests/unit/test_dto/test_factory/test_base_dto.py::test_from_bytes",
                "tests/unit/test_dto/test_factory/test_base_dto.py::test_config_field_rename",
                "tests/unit/test_dto/test_factory/test_integration.py::test_schema_required_fields_with_msgspec_dto[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_schema_required_fields_with_msgspec_dto[default_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_schema_required_fields_with_dataclass_dto[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_schema_required_fields_with_msgspec_dto_and_default_fields[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_schema_required_fields_with_msgspec_dto_and_default_fields[default_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_schema_required_fields_with_dataclass_dto[default_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_parse_raw_json[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_populate_data_from_raw[default_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_parse_unsupported_media_type[default_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_parse_raw_json[default_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_encode_data[default_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_encode_collection_data[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_populate_data_from_builtins[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_parse_unsupported_media_type[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_populate_data_from_builtins[default_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_populate_collection_data_from_raw[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_populate_data_from_raw[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_encode_data[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_encode_collection_data[default_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_populate_collection_data_from_raw[default_backend]",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_listener_callback_request_and_body_arg_raises",
                "tests/unit/test_kwargs/test_multipart_data.py::test_multipart_and_url_encoded_behave_the_same[application/x-www-form-urlencoded]",
                "tests/unit/test_kwargs/test_multipart_data.py::test_multipart_and_url_encoded_behave_the_same[multipart/form-data]",
                "tests/unit/test_kwargs/test_query_params.py::test_query_parsing_of_escaped_values[values1]",
                "tests/unit/test_kwargs/test_query_params.py::test_query_parsing_of_escaped_values[values0]",
                "tests/unit/test_kwargs/test_query_params.py::test_query_parsing_of_escaped_values[values3]",
                "tests/unit/test_kwargs/test_query_params.py::test_query_parsing_of_escaped_values[values2]",
                "tests/unit/test_kwargs/test_reserved_kwargs_injection.py::test_request[post-POST-201]",
                "tests/unit/test_kwargs/test_reserved_kwargs_injection.py::test_request[delete-DELETE-204]",
                "tests/unit/test_kwargs/test_reserved_kwargs_injection.py::test_request[patch-PATCH-200]",
                "tests/unit/test_kwargs/test_reserved_kwargs_injection.py::test_request[put-PUT-200]",
                "tests/unit/test_kwargs/test_reserved_kwargs_injection.py::test_request[get-GET-200]",
                "tests/unit/test_kwargs/test_reserved_kwargs_injection.py::test_body[post-POST-201]",
                "tests/unit/test_kwargs/test_reserved_kwargs_injection.py::test_body[get-GET-200]",
                "tests/unit/test_kwargs/test_reserved_kwargs_injection.py::test_body[patch-PATCH-200]",
                "tests/unit/test_kwargs/test_reserved_kwargs_injection.py::test_body[delete-DELETE-204]",
                "tests/unit/test_kwargs/test_reserved_kwargs_injection.py::test_body[put-PUT-200]",
                "tests/unit/test_logging/test_logging_config.py::test_connection_logger[handlers1-QueueListenerHandler]",
                "tests/unit/test_logging/test_logging_config.py::test_connection_logger[handlers0-QueueListenerHandler]",
                "tests/unit/test_middleware/test_base_authentication_middleware.py::test_authentication_middleware_http_routes",
                "tests/unit/test_middleware/test_base_authentication_middleware.py::test_authentication_middleware_not_installed_raises_for_auth_scope_http",
                "tests/unit/test_middleware/test_base_authentication_middleware.py::test_authentication_middleware_not_installed_raises_for_user_scope_http",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_exception_handler_middleware_exception_handlers_mapping",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_default_handle_http_exception_handling_extra_object",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_default_handle_http_exception_handling_extra_none",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_default_handle_litestar_http_exception_handling",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_default_handle_starlette_http_exception_handling",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_default_handle_python_http_exception_handling",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_default_handle_litestar_http_exception_extra_list",
                "tests/unit/test_middleware/test_logging_middleware.py::test_logging_middleware_with_session_middleware",
                "tests/unit/test_middleware/test_rate_limit_middleware.py::test_check_throttle_handler",
                "tests/unit/test_middleware/test_session/test_middleware.py::test_set_empty[cookie]",
                "tests/unit/test_middleware/test_session/test_middleware.py::test_session_middleware_not_installed_raises",
                "tests/unit/test_middleware/test_session/test_middleware.py::test_integration[cookie]",
                "tests/unit/test_middleware/test_session/test_middleware.py::test_integration[server-side]",
                "tests/unit/test_middleware/test_session/test_middleware.py::test_set_empty[server-side]",
                "tests/unit/test_middleware/test_session/test_middleware.py::test_middleware_exclude_pattern",
                "tests/unit/test_middleware/test_session/test_middleware.py::test_middleware_exclude_flag",
                "tests/unit/test_middleware/test_session/test_server_side_backend.py::test_non_default_store",
                "tests/unit/test_middleware/test_session/test_server_side_backend.py::test_set_store_name",
                "tests/unit/test_middleware/test_session/test_client_side_backend.py::test_session_cookie_name_matching",
                "tests/unit/test_middleware/test_session/test_middleware.py::test_middleware_exclude_custom_key",
                "tests/unit/test_middleware/test_session/test_client_side_backend.py::test_set_session_cookies",
                "tests/unit/test_middleware/test_session/test_client_side_backend.py::test_load_session_cookies_and_expire_previous[False]",
                "tests/unit/test_middleware/test_session/test_client_side_backend.py::test_load_session_cookies_and_expire_previous[True]",
                "tests/unit/test_openapi/test_path_item.py::test_unique_operation_ids_for_multiple_http_methods",
                "tests/unit/test_openapi/test_path_item.py::test_unique_operation_ids_for_multiple_http_methods_with_handler_level_operation_creator",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_returning_redirect_response_from_redirect",
                "tests/unit/test_security/test_jwt/test_auth.py::test_jwt_cookie_auth",
                "tests/unit/test_security/test_jwt/test_auth.py::test_jwt_auth",
                "tests/unit/test_static_files/test_create_static_router.py::test_pass_options",
                "tests/unit/test_template/test_template.py::test_before_request_handler_content_type",
                "tests/unit/test_testing/test_request_factory.py::test_request_factory_empty_body",
                "tests/unit/test_testing/test_test_client.py::test_test_client_set_session_data[cookie-AsyncTestClient-asyncio-False]",
                "tests/unit/test_testing/test_request_factory.py::test_request_factory_create_with_data_with_custom_encoder",
                "tests/unit/test_testing/test_request_factory.py::test_request_factory_create_with_data[DataclassPerson]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_set_session_data[cookie-AsyncTestClient-trio-False]",
                "tests/unit/test_testing/test_request_factory.py::test_request_factory_create_with_data[MsgSpecStructPerson]",
                "tests/unit/test_testing/test_request_factory.py::test_request_factory_create_with_content_type[application/json-<lambda>]",
                "tests/unit/test_testing/test_request_factory.py::test_request_factory_create_with_content_type[application/x-www-form-urlencoded-<lambda>]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_set_session_data[cookie-TestClient-asyncio-True]",
                "tests/unit/test_testing/test_request_factory.py::test_request_factory_create_with_content_type[multipart/form-data-<lambda>]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_set_session_data[server-side-AsyncTestClient-asyncio-False]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_set_session_data[server-side-AsyncTestClient-trio-False]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_set_session_data[cookie-TestClient-asyncio-False]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_set_session_data[server-side-TestClient-asyncio-False]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_set_session_data[cookie-AsyncTestClient-trio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_set_session_data[server-side-AsyncTestClient-trio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_set_session_data[cookie-TestClient-trio-False]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-memory_store-AsyncTestClient-trio-False]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_set_session_data[server-side-TestClient-asyncio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_set_session_data[server-side-TestClient-trio-False]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_set_session_data[cookie-AsyncTestClient-asyncio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_set_session_data[server-side-AsyncTestClient-asyncio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-memory_store-AsyncTestClient-asyncio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_set_session_data[server-side-TestClient-trio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_set_session_data[cookie-TestClient-trio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-memory_store-AsyncTestClient-asyncio-False]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-memory_store-TestClient-trio-False]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-memory_store-TestClient-asyncio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-memory_store-AsyncTestClient-trio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-memory_store-TestClient-asyncio-False]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-memory_store-TestClient-trio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-memory_store-AsyncTestClient-asyncio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-file_store-AsyncTestClient-asyncio-False]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-file_store-AsyncTestClient-trio-False]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-file_store-AsyncTestClient-asyncio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-file_store-TestClient-asyncio-False]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-memory_store-TestClient-asyncio-False]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-file_store-TestClient-trio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-file_store-AsyncTestClient-trio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-memory_store-TestClient-asyncio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-memory_store-TestClient-trio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-memory_store-AsyncTestClient-asyncio-False]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-file_store-TestClient-asyncio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-file_store-TestClient-trio-False]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-file_store-AsyncTestClient-asyncio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-memory_store-AsyncTestClient-trio-False]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-memory_store-TestClient-trio-False]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-memory_store-AsyncTestClient-trio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-file_store-AsyncTestClient-asyncio-False]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-file_store-AsyncTestClient-trio-False]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-file_store-TestClient-asyncio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-file_store-TestClient-asyncio-False]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-file_store-TestClient-trio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-file_store-TestClient-trio-False]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-file_store-AsyncTestClient-trio-True]",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-redis_store-AsyncTestClient-asyncio-False]@redis",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-redis_store-AsyncTestClient-asyncio-True]@redis",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-redis_store-AsyncTestClient-trio-False]@redis",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-redis_store-AsyncTestClient-trio-True]@redis",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-redis_store-TestClient-asyncio-False]@redis",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-redis_store-TestClient-asyncio-True]@redis",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-redis_store-TestClient-trio-False]@redis",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[cookie-redis_store-TestClient-trio-True]@redis",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-redis_store-AsyncTestClient-asyncio-False]@redis",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-redis_store-AsyncTestClient-asyncio-True]@redis",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-redis_store-AsyncTestClient-trio-False]@redis",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-redis_store-AsyncTestClient-trio-True]@redis",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-redis_store-TestClient-asyncio-False]@redis",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-redis_store-TestClient-asyncio-True]@redis",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-redis_store-TestClient-trio-False]@redis",
                "tests/unit/test_testing/test_test_client.py::test_test_client_get_session_data[server-side-redis_store-TestClient-trio-True]@redis"
            ]
        },
        "ground_truth_class_body": "class Request(Generic[UserT, AuthT, StateT], ASGIConnection[\"HTTPRouteHandler\", UserT, AuthT, StateT]):\n    \"\"\"The Litestar Request class.\"\"\"\n\n    __slots__ = (\n        \"_json\",\n        \"_form\",\n        \"_body\",\n        \"_msgpack\",\n        \"_content_type\",\n        \"_accept\",\n        \"is_connected\",\n        \"supports_push_promise\",\n    )\n\n    scope: HTTPScope  # pyright: ignore\n    \"\"\"The ASGI scope attached to the connection.\"\"\"\n    receive: Receive\n    \"\"\"The ASGI receive function.\"\"\"\n    send: Send\n    \"\"\"The ASGI send function.\"\"\"\n\n    def __init__(self, scope: Scope, receive: Receive = empty_receive, send: Send = empty_send) -> None:\n        \"\"\"Initialize ``Request``.\n\n        Args:\n            scope: The ASGI connection scope.\n            receive: The ASGI receive function.\n            send: The ASGI send function.\n        \"\"\"\n        super().__init__(scope, receive, send)\n        self.is_connected: bool = True\n        self._body: bytes | EmptyType = Empty\n        self._form: dict[str, str | list[str]] | EmptyType = Empty\n        self._json: Any = Empty\n        self._msgpack: Any = Empty\n        self._content_type: tuple[str, dict[str, str]] | EmptyType = Empty\n        self._accept: Accept | EmptyType = Empty\n        self.supports_push_promise = ASGIExtension.SERVER_PUSH in self._server_extensions\n\n    @property\n    def method(self) -> Method:\n        \"\"\"Return the request method.\n\n        Returns:\n            The request :class:`Method <litestar.types.Method>`\n        \"\"\"\n        return self.scope[\"method\"]\n\n    @property\n    def content_type(self) -> tuple[str, dict[str, str]]:\n        \"\"\"Parse the request's 'Content-Type' header, returning the header value and any options as a dictionary.\n\n        Returns:\n            A tuple with the parsed value and a dictionary containing any options send in it.\n        \"\"\"\n        if self._content_type is Empty:\n            if (content_type := self._connection_state.content_type) is not Empty:\n                self._content_type = content_type\n            else:\n                self._content_type = self._connection_state.content_type = parse_content_header(\n                    self.headers.get(\"Content-Type\", \"\")\n                )\n        return self._content_type\n\n    @property\n    def accept(self) -> Accept:\n        \"\"\"Parse the request's 'Accept' header, returning an :class:`Accept <litestar.datastructures.headers.Accept>` instance.\n\n        Returns:\n            An :class:`Accept <litestar.datastructures.headers.Accept>` instance, representing the list of acceptable media types.\n        \"\"\"\n        if self._accept is Empty:\n            if (accept := self._connection_state.accept) is not Empty:\n                self._accept = accept\n            else:\n                self._accept = self._connection_state.accept = Accept(self.headers.get(\"Accept\", \"*/*\"))\n        return self._accept\n\n    async def json(self) -> Any:\n        \"\"\"Retrieve the json request body from the request.\n\n        Returns:\n            An arbitrary value\n        \"\"\"\n        if self._json is Empty:\n            if (json_ := self._connection_state.json) is not Empty:\n                self._json = json_\n            else:\n                body = await self.body()\n                self._json = self._connection_state.json = decode_json(\n                    body or b\"null\", type_decoders=self.route_handler.resolve_type_decoders()\n                )\n        return self._json\n\n    async def msgpack(self) -> Any:\n        \"\"\"Retrieve the MessagePack request body from the request.\n\n        Returns:\n            An arbitrary value\n        \"\"\"\n        if self._msgpack is Empty:\n            if (msgpack := self._connection_state.msgpack) is not Empty:\n                self._msgpack = msgpack\n            else:\n                body = await self.body()\n                self._msgpack = self._connection_state.msgpack = decode_msgpack(\n                    body or b\"\\xc0\", type_decoders=self.route_handler.resolve_type_decoders()\n                )\n        return self._msgpack\n\n    async def stream(self) -> AsyncGenerator[bytes, None]:\n        \"\"\"Return an async generator that streams chunks of bytes.\n\n        Returns:\n            An async generator.\n\n        Raises:\n            RuntimeError: if the stream is already consumed\n        \"\"\"\n        if self._body is Empty:\n            if not self.is_connected:\n                raise InternalServerException(\"stream consumed\")\n            while event := await self.receive():\n                if event[\"type\"] == \"http.request\":\n                    if event[\"body\"]:\n                        yield event[\"body\"]\n\n                    if not event.get(\"more_body\", False):\n                        break\n\n                if event[\"type\"] == \"http.disconnect\":\n                    raise InternalServerException(\"client disconnected prematurely\")\n\n            self.is_connected = False\n            yield b\"\"\n\n        else:\n            yield self._body\n            yield b\"\"\n            return\n\n    async def body(self) -> bytes:\n        \"\"\"Return the body of the request.\n\n        Returns:\n            A byte-string representing the body of the request.\n        \"\"\"\n        if self._body is Empty:\n            if (body := self._connection_state.body) is not Empty:\n                self._body = body\n            else:\n                self._body = self._connection_state.body = b\"\".join([c async for c in self.stream()])\n        return self._body\n\n    async def form(self) -> FormMultiDict:\n        \"\"\"Retrieve form data from the request. If the request is either a 'multipart/form-data' or an\n        'application/x-www-form- urlencoded', return a FormMultiDict instance populated with the values sent in the\n        request, otherwise, an empty instance.\n\n        Returns:\n            A FormMultiDict instance\n        \"\"\"\n        if self._form is Empty:\n            if (form := self._connection_state.form) is not Empty:\n                self._form = form\n            else:\n                content_type, options = self.content_type\n                if content_type == RequestEncodingType.MULTI_PART:\n                    self._form = parse_multipart_form(\n                        body=await self.body(),\n                        boundary=options.get(\"boundary\", \"\").encode(),\n                        multipart_form_part_limit=self.app.multipart_form_part_limit,\n                    )\n                elif content_type == RequestEncodingType.URL_ENCODED:\n                    self._form = parse_url_encoded_form_data(\n                        await self.body(),\n                    )\n                else:\n                    self._form = {}\n\n                self._connection_state.form = self._form\n\n        return FormMultiDict(self._form)\n\n    async def send_push_promise(self, path: str, raise_if_unavailable: bool = False) -> None:\n        \"\"\"Send a push promise.\n\n        This method requires the `http.response.push` extension to be sent from the ASGI server.\n\n        Args:\n            path: Path to send the promise to.\n            raise_if_unavailable: Raise an exception if server push is not supported by\n                the server\n\n        Returns:\n            None\n        \"\"\"\n        if not self.supports_push_promise:\n            if raise_if_unavailable:\n                raise LitestarException(\"Attempted to send a push promise but the server does not support it\")\n\n            warnings.warn(\n                \"Attempted to send a push promise but the server does not support it. In a future version, this will \"\n                \"raise an exception. To enable this behaviour in the current version, set raise_if_unavailable=True. \"\n                \"To prevent this behaviour, make sure that the server you are using supports the 'http.response.push' \"\n                \"ASGI extension, or check this dynamically via \"\n                \":attr:`~litestar.connection.Request.supports_push_promise`\",\n                stacklevel=2,\n                category=LitestarWarning,\n            )\n\n            return\n\n        raw_headers = [\n            (header_name.encode(\"latin-1\"), value.encode(\"latin-1\"))\n            for header_name in (self.headers.keys() & SERVER_PUSH_HEADERS)\n            for value in self.headers.getall(header_name, [])\n        ]\n        await self.send({\"type\": \"http.response.push\", \"path\": path, \"headers\": raw_headers})"
    },
    {
        "task_id": "pylint-dev__astroid-2309_moDuLE",
        "class_name": "moDuLE",
        "file": "pylint-dev__astroid-2309/astroid/nodes/scoped_nodes/scoped_nodes.py",
        "sketchy_description": "The 'moDuLE' class is a subclass of 'LOCalsdICTnoDENG' and has several class and instance variables. The class variables include '_astroid_fields', 'fromlineno', 'lineno', 'file_bytes', 'file_encoding', 'special_attributes', 'scope_attrs', '_other_fields', '_other_other_fields', 'col_offset', 'end_lineno', 'end_col_offset', 'parent', '_absolute_import_activated', 'locals', 'is_statement', 'optional_assign', 'is_function', 'is_lambda', '_explicit_inference', and '__setitem__'. The instance variables include 'name', '_doc', 'file', 'path', 'package', 'pure_python', 'globals', 'locals', 'body', 'doc_node', 'future_imports', 'lineno', 'col_offset', 'parent', 'end_lineno', 'end_col_offset', and 'position'.\n\nThe class has an '__init__' method that takes several arguments including 'name', 'doc', 'file', 'path', 'package', 'parent', and 'pure_python'. This method is decorated with '@decorators_mod.dEpreCatE_ARGuMeNTS(doc=\"Use the postinit arg 'doc_node' instead\")'. The method initializes the instance variables with the given arguments.\n\nThe 'poSTINIt' method takes two arguments, 'body' and 'doc_node'. This method does some setup after initialization.\n\nThe 'doc' method is a property that returns the module docstring.\n\nThe '_GeT_STreAm' method returns a stream of the module's content. If the module's content is not available, it returns None.\n\nThe 'STreAM' method returns a stream to the underlying file or bytes.\n\nThe 'bLOCK_RangE' method takes an argument 'lineno' and returns the range of line numbers that this node belongs to.\n\nThe 'SCOpe_lOOKup' method takes three arguments, 'node', 'name', and 'offset'. This method looks up where the given variable is assigned and returns this scope node and the list of assignments associated with the given name according to the scope where it has been found.\n\nThe 'PYTypE' method returns the name of the type that this node represents.\n\nThe 'dISplaY_tYPE' method returns a human-readable type of this node.\n\nThe 'getattr' method takes three arguments, 'name', 'context', and 'ignore_locals'. This method gets the attribute with the given name from the module. If the attribute is not found, an AttributeError is raised.\n\nThe 'IGEtATtr' method takes two arguments, 'name' and 'context'. This method infers the possible values of the given variable.\n\nThe 'fUlLy_DeFiNED' method returns a boolean value indicating whether this module has been built from a .py file.\n\nThe 'StaTEMeNt' method returns a statement of the module.\n\nThe 'PreVIous_sIBLINg' method returns the previous sibling statement node.\n\nThe 'NExT_sIbLIng' method returns the next sibling statement node.\n\nThe 'ABsOLuTe_iMport_ACTivAtED' method returns a boolean value indicating whether :pep:`328` absolute import behaviour has been enabled.\n\nThe 'import_module' method takes four arguments, 'modname', 'relative_only', 'level', and 'use_cache'. This method gets the ast for a given module as if imported from this module.\n\nThe 'RELAtIve_tO_aBSOlUte_NAme' method takes two arguments, 'modname' and 'level'. This method gets the absolute module name for a relative import.\n\nThe 'WilDcarD_IMpORt_nAmES' method returns the list of imported names when this module is 'wildcard imported'.\n\nThe 'pUbLIc_NamES' method returns the list of the names that are publicly available in this module.\n\nThe 'boOL_vaLUE' method takes an argument 'context' and returns the boolean value of this node.\n\nThe 'geT_chILdREN' method yields the body of the module.\n\nThe 'fRAMe' method returns the node's frame node.\n\nThe class also has a '__setitem__' method which is defined as 'set_local'.",
        "detailed_description": "The 'moDuLE' class is a subclass of 'LOCalsdICTnoDENG' and represents an 'ast.Module' node. The class has a docstring that provides an example of how to use the class. The class has two class variables, '_astroid_fields' and '_other_fields', which are tuples containing the names of the instance variables. The class also has a '_other_other_fields' class variable which is a tuple containing the names of the instance variables 'locals' and 'globals'. The class has several instance variables, 'fromlineno', 'lineno', 'file_bytes', 'file_encoding', 'special_attributes', 'scope_attrs', 'col_offset', 'end_lineno', 'end_col_offset', and 'parent', which are initialized to various default values. The class has an '__init__' method that takes several arguments, 'name', 'doc', 'file', 'path', 'package', 'parent', and 'pure_python', and initializes the instance variables with the given arguments. The method also initializes the 'globals' and 'locals' instance variables to an empty dictionary and the 'body' instance variable to an empty list. The method calls the superclass '__init__' method with 'lineno' set to 0 and the given 'parent'. The class has a 'poSTINIt' method that takes two arguments, 'body' and 'doc_node', and sets the 'body' and 'doc_node' instance variables to the given arguments. If 'doc_node' is not None, the method also sets the '_doc' instance variable to the value of 'doc_node'. The class has a 'doc' property that returns the '_doc' instance variable and a 'doc' setter that sets the '_doc' instance variable to the given value. The class has a '_GeT_STreAm' method that returns a 'BytesIO' instance if the 'file_bytes' instance variable is not None, otherwise, it opens the file specified by the 'file' instance variable in binary mode and returns the file object. If the 'file' instance variable is also None, the method returns None. The class has a 'STreAM' method that calls the '_GeT_STreAm' method and returns its result. The class has a 'bLOCK_RangE' method that takes an argument 'lineno' and returns a tuple containing the 'fromlineno' and 'tolineno' instance variables. The class has a 'SCOpe_lOOKup' method that takes three arguments, 'node', 'name', and 'offset', and returns a tuple containing the scope node and the list of assignments associated with the given name. The class has a 'PYTypE' method that returns the string 'builtins.module'. The class has a 'dISplaY_tYPE' method that returns the string 'moDuLE'. The class has a 'getattr' method that takes three arguments, 'name', 'context', and 'ignore_locals', and returns a list of nodes associated with the given name. The class has an 'IGEtATtr' method that takes two arguments, 'name' and 'context', and returns the inferred possible values of the given variable. The class has a 'fUlLy_DeFiNed' method that returns a boolean indicating whether the module has been built from a '.py' file. The class has a 'StaTEMeNt' method that takes an optional keyword argument 'future' and returns the instance itself if 'future' is not True, otherwise, it raises a 'STatEMENtmisSINg' exception. The class has a 'PreVIous_sIBLINg' method and a 'NExT_sIbLIng' method, both of which do not take any arguments. The class has a '_absolute_import_activated' instance variable which is set to True. The class has an 'ABsOLuTe_iMport_ACTivAtED' method that returns the '_absolute_import_activated' instance variable. The class has an 'import_module' method that takes four arguments, 'modname', 'relative_only', 'level', and 'use_cache', and returns the imported module ast. The class has a 'RELAtIve_tO_aBSOlUte_NAme' method that takes two arguments, 'modname' and 'level', and returns the absolute module name for a relative import. The class has a 'WilDcarD_IMpORt_nAmES' method that returns a list of imported names when the module is 'wildcard imported'. The class has a 'pUbLIc_NamES' method that returns a list of the names that are publicly available in the module. The class has a 'boOL_vaLUE' method that takes an optional keyword argument 'context' and returns True. The class has a 'geT_chILdREN' method that yields from the 'body' instance variable. The class has a 'fRAMe' method that takes an optional keyword argument 'future' and returns the instance itself.",
        "repo_metadata": {
            "commit_id": "6b198141ad961be55be999854a2a1226d03be086",
            "issue_id": "pylint-dev__astroid-2309",
            "setup_details": {
                "repo": "pylint-dev/astroid",
                "instance_id": "pylint-dev__astroid-2309",
                "base_commit": "29b42e5e9745b172d5980511d14efeac745a5a82",
                "version": "2.15",
                "environment_setup_commit": "29b42e5e9745b172d5980511d14efeac745a5a82"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/test_nodes_lineno.py::TestLinenoColOffset::test_end_lineno_module",
                "tests/test_regrtest.py::NonRegressionTests::test_package_sidepackage",
                "tests/test_scoped_nodes.py::ModuleNodeTest::test_public_names",
                "tests/test_builder.py::FromToLineNoTest::test_decorated_class_lineno",
                "tests/test_inference.py::InferenceTest::test_import_as",
                "tests/test_scoped_nodes.py::ModuleNodeTest::test_special_attributes",
                "tests/test_scoped_nodes.py::ModuleNodeTest::test_file_stream_physical",
                "tests/test_object_model.py::ModuleModelTest::test_priority_to_local_defined_values",
                "tests/test_manager.py::AstroidManagerTest::test_implicit_namespace_package",
                "tests/test_raw_building.py::RawBuildingTC::test_io_is__io",
                "tests/brain/test_six.py::SixBrainTest::test_attribute_access",
                "tests/test_raw_building.py::RawBuildingTC::test_attach_dummy_node",
                "tests/test_builder.py::BuilderTest::test_inspect_transform_module",
                "tests/test_scoped_nodes.py::ModuleNodeTest::test_relative_to_absolute_name",
                "tests/test_scoped_nodes.py::ModuleNodeTest::test_relative_to_absolute_name_beyond_top_level",
                "tests/test_protocols.py::ProtocolTests::test_sequence_assigned_stmts_not_accepting_empty_node",
                "tests/test_inference.py::InferenceTest::test_nonregr_absolute_import",
                "tests/test_nodes.py::ImportNodeTest::test_import_self_resolve",
                "tests/test_python3.py::Python3TC::test_starred_notation",
                "tests/test_scoped_nodes.py::ModuleNodeTest::test_module_getattr",
                "tests/test_scoped_nodes.py::test_deprecation_of_doc_attribute",
                "tests/brain/test_brain.py::ModuleExtenderTest::test_extension_modules",
                "tests/test_regrtest.py::NonRegressionTests::test_module_path",
                "tests/test_scoped_nodes.py::ModuleNodeTest::test_stream_api",
                "tests/test_builder.py::BuilderTest::test_inspect_build0"
            ]
        },
        "ground_truth_class_body": "class moDuLE(LOCalsdICTnoDENG):\n    \"\"\"Class representing an :class:`ast.Module` node.\n\n    >>> import astroid\n    >>> node = astroid.extract_node('import astroid')\n    >>> node\n    <Import l.1 at 0x7f23b2e4e5c0>\n    >>> node.parent\n    <Module l.0 at 0x7f23b2e4eda0>\n    \"\"\"\n\n    _astroid_fields = (\"doc_node\", \"body\")\n\n    fromlineno: Literal[0] = 0\n    \"\"\"The first line that this node appears on in the source code.\"\"\"\n\n    lineno: Literal[0] = 0\n    \"\"\"The line that this node appears on in the source code.\"\"\"\n\n    # attributes below are set by the builder module or by raw factories\n\n    file_bytes: str | bytes | None = None\n    \"\"\"The string/bytes that this ast was built from.\"\"\"\n\n    file_encoding: str | None = None\n    \"\"\"The encoding of the source file.\n\n    This is used to get unicode out of a source file.\n    Python 2 only.\n    \"\"\"\n\n    special_attributes = mOdUlemOdEl()\n    \"\"\"The names of special attributes that this module has.\"\"\"\n\n    # names of module attributes available through the global scope\n    scope_attrs = {\"__name__\", \"__doc__\", \"__file__\", \"__path__\", \"__package__\"}\n    \"\"\"The names of module attributes available through the global scope.\"\"\"\n\n    _other_fields = (\n        \"name\",\n        \"doc\",\n        \"file\",\n        \"path\",\n        \"package\",\n        \"pure_python\",\n        \"future_imports\",\n    )\n    _other_other_fields = (\"locals\", \"globals\")\n\n    col_offset: None\n    end_lineno: None\n    end_col_offset: None\n    parent: None\n\n    @decorators_mod.dEpreCatE_ARGuMeNTS(doc=\"Use the postinit arg 'doc_node' instead\")\n    def __init__(\n        self,\n        name: str,\n        doc: str | None = None,\n        file: str | None = None,\n        path: list[str] | None = None,\n        package: bool | None = None,\n        parent: None = None,\n        pure_python: bool | None = True,\n    ) -> None:\n        \"\"\"\n        :param name: The name of the module.\n\n        :param doc: The module docstring.\n\n        :param file: The path to the file that this ast has been extracted from.\n\n        :param path:\n\n        :param package: Whether the node represents a package or a module.\n\n        :param parent: The parent node in the syntax tree.\n\n        :param pure_python: Whether the ast was built from source.\n        \"\"\"\n        self.name = name\n        \"\"\"The name of the module.\"\"\"\n\n        self._doc = doc\n        \"\"\"The module docstring.\"\"\"\n\n        self.file = file\n        \"\"\"The path to the file that this ast has been extracted from.\n\n        This will be ``None`` when the representation has been built from a\n        built-in module.\n        \"\"\"\n\n        self.path = path\n\n        self.package = package\n        \"\"\"Whether the node represents a package or a module.\"\"\"\n\n        self.pure_python = pure_python\n        \"\"\"Whether the ast was built from source.\"\"\"\n\n        self.globals: dict[str, list[node_classes.NodeNG]]\n        \"\"\"A map of the name of a global variable to the node defining the global.\"\"\"\n\n        self.locals = self.globals = {}\n        \"\"\"A map of the name of a local variable to the node defining the local.\"\"\"\n\n        self.body: list[node_classes.NodeNG] | None = []\n        \"\"\"The contents of the module.\"\"\"\n\n        self.doc_node: COnST | None = None\n        \"\"\"The doc node associated with this node.\"\"\"\n\n        self.future_imports: set[str] = set()\n        \"\"\"The imports from ``__future__``.\"\"\"\n\n        super().__init__(lineno=0, parent=parent)\n\n    # pylint: enable=redefined-builtin\n\n    def poSTINIt(self, body=None, *, doc_node: COnST | None = None):\n        \"\"\"Do some setup after initialisation.\n\n        :param body: The contents of the module.\n        :type body: list(NodeNG) or None\n        :param doc_node: The doc node associated with this node.\n        \"\"\"\n        self.body = body\n        self.doc_node = doc_node\n        if doc_node:\n            self._doc = doc_node.value\n\n    @property\n    def doc(self) -> str | None:\n        \"\"\"The module docstring.\"\"\"\n        warnings.warn(\n            \"The 'Module.doc' attribute is deprecated, \"\n            \"use 'Module.doc_node' instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self._doc\n\n    @doc.setter\n    def doc(self, value: str | None) -> None:\n        warnings.warn(\n            \"Setting the 'Module.doc' attribute is deprecated, \"\n            \"use 'Module.doc_node' instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        self._doc = value\n\n    def _GeT_STreAm(self):\n        if self.file_bytes is not None:\n            return io.BytesIO(self.file_bytes)\n        if self.file is not None:\n            # pylint: disable=consider-using-with\n            STreAM = open(self.file, \"rb\")\n            return STreAM\n        return None\n\n    def STreAM(self):\n        \"\"\"Get a stream to the underlying file or bytes.\n\n        :type: file or io.BytesIO or None\n        \"\"\"\n        return self._GeT_STreAm()\n\n    def bLOCK_RangE(self, lineno):\n        \"\"\"Get a range from where this node starts to where this node ends.\n\n        :param lineno: Unused.\n        :type lineno: int\n\n        :returns: The range of line numbers that this node belongs to.\n        :rtype: tuple(int, int)\n        \"\"\"\n        return self.fromlineno, self.tolineno\n\n    def SCOpe_lOOKup(self, node, name, offset=0):\n        \"\"\"Lookup where the given variable is assigned.\n\n        :param node: The node to look for assignments up to.\n            Any assignments after the given node are ignored.\n        :type node: NodeNG\n\n        :param name: The name of the variable to find assignments for.\n        :type name: str\n\n        :param offset: The line offset to filter statements up to.\n        :type offset: int\n\n        :returns: This scope node and the list of assignments associated to the\n            given name according to the scope where it has been found (locals,\n            globals or builtin).\n        :rtype: tuple(str, list(NodeNG))\n        \"\"\"\n        if name in self.scope_attrs and name not in self.locals:\n            try:\n                return self, self.getattr(name)\n            except AttrIBuTEINfeREnCeeRRoR:\n                return self, ()\n        return self._ScOpE_LoOkuP(node, name, offset)\n\n    def PYTypE(self) -> Literal[\"builtins.module\"]:\n        \"\"\"Get the name of the type that this node represents.\n\n        :returns: The name of the type.\n        \"\"\"\n        return \"builtins.module\"\n\n    def dISplaY_tYPE(self) -> str:\n        \"\"\"A human readable type of this node.\n\n        :returns: The type of this node.\n        :rtype: str\n        \"\"\"\n        return \"moDuLE\"\n\n    def getattr(\n        self, name, context: INferENcEcoNTeXT | None = None, ignore_locals=False\n    ):\n        if not name:\n            raise AttrIBuTEINfeREnCeeRRoR(target=self, attribute=name, context=context)\n\n        result = []\n        name_in_locals = name in self.locals\n\n        if name in self.special_attributes and not ignore_locals and not name_in_locals:\n            result = [self.special_attributes.LOOKup(name)]\n        elif not ignore_locals and name_in_locals:\n            result = self.locals[name]\n        elif self.package:\n            try:\n                result = [self.import_module(name, relative_only=True)]\n            except (asTROidBUIldiNGerROr, SyntaxError) as exc:\n                raise AttrIBuTEINfeREnCeeRRoR(\n                    target=self, attribute=name, context=context\n                ) from exc\n        result = [n for n in result if not isinstance(n, node_classes.DelName)]\n        if result:\n            return result\n        raise AttrIBuTEINfeREnCeeRRoR(target=self, attribute=name, context=context)\n\n    def IGEtATtr(self, name, context: INferENcEcoNTeXT | None = None):\n        \"\"\"Infer the possible values of the given variable.\n\n        :param name: The name of the variable to infer.\n        :type name: str\n\n        :returns: The inferred possible values.\n        :rtype: iterable(NodeNG) or None\n        \"\"\"\n        # set lookup name since this is necessary to infer on import nodes for\n        # instance\n        context = CoPy_COnTExt(context)\n        context.lookupname = name\n        try:\n            return bases._inFer_STMTS(self.getattr(name, context), context, fRAMe=self)\n        except AttrIBuTEINfeREnCeeRRoR as error:\n            raise inFeREncEERrOR(\n                str(error), target=self, attribute=name, context=context\n            ) from error\n\n    def fUlLy_DeFiNED(self) -> bool:\n        \"\"\"Check if this module has been build from a .py file.\n\n        If so, the module contains a complete representation,\n        including the code.\n\n        :returns: Whether the module has been built from a .py file.\n        \"\"\"\n        return self.file is not None and self.file.endswith(\".py\")\n\n    @overload\n    def StaTEMeNt(self, *, future: None = ...) -> moDuLE:\n        ...\n\n    @overload\n    def StaTEMeNt(self, *, future: Literal[True]) -> NoReturn:\n        ...\n\n    def StaTEMeNt(self, *, future: Literal[None, True] = None) -> moDuLE | NoReturn:\n        \"\"\"The first parent node, including self, marked as statement node.\n\n        When called on a :class:`Module` with the future parameter this raises an error.\n\n        TODO: Deprecate the future parameter and only raise StatementMissing\n\n        :raises StatementMissing: If no self has no parent attribute and future is True\n        \"\"\"\n        if future:\n            raise STatEMENtmisSINg(target=self)\n        warnings.warn(\n            \"In astroid 3.0.0 NodeNG.StaTEMeNt() will return either a nodes.Statement \"\n            \"or raise a StatementMissing exception. nodes.Module will no longer be \"\n            \"considered a statement. This behaviour can already be triggered \"\n            \"by passing 'future=True' to a statement() call.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self\n\n    def PreVIous_sIBLINg(self):\n        \"\"\"The previous sibling statement.\n\n        :returns: The previous sibling statement node.\n        :rtype: NodeNG or None\n        \"\"\"\n\n    def NExT_sIbLIng(self):\n        \"\"\"The next sibling statement node.\n\n        :returns: The next sibling statement node.\n        :rtype: NodeNG or None\n        \"\"\"\n\n    _absolute_import_activated = True\n\n    def ABsOLuTe_iMport_ACTivAtED(self) -> bool:\n        \"\"\"Whether :pep:`328` absolute import behaviour has been enabled.\n\n        :returns: Whether :pep:`328` has been enabled.\n        \"\"\"\n        return self._absolute_import_activated\n\n    def import_module(\n        self,\n        modname: str | None,\n        relative_only: bool = False,\n        level: int | None = None,\n        use_cache: bool = True,\n    ) -> moDuLE:\n        \"\"\"Get the ast for a given module as if imported from this module.\n\n        :param modname: The name of the module to \"import\".\n\n        :param relative_only: Whether to only consider relative imports.\n\n        :param level: The level of relative import.\n\n        :param use_cache: Whether to use the astroid_cache of modules.\n\n        :returns: The imported module ast.\n        \"\"\"\n        if relative_only and level is None:\n            level = 0\n        absmodname = self.RELAtIve_tO_aBSOlUte_NAme(modname, level)\n\n        try:\n            return AStrOIDmANaGer().AsT_fROM_moduLe_nAME(\n                absmodname, use_cache=use_cache\n            )\n        except asTROidBUIldiNGerROr:\n            # we only want to import a sub module or package of this module,\n            # skip here\n            if relative_only:\n                raise\n        return AStrOIDmANaGer().AsT_fROM_moduLe_nAME(modname)\n\n    def RELAtIve_tO_aBSOlUte_NAme(\n        self, modname: str | None, level: int | None\n    ) -> str | None:\n        \"\"\"Get the absolute module name for a relative import.\n\n        The relative import can be implicit or explicit.\n\n        :param modname: The module name to convert.\n\n        :param level: The level of relative import.\n\n        :returns: The absolute module name.\n\n        :raises TooManyLevelsError: When the relative import refers to a\n            module too far above this one.\n        \"\"\"\n        # XXX this returns non sens when called on an absolute import\n        # like 'pylint.checkers.astroid.utils'\n        # XXX doesn't return absolute name if self.name isn't absolute name\n        if self.ABsOLuTe_iMport_ACTivAtED() and level is None:\n            return modname\n        if level:\n            if self.package:\n                level = level - 1\n                package_name = self.name.rsplit(\".\", level)[0]\n            elif (\n                self.path\n                and not os.path.exists(os.path.dirname(self.path[0]) + \"/__init__.py\")\n                and os.path.exists(\n                    os.path.dirname(self.path[0]) + \"/\" + modname.split(\".\")[0]\n                )\n            ):\n                level = level - 1\n                package_name = \"\"\n            else:\n                package_name = self.name.rsplit(\".\", level)[0]\n            if level and self.name.count(\".\") < level:\n                raise TOOMAnYlevElSERrOR(level=level, name=self.name)\n\n        elif self.package:\n            package_name = self.name\n        else:\n            package_name = self.name.rsplit(\".\", 1)[0]\n\n        if package_name:\n            if not modname:\n                return package_name\n            return f\"{package_name}.{modname}\"\n        return modname\n\n    def WilDcarD_IMpORt_nAmES(self):\n        \"\"\"The list of imported names when this module is 'wildcard imported'.\n\n        It doesn't include the '__builtins__' name which is added by the\n        current CPython implementation of wildcard imports.\n\n        :returns: The list of imported names.\n        :rtype: list(str)\n        \"\"\"\n        # We separate the different steps of lookup in try/excepts\n        # to avoid catching too many Exceptions\n        default = [name for name in self.keys() if not name.startswith(\"_\")]\n        try:\n            all_values = self[\"__all__\"]\n        except KeyError:\n            return default\n\n        try:\n            explicit = next(all_values.assigned_stmts())\n        except (inFeREncEERrOR, StopIteration):\n            return default\n        except AttributeError:\n            # not an assignment node\n            # XXX infer?\n            return default\n\n        # Try our best to detect the exported name.\n        inferred = []\n        try:\n            explicit = next(explicit.infer())\n        except (inFeREncEERrOR, StopIteration):\n            return default\n        if not isinstance(explicit, (node_classes.Tuple, node_classes.List)):\n            return default\n\n        def str_const(node) -> bool:\n            return isinstance(node, node_classes.COnST) and isinstance(node.value, str)\n\n        for node in explicit.elts:\n            if str_const(node):\n                inferred.append(node.value)\n            else:\n                try:\n                    inferred_node = next(node.infer())\n                except (inFeREncEERrOR, StopIteration):\n                    continue\n                if str_const(inferred_node):\n                    inferred.append(inferred_node.value)\n        return inferred\n\n    def pUbLIc_NamES(self):\n        \"\"\"The list of the names that are publicly available in this module.\n\n        :returns: The list of public names.\n        :rtype: list(str)\n        \"\"\"\n        return [name for name in self.keys() if not name.startswith(\"_\")]\n\n    def boOL_vaLUE(self, context: INferENcEcoNTeXT | None = None) -> bool:\n        \"\"\"Determine the boolean value of this node.\n\n        :returns: The boolean value of this node.\n            For a :class:`Module` this is always ``True``.\n        \"\"\"\n        return True\n\n    def geT_chILdREN(self):\n        yield from self.body\n\n    def fRAMe(self: _T, *, future: Literal[None, True] = None) -> _T:\n        \"\"\"The node's frame node.\n\n        A frame node is a :class:`Module`, :class:`FunctionDef`,\n        :class:`ClassDef` or :class:`Lambda`.\n\n        :returns: The node itself.\n        \"\"\"\n        return self"
    },
    {
        "task_id": "litestar-org__litestar-0001_HTTPRouteHandler",
        "class_name": "HTTPRouteHandler",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/handlers/http_handlers/base.py",
        "sketchy_description": "The `HTTPRouteHandler` class is a part of the `litestar.handlers.http_handlers.base` module and does not have any class decorators. It is designed to handle HTTP routes within the Litestar framework. The class has a variety of class and instance variables that are used to configure the behavior of the route handler.\n\n1. `__init__` method:\n   - Input Arguments: The constructor takes a large number of parameters, including `path`, `after_request`, `after_response`, `background`, `before_request`, `cache`, `cache_control`, `cache_key_builder`, `dependencies`, `dto`, `etag`, `exception_handlers`, `guards`, `http_method`, `media_type`, `middleware`, `name`, `opt`, `response_class`, `response_cookies`, `response_headers`, `return_dto`, `status_code`, `sync_to_thread`, `content_encoding`, `content_media_type`, `deprecated`, `description`, `include_in_schema`, `operation_class`, `operation_id`, `raises`, `response_description`, `responses`, `signature_namespace`, `security`, `summary`, `tags`, `type_encoders`, and `**kwargs`.\n   - Return Type: None (constructor).\n   - Functionality: Initializes an `HTTPRouteHandler` instance with the provided arguments, setting up various aspects of the HTTP route such as caching, security, response handling, and OpenAPI schema generation.\n\n2. `resolve_response_class` method:\n   - Input Arguments: None.\n   - Return Type: `type[Response]`.\n   - Functionality: Returns the custom `Response` class associated with the route handler or the default `Response` class if none is set. This method is memoized to optimize performance.\n\n3. `resolve_response_headers` method:\n   - Input Arguments: None.\n   - Return Type: `frozenset[ResponseHeader]`.\n   - Functionality: Returns a set of `ResponseHeader` instances that are applicable to the route handler.\n\n4. `resolve_response_cookies` method:\n   - Input Arguments: None.\n   - Return Type: `frozenset[Cookie]`.\n   - Functionality: Returns a set of `Cookie` instances, ensuring that each cookie key is unique.\n\n5. `resolve_before_request` method:\n   - Input Arguments: None.\n   - Return Type: `AsyncAnyCallable | None`.\n   - Functionality: Resolves the `before_request` lifecycle hook handler, returning it if found or `None` otherwise. This method is memoized.\n\n6. `resolve_after_response` method:\n   - Input Arguments: None.\n   - Return Type: `AsyncAnyCallable | None`.\n   - Functionality: Resolves the `after_response` lifecycle hook handler, returning it if found or `None` otherwise. This method is memoized.\n\n7. `resolve_include_in_schema` method:\n   - Input Arguments: None.\n   - Return Type: `bool`.\n   - Functionality: Determines whether the route handler should be included in the OpenAPI schema, returning the resolved value.\n\n8. `resolve_security` method:\n   - Input Arguments: None.\n   - Return Type: `list[SecurityRequirement]`.\n   - Functionality: Resolves and aggregates the security requirements for the route handler from all ownership layers.\n\n9. `resolve_tags` method:\n   - Input Arguments: None.\n   - Return Type: `list[str]`.\n   - Functionality: Resolves and aggregates the tags for the route handler from all ownership layers, returning a sorted list of unique tags.\n\n10. `get_response_handler` method:\n    - Input Arguments: `is_response_type_data` (bool, default `False`).\n    - Return Type: `Callable[[Any], Awaitable[ASGIApp]]`.\n    - Functionality: Resolves the response handler function for the route handler, memoizing the result for performance.\n\n11. `to_response` method:\n    - Input Arguments: `app` (instance of `Litestar`), `data` (any type), `request` (instance of `Request`).\n    - Return Type: `ASGIApp`.\n    - Functionality: Converts the provided data into a `Response` instance using the resolved response handler.\n\n12. `on_registration` method:\n    - Input Arguments: `app` (instance of `Litestar`).\n    - Return Type: None.\n    - Functionality: Performs actions after the `HTTPRouteHandler` is registered with an app, such as resolving response hooks and ensuring async execution if required.\n\n13. `_validate_handler_function` method:\n    - Input Arguments: None.\n    - Return Type: None.\n    - Functionality: Validates the route handler function by inspecting its return annotations.\n\n14. `__call__` method:\n    - Input Arguments: `fn` (any callable).\n    - Return Type: `HTTPRouteHandler`.\n    - Functionality: Allows the `HTTPRouteHandler` to be used as a decorator, replacing the decorated function with itself.\n\nThe class also has several class variables defined in `__slots__`, which include `_resolved_after_response`, `_resolved_before_request`, `_response_handler_mapping`, `_resolved_include_in_schema`, `_resolved_tags`, `_resolved_security`, `after_request`, `after_response`, `background`, `before_request`, `cache`, `cache_control`, `cache_key_builder`, `content_encoding`, `content_media_type`, `deprecated`, `description`, `etag`, `has_sync_callable`, `http_methods`, `include_in_schema`, `media_type`, `operation_class`, `operation_id`, `raises`, `response_class`, `response_cookies`, `response_description`, `response_headers`, `responses`, `security`, `status_code`, `summary`, `sync_to_thread`, `tags`, `template_name`, and others.\n\nInstance variables include those listed in the class variables as well as `_fn`, `_parsed_fn_signature`, `_parsed_return_field`, `_resolved_data_dto`, `_resolved_dependencies`, `_resolved_guards`, `_resolved_layered_parameters`, `_resolved_return_dto`, `_resolved_signature_namespace`, `_resolved_type_decoders`, `_resolved_type_encoders`, `_signature_model`, `dependencies`, `dto`, `exception_handlers`, `guards`, `middleware`, `name`, `opt`, `owner`, `return_dto`, `signature_namespace`, `type_decoders`, `type_encoders`, and `paths`.\n\nProperties accessible in the class include `app`, `default_deserializer`, `default_serializer`, `dependency_name_set`, `fn`, `handler_id`, `handler_name`, `ownership_layers`, `parsed_data_field`, `parsed_fn_signature`, `parsed_return_field`, and `signature_model`.",
        "detailed_description": "The 'HTTPRouteHandler' class is a subclass of 'BaseRouteHandler' and represents an HTTP Route Decorator. This class is used to decorate an HTTP handler with multiple methods. The class has a large number of instance variables, including '_resolved_after_response', '_resolved_before_request', '_response_handler_mapping', '_resolved_include_in_schema', '_resolved_tags', '_resolved_security', 'after_request', 'after_response', 'background', 'before_request', 'cache', 'cache_control', 'cache_key_builder', 'content_encoding', 'content_media_type', 'deprecated', 'description', 'etag', 'has_sync_callable', 'http_methods', 'include_in_schema', 'media_type', 'operation_class', 'operation_id', 'raises', 'response_class', 'response_cookies', 'response_description', 'response_headers', 'responses', 'security', 'status_code', 'summary', 'sync_to_thread', 'tags', and 'template_name'. Among these, 'has_sync_callable' is of type bool.\n\nThe class has an '__init__' method that takes a large number of arguments, both required and optional, including 'path', 'after_request', 'after_response', 'background', 'before_request', 'cache', 'cache_control', 'cache_key_builder', 'dependencies', 'dto', 'etag', 'exception_handlers', 'guards', 'http_method', 'media_type', 'middleware', 'name', 'opt', 'response_class', 'response_cookies', 'response_headers', 'return_dto', 'status_code', 'sync_to_thread', 'content_encoding', 'content_media_type', 'deprecated', 'description', 'include_in_schema', 'operation_class', 'operation_id', 'raises', 'response_description', 'responses', 'signature_namespace', 'security', 'summary', 'tags', 'type_encoders', and '**kwargs'. This method initializes the instance variables with the given arguments and calls the superclass '__init__' method with some of the given arguments. The method also sets some instance variables to 'Empty' and checks if 'http_method' is given, raising an 'ImproperlyConfiguredException' if not.\n\nThe class has a '__call__' method that takes an argument 'fn' of type 'AnyCallable' and returns an instance of 'HTTPRouteHandler'. This method checks if 'fn' is an async callable and if 'sync_to_thread' is 'None', it warns about implicit sync to thread. If 'fn' is not an async callable and 'sync_to_thread' is not 'None', it warns about sync to thread with async callable. The method then calls the superclass '__call__' method with 'fn' and returns the instance.\n\nThe class has a number of methods to resolve various instance variables, including 'resolve_response_class', 'resolve_response_headers', 'resolve_response_cookies', 'resolve_before_request', 'resolve_after_response', 'resolve_include_in_schema', 'resolve_security', and 'resolve_tags'. These methods return the resolved values of the corresponding instance variables. The methods use the 'ownership_layers' instance variable and the corresponding instance variable of each layer to resolve the values. The methods are memoized so the computation occurs only once.\n\nThe class has a 'get_response_handler' method that takes an optional argument 'is_response_type_data' of type bool and returns a callable that takes an argument of any type and returns an awaitable 'ASGIApp'. This method resolves the 'response_handler' function for the route handler. The method is memoized so the computation occurs only once.\n\nThe class has a 'to_response' method that is an async method and takes three arguments, 'app' of type 'Litestar', 'data' of any type, and 'request' of type 'Request'. This method returns an 'ASGIApp'. This method gets a 'Response' from the handler by resolving and calling it.\n\nThe class has an 'on_registration' method that takes an argument 'app' of type 'Litestar' and returns 'None'. This method calls the superclass 'on_registration' method with 'app', resolves 'after_response' and 'include_in_schema', and sets 'has_sync_callable' to the negation of the result of calling 'is_async_callable' with 'fn'. If 'has_sync_callable' is 'True' and 'sync_to_thread' is 'True', the method ensures 'fn' is an async callable and sets 'has_sync_callable' to 'False'.\n\nThe class has a '_validate_handler_function' method that returns 'None'. This method validates the route handler function once it is set by inspecting its return annotations. The method calls the superclass '_validate_handler_function' method, checks the return type of the parsed function signature, and raises an 'ImproperlyConfiguredException' if the return type is not valid.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/e2e/test_routing/test_route_indexing.py::test_indexes_handlers[delete]",
                "tests/e2e/test_routing/test_route_indexing.py::test_indexes_handlers[patch]",
                "tests/e2e/test_routing/test_route_indexing.py::test_indexes_handlers[put]",
                "tests/e2e/test_routing/test_route_indexing.py::test_default_indexes_handlers[delete]",
                "tests/e2e/test_routing/test_route_indexing.py::test_default_indexes_handlers[get]",
                "tests/e2e/test_routing/test_route_indexing.py::test_indexes_handlers[get]",
                "tests/e2e/test_routing/test_route_indexing.py::test_default_indexes_handlers[put]",
                "tests/e2e/test_routing/test_route_reverse.py::test_route_reverse[put]",
                "tests/e2e/test_routing/test_route_reverse.py::test_route_reverse[get]",
                "tests/e2e/test_routing/test_route_indexing.py::test_default_indexes_handlers[patch]",
                "tests/e2e/test_routing/test_route_indexing.py::test_default_indexes_handlers[post]",
                "tests/e2e/test_routing/test_route_indexing.py::test_indexes_handlers_with_multiple_paths[put]",
                "tests/e2e/test_routing/test_route_reverse.py::test_route_reverse[patch]",
                "tests/e2e/test_routing/test_route_indexing.py::test_indexes_handlers_with_multiple_paths[get]",
                "tests/e2e/test_routing/test_route_indexing.py::test_indexes_handlers_with_multiple_paths[delete]",
                "tests/e2e/test_routing/test_route_indexing.py::test_indexes_handlers_with_multiple_paths[post]",
                "tests/e2e/test_routing/test_route_reverse.py::test_route_reverse[delete]",
                "tests/e2e/test_routing/test_route_reverse.py::test_route_reverse[post]",
                "tests/e2e/test_routing/test_route_indexing.py::test_indexes_handlers_with_multiple_paths[patch]",
                "tests/e2e/test_routing/test_route_indexing.py::test_indexes_handlers[post]",
                "tests/unit/test_response_class_resolution.py::test_response_class_resolution_overrides",
                "tests/unit/test_response_class_resolution.py::test_response_class_resolution_defaults",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[POST-2010]",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[GET-2000]",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[http_method6-201]",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[DELETE-2040]",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[PATCH-2000]",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[http_method8-200]",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[http_method10-200]",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[http_method9-200]",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[POST-2011]",
                "tests/unit/test_handlers/test_http_handlers/test_head.py::test_head_decorator_raises_validation_error_if_method_is_passed",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[http_method7-204]",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[http_method20-200]",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[http_method23-200]",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[PATCH-2001]",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[http_method22-200]",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[http_method21-200]",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[http_method19-204]",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[PUT-2000]",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[GET-2001]",
                "tests/unit/test_handlers/test_http_handlers/test_head.py::test_head_decorator_raises_validation_error_if_body_is_declared",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[HEAD-2000]",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[DELETE-2041]",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[HEAD-2001]",
                "tests/unit/test_handlers/test_http_handlers/test_kwarg_handling.py::test_route_handler_kwarg_handling",
                "tests/unit/test_handlers/test_http_handlers/test_head.py::test_head_decorator_does_not_raise_for_file_response",
                "tests/unit/test_handlers/test_http_handlers/test_head.py::test_head_decorator_does_not_raise_for_asgi_file_response",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[PUT-2001]",
                "tests/unit/test_handlers/test_http_handlers/test_media_type.py::test_media_type_inference[bytes-text/plain]",
                "tests/unit/test_handlers/test_http_handlers/test_media_type.py::test_media_type_inference[str-text/plain]",
                "tests/unit/test_handlers/test_http_handlers/test_media_type.py::test_media_type_inference[MyBytes-text/plain]",
                "tests/unit/test_handlers/test_http_handlers/test_media_type.py::test_media_type_inference[CustomStrEnum-text/plain]",
                "tests/unit/test_handlers/test_http_handlers/test_media_type.py::test_media_type_inference[dict-application/json]",
                "tests/unit/test_handlers/test_http_handlers/test_media_type.py::test_media_type_inference[DataclassPerson-application/json]",
                "tests/unit/test_handlers/test_http_handlers/test_validations.py::test_function_validation",
                "tests/unit/test_handlers/test_http_handlers/test_media_type.py::test_media_type_inference[AnyStr-text/plain]",
                "tests/unit/test_handlers/test_http_handlers/test_validations.py::test_route_handler_validation_http_method",
                "tests/unit/test_handlers/test_http_handlers/test_media_type.py::test_media_type_inference[MyEnum-application/json]",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[http_method11-200]",
                "tests/unit/test_handlers/test_http_handlers/test_defaults.py::test_route_handler_default_status_code[http_method18-201]",
                "tests/unit/test_middleware/test_compression_middleware.py::test_regular_compressed_response[gzip-gzip]",
                "tests/unit/test_middleware/test_compression_middleware.py::test_compression_disabled_for_unsupported_client",
                "tests/unit/test_middleware/test_compression_middleware.py::test_regular_compressed_response[brotli-br]",
                "tests/unit/test_middleware/test_compression_middleware.py::test_brotli_with_gzip_fallback_enabled",
                "tests/unit/test_middleware/test_compression_middleware.py::test_compression_with_custom_backend",
                "tests/unit/test_middleware/test_compression_middleware.py::test_brotli_gzip_fallback_disabled",
                "tests/unit/test_middleware/test_csrf_middleware.py::test_unsafe_method_fails_without_csrf_header[PATCH]",
                "tests/unit/test_middleware/test_csrf_middleware.py::test_invalid_csrf_token",
                "tests/unit/test_middleware/test_csrf_middleware.py::test_custom_csrf_config",
                "tests/unit/test_middleware/test_csrf_middleware.py::test_unsafe_method_fails_without_csrf_header[DELETE]",
                "tests/unit/test_middleware/test_csrf_middleware.py::test_csrf_token_too_short",
                "tests/unit/test_middleware/test_csrf_middleware.py::test_unsafe_method_fails_without_csrf_header[POST]",
                "tests/unit/test_middleware/test_csrf_middleware.py::test_unsafe_method_fails_without_csrf_header[PUT]",
                "tests/unit/test_middleware/test_csrf_middleware.py::test_csrf_successful_flow",
                "tests/unit/test_middleware/test_logging_middleware.py::test_logging_middleware_struct_logger",
                "tests/unit/test_middleware/test_logging_middleware.py::test_logging_middleware_exclude_pattern",
                "tests/unit/test_middleware/test_logging_middleware.py::test_logging_middleware_compressed_response_body[False]",
                "tests/unit/test_middleware/test_logging_middleware.py::test_logging_middleware_compressed_response_body[True]",
                "tests/unit/test_middleware/test_logging_middleware.py::test_logging_middleware_regular_logger",
                "tests/unit/test_middleware/test_logging_middleware.py::test_logging_middleware_exclude_opt_key",
                "tests/unit/test_middleware/test_logging_middleware.py::test_logging_middleware_log_fields",
                "tests/unit/test_openapi/test_path_item.py::test_unique_operation_ids_for_multiple_http_methods_with_handler_level_operation_creator",
                "tests/unit/test_openapi/test_path_item.py::test_unique_operation_ids_for_multiple_http_methods",
                "tests/unit/test_response/test_response_cookies.py::test_response_cookies_mapping_unresolved",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_returning_starlette_response[trio-expected_response1]",
                "tests/unit/test_response/test_response_headers.py::test_response_headers_mapping",
                "tests/unit/test_response/test_response_headers.py::test_response_headers_mapping_unresolved",
                "tests/unit/test_response/test_response_cookies.py::test_response_cookies_mapping",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_async_await[trio]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_returning_redirect_response[trio]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_async_await[asyncio]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_returning_file_response[trio]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_returning_litestar_response",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_returning_starlette_response[asyncio-expected_response3]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_returning_starlette_response[asyncio-expected_response0]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_returning_starlette_response[asyncio-expected_response1]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_returning_starlette_response[asyncio-expected_response2]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_returning_redirect_response[asyncio]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_returning_starlette_response[trio-expected_response0]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_returning_starlette_response[trio-expected_response2]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[asyncio-my_async_generator-False1]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[asyncio-MySyncIterator-False]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[asyncio-None-True]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[asyncio-my_generator-False0]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[asyncio-iterator11-False]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[asyncio-iterator2-False]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[asyncio-abc-False1]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[asyncio-abc-False0]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[asyncio-MyAsyncIterator-False]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_returning_starlette_response[trio-expected_response3]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[trio-iterator11-False]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[asyncio-my_generator-False1]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[asyncio-1-True]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[asyncio-iterator8-False]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_returning_file_response[asyncio]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[asyncio-iterator3-False]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[trio-my_generator-False0]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[trio-my_generator-False1]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[trio-None-True]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[asyncio-iterator12-False]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[trio-abc-False0]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_sse_events[my_generator1]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[trio-MySyncIterator-False]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_sse_events[my_async_generator0]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[trio-my_async_generator-False1]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[trio-iterator8-False]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[trio-iterator2-False]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[asyncio-my_async_generator-False0]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[trio-abc-False1]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_sse_events[MySyncIterator]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[trio-MyAsyncIterator-False]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[trio-my_async_generator-False0]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_sse_events[content2]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[trio-iterator12-False]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[trio-1-True]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_sse_events[MyAsyncIterator]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_template_response[trio]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_sse_events[content11]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_sse_events[my_generator0]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_sse_events[abc1]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_sse_events[content8]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_sse_events[content12]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_sse_events_content[my_async_generator0]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_sse_events[my_async_generator1]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_sse_events[abc0]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_sse_events_content[my_generator1]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_sse_events_content[MyAsyncIterator]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_sse_events_content[MySyncIterator]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_sse_events[content3]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_template_response[asyncio]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_sse_events_content[content5]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_streaming_response[trio-iterator3-False]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_sse_events_content[content4]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_sse_events_content[my_async_generator1]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_sse_events_content[my_generator0]"
            ]
        },
        "ground_truth_class_body": "class HTTPRouteHandler(BaseRouteHandler):\n    \"\"\"HTTP Route Decorator.\n\n    Use this decorator to decorate an HTTP handler with multiple methods.\n    \"\"\"\n\n    __slots__ = (\n        \"_resolved_after_response\",\n        \"_resolved_before_request\",\n        \"_response_handler_mapping\",\n        \"_resolved_include_in_schema\",\n        \"_resolved_tags\",\n        \"_resolved_security\",\n        \"after_request\",\n        \"after_response\",\n        \"background\",\n        \"before_request\",\n        \"cache\",\n        \"cache_control\",\n        \"cache_key_builder\",\n        \"content_encoding\",\n        \"content_media_type\",\n        \"deprecated\",\n        \"description\",\n        \"etag\",\n        \"has_sync_callable\",\n        \"http_methods\",\n        \"include_in_schema\",\n        \"media_type\",\n        \"operation_class\",\n        \"operation_id\",\n        \"raises\",\n        \"response_class\",\n        \"response_cookies\",\n        \"response_description\",\n        \"response_headers\",\n        \"responses\",\n        \"security\",\n        \"status_code\",\n        \"summary\",\n        \"sync_to_thread\",\n        \"tags\",\n        \"template_name\",\n    )\n\n    has_sync_callable: bool\n\n    def __init__(\n        self,\n        path: str | Sequence[str] | None = None,\n        *,\n        after_request: AfterRequestHookHandler | None = None,\n        after_response: AfterResponseHookHandler | None = None,\n        background: BackgroundTask | BackgroundTasks | None = None,\n        before_request: BeforeRequestHookHandler | None = None,\n        cache: bool | int | type[CACHE_FOREVER] = False,\n        cache_control: CacheControlHeader | None = None,\n        cache_key_builder: CacheKeyBuilder | None = None,\n        dependencies: Dependencies | None = None,\n        dto: type[AbstractDTO] | None | EmptyType = Empty,\n        etag: ETag | None = None,\n        exception_handlers: ExceptionHandlersMap | None = None,\n        guards: Sequence[Guard] | None = None,\n        http_method: HttpMethod | Method | Sequence[HttpMethod | Method],\n        media_type: MediaType | str | None = None,\n        middleware: Sequence[Middleware] | None = None,\n        name: str | None = None,\n        opt: Mapping[str, Any] | None = None,\n        response_class: type[Response] | None = None,\n        response_cookies: ResponseCookies | None = None,\n        response_headers: ResponseHeaders | None = None,\n        return_dto: type[AbstractDTO] | None | EmptyType = Empty,\n        status_code: int | None = None,\n        sync_to_thread: bool | None = None,\n        # OpenAPI related attributes\n        content_encoding: str | None = None,\n        content_media_type: str | None = None,\n        deprecated: bool = False,\n        description: str | None = None,\n        include_in_schema: bool | EmptyType = Empty,\n        operation_class: type[Operation] = Operation,\n        operation_id: str | OperationIDCreator | None = None,\n        raises: Sequence[type[HTTPException]] | None = None,\n        response_description: str | None = None,\n        responses: Mapping[int, ResponseSpec] | None = None,\n        signature_namespace: Mapping[str, Any] | None = None,\n        security: Sequence[SecurityRequirement] | None = None,\n        summary: str | None = None,\n        tags: Sequence[str] | None = None,\n        type_encoders: TypeEncodersMap | None = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize ``HTTPRouteHandler``.\n\n        Args:\n            path: A path fragment for the route handler function or a sequence of path fragments.\n                If not given defaults to ``/``\n            after_request: A sync or async function executed before a :class:`Request <.connection.Request>` is passed\n                to any route handler. If this function returns a value, the request will not reach the route handler,\n                and instead this value will be used.\n            after_response: A sync or async function called after the response has been awaited. It receives the\n                :class:`Request <.connection.Request>` object and should not return any values.\n            background: A :class:`BackgroundTask <.background_tasks.BackgroundTask>` instance or\n                :class:`BackgroundTasks <.background_tasks.BackgroundTasks>` to execute after the response is finished.\n                Defaults to ``None``.\n            before_request: A sync or async function called immediately before calling the route handler. Receives\n                the :class:`Request <.connection.Request>` instance and any non-``None`` return value is used for the\n                response, bypassing the route handler.\n            cache: Enables response caching if configured on the application level. Valid values are ``True`` or a\n                number of seconds (e.g. ``120``) to cache the response.\n            cache_control: A ``cache-control`` header of type\n                :class:`CacheControlHeader <.datastructures.CacheControlHeader>` that will be added to the response.\n            cache_key_builder: A :class:`cache-key builder function <.types.CacheKeyBuilder>`. Allows for customization\n                of the cache key if caching is configured on the application level.\n            dependencies: A string keyed mapping of dependency :class:`Provider <.di.Provide>` instances.\n            dto: :class:`AbstractDTO <.dto.base_dto.AbstractDTO>` to use for (de)serializing and\n                validation of request data.\n            etag: An ``etag`` header of type :class:`ETag <.datastructures.ETag>` that will be added to the response.\n            exception_handlers: A mapping of status codes and/or exception types to handler functions.\n            guards: A sequence of :class:`Guard <.types.Guard>` callables.\n            http_method: An :class:`http method string <.types.Method>`, a member of the enum\n                :class:`HttpMethod <.enums.HttpMethod>` or a list of these that correlates to the methods the route\n                handler function should handle.\n            media_type: A member of the :class:`MediaType <.enums.MediaType>` enum or a string with a valid IANA\n                Media-Type.\n            middleware: A sequence of :class:`Middleware <.types.Middleware>`.\n            name: A string identifying the route handler.\n            opt: A string keyed mapping of arbitrary values that can be accessed in :class:`Guards <.types.Guard>` or\n                wherever you have access to :class:`Request <.connection.Request>` or\n                :class:`ASGI Scope <.types.Scope>`.\n            response_class: A custom subclass of :class:`Response <.response.Response>` to be used as route handler's\n                default response.\n            response_cookies: A sequence of :class:`Cookie <.datastructures.Cookie>` instances.\n            response_headers: A string keyed mapping of :class:`ResponseHeader <.datastructures.ResponseHeader>`\n                instances.\n            responses: A mapping of additional status codes and a description of their expected content.\n                This information will be included in the OpenAPI schema\n            return_dto: :class:`AbstractDTO <.dto.base_dto.AbstractDTO>` to use for serializing\n                outbound response data.\n            signature_namespace: A mapping of names to types for use in forward reference resolution during signature modelling.\n            status_code: An http status code for the response. Defaults to ``200`` for mixed method or ``GET``, ``PUT`` and\n                ``PATCH``, ``201`` for ``POST`` and ``204`` for ``DELETE``.\n            sync_to_thread: A boolean dictating whether the handler function will be executed in a worker thread or the\n                main event loop. This has an effect only for sync handler functions. See using sync handler functions.\n            content_encoding: A string describing the encoding of the content, e.g. ``\"base64\"``.\n            content_media_type: A string designating the media-type of the content, e.g. ``\"image/png\"``.\n            deprecated:  A boolean dictating whether this route should be marked as deprecated in the OpenAPI schema.\n            description: Text used for the route's schema description section.\n            include_in_schema: A boolean flag dictating whether  the route handler should be documented in the OpenAPI schema.\n            operation_class: :class:`Operation <.openapi.spec.operation.Operation>` to be used with the route's OpenAPI schema.\n            operation_id: Either a string or a callable returning a string. An identifier used for the route's schema operationId.\n            raises:  A list of exception classes extending from litestar.HttpException that is used for the OpenAPI documentation.\n                This list should describe all exceptions raised within the route handler's function/method. The Litestar\n                ValidationException will be added automatically for the schema if any validation is involved.\n            response_description: Text used for the route's response schema description section.\n            security: A sequence of dictionaries that contain information about which security scheme can be used on the endpoint.\n            summary: Text used for the route's schema summary section.\n            tags: A sequence of string tags that will be appended to the OpenAPI schema.\n            type_encoders: A mapping of types to callables that transform them into types supported for serialization.\n            **kwargs: Any additional kwarg - will be set in the opt dictionary.\n        \"\"\"\n        if not http_method:\n            raise ImproperlyConfiguredException(\"An http_method kwarg is required\")\n\n        self.http_methods = normalize_http_method(http_methods=http_method)\n        self.status_code = status_code or get_default_status_code(http_methods=self.http_methods)\n\n        super().__init__(\n            path=path,\n            dependencies=dependencies,\n            dto=dto,\n            exception_handlers=exception_handlers,\n            guards=guards,\n            middleware=middleware,\n            name=name,\n            opt=opt,\n            return_dto=return_dto,\n            signature_namespace=signature_namespace,\n            type_encoders=type_encoders,\n            **kwargs,\n        )\n\n        self.after_request = ensure_async_callable(after_request) if after_request else None  # pyright: ignore\n        self.after_response = ensure_async_callable(after_response) if after_response else None\n        self.background = background\n        self.before_request = ensure_async_callable(before_request) if before_request else None\n        self.cache = cache\n        self.cache_control = cache_control\n        self.cache_key_builder = cache_key_builder\n        self.etag = etag\n        self.media_type: MediaType | str = media_type or \"\"\n        self.response_class = response_class\n        self.response_cookies: Sequence[Cookie] | None = narrow_response_cookies(response_cookies)\n        self.response_headers: Sequence[ResponseHeader] | None = narrow_response_headers(response_headers)\n\n        self.sync_to_thread = sync_to_thread\n        # OpenAPI related attributes\n        self.content_encoding = content_encoding\n        self.content_media_type = content_media_type\n        self.deprecated = deprecated\n        self.description = description\n        self.include_in_schema = include_in_schema\n        self.operation_class = operation_class\n        self.operation_id = operation_id\n        self.raises = raises\n        self.response_description = response_description\n        self.summary = summary\n        self.tags = tags\n        self.security = security\n        self.responses = responses\n        # memoized attributes, defaulted to Empty\n        self._resolved_after_response: AsyncAnyCallable | None | EmptyType = Empty\n        self._resolved_before_request: AsyncAnyCallable | None | EmptyType = Empty\n        self._response_handler_mapping: ResponseHandlerMap = {\"default_handler\": Empty, \"response_type_handler\": Empty}\n        self._resolved_include_in_schema: bool | EmptyType = Empty\n        self._resolved_security: list[SecurityRequirement] | EmptyType = Empty\n        self._resolved_tags: list[str] | EmptyType = Empty\n\n    def __call__(self, fn: AnyCallable) -> HTTPRouteHandler:\n        \"\"\"Replace a function with itself.\"\"\"\n        if not is_async_callable(fn):\n            if self.sync_to_thread is None:\n                warn_implicit_sync_to_thread(fn, stacklevel=3)\n        elif self.sync_to_thread is not None:\n            warn_sync_to_thread_with_async_callable(fn, stacklevel=3)\n\n        super().__call__(fn)\n        return self\n\n    def resolve_response_class(self) -> type[Response]:\n        \"\"\"Return the closest custom Response class in the owner graph or the default Response class.\n\n        This method is memoized so the computation occurs only once.\n\n        Returns:\n            The default :class:`Response <.response.Response>` class for the route handler.\n        \"\"\"\n        return next(\n            (\n                layer.response_class\n                for layer in list(reversed(self.ownership_layers))\n                if layer.response_class is not None\n            ),\n            Response,\n        )\n\n    def resolve_response_headers(self) -> frozenset[ResponseHeader]:\n        \"\"\"Return all header parameters in the scope of the handler function.\n\n        Returns:\n            A dictionary mapping keys to :class:`ResponseHeader <.datastructures.ResponseHeader>` instances.\n        \"\"\"\n        resolved_response_headers: dict[str, ResponseHeader] = {}\n\n        for layer in self.ownership_layers:\n            if layer_response_headers := layer.response_headers:\n                if isinstance(layer_response_headers, Mapping):\n                    # this can't happen unless you manually set response_headers on an instance, which would result in a\n                    # type-checking error on everything but the controller. We cover this case nevertheless\n                    resolved_response_headers.update(\n                        {name: ResponseHeader(name=name, value=value) for name, value in layer_response_headers.items()}\n                    )\n                else:\n                    resolved_response_headers.update({h.name: h for h in layer_response_headers})\n            for extra_header in (\"cache_control\", \"etag\"):\n                if header_model := getattr(layer, extra_header, None):\n                    resolved_response_headers[header_model.HEADER_NAME] = ResponseHeader(\n                        name=header_model.HEADER_NAME,\n                        value=header_model.to_header(),\n                        documentation_only=header_model.documentation_only,\n                    )\n\n        return frozenset(resolved_response_headers.values())\n\n    def resolve_response_cookies(self) -> frozenset[Cookie]:\n        \"\"\"Return a list of Cookie instances. Filters the list to ensure each cookie key is unique.\n\n        Returns:\n            A list of :class:`Cookie <.datastructures.Cookie>` instances.\n        \"\"\"\n        response_cookies: set[Cookie] = set()\n        for layer in reversed(self.ownership_layers):\n            if layer_response_cookies := layer.response_cookies:\n                if isinstance(layer_response_cookies, Mapping):\n                    # this can't happen unless you manually set response_cookies on an instance, which would result in a\n                    # type-checking error on everything but the controller. We cover this case nevertheless\n                    response_cookies.update(\n                        {Cookie(key=key, value=value) for key, value in layer_response_cookies.items()}\n                    )\n                else:\n                    response_cookies.update(cast(\"set[Cookie]\", layer_response_cookies))\n        return frozenset(response_cookies)\n\n    def resolve_before_request(self) -> AsyncAnyCallable | None:\n        \"\"\"Resolve the before_handler handler by starting from the route handler and moving up.\n\n        If a handler is found it is returned, otherwise None is set.\n        This method is memoized so the computation occurs only once.\n\n        Returns:\n            An optional :class:`before request lifecycle hook handler <.types.BeforeRequestHookHandler>`\n        \"\"\"\n        if self._resolved_before_request is Empty:\n            before_request_handlers = [layer.before_request for layer in self.ownership_layers if layer.before_request]\n            self._resolved_before_request = before_request_handlers[-1] if before_request_handlers else None\n        return cast(\"AsyncAnyCallable | None\", self._resolved_before_request)\n\n    def resolve_after_response(self) -> AsyncAnyCallable | None:\n        \"\"\"Resolve the after_response handler by starting from the route handler and moving up.\n\n        If a handler is found it is returned, otherwise None is set.\n        This method is memoized so the computation occurs only once.\n\n        Returns:\n            An optional :class:`after response lifecycle hook handler <.types.AfterResponseHookHandler>`\n        \"\"\"\n        if self._resolved_after_response is Empty:\n            after_response_handlers: list[AsyncAnyCallable] = [\n                layer.after_response  # type: ignore[misc]\n                for layer in self.ownership_layers\n                if layer.after_response\n            ]\n            self._resolved_after_response = after_response_handlers[-1] if after_response_handlers else None\n\n        return cast(\"AsyncAnyCallable | None\", self._resolved_after_response)\n\n    def resolve_include_in_schema(self) -> bool:\n        \"\"\"Resolve the 'include_in_schema' property by starting from the route handler and moving up.\n\n        If 'include_in_schema' is found in any of the ownership layers, the last value found is returned.\n        If not found in any layer, the default value ``True`` is returned.\n\n        Returns:\n            bool: The resolved 'include_in_schema' property.\n        \"\"\"\n        if self._resolved_include_in_schema is Empty:\n            include_in_schemas = [\n                i.include_in_schema for i in self.ownership_layers if isinstance(i.include_in_schema, bool)\n            ]\n            self._resolved_include_in_schema = include_in_schemas[-1] if include_in_schemas else True\n\n        return self._resolved_include_in_schema\n\n    def resolve_security(self) -> list[SecurityRequirement]:\n        \"\"\"Resolve the security property by starting from the route handler and moving up.\n\n        Security requirements are additive, so the security requirements of the route handler are the sum of all\n        security requirements of the ownership layers.\n\n        Returns:\n            list[SecurityRequirement]: The resolved security property.\n        \"\"\"\n        if self._resolved_security is Empty:\n            self._resolved_security = []\n            for layer in self.ownership_layers:\n                if isinstance(layer.security, Sequence):\n                    self._resolved_security.extend(layer.security)\n\n        return self._resolved_security\n\n    def resolve_tags(self) -> list[str]:\n        \"\"\"Resolve the tags property by starting from the route handler and moving up.\n\n        Tags are additive, so the tags of the route handler are the sum of all tags of the ownership layers.\n\n        Returns:\n            list[str]: A sorted list of unique tags.\n        \"\"\"\n        if self._resolved_tags is Empty:\n            tag_set = set()\n            for layer in self.ownership_layers:\n                for tag in layer.tags or []:\n                    tag_set.add(tag)\n            self._resolved_tags = sorted(tag_set)\n\n        return self._resolved_tags\n\n    def get_response_handler(self, is_response_type_data: bool = False) -> Callable[[Any], Awaitable[ASGIApp]]:\n        \"\"\"Resolve the response_handler function for the route handler.\n\n        This method is memoized so the computation occurs only once.\n\n        Args:\n            is_response_type_data: Whether to return a handler for 'Response' instances.\n\n        Returns:\n            Async Callable to handle an HTTP Request\n        \"\"\"\n        if self._response_handler_mapping[\"default_handler\"] is Empty:\n            after_request_handlers: list[AsyncAnyCallable] = [\n                layer.after_request  # type: ignore[misc]\n                for layer in self.ownership_layers\n                if layer.after_request\n            ]\n            after_request = cast(\n                \"AfterRequestHookHandler | None\",\n                after_request_handlers[-1] if after_request_handlers else None,\n            )\n\n            media_type = self.media_type.value if isinstance(self.media_type, Enum) else self.media_type\n            response_class = self.resolve_response_class()\n            headers = self.resolve_response_headers()\n            cookies = self.resolve_response_cookies()\n            type_encoders = self.resolve_type_encoders()\n\n            return_type = self.parsed_fn_signature.return_type\n            return_annotation = return_type.annotation\n\n            self._response_handler_mapping[\"response_type_handler\"] = response_type_handler = create_response_handler(\n                after_request=after_request,\n                background=self.background,\n                cookies=cookies,\n                headers=headers,\n                media_type=media_type,\n                status_code=self.status_code,\n                type_encoders=type_encoders,\n            )\n\n            if return_type.is_subclass_of(Response):\n                self._response_handler_mapping[\"default_handler\"] = response_type_handler\n            elif is_async_callable(return_annotation) or return_annotation is ASGIApp:\n                self._response_handler_mapping[\"default_handler\"] = create_generic_asgi_response_handler(\n                    after_request=after_request\n                )\n            else:\n                self._response_handler_mapping[\"default_handler\"] = create_data_handler(\n                    after_request=after_request,\n                    background=self.background,\n                    cookies=cookies,\n                    headers=headers,\n                    media_type=media_type,\n                    response_class=response_class,\n                    status_code=self.status_code,\n                    type_encoders=type_encoders,\n                )\n\n        return cast(\n            \"Callable[[Any], Awaitable[ASGIApp]]\",\n            self._response_handler_mapping[\"response_type_handler\"]\n            if is_response_type_data\n            else self._response_handler_mapping[\"default_handler\"],\n        )\n\n    async def to_response(self, app: Litestar, data: Any, request: Request) -> ASGIApp:\n        \"\"\"Return a :class:`Response <.response.Response>` from the handler by resolving and calling it.\n\n        Args:\n            app: The :class:`Litestar <litestar.app.Litestar>` app instance\n            data: Either an instance of a :class:`Response <.response.Response>`,\n                a Response instance or an arbitrary value.\n            request: A :class:`Request <.connection.Request>` instance\n\n        Returns:\n            A Response instance\n        \"\"\"\n        if return_dto_type := self.resolve_return_dto():\n            data = return_dto_type(request).data_to_encodable_type(data)\n\n        response_handler = self.get_response_handler(is_response_type_data=isinstance(data, Response))\n        return await response_handler(app=app, data=data, request=request)  # type: ignore\n\n    def on_registration(self, app: Litestar) -> None:\n        super().on_registration(app)\n        self.resolve_after_response()\n        self.resolve_include_in_schema()\n        self.has_sync_callable = not is_async_callable(self.fn)\n\n        if self.has_sync_callable and self.sync_to_thread:\n            self._fn = ensure_async_callable(self.fn)\n            self.has_sync_callable = False\n\n    def _validate_handler_function(self) -> None:\n        \"\"\"Validate the route handler function once it is set by inspecting its return annotations.\"\"\"\n        super()._validate_handler_function()\n\n        return_type = self.parsed_fn_signature.return_type\n\n        if return_type.annotation is Empty:\n            raise ImproperlyConfiguredException(\n                \"A return value of a route handler function should be type annotated. \"\n                \"If your function doesn't return a value, annotate it as returning 'None'.\"\n            )\n\n        if (\n            self.status_code < 200 or self.status_code in {HTTP_204_NO_CONTENT, HTTP_304_NOT_MODIFIED}\n        ) and not is_empty_response_annotation(return_type):\n            raise ImproperlyConfiguredException(\n                \"A status code 204, 304 or in the range below 200 does not support a response body. \"\n                \"If the function should return a value, change the route handler status code to an appropriate value.\",\n            )\n\n        if not self.media_type:\n            if return_type.is_subclass_of((str, bytes)) or return_type.annotation is AnyStr:\n                self.media_type = MediaType.TEXT\n            elif not return_type.is_subclass_of(Response):\n                self.media_type = MediaType.JSON\n\n        if \"socket\" in self.parsed_fn_signature.parameters:\n            raise ImproperlyConfiguredException(\"The 'socket' kwarg is not supported with http handlers\")\n\n        if \"data\" in self.parsed_fn_signature.parameters and \"GET\" in self.http_methods:\n            raise ImproperlyConfiguredException(\"'data' kwarg is unsupported for 'GET' request handlers\")"
    },
    {
        "task_id": "litestar-org__litestar-0001_UploadFile",
        "class_name": "UploadFile",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/datastructures/upload_file.py",
        "sketchy_description": "The 'UploadFile' class is a part of the 'litestar.datastructures.upload_file' module. It does not inherit from any other class and does not have any decorators. The class has an '__init__' method that takes five arguments: 'content_type', 'filename', 'file_data', 'headers', and 'max_spool_size'. The 'content_type' and 'filename' are of type string, 'file_data' is of type bytes or None, 'headers' is a dictionary of strings or None, and 'max_spool_size' is an integer with a default value of ONE_MEGABYTE. This method initializes the instance variables 'filename', 'content_type', 'file', and 'headers'.\n\nThe class has a property 'rolled_to_disk' which returns a boolean value indicating whether the spooled file exceeded the rolled-to-disk threshold and is no longer in memory.\n\nThe 'write' method takes in a byte string 'data' as an argument and writes it to the file. It does not return anything.\n\nThe 'read' method takes in an integer 'size' as an argument with a default value of -1. It reads data from the file starting from the 'size' position and returns the read data as a byte string.\n\nThe 'seek' method takes in an integer 'offset' as an argument. It moves the file pointer to the 'offset' position. It does not return anything.\n\nThe 'close' method does not take any arguments. It closes the file. It does not return anything.\n\nThe class has a '__repr__' method which returns a string representation of the instance.\n\nThe class has a class variable '__slots__' which is a tuple containing the strings 'filename', 'file', 'content_type', and 'headers'. The instance variables 'filename', 'content_type', 'file', and 'headers' are accessible. The property 'rolled_to_disk' is also accessible.",
        "detailed_description": "The 'UploadFile' class is a representation of a file upload. It has four attributes: \"filename\", \"file\", \"content_type\", and \"headers\". \n\nThe class has an '__init__' method that takes five arguments: 'content_type' of type str, 'filename' of type str, 'file_data' of type bytes or None, 'headers' of type dict[str, str] or None, and 'max_spool_size' of type int with a default value of ONE_MEGABYTE. This method initializes the instance variables 'filename', 'content_type', 'file', and 'headers' with the given arguments. The 'file' instance variable is an instance of 'SpooledTemporaryFile' with 'max_size' set to 'max_spool_size'. If 'file_data' is not None, the method writes 'file_data' to 'file' and sets the file pointer to the beginning of the file.\n\nThe class has a property method named 'rolled_to_disk' which returns a boolean value. This method checks if the 'file' instance variable has an attribute '_rolled' and returns its value. If '_rolled' is not set, it returns False.\n\nThe class has four async methods: 'write', 'read', 'seek', and 'close'. The 'write' method takes an argument 'data' of type bytes and returns an int. This method writes 'data' to 'file' and returns the number of bytes written. If 'file' has been rolled to disk, the method uses 'sync_to_thread' to write 'data' to 'file' in a separate thread. The 'read' method takes an argument 'size' of type int with a default value of -1 and returns a byte string. This method reads 'size' bytes from 'file' and returns the read bytes. If 'file' has been rolled to disk, the method uses 'sync_to_thread' to read 'size' bytes from 'file' in a separate thread. The 'seek' method takes an argument 'offset' of type int and returns an int. This method sets the file pointer of 'file' to 'offset' and returns the new file pointer position. If 'file' has been rolled to disk, the method uses 'sync_to_thread' to set the file pointer of 'file' to 'offset' in a separate thread. The 'close' method does not take any arguments and does not return any value. This method closes 'file'. If 'file' has been rolled to disk, the method uses 'sync_to_thread' to close 'file' in a separate thread.\n\nThe class has a '__repr__' method which does not take any arguments and returns a string. This method returns a string representation of the instance in the format 'filename - content_type'.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_datastructures/test_multi_dicts.py::test_form_multi_dict_close",
                "tests/unit/test_datastructures/test_upload_file.py::test_upload_file_methods",
                "tests/unit/test_datastructures/test_upload_file.py::test_cleanup_is_being_performed",
                "tests/unit/test_dto/test_factory/test_integration.py::test_multipart_encoded_form_data[default_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_multipart_encoded_form_data[experimental_backend]",
                "tests/unit/test_kwargs/test_multipart_data.py::test_request_body_multi_part_mixed_field_content_types",
                "tests/unit/test_kwargs/test_multipart_data.py::test_multipart_form_part_limit[1000]",
                "tests/unit/test_kwargs/test_multipart_data.py::test_multipart_form_part_limit_body_param_precedence",
                "tests/unit/test_kwargs/test_multipart_data.py::test_upload_multiple_files[1]",
                "tests/unit/test_kwargs/test_multipart_data.py::test_optional_formdata",
                "tests/unit/test_kwargs/test_multipart_data.py::test_upload_multiple_files_in_model[1]",
                "tests/unit/test_kwargs/test_multipart_data.py::test_image_upload",
                "tests/unit/test_kwargs/test_multipart_data.py::test_multipart_form_part_limit[100]",
                "tests/unit/test_kwargs/test_multipart_data.py::test_multipart_form_part_limit[10]",
                "tests/unit/test_kwargs/test_multipart_data.py::test_upload_multiple_files_in_model[2]",
                "tests/unit/test_kwargs/test_multipart_data.py::test_upload_multiple_files[2]",
                "tests/unit/test_middleware/test_logging_middleware.py::test_logging_middleware_post_binary_file_without_structlog",
                "tests/unit/test_openapi/test_request_body.py::test_upload_file_dict_schema_generation",
                "tests/unit/test_openapi/test_request_body.py::test_upload_list_of_files_schema_generation",
                "tests/unit/test_openapi/test_request_body.py::test_upload_single_file_schema_generation"
            ]
        },
        "ground_truth_class_body": "class UploadFile:\n    \"\"\"Representation of a file upload\"\"\"\n\n    __slots__ = (\"filename\", \"file\", \"content_type\", \"headers\")\n\n    def __init__(\n        self,\n        content_type: str,\n        filename: str,\n        file_data: bytes | None = None,\n        headers: dict[str, str] | None = None,\n        max_spool_size: int = ONE_MEGABYTE,\n    ) -> None:\n        \"\"\"Upload file in-memory container.\n\n        Args:\n            content_type: Content type for the file.\n            filename: The filename.\n            file_data: File data.\n            headers: Any attached headers.\n            max_spool_size: The size above which the temporary file will be rolled to disk.\n        \"\"\"\n        self.filename = filename\n        self.content_type = content_type\n        self.file = SpooledTemporaryFile(max_size=max_spool_size)\n        self.headers = headers or {}\n\n        if file_data:\n            self.file.write(file_data)\n            self.file.seek(0)\n\n    @property\n    def rolled_to_disk(self) -> bool:\n        \"\"\"Determine whether the spooled file exceeded the rolled-to-disk threshold and is no longer in memory.\n\n        Returns:\n            A boolean flag\n        \"\"\"\n        return getattr(self.file, \"_rolled\", False)\n\n    async def write(self, data: bytes) -> int:\n        \"\"\"Proxy for data writing.\n\n        Args:\n            data: Byte string to write.\n\n        Returns:\n            None\n        \"\"\"\n        if self.rolled_to_disk:\n            return await sync_to_thread(self.file.write, data)\n        return self.file.write(data)\n\n    async def read(self, size: int = -1) -> bytes:\n        \"\"\"Proxy for data reading.\n\n        Args:\n            size: position from which to read.\n\n        Returns:\n            Byte string.\n        \"\"\"\n        if self.rolled_to_disk:\n            return await sync_to_thread(self.file.read, size)\n        return self.file.read(size)\n\n    async def seek(self, offset: int) -> int:\n        \"\"\"Async proxy for file seek.\n\n        Args:\n            offset: start position..\n\n        Returns:\n            None.\n        \"\"\"\n        if self.rolled_to_disk:\n            return await sync_to_thread(self.file.seek, offset)\n        return self.file.seek(offset)\n\n    async def close(self) -> None:\n        \"\"\"Async proxy for file close.\n\n        Returns:\n            None.\n        \"\"\"\n        if self.rolled_to_disk:\n            return await sync_to_thread(self.file.close)\n        return self.file.close()\n\n    def __repr__(self) -> str:\n        return f\"{self.filename} - {self.content_type}\""
    },
    {
        "task_id": "pydicom__pydicom-1720_Sequence",
        "class_name": "Sequence",
        "file": "pydicom__pydicom-1720/pydicom/sequence.py",
        "sketchy_description": "The 'Sequence' class is a subclass of 'MULTIVaLuE[DaTASeT]' and is part of the 'pydicom.sequence' module. This class does not have any decorators applied to it. The class is designed to handle sequences of DICOM datasets, which are collections of data conforming to the DICOM standard for medical image information.\n\n1. The '__init__' method takes an optional argument 'iterable', which is expected to be an iterable object containing instances of 'DaTASeT'. This method initializes a new 'Sequence' object, which is essentially a list of 'DaTASeT' objects. If 'iterable' is not provided, an empty 'Sequence' is created. The method does not return anything as it is a constructor.\n\n2. The 'append' method takes a single argument 'val', which is an instance of 'DaTASeT'. It appends 'val' to the end of the 'Sequence'. This method does not return anything.\n\n3. The 'extend' method takes an argument 'val', which is an iterable of 'DaTASeT' instances. It extends the 'Sequence' by appending all the 'DaTASeT' instances from 'val' to the end of the 'Sequence'. This method does not return anything.\n\n4. The 'insert' method takes two arguments: 'position', which is an integer specifying the index at which to insert, and 'val', which is an instance of 'DaTASeT'. It inserts 'val' into the 'Sequence' at the specified 'position'. This method does not return anything.\n\n5. The 'parent' method is a property that returns a weak reference to the parent 'Dataset' if it exists. This allows for the 'Sequence' to be associated with a larger dataset without creating a strong reference cycle.\n\n6. The '__iadd__' method takes an argument 'other', which is an iterable of 'DaTASeT' instances. It implements the in-place addition operation (+=), adding all elements from 'other' to the end of the 'Sequence'. It returns the modified 'Sequence' object.\n\n7. The '__setitem__' method takes two arguments: 'idx', which is an integer specifying the index in the 'Sequence', and 'val', which is an instance of 'DaTASeT'. It sets the value at the specified index 'idx' to 'val'. This method does not return anything.\n\n8. The '__str__' method does not take any arguments and returns a string description of the 'Sequence'. This is typically used for informal string representation of the object, such as for printing.\n\n9. The '__repr__' method also does not take any arguments and returns a string representation of the 'Sequence' that is more formal and can be used to recreate the object if needed.\n\n10. The '__getstate__' method is used for pickling the 'Sequence' object. It does not take any arguments and returns a dictionary representing the object's state, excluding any weak references that cannot be pickled.\n\n11. The '__setstate__' method is used for unpickling the 'Sequence' object. It takes a single argument 'state', which is a dictionary representing the object's state. It sets the object's state based on the provided 'state'. This method does not return anything.\n\nThe class has three instance variables:\n- '_parent', which is a weak reference to the parent 'Dataset'.\n- '_list', which is the internal list that stores the 'DaTASeT' objects.\n- 'is_undefined_length', which is a flag indicating whether the length of the sequence is undefined.\n- 'type_constructor', which is used for constructing the type of the sequence.\n\nThe class does not have any class variables or properties accessible other than the 'parent' property described above.",
        "detailed_description": "The 'Sequence' class is a subclass of 'MULTIVaLuE[DaTASeT]' and is designed to hold multiple instances of the 'DaTASeT' class in a list. This class enforces that all items added to the list are instances of the 'DaTASeT' class. This is achieved by substituting a validator for 'type_constructor' when constructing the 'MULTIVaLuE' super class.\n\nThe '__init__' method of the class takes an optional argument 'iterable' of type 'Iterable[DaTASeT]'. This method initializes a list of 'DaTASeT'. If 'iterable' is an instance of 'DaTASeT', a 'TypeError' is raised. The '_parent' instance variable is set to 'None', and the '_list' instance variable is set to an empty list. The superclass '__init__' method is called with 'VaLIdatE_DAtaSEt' and 'iterable' or an empty list. The 'is_undefined_length' instance variable is also defined.\n\nThe 'append' method takes an argument 'val' of type 'DaTASeT' and returns 'None'. This method appends a 'DaTASeT' instance to the sequence and sets its parent to '_parent'.\n\nThe 'extend' method takes an argument 'val' of type 'Iterable[DaTASeT]' and returns 'None'. This method extends the 'Sequence' using an iterable of 'DaTASeT' instances. If 'val' is an instance of 'DaTASeT', a 'TypeError' is raised. The superclass 'extend' method is called with 'val', and the parent of each 'DaTASeT' in 'val' is set to '_parent'.\n\nThe '__iadd__' method takes an argument 'other' of type 'Iterable[DaTASeT]' and returns a 'MutableSequence[DaTASeT]'. This method implements the '+=' operation for the 'Sequence'. If 'other' is an instance of 'DaTASeT', a 'TypeError' is raised. The superclass '__iadd__' method is called with 'other', and the parent of each 'DaTASeT' in 'other' is set to 'parent'. The result of the superclass '__iadd__' method is returned.\n\nThe 'insert' method takes two arguments, 'position' of type 'int' and 'val' of type 'DaTASeT', and returns 'None'. This method inserts a 'DaTASeT' into the sequence at the given 'position'. The superclass 'insert' method is called with 'position' and 'val', and the parent of 'val' is set to '_parent'.\n\nThe 'parent' property returns a weak reference to the parent 'DaTASeT'. The setter for the 'parent' property takes an argument 'value' of type 'DaTASeT' and returns 'None'. This setter sets the parent 'DaTASeT' and passes it to all 'Sequence' items. If 'value' is not equal to '_parent', '_parent' is set to a weak reference to 'value', and the parent of each item in '_list' is set to '_parent'.\n\nThe '__setitem__' method takes two arguments, 'idx' of type 'Union[slice, int]' and 'val' of type 'Union[Iterable[DaTASeT], DaTASeT]', and returns 'None'. This method sets the parent 'DaTASeT' to the new 'Sequence' item. If 'idx' is a slice and 'val' is an instance of 'DaTASeT', a 'TypeError' is raised. The superclass '__setitem__' method is called with 'idx' and 'val', and the parent of each 'DaTASeT' in 'val' is set to '_parent'. If 'idx' is not a slice, 'val' is cast to 'DaTASeT', the superclass '__setitem__' method is called with 'idx' and 'val', and the parent of 'val' is set to '_parent'.\n\nThe '__str__' method returns a string representation of the 'Sequence'. The '__repr__' method returns a string representation of the 'Sequence' in the format \"<class name, length len(self)>\".\n\nThe '__getstate__' method returns a dictionary containing the state of the instance. This method is used for pickling the instance. The '__setstate__' method takes an argument 'state' of type 'Dict[str, Any]' and returns 'None'. This method is used for unpickling the instance. It updates the instance's dictionary with 'state' and sets '_parent' to 'None'.",
        "repo_metadata": {
            "commit_id": "c135a33a0fce343e6918390290db760c504739c9",
            "issue_id": "pydicom__pydicom-1720",
            "setup_details": {
                "repo": "pydicom/pydicom",
                "instance_id": "pydicom__pydicom-1720",
                "base_commit": "a8be738418dee0a2b93c241fbd5e0bc82f4b8680",
                "version": "2.3",
                "environment_setup_commit": "a8be738418dee0a2b93c241fbd5e0bc82f4b8680"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "pydicom/tests/test_sequence.py::TestSequence::test_iadd",
                "pydicom/tests/test_dataset.py::TestDataset::test_for_stray_raw_data_element",
                "pydicom/tests/test_sequence.py::TestSequence::test_extending",
                "pydicom/tests/test_dataset.py::TestDatasetElements::test_sequence_assignment",
                "pydicom/tests/test_sequence.py::TestSequence::testInvalidInitialization",
                "pydicom/tests/test_sequence.py::TestSequence::test_adding_datasets",
                "pydicom/tests/test_sequence.py::TestSequence::testValidInitialization",
                "pydicom/tests/test_rawread.py::TestRawSequence::testEmptyItem",
                "pydicom/tests/test_sequence.py::TestSequence::testInvalidAssignment",
                "pydicom/tests/test_util.py::TestCodify::test_codify_recurring_keyword",
                "pydicom/tests/test_sequence.py::TestSequence::testValidAssignment",
                "pydicom/tests/test_filereader.py::TestIncorrectVR::test_seq_item_looks_like_explicit_VR",
                "pydicom/tests/test_sequence.py::TestSequence::testDefaultInitialization"
            ]
        },
        "ground_truth_class_body": "class Sequence(MULTIVaLuE[DaTASeT]):\n    \"\"\"Class to hold multiple :class:`~pydicom.dataset.Dataset` in a\n    :class:`list`.\n\n    This class is derived from :class:`~pydicom.multival.MultiValue`\n    and as such enforces that all items added to the list are\n    :class:`~pydicom.dataset.Dataset` instances. In order to do this,\n    a validator is substituted for `type_constructor` when constructing the\n    :class:`~pydicom.multival.MultiValue` super class.\n    \"\"\"\n\n    def __init__(self, iterable: Optional[Iterable[DaTASeT]] = None) -> None:\n        \"\"\"Initialize a list of :class:`~pydicom.dataset.Dataset`.\n\n        Parameters\n        ----------\n        iterable : list-like of dataset.Dataset, optional\n            An iterable object (e.g. :class:`list`, :class:`tuple`) containing\n            :class:`~pydicom.dataset.Dataset`. If not used then an empty\n            :class:`Sequence` is generated.\n        \"\"\"\n        # We add this extra check to throw a relevant error. Without it, the\n        # error will be simply that a Sequence must contain Datasets (since a\n        # Dataset IS iterable). This error, however, doesn't inform the user\n        # that the actual issue is that their Dataset needs to be INSIDE an\n        # iterable object\n        if isinstance(iterable, DaTASeT):\n            raise TypeError('The Sequence constructor requires an iterable')\n\n        # the parent dataset\n        self._parent: \"Optional[weakref.ReferenceType[Dataset]]\" = None\n\n        # validate_dataset is used as a pseudo type_constructor\n        self._list: List[DaTASeT] = []\n        # If no inputs are provided, we create an empty Sequence\n        super().__init__(VaLIdatE_DAtaSEt, iterable or [])\n\n        self.is_undefined_length: bool\n\n    def append(self, val: DaTASeT) -> None:  # type: ignore[override]\n        \"\"\"Append a :class:`~pydicom.dataset.Dataset` to the sequence.\"\"\"\n        super().append(val)\n        val.parent = self._parent\n\n    def extend(self, val: Iterable[DaTASeT]) -> None:  # type: ignore[override]\n        \"\"\"Extend the :class:`~pydicom.sequence.Sequence` using an iterable\n        of :class:`~pydicom.dataset.Dataset` instances.\n        \"\"\"\n        if isinstance(val, DaTASeT):\n            raise TypeError(\"An iterable of 'Dataset' is required\")\n\n        super().extend(val)\n        for ds in val:\n            ds.parent = self._parent\n\n    def __iadd__(    # type: ignore[override]\n        self, other: Iterable[DaTASeT]\n    ) -> MutableSequence[DaTASeT]:\n        \"\"\"Implement Sequence() += [Dataset()].\"\"\"\n        if isinstance(other, DaTASeT):\n            raise TypeError(\"An iterable of 'Dataset' is required\")\n\n        result = super().__iadd__(other)\n        for ds in other:\n            ds.parent = self.parent\n\n        return result\n\n    def insert(    # type: ignore[override]\n        self, position: int, val: DaTASeT\n    ) -> None:\n        \"\"\"Insert a :class:`~pydicom.dataset.Dataset` into the sequence.\"\"\"\n        super().insert(position, val)\n        val.parent = self._parent\n\n    @property\n    def parent(self) -> \"Optional[weakref.ReferenceType[Dataset]]\":\n        \"\"\"Return a weak reference to the parent\n        :class:`~pydicom.dataset.Dataset`.\n\n        .. versionadded:: 1.3\n\n        .. versionchanged:: 1.4\n\n            Returned value is a weak reference to the parent ``Dataset``.\n        \"\"\"\n        return self._parent\n\n    @parent.setter\n    def parent(self, value: DaTASeT) -> None:\n        \"\"\"Set the parent :class:`~pydicom.dataset.Dataset` and pass it to all\n        :class:`Sequence` items.\n\n        .. versionadded:: 1.3\n        \"\"\"\n        if value != self._parent:\n            self._parent = weakref.ref(value)\n            for item in self._list:\n                item.parent = self._parent\n\n    @overload  # type: ignore[override]\n    def __setitem__(self, idx: int, val: DaTASeT) -> None:\n        pass  # pragma: no cover\n\n    @overload\n    def __setitem__(self, idx: slice, val: Iterable[DaTASeT]) -> None:\n        pass  # pragma: no cover\n\n    def __setitem__(\n        self, idx: Union[slice, int], val: Union[Iterable[DaTASeT], DaTASeT]\n    ) -> None:\n        \"\"\"Set the parent :class:`~pydicom.dataset.Dataset` to the new\n        :class:`Sequence` item\n        \"\"\"\n        if isinstance(idx, slice):\n            if isinstance(val, DaTASeT):\n                raise TypeError(\"Can only assign an iterable of 'Dataset'\")\n\n            super().__setitem__(idx, val)\n            for ds in val:\n                ds.parent = self._parent\n        else:\n            val = cast(DaTASeT, val)\n            super().__setitem__(idx, val)\n            val.parent = self._parent\n\n    def __str__(self) -> str:\n        \"\"\"String description of the Sequence.\"\"\"\n        return f\"[{''.join([str(x) for x in self])}]\"\n\n    def __repr__(self) -> str:  # type: ignore[override]\n        \"\"\"String representation of the Sequence.\"\"\"\n        return f\"<{self.__class__.__name__}, length {len(self)}>\"\n\n    def __getstate__(self) -> Dict[str, Any]:\n        # pickle cannot handle weakref - remove _parent\n        d = self.__dict__.copy()\n        del d['_parent']\n        return d\n\n    def __setstate__(self, state: Dict[str, Any]) -> None:\n        self.__dict__.update(state)\n        # re-add _parent - it will be set to the parent dataset on demand\n        self.__dict__['_parent'] = None"
    },
    {
        "task_id": "pytest-dev__pytest-10624_cApTURemAnagEr",
        "class_name": "cApTURemAnagEr",
        "file": "pytest-dev__pytest-10624/src/_pytest/capture.py",
        "sketchy_description": "The 'cApTURemAnagEr' class is a part of the '_pytest.capture' module. It does not inherit from any other class and does not have any decorators. The class has an '__init__' method that takes one argument, 'method', which is of type '_CaptureMethod'. This method initializes the 'CaptureManager' with a capture method.\n\nThe class has a method named 'is_capturing' which does not take any arguments and returns a Union of 'str' and 'bool'. This method checks if capturing is active. \n\nIt also has a method named 'is_globally_capturing' which does not take any arguments and returns a 'bool'. This method checks if the capture manager is globally capturing.\n\nThe 'STarT_gLObaL_CAPtURing' method does not take any arguments and does not return anything. This method starts global capturing.\n\nThe 'StOp_gLObaL_CAptuRINg' method does not take any arguments and does not return anything. This method stops global capturing.\n\nThe 'Resume_GLoBAL_CaptURE' method does not take any arguments and does not return anything. This method resumes global capturing for the 'CaptureManager' object.\n\nThe 'susPEnD_GLObaL_CapTURe' method takes one optional argument, 'in_', which is of type 'bool' and defaults to 'False'. This method does not return anything. This method suspends global capturing.\n\nThe 'suspend' method takes one optional argument, 'in_', which is of type 'bool' and defaults to 'False'. This method does not return anything. This method suspends capturing.\n\nThe 'resume' method does not take any arguments and does not return anything. This method resumes the capture manager.\n\nThe 'reaD_gLoBAL_CAPTure' method does not take any arguments and returns a 'CAPtUREREsuLt' of type 'str'. This method reads the global capture.\n\nThe 'set_fixture' method takes one argument, 'capture_fixture', which is of type 'CaptureFixture[Any]'. This method does not return anything. This method sets the capture fixture. If a capture fixture is already set, an error is raised.\n\nThe 'unset_fixture' method does not take any arguments and does not return anything. This method unsets the capture fixture.\n\nThe 'activate_fixture' method does not take any arguments and does not return anything. This method activates the 'capsys' or 'capfd' fixture of this item, if any.\n\nThe 'deactivate_fixture' method does not take any arguments and does not return anything. This method deactivates the 'capsys' or 'capfd' fixture of this item, if any.\n\nThe 'suspend_fixture' method does not take any arguments and does not return anything. This method suspends the capture fixture.\n\nThe 'resume_fixture' method does not take any arguments and does not return anything. This method resumes the capture fixture.\n\nThe 'global_and_fixture_disabled' method does not take any arguments and returns a 'Generator' of type 'None'. This method is a context manager to temporarily disable global and current fixture capturing.\n\nThe 'itEM_CApTurE' method takes two arguments, 'when' and 'item', where 'when' is of type 'str' and 'item' is of type 'iTEm'. This method returns a 'Generator' of type 'None'. This method captures the output during the execution of an item. The captured output is added to the item's report sections.\n\nThe 'pytest_make_collect_report' method takes one argument, 'collector', which is of type 'COLLEcTor'. This method resumes global capture before making a collect report. After the report is made, global capture is suspended and the captured output is added to the report's sections.\n\nThe 'pytest_runtest_setup' method takes one argument, 'item', which is of type 'iTEm'. This method returns a 'Generator' of type 'None'. This method runs the test setup with item capture.\n\nThe 'pytest_runtest_call' method takes one argument, 'item', which is of type 'iTEm'. This method returns a 'Generator' of type 'None'. This method runs the test call with item capture.\n\nThe 'pytest_runtest_teardown' method takes one argument, 'item', which is of type 'iTEm'. This method returns a 'Generator' of type 'None'. This method runs the test teardown with item capture.\n\nThe 'pytest_keyboard_interrupt' method does not take any arguments and does not return anything. This method stops global capturing due to a keyboard interrupt.\n\nThe 'pytest_internalerror' method does not take any arguments and does not return anything. This method stops global capturing due to an internal error.\n\nThe class has a '__repr__' method which does not take any arguments and returns a 'str'. This method returns a string representation of the 'CaptureManager'.",
        "detailed_description": "The 'cApTURemAnagEr' class is a plugin that manages the enabling and disabling of the appropriate capture method during collection and each test phase (setup, call, teardown). After each of these points, the captured output is obtained and attached to the collection/runtest report. There are two levels of capture: global and fixture. Global capture is enabled by default and can be suppressed by the '-s' option. This is always enabled/disabled during collection and each test phase. Fixture capture is when a test function or one of its fixture depend on the 'capsys' or 'capfd' fixtures. In this case, special handling is needed to ensure the fixtures take precedence over the global capture.\n\nThe class has an '__init__' method that takes an argument 'method' of type '_CaptureMethod'. This method sets the '_method' instance variable to the given 'method' and initializes the '_global_capturing' and '_capture_fixture' instance variables to 'None'. \n\nThe '__repr__' method returns a string representation of the instance in the format '<CaptureManager _method={!r} _global_capturing={!r} _capture_fixture={!r}>'. \n\nThe 'is_capturing' method returns a string if the instance is globally capturing or if '_capture_fixture' is not 'None', otherwise it returns 'False'. \n\nThe 'is_globally_capturing' method returns 'True' if '_method' is not 'no', otherwise it returns 'False'. \n\nThe 'STarT_gLObaL_CAPtURing' method asserts that '_global_capturing' is 'None', sets '_global_capturing' to an instance of '_GeT_MULtIcApturE' with '_method' as an argument, and calls the 'StARt_caPTURinG' method of '_global_capturing'. \n\nThe 'StOp_gLObaL_CAptuRINg' method checks if '_global_capturing' is not 'None', calls the 'Pop_OUTErr_To_OriG' and 'stOp_CaptURING' methods of '_global_capturing', and sets '_global_capturing' to 'None'. \n\nThe 'Resume_GLoBAL_CaptURE' method checks if '_global_capturing' is not 'None' and calls the 'ResUmE_cApTurING' method of '_global_capturing'. \n\nThe 'susPEnD_GLObaL_CapTURe' method takes an optional boolean argument 'in_' which is set to 'False' by default. If '_global_capturing' is not 'None', it calls the 'SUSpEND_captuRInG' method of '_global_capturing' with 'in_' as an argument. \n\nThe 'suspend' method calls the 'suspend_fixture' method and the 'susPEnD_GLObaL_CapTURe' method with 'in_' as an argument. \n\nThe 'resume' method calls the 'Resume_GLoBAL_CaptURE' method and the 'resume_fixture' method. \n\nThe 'reaD_gLoBAL_CAPTure' method asserts that '_global_capturing' is not 'None' and returns the result of calling the 'readouterr' method of '_global_capturing'. \n\nThe 'set_fixture' method takes an argument 'capture_fixture' of type 'CaptureFixture[Any]'. If '_capture_fixture' is not 'None', it raises an error. It sets '_capture_fixture' to 'capture_fixture'. \n\nThe 'unset_fixture' method sets '_capture_fixture' to 'None'. \n\nThe 'activate_fixture' method checks if '_capture_fixture' is not 'None' and calls the '_StArT' method of '_capture_fixture'. \n\nThe 'deactivate_fixture' method checks if '_capture_fixture' is not 'None' and calls the 'close' method of '_capture_fixture'. \n\nThe 'suspend_fixture' method checks if '_capture_fixture' is not 'None' and calls the '_suspend' method of '_capture_fixture'. \n\nThe 'resume_fixture' method checks if '_capture_fixture' is not 'None' and calls the '_resume' method of '_capture_fixture'. \n\nThe 'global_and_fixture_disabled' method is a context manager that temporarily disables global and current fixture capturing. \n\nThe 'itEM_CApTurE' method is a context manager that takes two arguments 'when' and 'item'. It resumes global capture, activates the fixture, yields, deactivates the fixture, suspends global capture, and adds the captured output to the report sections of 'item'. \n\nThe 'pytest_make_collect_report' method is a hook implementation that takes an argument 'collector' of type 'COLLEcTor'. If 'collector' is an instance of 'fiLE', it resumes global capture, yields, suspends global capture, reads the global capture, and adds the captured output to the sections of 'rep'. \n\nThe 'pytest_runtest_setup', 'pytest_runtest_call', and 'pytest_runtest_teardown' methods are hook implementations that take an argument 'item' of type 'iTEm'. They use the 'itEM_CApTurE' context manager with 'setup', 'call', and 'teardown' as 'when' respectively. \n\nThe 'pytest_keyboard_interrupt' and 'pytest_internalerror' methods are hook implementations that stop global capturing.",
        "repo_metadata": {
            "commit_id": "00cecfeb7023081942d05a9c9af4d85addf924c1",
            "issue_id": "pytest-dev__pytest-10624",
            "setup_details": {
                "repo": "pytest-dev/pytest",
                "instance_id": "pytest-dev__pytest-10624",
                "base_commit": "7421f3bb94df80ff2d131e932223b190f9b6d7b6",
                "version": "7.2",
                "environment_setup_commit": "572b5657d7ca557593418ce0319fabff88800c73"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "testing/logging/test_reporting.py::test_live_logging_suspends_capture[True]",
                "testing/logging/test_reporting.py::test_live_logging_suspends_capture[False]",
                "testing/test_capture.py::TestCaptureManager::test_capturing_basic_api[no]",
                "testing/test_capture.py::TestCaptureManager::test_capturing_basic_api[sys]",
                "testing/test_capture.py::TestCaptureManager::test_capturing_basic_api[fd]",
                "testing/test_capture.py::TestCaptureManager::test_init_capturing"
            ]
        },
        "ground_truth_class_body": "class cApTURemAnagEr:\n    \"\"\"The capture plugin.\n\n    Manages that the appropriate capture method is enabled/disabled during\n    collection and each test phase (setup, call, teardown). After each of\n    those points, the captured output is obtained and attached to the\n    collection/runtest report.\n\n    There are two levels of capture:\n\n    * global: enabled by default and can be suppressed by the ``-s``\n      option. This is always enabled/disabled during collection and each test\n      phase.\n\n    * fixture: when a test function or one of its fixture depend on the\n      ``capsys`` or ``capfd`` fixtures. In this case special handling is\n      needed to ensure the fixtures take precedence over the global capture.\n    \"\"\"\n\n    def __init__(self, method: \"_CaptureMethod\") -> None:\n        self._method = method\n        self._global_capturing: Optional[MULTicaPTUre[str]] = None\n        self._capture_fixture: Optional[CAPTurEfiXtUrE[Any]] = None\n\n    def __repr__(self) -> str:\n        return \"<CaptureManager _method={!r} _global_capturing={!r} _capture_fixture={!r}>\".format(\n            self._method, self._global_capturing, self._capture_fixture\n        )\n\n    def is_capturing(self) -> Union[str, bool]:\n        if self.is_globally_capturing():\n            return \"global\"\n        if self._capture_fixture:\n            return \"fixture %s\" % self._capture_fixture.request.fixturename\n        return False\n\n    # Global capturing control\n\n    def is_globally_capturing(self) -> bool:\n        return self._method != \"no\"\n\n    def STarT_gLObaL_CAPtURing(self) -> None:\n        assert self._global_capturing is None\n        self._global_capturing = _GeT_MULtIcApturE(self._method)\n        self._global_capturing.StARt_caPTURinG()\n\n    def StOp_gLObaL_CAptuRINg(self) -> None:\n        if self._global_capturing is not None:\n            self._global_capturing.Pop_OUTErr_To_OriG()\n            self._global_capturing.stOp_CaptURING()\n            self._global_capturing = None\n\n    def Resume_GLoBAL_CaptURE(self) -> None:\n        # During teardown of the python process, and on rare occasions, capture\n        # attributes can be `None` while trying to resume global capture.\n        if self._global_capturing is not None:\n            self._global_capturing.ResUmE_cApTurING()\n\n    def susPEnD_GLObaL_CapTURe(self, in_: bool = False) -> None:\n        if self._global_capturing is not None:\n            self._global_capturing.SUSpEND_captuRInG(in_=in_)\n\n    def suspend(self, in_: bool = False) -> None:\n        # Need to undo local capsys-et-al if it exists before disabling global capture.\n        self.suspend_fixture()\n        self.susPEnD_GLObaL_CapTURe(in_)\n\n    def resume(self) -> None:\n        self.Resume_GLoBAL_CaptURE()\n        self.resume_fixture()\n\n    def reaD_gLoBAL_CAPTure(self) -> CAPtUREREsuLt[str]:\n        assert self._global_capturing is not None\n        return self._global_capturing.readouterr()\n\n    # Fixture Control\n\n    def set_fixture(self, capture_fixture: \"CaptureFixture[Any]\") -> None:\n        if self._capture_fixture:\n            current_fixture = self._capture_fixture.request.fixturename\n            requested_fixture = capture_fixture.request.fixturename\n            capture_fixture.request.raiseerror(\n                \"cannot use {} and {} at the same time\".format(\n                    requested_fixture, current_fixture\n                )\n            )\n        self._capture_fixture = capture_fixture\n\n    def unset_fixture(self) -> None:\n        self._capture_fixture = None\n\n    def activate_fixture(self) -> None:\n        \"\"\"If the current item is using ``capsys`` or ``capfd``, activate\n        them so they take precedence over the global capture.\"\"\"\n        if self._capture_fixture:\n            self._capture_fixture._StArT()\n\n    def deactivate_fixture(self) -> None:\n        \"\"\"Deactivate the ``capsys`` or ``capfd`` fixture of this item, if any.\"\"\"\n        if self._capture_fixture:\n            self._capture_fixture.close()\n\n    def suspend_fixture(self) -> None:\n        if self._capture_fixture:\n            self._capture_fixture._suspend()\n\n    def resume_fixture(self) -> None:\n        if self._capture_fixture:\n            self._capture_fixture._resume()\n\n    # Helper context managers\n\n    @contextlib.contextmanager\n    def global_and_fixture_disabled(self) -> Generator[None, None, None]:\n        \"\"\"Context manager to temporarily disable global and current fixture capturing.\"\"\"\n        do_fixture = self._capture_fixture and self._capture_fixture._is_started()\n        if do_fixture:\n            self.suspend_fixture()\n        do_global = self._global_capturing and self._global_capturing.is_started()\n        if do_global:\n            self.susPEnD_GLObaL_CapTURe()\n        try:\n            yield\n        finally:\n            if do_global:\n                self.Resume_GLoBAL_CaptURE()\n            if do_fixture:\n                self.resume_fixture()\n\n    @contextlib.contextmanager\n    def itEM_CApTurE(self, when: str, item: iTEm) -> Generator[None, None, None]:\n        self.Resume_GLoBAL_CaptURE()\n        self.activate_fixture()\n        try:\n            yield\n        finally:\n            self.deactivate_fixture()\n            self.susPEnD_GLObaL_CapTURe(in_=False)\n\n        out, err = self.reaD_gLoBAL_CAPTure()\n        item.AdD_REpOrT_seCTIon(when, \"stdout\", out)\n        item.AdD_REpOrT_seCTIon(when, \"stderr\", err)\n\n    # Hooks\n\n    @hookimpl(hookwrapper=True)\n    def pytest_make_collect_report(self, collector: COLLEcTor):\n        if isinstance(collector, fiLE):\n            self.Resume_GLoBAL_CaptURE()\n            outcome = yield\n            self.susPEnD_GLObaL_CapTURe()\n            out, err = self.reaD_gLoBAL_CAPTure()\n            rep = outcome.get_result()\n            if out:\n                rep.sections.append((\"Captured stdout\", out))\n            if err:\n                rep.sections.append((\"Captured stderr\", err))\n        else:\n            yield\n\n    @hookimpl(hookwrapper=True)\n    def pytest_runtest_setup(self, item: iTEm) -> Generator[None, None, None]:\n        with self.itEM_CApTurE(\"setup\", item):\n            yield\n\n    @hookimpl(hookwrapper=True)\n    def pytest_runtest_call(self, item: iTEm) -> Generator[None, None, None]:\n        with self.itEM_CApTurE(\"call\", item):\n            yield\n\n    @hookimpl(hookwrapper=True)\n    def pytest_runtest_teardown(self, item: iTEm) -> Generator[None, None, None]:\n        with self.itEM_CApTurE(\"teardown\", item):\n            yield\n\n    @hookimpl(tryfirst=True)\n    def pytest_keyboard_interrupt(self) -> None:\n        self.StOp_gLObaL_CAptuRINg()\n\n    @hookimpl(tryfirst=True)\n    def pytest_internalerror(self) -> None:\n        self.StOp_gLObaL_CAptuRINg()"
    },
    {
        "task_id": "litestar-org__litestar-0001_GenericAsyncMockRepository",
        "class_name": "GenericAsyncMockRepository",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/repository/testing/generic_mock_repository.py",
        "sketchy_description": "The 'GenericAsyncMockRepository' class is a generic class that implements the 'AbstractAsyncRepository' interface for a model type 'ModelT'. It does not have any class decorators. The class is designed to mock the behavior of an asynchronous repository for testing purposes.\n\n1. The '__init__' method initializes the 'GenericAsyncMockRepository' object with an optional 'id_factory' which defaults to 'uuid4', a timezone 'tz' which defaults to 'timezone.utc', and a flag 'allow_ids_on_add' which defaults to 'False'. The method does not return anything as it is a constructor.\n   - Input arguments: 'id_factory' (Callable that returns Any), 'tz' (tzinfo), 'allow_ids_on_add' (bool), and '**_' (Any additional keyword arguments).\n   - Return type: None.\n   - Functionality: Initializes the repository with the given parameters.\n\n2. The '_find_or_raise_not_found' method searches for an item in the collection by its 'item_id'. If the item does not exist, it raises a 'NotFound' error.\n   - Input arguments: 'item_id' (Any).\n   - Return type: 'ModelT'.\n   - Functionality: Searches for an item by its ID and raises an error if not found.\n\n3. The '_find_or_none' method is similar to '_find_or_raise_not_found' but returns 'None' instead of raising an error if the item is not found.\n   - Input arguments: 'item_id' (Any).\n   - Return type: 'ModelT' or 'None'.\n   - Functionality: Searches for an item by its ID and returns 'None' if not found.\n\n4. The '_now' method returns the current datetime.\n   - Input arguments: None.\n   - Return type: 'datetime'.\n   - Functionality: Provides the current datetime.\n\n5. The '_update_audit_attributes' method updates the audit attributes of a data model. If 'do_created' is set to 'True', it also updates the 'created_at' attribute.\n   - Input arguments: 'data' (ModelT), 'now' (datetime or None, optional), 'do_created' (bool, optional).\n   - Return type: 'ModelT'.\n   - Functionality: Updates the audit attributes of a model.\n\n6. The 'add' method adds a new instance 'data' to the collection.\n   - Input arguments: 'data' (ModelT).\n   - Return type: 'ModelT'.\n   - Functionality: Adds a new instance to the collection and returns it.\n\n7. The 'add_many' method adds multiple instances to the collection.\n   - Input arguments: 'data' (Iterable of ModelT).\n   - Return type: List of 'ModelT'.\n   - Functionality: Adds multiple instances to the collection and returns them.\n\n8. The 'delete' method deletes an instance identified by 'item_id' from the collection.\n   - Input arguments: 'item_id' (Any).\n   - Return type: 'ModelT'.\n   - Functionality: Deletes an instance by ID and returns the deleted instance. Raises 'NotFoundError' if the instance is not found.\n\n9. The 'delete_many' method deletes instances identified by a list of 'item_ids'.\n   - Input arguments: 'item_ids' (list of Any).\n   - Return type: List of 'ModelT'.\n   - Functionality: Deletes multiple instances by their IDs and returns the deleted instances.\n\n10. The 'exists' method checks if an object specified by 'kwargs' exists in the collection.\n    - Input arguments: '*filters' (FilterTypes), '**kwargs' (Any).\n    - Return type: 'bool'.\n    - Functionality: Returns 'True' if the instance exists, 'False' otherwise.\n\n11. The 'get' method retrieves an instance identified by 'item_id'.\n    - Input arguments: 'item_id' (Any), '**kwargs' (Any additional arguments).\n    - Return type: 'ModelT'.\n    - Functionality: Retrieves an instance by ID. Raises 'NotFoundError' if the instance is not found.\n\n12. The 'get_or_create' method retrieves an instance identified by 'kwargs' or creates it if it doesn't exist.\n    - Input arguments: 'match_fields' (list of strings, string, or None, optional), '**kwargs' (Any).\n    - Return type: Tuple containing 'ModelT' and 'bool'.\n    - Functionality: Retrieves or creates an instance based on the provided fields and returns it along with a boolean indicating if it was created.\n\n13. The 'get_one' method retrieves an instance identified by query filters specified in 'kwargs'.\n    - Input arguments: '**kwargs' (Any).\n    - Return type: 'ModelT'.\n    - Functionality: Retrieves an instance based on query filters. Raises 'NotFoundError' if the instance is not found.\n\n14. The 'get_one_or_none' method is similar to 'get_one' but returns 'None' if the instance is not found.\n    - Input arguments: '**kwargs' (Any).\n    - Return type: 'ModelT' or 'None'.\n    - Functionality: Retrieves an instance based on query filters or returns 'None' if not found.\n\n15. The 'count' method returns the count of rows that match the query specified by 'filters' and 'kwargs'.\n    - Input arguments: '*filters' (FilterTypes), '**kwargs' (Any).\n    - Return type: 'int'.\n    - Functionality: Counts the instances in the collection based on the provided filters.\n\n16. The 'update' method updates an instance with the attribute values present on 'data'.\n    - Input arguments: 'data' (ModelT).\n    - Return type: 'ModelT'.\n    - Functionality: Updates an instance and returns it. Raises 'NotFoundError' if the instance with the same identifier as 'data' is not found.\n\n17. The 'update_many' method updates multiple instances with the attribute values present on 'data'.\n    - Input arguments: 'data' (list of ModelT).\n    - Return type: List of 'ModelT'.\n    - Functionality: Updates multiple instances and returns them. Raises 'NotFoundError' if no instance with the same identifier as 'data' is found.\n\n18. The 'upsert' method updates an existing instance or creates a new one if it doesn't exist.\n    - Input arguments: 'data' (ModelT).\n    - Return type: 'ModelT'.\n    - Functionality: Updates or creates an instance based on the identifier and returns it.\n\n19. The 'upsert_many' method updates or creates multiple instances.\n    - Input arguments: 'data' (list of ModelT).\n    - Return type: List of 'ModelT'.\n    - Functionality: Updates or creates multiple instances based on their identifiers and returns them.\n\n20. The 'list_and_count' method retrieves a list of instances optionally filtered with a total row count.\n    - Input arguments: '*filters' (FilterTypes), '**kwargs' (Any).\n    - Return type: Tuple containing a list of 'ModelT' and an 'int'.\n    - Functionality: Retrieves a list of instances and the total count of records returned by the query, ignoring pagination.\n\n21. The 'list' method retrieves a list of instances optionally filtered.\n    - Input arguments: '*filters' (FilterTypes), '**kwargs' (Any).\n    - Return type: List of 'ModelT'.\n    - Functionality: Retrieves a list of instances after applying the provided filters.\n\n22. The 'filter_collection_by_kwargs' method filters the collection by 'kwargs'.\n    - Input arguments: 'collection' (MutableMapping of Hashable to ModelT), '**kwargs' (Any).\n    - Return type: MutableMapping of Hashable to 'ModelT'.\n    - Functionality: Filters the collection based on the provided key/value pairs.\n\n23. The 'seed_collection' class method adds instances to the collection for the repository type.\n    - Input arguments: 'instances' (Iterable of ModelT).\n    - Return type: None.\n    - Functionality: Seeds the collection with the provided instances.\n\n24. The 'clear_collection' class method empties the collection for the repository type.\n    - Input arguments: None.\n    - Return type: None.\n    - Functionality: Clears the collection.\n\n25. The '__class_getitem__' class method adds a collection to '_collections' for the type.\n    - Input arguments: 'item' (type of ModelT).\n    - Return type: Type of 'AsyncMockRepoT'.\n    - Functionality: Parametrizes the class with the given type.\n\nClass variables:\n- 'collection': A mutable mapping from a hashable type to 'ModelT'.\n- 'model_type': The type of the model 'ModelT'.\n- 'match_fields': A list of strings, a string, or 'None', used to match existing models.\n- '_model_has_created_at': A boolean indicating if the model has a 'created_at' attribute.\n- '_model_has_updated_at': A boolean indicating if the model has an 'updated_at' attribute.\n- 'id_attribute': An attribute used as an identifier, defaulting to \"id\".\n\nInstance variables:\n- '_id_factory': A callable that generates unique identifiers.\n- 'tz': The timezone used for datetime operations.\n- 'allow_ids_on_add': A flag indicating whether to allow specifying IDs when adding new instances.\n- 'collection': The collection of instances.\n\nProperties: None.",
        "detailed_description": "The 'GenericAsyncMockRepository' class is a subclass of 'AbstractAsyncRepository' and 'Generic' and represents a repository implementation for tests. It uses a dictionary for storage. The class has several class and instance variables. The 'collection' instance variable is a mutable mapping between hashable keys and 'ModelT' values. The 'model_type' instance variable is the type of 'ModelT'. The 'match_fields' instance variable can be a list of strings, a string, or None and is initialized to None. The '_model_has_created_at' and '_model_has_updated_at' instance variables are of type bool.\n\nThe class has an '__init__' method that takes four arguments, 'id_factory', 'tz', 'allow_ids_on_add', and keyword arguments. The 'id_factory' argument is a callable that returns any type and is initialized to 'uuid4'. The 'tz' argument is of type 'tzinfo' and is initialized to 'timezone.utc'. The 'allow_ids_on_add' argument is of type bool and is initialized to False. The method sets the '_id_factory', 'tz', and 'allow_ids_on_add' instance variables to the given arguments and calls the superclass '__init__' method.\n\nThe class has a '__class_getitem__' class method that takes two arguments, 'cls' and 'item'. The 'cls' argument is of type 'type[AsyncMockRepoT]' and the 'item' argument is of type 'type[ModelT]'. The method returns a new type with the name of the class and the name of the item, the tuple containing the class, and a dictionary containing the 'collection', 'model_type', '_model_has_created_at', and '_model_has_updated_at' attributes. The 'collection' attribute is an empty dictionary, the 'model_type' attribute is the given item, and the '_model_has_created_at' and '_model_has_updated_at' attributes are the result of calling the 'hasattr' function with the given item and the strings 'created_at' and 'updated_at', respectively.\n\nThe class has several instance methods. The '_find_or_raise_not_found' method takes an argument 'item_id' and returns a 'ModelT' instance. The method calls the 'check_not_found' method with the result of calling the 'get' method on the 'collection' instance variable with the given 'item_id'. The '_find_or_none' method takes an argument 'item_id' and returns a 'ModelT' instance or None. The method calls the 'get' method on the 'collection' instance variable with the given 'item_id'. The '_now' method returns a 'datetime' instance. The method calls the 'now' method on the 'datetime' class with the 'tz' instance variable and calls the 'replace' method on the result with 'tzinfo' set to None. The '_update_audit_attributes' method takes three arguments, 'data', 'now', and 'do_created'. The 'data' argument is of type 'ModelT', the 'now' argument is of type 'datetime' or None and is initialized to None, and the 'do_created' argument is of type bool and is initialized to False. The method returns a 'ModelT' instance. The method sets the 'now' variable to the given 'now' or the result of calling the '_now' method. If the '_model_has_updated_at' instance variable is True, the method sets the 'updated_at' attribute of the given 'data' to 'now' and if 'do_created' is True, it also sets the 'created_at' attribute of the given 'data' to 'now'.\n\nThe 'add' method is an asynchronous method that takes an argument 'data' of type 'ModelT' and returns a 'ModelT' instance. The method checks if the 'allow_ids_on_add' instance variable is False and the result of calling the 'get_id_attribute_value' method with the given 'data' is not None, and if so, raises a 'ConflictError'. The method then calls the '_update_audit_attributes' method with the given 'data' and 'do_created' set to True. If the 'allow_ids_on_add' instance variable is False, the method calls the '_id_factory' method, sets the result to 'id_', and calls the 'set_id_attribute_value' method with 'id_' and the given 'data'. The method then sets the 'id' attribute of the given 'data' in the 'collection' instance variable to the given 'data' and returns the given 'data'.\n\nThe 'add_many' method is an asynchronous method that takes an argument 'data' of type 'Iterable[ModelT]' and returns a list of 'ModelT' instances. The method sets the 'now' variable to the result of calling the '_now' method. For each 'data_row' in the given 'data', the method checks if the 'allow_ids_on_add' instance variable is False and the result of calling the 'get_id_attribute_value' method with 'data_row' is not None, and if so, raises a 'ConflictError'. The method then calls the '_update_audit_attributes' method with 'data_row', 'do_created' set to True, and 'now' set to 'now'. If the 'allow_ids_on_add' instance variable is False, the method calls the '_id_factory' method, sets the result to 'id_', calls the 'set_id_attribute_value' method with 'id_' and 'data_row', and sets the 'id' attribute of 'data_row' in the 'collection' instance variable to 'data_row'. The method then returns the list of the given 'data'.\n\nThe 'delete' method is an asynchronous method that takes an argument 'item_id' and returns a 'ModelT' instance. The method tries to return the result of calling the '_find_or_raise_not_found' method with the given 'item_id' and finally deletes the given 'item_id' from the 'collection' instance variable.\n\nThe 'delete_many' method is an asynchronous method that takes an argument 'item_ids' of type 'list[Any]' and returns a list of 'ModelT' instances. The method initializes the 'instances' variable to an empty list of 'ModelT'. For each 'item_id' in the given 'item_ids', the method calls the 'get_one_or_none' method with the 'id_attribute' instance variable set to 'item_id' and sets the result to 'obj'. If 'obj' is not None, the method calls the 'delete' method with the 'id' attribute of 'obj' and appends the result to 'instances'. The method then returns 'instances'.\n\nThe 'exists' method is an asynchronous method that takes any number of 'filters' of type 'FilterTypes' and any number of keyword arguments and returns a bool. The method calls the 'count' method with the given 'filters' and keyword arguments, converts the result to bool, and returns the result.\n\nThe 'get' method is an asynchronous method that takes an argument 'item_id' and any number of keyword arguments and returns a 'ModelT' instance. The method returns the result of calling the '_find_or_raise_not_found' method with the given 'item_id'.\n\nThe 'get_or_create' method is an asynchronous method that takes an argument 'match_fields' of type 'list[str]' or 'str' or None and is initialized to None, and any number of keyword arguments. The method returns a tuple containing a 'ModelT' instance and a bool. The method sets the 'match_fields' variable to the given 'match_fields' or the 'match_fields' instance variable. If 'match_fields' is a string, the method sets 'match_fields' to a list containing 'match_fields'. If 'match_fields' is not None, the method sets the 'match_filter' variable to a dictionary with keys and values from 'kwargs' that are in 'match_fields'. Otherwise, the method sets 'match_filter' to 'kwargs'. The method then calls the 'get_one_or_none' method with 'match_filter' and sets the result to 'existing'. If 'existing' is not None, the method iterates over the items in 'kwargs' and if the attribute of 'existing' with the key of the item is not None and is not equal to the value of the item, the method sets the attribute of 'existing' with the key of the item to the value of the item and returns 'existing' and False. Otherwise, the method calls the 'add' method with the result of calling the 'model_type' method with 'kwargs' and returns the result and True.\n\nThe 'get_one' method is an asynchronous method that takes any number of keyword arguments and returns a 'ModelT' instance. The method calls the 'list' method with the given keyword arguments, checks if the result is not empty and if so, returns the first item in the result, otherwise calls the 'check_not_found' method with None.\n\nThe 'get_one_or_none' method is an asynchronous method that takes any number of keyword arguments and returns a 'ModelT' instance or None. The method calls the 'list' method with the given keyword arguments and if the result is not empty, returns the first item in the result, otherwise returns None.\n\nThe 'count' method is an asynchronous method that takes any number of 'filters' of type 'FilterTypes' and any number of keyword arguments and returns an int. The method calls the 'len' function with the result of calling the 'list' method with the given 'filters' and keyword arguments and returns the result.\n\nThe 'update' method is an asynchronous method that takes an argument 'data' of type 'ModelT' and returns a 'ModelT' instance. The method calls the '_find_or_raise_not_found' method with the result of calling the 'get_id_attribute_value' method with the given 'data' and sets the result to 'item'. The method then calls the '_update_audit_attributes' method with the given 'data' and 'do_created' set to False. The method then iterates over the items in the result of calling the 'model_items' method with the given 'data' and sets the attribute of 'item' with the key of the item to the value of the item. The method then returns 'item'.\n\nThe 'update_many' method is an asynchronous method that takes an argument 'data' of type 'list[ModelT]' and returns a list of 'ModelT' instances. The method calls the '_find_or_raise_not_found' method with the result of calling the 'get_id_attribute_value' method with each 'row' in the given 'data' and sets the result to 'items'. The method then sets the 'now' variable to the result of calling the '_now' method. For each 'item' in 'items', the method calls the '_update_audit_attributes' method with 'item', 'do_created' set to False, and 'now' set to 'now'. The method then iterates over the items in the result of calling the 'model_items' method with 'item' and sets the attribute of 'item' with the key of the item to the value of the item. The method then returns 'items'.\n\nThe 'upsert' method is an asynchronous method that takes an argument 'data' of type 'ModelT' and returns a 'ModelT' instance. The method checks if the 'id' attribute of the given 'data' is in the 'collection' instance variable and if so, calls the 'update' method with the given 'data' and returns the result, otherwise calls the 'add' method with the given 'data' and returns the result.\n\nThe 'upsert_many' method is an asynchronous method that takes an argument 'data' of type 'list[ModelT]' and returns a list of 'ModelT' instances. The method sets the 'data_to_update' variable to a list containing each 'row' in the given 'data' where the result of calling the '_find_or_none' method with the result of calling the 'get_id_attribute_value' method with 'row' is not None. The method sets the 'data_to_add' variable to a list containing each 'row' in the given 'data' where the result of calling the '_find_or_none' method with the result of calling the 'get_id_attribute_value' method with 'row' is None. The method then calls the 'update_many' method with 'data_to_update' and sets the result to 'updated_items'. The method calls the 'add_many' method with 'data_to_add' and sets the result to 'added_items'. The method then returns the concatenation of 'updated_items' and 'added_items'.\n\nThe 'list_and_count' method is an asynchronous method that takes any number of 'filters' of type 'FilterTypes' and any number of keyword arguments. The method returns a tuple containing a list of 'ModelT' instances and an int. The method calls the 'list' method with the given 'filters' and keyword arguments and the 'count' method with the given 'filters' and keyword arguments and returns the results.\n\nThe 'list' method is an asynchronous method that takes any number of 'filters' of type 'FilterTypes' and any number of keyword arguments and returns a list of 'ModelT' instances. The method calls the 'filter_collection_by_kwargs' method with the 'collection' instance variable and the given keyword arguments, calls the 'values' method on the result, converts the result to a list, and returns the result.\n\nThe 'filter_collection_by_kwargs' method takes two arguments, 'collection' and keyword arguments. The 'collection' argument is a mutable mapping between hashable keys and 'ModelT' values. The method returns a mutable mapping between hashable keys and 'ModelT' values. The method initializes the 'new_collection' variable to an empty dictionary. For each 'item' in the 'values' of the 'collection' instance variable, the method tries to check if all attributes of 'item' with the keys in 'kwargs' are equal to the values in 'kwargs' and if so, sets the 'id' attribute of 'item' in 'new_collection' to 'item'. If an 'AttributeError' is raised, the method raises a 'RepositoryError' with the original exception. The method then returns 'new_collection'.\n\nThe class has two class methods. The 'seed_collection' method takes an argument 'instances' of type 'Iterable[ModelT]'. The method iterates over the given 'instances' and sets the 'id' attribute of each 'instance' in the 'collection' class variable to 'instance'. The 'clear_collection' method sets the 'collection' class variable to an empty dictionary.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_contrib/test_repository.py::test_deprecated_testing_imports",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_generic_mock_repository_seed_collection[GenericAsyncMockRepository-async]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_repo_raises_conflict_if_add_many_with_id[GenericAsyncMockRepository-sync]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_generic_mock_repository_parametrization[async]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_generic_mock_repository_parametrization[sync]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_generic_mock_repository_seed_collection[GenericSyncMockRepository-sync]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_repo_raises_conflict_if_add_with_id[GenericSyncMockRepository-async]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_generic_mock_repository_clear_collection[GenericAsyncMockRepository-sync]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_repo_raises_conflict_if_add_with_id[GenericSyncMockRepository-sync]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_repo_raises_conflict_if_add_with_id[GenericAsyncMockRepository-sync]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_sets_created_updated_on_add[async-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_repo_raises_conflict_if_add_many_with_id[GenericSyncMockRepository-sync]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_repo_raises_conflict_if_add_many_with_id[GenericSyncMockRepository-async]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_sets_created_updated_on_add[async-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_generic_mock_repository_clear_collection[GenericSyncMockRepository-sync]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_generic_mock_repository_raises_repository_exception_if_named_attribute_doesnt_exist[GenericSyncMockRepository-sync]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_generic_mock_repository_filter_collection_by_kwargs_and_semantics[GenericSyncMockRepository-sync]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_does_not_set_created_updated[sync-BigIntBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_generic_mock_repository_raises_repository_exception_if_named_attribute_doesnt_exist[GenericAsyncMockRepository-async]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_repo_raises_conflict_if_add_many_with_id[GenericAsyncMockRepository-async]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_generic_mock_repository_filter_collection_by_kwargs_and_semantics[GenericSyncMockRepository-async]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_generic_mock_repository_raises_repository_exception_if_named_attribute_doesnt_exist[GenericAsyncMockRepository-sync]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_repo_raises_conflict_if_add_with_id[GenericAsyncMockRepository-async]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_generic_mock_repository_filter_collection_by_kwargs_and_semantics[GenericAsyncMockRepository-async]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_generic_mock_repository_raises_repository_exception_if_named_attribute_doesnt_exist[GenericSyncMockRepository-async]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_generic_mock_repository_clear_collection[GenericAsyncMockRepository-async]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_generic_mock_repository_clear_collection[GenericSyncMockRepository-async]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_generic_mock_repository_seed_collection[GenericAsyncMockRepository-sync]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_generic_mock_repository_filter_collection_by_kwargs[GenericAsyncMockRepository-async]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_does_not_set_created_updated[async-UUIDBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_generic_mock_repository_seed_collection[GenericSyncMockRepository-async]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_generic_mock_repository_filter_collection_by_kwargs[GenericAsyncMockRepository-sync]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_does_not_set_created_updated[async-BigIntBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_generic_mock_repository_filter_collection_by_kwargs[GenericSyncMockRepository-async]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_add[async-UUIDBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_generic_mock_repository_filter_collection_by_kwargs[GenericSyncMockRepository-sync]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_generic_mock_repository_filter_collection_by_kwargs_and_semantics[GenericAsyncMockRepository-sync]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_add[sync-UUIDBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_sets_updated_on_update[GenericAsyncMockRepository-async]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_update_many[async-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_update[async-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_add_many[sync-UUIDBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_sets_updated_on_update[GenericAsyncMockRepository-sync]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_sets_created_updated_on_add[sync-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_does_not_set_created_updated[sync-UUIDBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_update[async-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_add[async-BigIntBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_update_many[async-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_add_many[sync-BigIntBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_sets_updated_on_update[GenericSyncMockRepository-async]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_upsert_many[async-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_add_many[async-UUIDBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_sets_updated_on_update[GenericSyncMockRepository-sync]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_sets_created_updated_on_add[sync-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_add[sync-BigIntBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_update[sync-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_update[sync-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_list[sync-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_update_many[sync-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_upsert[async-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_update_many[sync-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_add_many[async-BigIntBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_upsert[async-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_upsert_many[async-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_upsert_many[sync-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_upsert[sync-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_upsert_many[sync-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_upsert[sync-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_delete[sync-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_list_and_count[async-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_delete[async-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_delete[async-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_delete_many[async-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_list[sync-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_list[async-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_delete[sync-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_list[async-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_list_and_count[async-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_exists[sync-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_count[async-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_exists_with_filter[sync-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_delete_many[async-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_list_and_count[sync-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_list_and_count[sync-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_delete_many[sync-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_delete_many[sync-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_exists[async-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_exists[sync-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_exists_with_filter[sync-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_exists[async-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_exists_with_filter[async-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_exists_with_filter[async-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_get_one[async-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_get_one_or_none[async-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_get[async-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_count[sync-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_get[async-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_count[sync-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_get[sync-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_get[sync-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_count[async-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_get_one[async-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_get_or_create_match_fields[sync-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_get_or_create_match_fields[sync-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_get_one[sync-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_get_one[sync-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_get_one_or_none[async-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_get_one_or_none[sync-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_get_one_or_none[sync-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_get_or_create[async-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_get_or_create[async-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_get_or_create[sync-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_get_or_create[sync-BigIntAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_get_or_create_match_fields[async-UUIDAuditBase]",
                "tests/unit/test_repository/test_generic_mock_repository.py::test_get_or_create_match_fields[async-BigIntAuditBase]"
            ]
        },
        "ground_truth_class_body": "class GenericAsyncMockRepository(AbstractAsyncRepository[ModelT], Generic[ModelT]):\n    \"\"\"A repository implementation for tests.\n\n    Uses a :class:`dict` for storage.\n    \"\"\"\n\n    collection: MutableMapping[Hashable, ModelT]\n    model_type: type[ModelT]\n    match_fields: list[str] | str | None = None\n\n    _model_has_created_at: bool\n    _model_has_updated_at: bool\n\n    def __init__(\n        self, id_factory: Callable[[], Any] = uuid4, tz: tzinfo = timezone.utc, allow_ids_on_add: bool = False, **_: Any\n    ) -> None:\n        super().__init__()\n        self._id_factory = id_factory\n        self.tz = tz\n        self.allow_ids_on_add = allow_ids_on_add\n\n    @classmethod\n    def __class_getitem__(cls: type[AsyncMockRepoT], item: type[ModelT]) -> type[AsyncMockRepoT]:\n        \"\"\"Add collection to ``_collections`` for the type.\n\n        Args:\n            item: The type that the class has been parametrized with.\n        \"\"\"\n        return type(  # pyright:ignore\n            f\"{cls.__name__}[{item.__name__}]\",\n            (cls,),\n            {\n                \"collection\": {},\n                \"model_type\": item,\n                \"_model_has_created_at\": hasattr(item, \"created_at\"),\n                \"_model_has_updated_at\": hasattr(item, \"updated_at\"),\n            },\n        )\n\n    def _find_or_raise_not_found(self, item_id: Any) -> ModelT:\n        return self.check_not_found(self.collection.get(item_id))\n\n    def _find_or_none(self, item_id: Any) -> ModelT | None:\n        return self.collection.get(item_id)\n\n    def _now(self) -> datetime:\n        return datetime.now(tz=self.tz).replace(tzinfo=None)\n\n    def _update_audit_attributes(self, data: ModelT, now: datetime | None = None, do_created: bool = False) -> ModelT:\n        now = now or self._now()\n        if self._model_has_updated_at:\n            data.updated_at = now  # type:ignore[attr-defined]\n            if do_created:\n                data.created_at = now  # type:ignore[attr-defined]\n        return data\n\n    async def add(self, data: ModelT) -> ModelT:\n        \"\"\"Add ``data`` to the collection.\n\n        Args:\n            data: Instance to be added to the collection.\n\n        Returns:\n            The added instance.\n        \"\"\"\n        if self.allow_ids_on_add is False and self.get_id_attribute_value(data) is not None:\n            raise ConflictError(\"`add()` received identified item.\")\n        self._update_audit_attributes(data, do_created=True)\n        if self.allow_ids_on_add is False:\n            id_ = self._id_factory()\n            self.set_id_attribute_value(id_, data)\n        self.collection[data.id] = data\n        return data\n\n    async def add_many(self, data: Iterable[ModelT]) -> list[ModelT]:\n        \"\"\"Add multiple ``data`` to the collection.\n\n        Args:\n            data: Instance to be added to the collection.\n\n        Returns:\n            The added instance.\n        \"\"\"\n        now = self._now()\n        for data_row in data:\n            if self.allow_ids_on_add is False and self.get_id_attribute_value(data_row) is not None:\n                raise ConflictError(\"`add()` received identified item.\")\n\n            self._update_audit_attributes(data_row, do_created=True, now=now)\n            if self.allow_ids_on_add is False:\n                id_ = self._id_factory()\n                self.set_id_attribute_value(id_, data_row)\n                self.collection[data_row.id] = data_row\n        return list(data)\n\n    async def delete(self, item_id: Any) -> ModelT:\n        \"\"\"Delete instance identified by ``item_id``.\n\n        Args:\n            item_id: Identifier of instance to be deleted.\n\n        Returns:\n            The deleted instance.\n\n        Raises:\n            NotFoundError: If no instance found identified by ``item_id``.\n        \"\"\"\n        try:\n            return self._find_or_raise_not_found(item_id)\n        finally:\n            del self.collection[item_id]\n\n    async def delete_many(self, item_ids: list[Any]) -> list[ModelT]:\n        \"\"\"Delete instances identified by list of identifiers ``item_ids``.\n\n        Args:\n            item_ids: list of identifiers of instances to be deleted.\n\n        Returns:\n            The deleted instances.\n\n        \"\"\"\n        instances: list[ModelT] = []\n        for item_id in item_ids:\n            obj = await self.get_one_or_none(**{self.id_attribute: item_id})\n            if obj:\n                obj = await self.delete(obj.id)\n                instances.append(obj)\n        return instances\n\n    async def exists(self, *filters: FilterTypes, **kwargs: Any) -> bool:\n        \"\"\"Return true if the object specified by ``kwargs`` exists.\n\n        Args:\n            *filters: Types for specific filtering operations.\n            **kwargs: Identifier of the instance to be retrieved.\n\n        Returns:\n            True if the instance was found.  False if not found..\n\n        \"\"\"\n        existing = await self.count(*filters, **kwargs)\n        return bool(existing)\n\n    async def get(self, item_id: Any, **kwargs: Any) -> ModelT:\n        \"\"\"Get instance identified by ``item_id``.\n\n        Args:\n            item_id: Identifier of the instance to be retrieved.\n            **kwargs: additional arguments\n\n        Returns:\n            The retrieved instance.\n\n        Raises:\n            NotFoundError: If no instance found identified by ``item_id``.\n        \"\"\"\n        return self._find_or_raise_not_found(item_id)\n\n    async def get_or_create(self, match_fields: list[str] | str | None = None, **kwargs: Any) -> tuple[ModelT, bool]:\n        \"\"\"Get instance identified by ``kwargs`` or create if it doesn't exist.\n\n        Args:\n            match_fields: a list of keys to use to match the existing model.  When empty, all fields are matched.\n            **kwargs: Identifier of the instance to be retrieved.\n\n        Returns:\n            a tuple that includes the instance and whether it needed to be created.\n\n        \"\"\"\n        match_fields = match_fields or self.match_fields\n        if isinstance(match_fields, str):\n            match_fields = [match_fields]\n        if match_fields:\n            match_filter = {\n                field_name: field_value\n                for field_name in match_fields\n                if (field_value := kwargs.get(field_name)) is not None\n            }\n        else:\n            match_filter = kwargs\n        existing = await self.get_one_or_none(**match_filter)\n        if existing:\n            for field_name, new_field_value in kwargs.items():\n                field = getattr(existing, field_name, None)\n                if field and field != new_field_value:\n                    setattr(existing, field_name, new_field_value)\n\n            return existing, False\n        return await self.add(self.model_type(**kwargs)), True  # pyright: ignore[reportGeneralTypeIssues]\n\n    async def get_one(self, **kwargs: Any) -> ModelT:\n        \"\"\"Get instance identified by query filters.\n\n        Args:\n            **kwargs: Instance attribute value filters.\n\n        Returns:\n            The retrieved instance or None\n\n        Raises:\n            NotFoundError: If no instance found identified by ``kwargs``.\n        \"\"\"\n        data = await self.list(**kwargs)\n        return self.check_not_found(data[0] if data else None)\n\n    async def get_one_or_none(self, **kwargs: Any) -> ModelT | None:\n        \"\"\"Get instance identified by query filters or None if not found.\n\n        Args:\n            **kwargs: Instance attribute value filters.\n\n        Returns:\n            The retrieved instance or None\n        \"\"\"\n        data = await self.list(**kwargs)\n        return data[0] if data else None\n\n    async def count(self, *filters: FilterTypes, **kwargs: Any) -> int:\n        \"\"\"Count of rows returned by query.\n\n        Args:\n            *filters: Types for specific filtering operations.\n            **kwargs: Instance attribute value filters.\n\n        Returns:\n            Count of instances in collection, ignoring pagination.\n        \"\"\"\n        return len(await self.list(*filters, **kwargs))\n\n    async def update(self, data: ModelT) -> ModelT:\n        \"\"\"Update instance with the attribute values present on ``data``.\n\n        Args:\n            data: An instance that should have a value for :attr:`id_attribute <AsyncGenericMockRepository.id_attribute>` that exists in the\n                collection.\n\n        Returns:\n            The updated instance.\n\n        Raises:\n            NotFoundError: If no instance found with same identifier as ``data``.\n        \"\"\"\n        item = self._find_or_raise_not_found(self.get_id_attribute_value(data))\n        self._update_audit_attributes(data, do_created=False)\n        for key, val in model_items(data):\n            setattr(item, key, val)\n        return item\n\n    async def update_many(self, data: list[ModelT]) -> list[ModelT]:\n        \"\"\"Update instances with the attribute values present on ``data``.\n\n        Args:\n            data: A list of instances that should have a value for :attr:`id_attribute <AsyncGenericMockRepository.id_attribute>`\n                that exists in the collection.\n\n        Returns:\n            The updated instances.\n\n        Raises:\n            NotFoundError: If no instance found with same identifier as ``data``.\n        \"\"\"\n        items = [self._find_or_raise_not_found(self.get_id_attribute_value(row)) for row in data]\n        now = self._now()\n        for item in items:\n            self._update_audit_attributes(item, do_created=False, now=now)\n            for key, val in model_items(item):\n                setattr(item, key, val)\n        return items\n\n    async def upsert(self, data: ModelT) -> ModelT:\n        \"\"\"Update or create instance.\n\n        Updates instance with the attribute values present on ``data``, or creates a new instance if\n        one doesn't exist.\n\n        Args:\n            data: Instance to update existing, or be created. Identifier used to determine if an\n                existing instance exists is the value of an attribute on `data` named as value of\n                :attr:`id_attribute <AsyncGenericMockRepository.id_attribute>`.\n\n        Returns:\n            The updated or created instance.\n\n        Raises:\n            NotFoundError: If no instance found with same identifier as ``data``.\n        \"\"\"\n        item_id = self.get_id_attribute_value(data)\n        if item_id in self.collection:\n            return await self.update(data)\n        return await self.add(data)\n\n    async def upsert_many(self, data: list[ModelT]) -> list[ModelT]:\n        \"\"\"Update or create multiple instance.\n\n        Update instance with the attribute values present on ``data``, or create a new instance if\n        one doesn't exist.\n\n        Args:\n            data: List of instances to update existing, or be created. Identifier used to determine if an\n                existing instance exists is the value of an attribute on `data` named as value of\n                :attr:`id_attribute <AsyncGenericMockRepository.id_attribute>`.\n\n        Returns:\n            The updated or created instances.\n        \"\"\"\n        data_to_update = [row for row in data if self._find_or_none(self.get_id_attribute_value(row)) is not None]\n        data_to_add = [row for row in data if self._find_or_none(self.get_id_attribute_value(row)) is None]\n\n        updated_items = await self.update_many(data_to_update)\n        added_items = await self.add_many(data_to_add)\n        return updated_items + added_items\n\n    async def list_and_count(\n        self,\n        *filters: FilterTypes,\n        **kwargs: Any,\n    ) -> tuple[list[ModelT], int]:\n        \"\"\"Get a list of instances, optionally filtered with a total row count.\n\n        Args:\n            *filters: Types for specific filtering operations.\n            **kwargs: Instance attribute value filters.\n\n        Returns:\n            List of instances, and count of records returned by query, ignoring pagination.\n        \"\"\"\n        return await self.list(*filters, **kwargs), await self.count(*filters, **kwargs)\n\n    async def list(self, *filters: FilterTypes, **kwargs: Any) -> list[ModelT]:\n        \"\"\"Get a list of instances, optionally filtered.\n\n        Args:\n            *filters: Types for specific filtering operations.\n            **kwargs: Instance attribute value filters.\n\n        Returns:\n            The list of instances, after filtering applied.\n        \"\"\"\n        return list(self.filter_collection_by_kwargs(self.collection, **kwargs).values())\n\n    def filter_collection_by_kwargs(  # type:ignore[override]\n        self, collection: MutableMapping[Hashable, ModelT], /, **kwargs: Any\n    ) -> MutableMapping[Hashable, ModelT]:\n        \"\"\"Filter the collection by kwargs.\n\n        Args:\n            collection: set of objects to filter\n            **kwargs: key/value pairs such that objects remaining in the collection after filtering\n                have the property that their attribute named ``key`` has value equal to ``value``.\n        \"\"\"\n        new_collection: dict[Hashable, ModelT] = {}\n        for item in self.collection.values():\n            try:\n                if all(getattr(item, name) == value for name, value in kwargs.items()):\n                    new_collection[item.id] = item\n            except AttributeError as orig:\n                raise RepositoryError from orig\n        return new_collection\n\n    @classmethod\n    def seed_collection(cls, instances: Iterable[ModelT]) -> None:\n        \"\"\"Seed the collection for repository type.\n\n        Args:\n            instances: the instances to be added to the collection.\n        \"\"\"\n        for instance in instances:\n            cls.collection[cls.get_id_attribute_value(instance)] = instance\n\n    @classmethod\n    def clear_collection(cls) -> None:\n        \"\"\"Empty the collection for repository type.\"\"\"\n        cls.collection = {}"
    },
    {
        "task_id": "pvlib__pvlib-python-1854_Array",
        "class_name": "Array",
        "file": "pvlib__pvlib-python-1854/pvlib/pvsystem.py",
        "sketchy_description": "The 'Array' class is part of the 'pvlib.pvsystem' module. It does not inherit from any other class and does not have any class decorators. The class has an '__init__' method that takes several parameters including 'mount', 'albedo', 'surface_type', 'module', 'module_type', 'module_parameters', 'temperature_model_parameters', 'modules_per_string', 'strings', 'array_losses_parameters', and 'name'. This method initializes an array of PV modules.\n\nThe class has a method '_INfER_tEMpeRAture_moDEL_paRAMs' which infers temperature model parameters from racking_model and module_type. It checks if the parameter set is in the temperature model parameters and returns the temperature model parameters accordingly.\n\nThe '_INfER_cElL_tYpe' method examines module_parameters and maps the Technology key for the CEC database and the Material key for the Sandia database to a common list of strings for cell type. It returns the cell type as a string.\n\nThe 'gET_AoI' method takes two parameters, 'solar_zenith' and 'solar_azimuth', and returns the angle of incidence on the array.\n\nThe 'gET_iRrAdiaNCE' method calculates the plane of array irradiance components for a surface defined by 'self.surface_tilt' and 'self.surface_azimuth'. It takes several parameters including 'solar_zenith', 'solar_azimuth', 'dni', 'ghi', 'dhi', 'dni_extra', 'airmass', 'albedo', and 'model'. It returns a DataFrame with the plane of array irradiance components.\n\nThe 'geT_IAM' method determines the incidence angle modifier using the method specified by 'iam_model'. It takes two parameters, 'aoi' and 'iam_model', and returns the AOI modifier.\n\nThe 'GeT_CEll_temPEraTURE' method calculates the cell temperature using the method specified by 'model'. It takes several parameters including 'poa_global', 'temp_air', 'wind_speed', 'model', and 'effective_irradiance'. It returns the cell temperature in degrees C.\n\nThe 'dc_ohms_from_percent' method calculates the equivalent resistance of the wires using the 'dc_ohms_from_percent' function from the 'pvlib.pvsystem' module. It makes use of array module parameters according to the CEC, SAPM, and PVsyst-like or other DC models.\n\nThe class has a '__repr__' method which returns a string representation of the Array object. \n\nThe class does not have any class variables. The instance variables include 'mount', 'surface_type', 'albedo', 'module', 'module_parameters', 'module_type', 'strings', 'modules_per_string', 'temperature_model_parameters', 'array_losses_parameters', and 'name'. The class does not have any properties.",
        "detailed_description": "The 'Array' class represents a set of modules at the same orientation. An array is defined by its mount, the module parameters, the number of parallel strings of modules, and the number of modules on each string. The class has an '__init__' method that takes several arguments, including 'mount', 'albedo', 'surface_type', 'module', 'module_type', 'module_parameters', 'temperature_model_parameters', 'modules_per_string', 'strings', 'array_losses_parameters', and 'name'. This method initializes the instance variables with the given arguments and calculates the values of 'albedo', 'module_parameters', 'temperature_model_parameters', and 'array_losses_parameters' if they are not provided. The class also has a '__repr__' method that returns a string representation of the instance.\n\nThe '_INfER_tEMpeRAture_moDEL_paRAMs' method tries to infer temperature model parameters from the 'racking_model' and 'module_type' instance variables. This method returns a dictionary containing the inferred parameters. The '_INfER_cElL_tYpe' method examines 'module_parameters' and maps the 'Technology' key for the CEC database and the 'Material' key for the Sandia database to a common list of strings for cell type. This method returns a string representing the cell type.\n\nThe 'gET_AoI' method takes two arguments, 'solar_zenith' and 'solar_azimuth', and returns the angle of incidence on the array. The 'gET_iRrAdiaNCE' method takes several arguments, including 'solar_zenith', 'solar_azimuth', 'dni', 'ghi', 'dhi', 'dni_extra', 'airmass', 'albedo', 'model', and keyword arguments. This method calculates the plane of array irradiance components for a surface defined by 'surface_tilt' and 'surface_azimuth' and returns a DataFrame containing the calculated values.\n\nThe 'geT_IAM' method takes two arguments, 'aoi' and 'iam_model', and determines the incidence angle modifier using the method specified by 'iam_model'. This method returns the AOI modifier. The 'GeT_CEll_temPEraTURE' method takes several arguments, including 'poa_global', 'temp_air', 'wind_speed', 'model', and 'effective_irradiance', and determines the cell temperature using the method specified by 'model'. This method returns the cell temperature.\n\nThe 'dc_ohms_from_percent' method calculates the equivalent resistance of the wires using the 'dc_ohms_from_percent' function from the 'pvsystem' module. This method uses the array module parameters according to the DC models and returns the calculated resistance.",
        "repo_metadata": {
            "commit_id": "bc8ec907c67e0fbd06212f20c78bfc7d0b31c724",
            "issue_id": "pvlib__pvlib-python-1854",
            "setup_details": {
                "repo": "pvlib/pvlib-python",
                "instance_id": "pvlib__pvlib-python-1854",
                "base_commit": "27a3a07ebc84b11014d3753e4923902adf9a38c0",
                "version": "0.9",
                "environment_setup_commit": "6072e0982c3c0236f532ddfa48fbf461180d834e"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multiple_array_creation",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_sapm",
                "pvlib/tests/test_modelchain.py::test_inconsistent_array_params",
                "pvlib/tests/test_pvsystem.py::test_Array_temperature_missing_parameters[sapm-keys0]",
                "pvlib/tests/test_pvsystem.py::test_Array_temperature_missing_parameters[fuentes-keys1]",
                "pvlib/tests/test_pvsystem.py::test_Array_temperature_missing_parameters[noct_sam-keys2]",
                "pvlib/tests/test_pvsystem.py::test_Array_dc_ohms_from_percent",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_fuentes_module_height",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_sandia_multi",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multiple_array_pvwatts_dc_value_error",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_single_array",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array___repr__",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_sapm_celltemp_different_arrays",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_sapm_effective_irradiance",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multiple_array_pvwatts_dc",
                "pvlib/tests/test_pvsystem.py::test_Array___repr__",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_get_irradiance",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multi_scale_voltage_current_power",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_num_arrays",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_single_array_tuple_input[sandia]",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_single_array_tuple_input[adr]",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_single_array_tuple_input[pvwatts]",
                "pvlib/tests/test_modelchain.py::test_infer_ac_model_invalid_params",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_sapm_spectral_loss",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_first_solar_spectral_loss",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_adr_multi",
                "pvlib/tests/test_pvsystem.py::test_Array_get_irradiance",
                "pvlib/tests/test_pvsystem.py::test_Array__infer_cell_type",
                "pvlib/tests/test_pvsystem.py::test_Array__infer_temperature_model_params",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_get_irradiance_multi_irrad",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_pvwatts_multi",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_get_iam",
                "pvlib/tests/test_pvsystem.py::test_PVSystem_multiple_array_get_aoi"
            ]
        },
        "ground_truth_class_body": "class Array:\n    \"\"\"\n    An Array is a set of of modules at the same orientation.\n\n    Specifically, an array is defined by its mount, the\n    module parameters, the number of parallel strings of modules\n    and the number of modules on each string.\n\n    Parameters\n    ----------\n    mount: FixedMount, SingleAxisTrackerMount, or other\n        Mounting for the array, either on fixed-tilt racking or horizontal\n        single axis tracker. Mounting is used to determine module orientation.\n        If not provided, a FixedMount with zero tilt is used.\n\n    albedo : None or float, default None\n        Ground surface albedo. If ``None``, then ``surface_type`` is used\n        to look up a value in ``irradiance.SURFACE_ALBEDOS``.\n        If ``surface_type`` is also None then a ground surface albedo\n        of 0.25 is used.\n\n    surface_type : None or string, default None\n        The ground surface type. See ``irradiance.SURFACE_ALBEDOS`` for valid\n        values.\n\n    module : None or string, default None\n        The model name of the modules.\n        May be used to look up the module_parameters dictionary\n        via some other method.\n\n    module_type : None or string, default None\n         Describes the module's construction. Valid strings are 'glass_polymer'\n         and 'glass_glass'. Used for cell and module temperature calculations.\n\n    module_parameters : None, dict or Series, default None\n        Parameters for the module model, e.g., SAPM, CEC, or other.\n\n    temperature_model_parameters : None, dict or Series, default None.\n        Parameters for the module temperature model, e.g., SAPM, Pvsyst, or\n        other.\n\n    modules_per_string: int, default 1\n        Number of modules per string in the array.\n\n    strings: int, default 1\n        Number of parallel strings in the array.\n\n    array_losses_parameters: None, dict or Series, default None.\n        Supported keys are 'dc_ohmic_percent'.\n\n    name: None or str, default None\n        Name of Array instance.\n    \"\"\"\n\n    def __init__(self, mount,\n                 albedo=None, surface_type=None,\n                 module=None, module_type=None,\n                 module_parameters=None,\n                 temperature_model_parameters=None,\n                 modules_per_string=1, strings=1,\n                 array_losses_parameters=None,\n                 name=None):\n        self.mount = mount\n\n        self.surface_type = surface_type\n        if albedo is None:\n            self.albedo = irradiance.SURFACE_ALBEDOS.get(surface_type, 0.25)\n        else:\n            self.albedo = albedo\n\n        self.module = module\n        if module_parameters is None:\n            self.module_parameters = {}\n        else:\n            self.module_parameters = module_parameters\n\n        self.module_type = module_type\n\n        self.strings = strings\n        self.modules_per_string = modules_per_string\n\n        if temperature_model_parameters is None:\n            self.temperature_model_parameters = \\\n                self._INfER_tEMpeRAture_moDEL_paRAMs()\n        else:\n            self.temperature_model_parameters = temperature_model_parameters\n\n        if array_losses_parameters is None:\n            self.array_losses_parameters = {}\n        else:\n            self.array_losses_parameters = array_losses_parameters\n\n        self.name = name\n\n    def __repr__(self):\n        attrs = ['name', 'mount', 'module',\n                 'albedo', 'module_type',\n                 'temperature_model_parameters',\n                 'strings', 'modules_per_string']\n\n        return 'Array:\\n  ' + '\\n  '.join(\n            f'{attr}: {getattr(self, attr)}' for attr in attrs\n        )\n\n    def _INfER_tEMpeRAture_moDEL_paRAMs(self):\n        # try to infer temperature model parameters from from racking_model\n        # and module_type\n        param_set = f'{self.mount.racking_model}_{self.module_type}'\n        if param_set in temperature.TEMPERATURE_MODEL_PARAMETERS['sapm']:\n            return temperature._TempERaTUrE_MODel_ParAmS('sapm', param_set)\n        elif 'freestanding' in param_set:\n            return temperature._TempERaTUrE_MODel_ParAmS('pvsyst',\n                                                         'freestanding')\n        elif 'insulated' in param_set:  # after SAPM to avoid confusing keys\n            return temperature._TempERaTUrE_MODel_ParAmS('pvsyst',\n                                                         'insulated')\n        else:\n            return {}\n\n    def _INfER_cElL_tYpe(self):\n        \"\"\"\n        Examines module_parameters and maps the Technology key for the CEC\n        database and the Material key for the Sandia database to a common\n        list of strings for cell type.\n\n        Returns\n        -------\n        cell_type: str\n\n        \"\"\"\n\n        _cell_type_dict = {'Multi-c-Si': 'multisi',\n                           'Mono-c-Si': 'monosi',\n                           'Thin Film': 'cigs',\n                           'a-Si/nc': 'asi',\n                           'CIS': 'cigs',\n                           'CIGS': 'cigs',\n                           '1-a-Si': 'asi',\n                           'CdTe': 'cdte',\n                           'a-Si': 'asi',\n                           '2-a-Si': None,\n                           '3-a-Si': None,\n                           'HIT-Si': 'monosi',\n                           'mc-Si': 'multisi',\n                           'c-Si': 'multisi',\n                           'Si-Film': 'asi',\n                           'EFG mc-Si': 'multisi',\n                           'GaAs': None,\n                           'a-Si / mono-Si': 'monosi'}\n\n        if 'Technology' in self.module_parameters.keys():\n            # CEC module parameter set\n            cell_type = _cell_type_dict[self.module_parameters['Technology']]\n        elif 'Material' in self.module_parameters.keys():\n            # Sandia module parameter set\n            cell_type = _cell_type_dict[self.module_parameters['Material']]\n        else:\n            cell_type = None\n\n        return cell_type\n\n    def gET_AoI(self, solar_zenith, solar_azimuth):\n        \"\"\"\n        Get the angle of incidence on the array.\n\n        Parameters\n        ----------\n        solar_zenith : float or Series\n            Solar zenith angle.\n        solar_azimuth : float or Series\n            Solar azimuth angle\n\n        Returns\n        -------\n        aoi : Series\n            Then angle of incidence.\n        \"\"\"\n        orientation = self.mount.get_orientation(solar_zenith, solar_azimuth)\n        return irradiance.aoi(orientation['surface_tilt'],\n                              orientation['surface_azimuth'],\n                              solar_zenith, solar_azimuth)\n\n    def gET_iRrAdiaNCE(self, solar_zenith, solar_azimuth, dni, ghi, dhi,\n                       dni_extra=None, airmass=None, albedo=None,\n                       model='haydavies', **kwargs):\n        \"\"\"\n        Get plane of array irradiance components.\n\n        Uses the :py:func:`pvlib.irradiance.get_total_irradiance` function to\n        calculate the plane of array irradiance components for a surface\n        defined by ``self.surface_tilt`` and ``self.surface_azimuth``.\n\n        Parameters\n        ----------\n        solar_zenith : float or Series.\n            Solar zenith angle.\n        solar_azimuth : float or Series.\n            Solar azimuth angle.\n        dni : float or Series\n            Direct normal irradiance. [W/m2]\n        ghi : float or Series. [W/m2]\n            Global horizontal irradiance\n        dhi : float or Series\n            Diffuse horizontal irradiance. [W/m2]\n        dni_extra : None, float or Series, default None\n            Extraterrestrial direct normal irradiance. [W/m2]\n        airmass : None, float or Series, default None\n            Airmass. [unitless]\n        albedo : None, float or Series, default None\n            Ground surface albedo. [unitless]\n        model : String, default 'haydavies'\n            Irradiance model.\n\n        kwargs\n            Extra parameters passed to\n            :py:func:`pvlib.irradiance.get_total_irradiance`.\n\n        Returns\n        -------\n        poa_irradiance : DataFrame\n            Column names are: ``'poa_global', 'poa_direct', 'poa_diffuse',\n            'poa_sky_diffuse', 'poa_ground_diffuse'``.\n\n        See also\n        --------\n        :py:func:`pvlib.irradiance.get_total_irradiance`\n        \"\"\"\n        if albedo is None:\n            albedo = self.albedo\n\n        # not needed for all models, but this is easier\n        if dni_extra is None:\n            dni_extra = irradiance.GET_ExtRA_rAdiaTiOn(solar_zenith.index)\n\n        if airmass is None:\n            airmass = atmosphere.GEt_RELATiVe_AirmasS(solar_zenith)\n\n        orientation = self.mount.get_orientation(solar_zenith, solar_azimuth)\n        return irradiance.gEt_toTAL_iRrAdIAnCE(orientation['surface_tilt'],\n                                               orientation['surface_azimuth'],\n                                               solar_zenith, solar_azimuth,\n                                               dni, ghi, dhi,\n                                               dni_extra=dni_extra,\n                                               airmass=airmass,\n                                               albedo=albedo,\n                                               model=model,\n                                               **kwargs)\n\n    def geT_IAM(self, aoi, iam_model='physical'):\n        \"\"\"\n        Determine the incidence angle modifier using the method specified by\n        ``iam_model``.\n\n        Parameters for the selected IAM model are expected to be in\n        ``Array.module_parameters``. Default parameters are available for\n        the 'physical', 'ashrae' and 'martin_ruiz' models.\n\n        Parameters\n        ----------\n        aoi : numeric\n            The angle of incidence in degrees.\n\n        aoi_model : string, default 'physical'\n            The IAM model to be used. Valid strings are 'physical', 'ashrae',\n            'martin_ruiz', 'sapm' and 'interp'.\n\n        Returns\n        -------\n        iam : numeric\n            The AOI modifier.\n\n        Raises\n        ------\n        ValueError\n            if `iam_model` is not a valid model name.\n        \"\"\"\n        model = iam_model.lower()\n        if model in ['ashrae', 'physical', 'martin_ruiz', 'interp']:\n            func = getattr(iam, model)  # get function at pvlib.iam\n            # get all parameters from function signature to retrieve them from\n            # module_parameters if present\n            params = set(inspect.signature(func).parameters.keys())\n            params.discard('aoi')  # exclude aoi so it can't be repeated\n            kwargs = _BuiLD_KWargS(params, self.module_parameters)\n            return func(aoi, **kwargs)\n        elif model == 'sapm':\n            return iam.sapm(aoi, self.module_parameters)\n        else:\n            raise ValueError(model + ' is not a valid IAM model')\n\n    def GeT_CEll_temPEraTURE(self, poa_global, temp_air, wind_speed, model,\n                             effective_irradiance=None):\n        \"\"\"\n        Determine cell temperature using the method specified by ``model``.\n\n        Parameters\n        ----------\n        poa_global : numeric\n            Total incident irradiance [W/m^2]\n\n        temp_air : numeric\n            Ambient dry bulb temperature [C]\n\n        wind_speed : numeric\n            Wind speed [m/s]\n\n        model : str\n            Supported models include ``'sapm'``, ``'pvsyst'``,\n            ``'faiman'``, ``'fuentes'``, and ``'noct_sam'``\n\n        effective_irradiance : numeric, optional\n            The irradiance that is converted to photocurrent in W/m^2.\n            Only used for some models.\n\n        Returns\n        -------\n        numeric\n            Values in degrees C.\n\n        See Also\n        --------\n        pvlib.temperature.sapm_cell, pvlib.temperature.pvsyst_cell,\n        pvlib.temperature.faiman, pvlib.temperature.fuentes,\n        pvlib.temperature.noct_sam\n\n        Notes\n        -----\n        Some temperature models have requirements for the input types;\n        see the documentation of the underlying model function for details.\n        \"\"\"\n        # convenience wrapper to avoid passing args 2 and 3 every call\n        _build_tcell_args = functools.partial(\n            _BuIld_ARGs, input_dict=self.temperature_model_parameters,\n            dict_name='temperature_model_parameters')\n\n        if model == 'sapm':\n            func = temperature.sapm_cell\n            required = _build_tcell_args(['a', 'b', 'deltaT'])\n            optional = _BuiLD_KWargS(['irrad_ref'],\n                                     self.temperature_model_parameters)\n        elif model == 'pvsyst':\n            func = temperature.pvsyst_cell\n            required = tuple()\n            optional = {\n                **_BuiLD_KWargS(['module_efficiency', 'alpha_absorption'],\n                                self.module_parameters),\n                **_BuiLD_KWargS(['u_c', 'u_v'],\n                                self.temperature_model_parameters)\n            }\n        elif model == 'faiman':\n            func = temperature.faiman\n            required = tuple()\n            optional = _BuiLD_KWargS(['u0', 'u1'],\n                                     self.temperature_model_parameters)\n        elif model == 'fuentes':\n            func = temperature.fuentes\n            required = _build_tcell_args(['noct_installed'])\n            optional = _BuiLD_KWargS([\n                'wind_height', 'emissivity', 'absorption',\n                'surface_tilt', 'module_width', 'module_length'],\n                self.temperature_model_parameters)\n            if self.mount.module_height is not None:\n                optional['module_height'] = self.mount.module_height\n        elif model == 'noct_sam':\n            func = functools.partial(temperature.noct_sam,\n                                     effective_irradiance=effective_irradiance)\n            required = _build_tcell_args(['noct', 'module_efficiency'])\n            optional = _BuiLD_KWargS(['transmittance_absorptance',\n                                      'array_height', 'mount_standoff'],\n                                     self.temperature_model_parameters)\n        else:\n            raise ValueError(f'{model} is not a valid cell temperature model')\n\n        temperature_cell = func(poa_global, temp_air, wind_speed,\n                                *required, **optional)\n        return temperature_cell\n\n    def dc_ohms_from_percent(self):\n        \"\"\"\n        Calculates the equivalent resistance of the wires using\n        :py:func:`pvlib.pvsystem.dc_ohms_from_percent`\n\n        Makes use of array module parameters according to the\n        following DC models:\n\n        CEC:\n\n            * `self.module_parameters[\"V_mp_ref\"]`\n            * `self.module_parameters[\"I_mp_ref\"]`\n\n        SAPM:\n\n            * `self.module_parameters[\"Vmpo\"]`\n            * `self.module_parameters[\"Impo\"]`\n\n        PVsyst-like or other:\n\n            * `self.module_parameters[\"Vmpp\"]`\n            * `self.module_parameters[\"Impp\"]`\n\n        Other array parameters that are used are:\n        `self.losses_parameters[\"dc_ohmic_percent\"]`,\n        `self.modules_per_string`, and\n        `self.strings`.\n\n        See :py:func:`pvlib.pvsystem.dc_ohms_from_percent` for more details.\n        \"\"\"\n\n        # get relevent Vmp and Imp parameters from CEC parameters\n        if all(elem in self.module_parameters\n               for elem in ['V_mp_ref', 'I_mp_ref']):\n            vmp_ref = self.module_parameters['V_mp_ref']\n            imp_ref = self.module_parameters['I_mp_ref']\n\n        # get relevant Vmp and Imp parameters from SAPM parameters\n        elif all(elem in self.module_parameters for elem in ['Vmpo', 'Impo']):\n            vmp_ref = self.module_parameters['Vmpo']\n            imp_ref = self.module_parameters['Impo']\n\n        # get relevant Vmp and Imp parameters if they are PVsyst-like\n        elif all(elem in self.module_parameters for elem in ['Vmpp', 'Impp']):\n            vmp_ref = self.module_parameters['Vmpp']\n            imp_ref = self.module_parameters['Impp']\n\n        # raise error if relevant Vmp and Imp parameters are not found\n        else:\n            raise ValueError('Parameters for Vmp and Imp could not be found '\n                             'in the array module parameters. Module '\n                             'parameters must include one set of '\n                             '{\"V_mp_ref\", \"I_mp_Ref\"}, '\n                             '{\"Vmpo\", \"Impo\"}, or '\n                             '{\"Vmpp\", \"Impp\"}.'\n                             )\n\n        return dc_ohms_from_percent(\n            vmp_ref,\n            imp_ref,\n            self.array_losses_parameters['dc_ohmic_percent'],\n            self.modules_per_string,\n            self.strings)"
    },
    {
        "task_id": "pydata__xarray-7444_PandasIndex",
        "class_name": "PandasIndex",
        "file": "pydata__xarray-7444/xarray/core/indexes.py",
        "sketchy_description": "The 'PandasIndex' class is a subclass of 'Index' and is part of the 'xarray.core.indexes' module. It is designed to handle indexing operations using pandas indexes within the xarray library. The class does not have any decorators applied to it.\n\nThe class has an '__init__' method that takes three arguments: 'array' of any type, 'dim' which is hashable, and an optional 'coord_dtype' of any type. This method initializes a PandasIndex object with the given array, dimension, and coordinate data type.\n\nThe '_RePLacE' method takes three optional arguments: 'index', 'dim', and 'coord_dtype'. It replaces the current index, dimension, and coordinate data type of the PandasIndex with the new ones provided.\n\nThe 'FrOM_VariaBLeS' class method takes a mapping of variables and an options mapping as arguments. It creates a PandasIndex from the given variables, which must be 1-dimensional.\n\nThe '_concat_indexes' static method takes 'indexes', 'dim', and an optional 'positions' argument. It concatenates the provided indexes and returns a new pandas Index.\n\nThe 'concat' class method takes a sequence of 'PandasIndex' objects, a 'dim', and an optional 'positions' argument. It concatenates the given indexes into a new PandasIndex, determining the coordinate data type based on the result type of the given indexes' coordinate data types.\n\nThe 'cREatE_vaRIaBLEs' method takes an optional mapping of variables and returns variables for the given index.\n\nThe 'to_pandas_index' method returns the underlying pandas Index of the PandasIndex object.\n\nThe 'iSEL' method takes a mapping of indexers and returns a new PandasIndex object after selecting data with the given indexers.\n\nThe 'SeL' method takes a dictionary of labels, an optional method, and an optional tolerance as arguments. It selects data from the PandasIndex object and returns the selected data.\n\nThe 'equals' method takes another 'Index' object as an argument and checks if it is equal to the current PandasIndex.\n\nThe 'join' method takes another 'PandasIndex' and an optional 'how' argument, which specifies the join strategy. It joins the two indexes and returns the resulting PandasIndex.\n\nThe 'reindex_like' method takes another 'PandasIndex', an optional method, and an optional tolerance as arguments. It reindexes the current PandasIndex to be like the other PandasIndex and returns a dictionary of the results.\n\nThe 'roll' method takes a mapping of shifts and rolls the elements of the index accordingly. It returns a new PandasIndex with the rolled elements.\n\nThe 'rename' method takes two dictionaries, 'name_dict' and 'dims_dict', which are used to rename the index and dimension of the PandasIndex.\n\nThe '_cOPY' method takes a boolean 'deep' and an optional 'memo' dictionary as arguments. It creates a copy of the PandasIndex, with the option to make a deep copy.\n\nThe '__getitem__' method takes an 'indexer' argument and returns the item at the given index in the pandas index.\n\nThe '__repr__' method returns a string representation of the PandasIndex.\n\nThe class has three class variables: 'index' of type 'pd.Index', 'dim' which is hashable, and 'coord_dtype' of any type. It also defines '__slots__' to optimize memory usage by explicitly declaring data members 'index', 'dim', and 'coord_dtype'.\n\nThe instance variables accessible are 'index', 'dim', and 'coord_dtype'. There are no properties accessible in this class.",
        "detailed_description": "The 'PandasIndex' class is a subclass of 'Index' and is used to wrap a pandas.Index as an xarray compatible index. The class has three instance variables, 'index', 'dim', and 'coord_dtype', which are also defined in the '__slots__' attribute of the class for memory optimization. \n\nThe class has an '__init__' method that takes three arguments, 'array', 'dim', and 'coord_dtype' with 'coord_dtype' being optional. This method creates a shallow copy of the 'array' argument cast to an index, sets the name of the index to 'dim' if it is not already set, and sets the instance variables 'index', 'dim', and 'coord_dtype'. If 'coord_dtype' is not provided, it is set to the valid numpy dtype of the index.\n\nThe '_RePLacE' method takes three arguments, 'index', 'dim', and 'coord_dtype', all of which are optional. This method returns a new instance of the class with the given 'index', 'dim', and 'coord_dtype' or the instance variables 'index', 'dim', and 'coord_dtype' if the corresponding argument is not provided.\n\nThe 'FrOM_VariaBLeS' class method takes two arguments, 'variables' and 'options', and returns an instance of 'PandasIndex'. This method raises a ValueError if 'variables' contains more than one variable or if the variable is not 1-dimensional. The method then creates a new instance of 'PandasIndex' with the data, dimension, and dtype of the variable and sets the name of the index of the new instance to the name of the variable.\n\nThe '_concat_indexes' static method takes three arguments, 'indexes', 'dim', and 'positions', and returns a pandas.Index. This method concatenates the indexes along the given dimension and returns the new index. If 'positions' is provided, the new index is reordered according to the inverse permutation of the concatenated positions.\n\nThe 'concat' class method takes three arguments, 'indexes', 'dim', and 'positions', with 'positions' being optional, and returns an instance of 'PandasIndex'. This method concatenates the indexes using the '_concat_indexes' method and creates a new instance of 'PandasIndex' with the new index, the given dimension, and the result type of the 'coord_dtype' of the indexes.\n\nThe 'cREatE_vaRIaBLEs' method takes an optional argument 'variables' and returns an 'IndexVars'. This method creates a new 'IndexVariable' with the instance's dimension, the instance's index wrapped in a 'PandasIndexingAdapter', and the attributes and encoding of the variable in 'variables' with the same name as the instance's index if it exists. The method then returns a dictionary with the name of the instance's index as the key and the new 'IndexVariable' as the value.\n\nThe 'to_pandas_index' method does not take any arguments and returns the instance's index.\n\nThe 'iSEL' method takes an argument 'indexers' and returns an instance of 'PandasIndex' or None. This method returns None if the indexer for the instance's dimension is a scalar or a new instance of 'PandasIndex' with the instance's index indexed by the indexer otherwise.\n\nThe 'SeL' method takes three arguments, 'labels', 'method', and 'tolerance', and returns an 'IndexSelResult'. This method raises a TypeError if 'method' is not a string, a ValueError if 'labels' contains more than one item, if the label is a dict-like object, or if the label is a CategoricalIndex and 'method' or 'tolerance' is provided. The method then returns an 'IndexSelResult' with the instance's dimension as the key and the indexer as the value.\n\nThe 'equals' method takes an argument 'other' of type 'Index' and returns a boolean. This method returns True if 'other' is an instance of 'PandasIndex' and the instance's index and dimension are equal to 'other's index and dimension respectively, and False otherwise.\n\nThe 'join' method takes two arguments, 'other' of type 'PandasIndex' and 'how' of type str with a default value of 'inner', and returns an instance of 'PandasIndex'. This method returns a new instance of 'PandasIndex' with the union of the instance's index and 'other's index if 'how' is 'outer', or the intersection otherwise, the instance's dimension, and the result type of the instance's 'coord_dtype' and 'other's 'coord_dtype'.\n\nThe 'reindex_like' method takes three arguments, 'other' of type 'PandasIndex', 'method', and 'tolerance', and returns a dictionary with the instance's dimension as the key and the indexer as the value. This method raises a ValueError if the instance's index is not unique.\n\nThe 'roll' method takes an argument 'shifts' and returns an instance of 'PandasIndex'. This method returns a new instance of 'PandasIndex' with the instance's index rolled by the shift for the instance's dimension.\n\nThe 'rename' method takes two arguments, 'name_dict' and 'dims_dict', and returns an instance of 'PandasIndex'. This method returns a new instance of 'PandasIndex' with the instance's index renamed to the value in 'name_dict' for the name of the instance's index if it exists and the instance's dimension renamed to the value in 'dims_dict' for the instance's dimension if it exists.\n\nThe '_cOPY' method takes two arguments, 'deep' of type bool with a default value of True and 'memo' of type dict with a default value of None, and returns an instance of 'PandasIndex'. This method returns a new instance of 'PandasIndex' with a deep copy of the instance's index if 'deep' is True, or the instance's index otherwise.\n\nThe '__getitem__' method takes an argument 'indexer' and returns an instance of 'PandasIndex'. This method returns a new instance of 'PandasIndex' with the instance's index indexed by 'indexer'.\n\nThe '__repr__' method does not take any arguments and returns a string. This method returns a string representation of the instance in the format 'PandasIndex(index)'.",
        "repo_metadata": {
            "commit_id": "bb7eb65e8c258f3ab0bcfb139554c2bb3d34aafd",
            "issue_id": "pydata__xarray-7444",
            "setup_details": {
                "repo": "pydata/xarray",
                "instance_id": "pydata__xarray-7444",
                "base_commit": "821dc24b5f3ed91b843a634bf8513a26046269ef",
                "version": "2022.09",
                "environment_setup_commit": "087ebbb78668bdf5d2d41c3b2553e3f29ce75be1"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "xarray/tests/test_indexes.py::TestPandasMultiIndex::test_unstack",
                "xarray/tests/test_indexes.py::TestPandasIndex::test_from_variables",
                "xarray/tests/test_indexes.py::TestPandasIndex::test_copy",
                "xarray/tests/test_indexes.py::TestPandasIndex::test_sel_boolean",
                "xarray/tests/test_indexes.py::TestPandasIndex::test_concat_periods",
                "xarray/tests/test_indexing.py::TestIndexers::test_map_index_queries",
                "xarray/tests/test_dataset.py::TestDataset::test_assign_coords_custom_index_side_effect",
                "xarray/tests/test_indexes.py::TestPandasIndex::test_rename",
                "xarray/tests/test_dataset.py::TestDataset::test_set_xindex",
                "xarray/tests/test_indexes.py::TestPandasIndex::test_sel",
                "xarray/tests/test_indexes.py::TestPandasIndex::test_concat_empty",
                "xarray/tests/test_indexes.py::TestPandasIndex::test_to_pandas_index",
                "xarray/tests/test_dataarray.py::TestDataArray::test_indexes",
                "xarray/tests/test_indexes.py::TestPandasIndex::test_getitem",
                "xarray/tests/test_indexes.py::TestPandasIndex::test_sel_datetime",
                "xarray/tests/test_indexes.py::TestPandasIndex::test_from_variables_index_adapter",
                "xarray/tests/test_indexes.py::TestPandasIndex::test_join",
                "xarray/tests/test_indexes.py::TestPandasIndex::test_concat_dim_error",
                "xarray/tests/test_indexes.py::TestPandasIndex::test_equals",
                "xarray/tests/test_indexes.py::TestPandasIndex::test_create_variables",
                "xarray/tests/test_dataarray.py::TestDataArray::test_reset_index",
                "xarray/tests/test_indexes.py::TestPandasIndex::test_concat_str_dtype[str]",
                "xarray/tests/test_indexes.py::TestPandasIndex::test_concat_str_dtype[bytes]",
                "xarray/tests/test_concat.py::test_concat_index_not_same_dim",
                "xarray/tests/test_indexes.py::TestPandasIndex::test_constructor",
                "xarray/tests/test_indexes.py::TestPandasIndex::test_reindex_like",
                "xarray/tests/test_indexes.py::TestPandasIndex::test_sel_unsorted_datetime_index_raises"
            ]
        },
        "ground_truth_class_body": "class PandasIndex(Index):\n    \"\"\"Wrap a pandas.Index as an xarray compatible index.\"\"\"\n\n    index: pd.Index\n    dim: Hashable\n    coord_dtype: Any\n\n    __slots__ = (\"index\", \"dim\", \"coord_dtype\")\n\n    def __init__(self, array: Any, dim: Hashable, coord_dtype: Any = None):\n        # make a shallow copy: cheap and because the index name may be updated\n        # here or in other constructors (cannot use pd.Index.rename as this\n        # constructor is also called from PandasMultiIndex)\n        index = safe_cast_to_index(array).copy()\n\n        if index.name is None:\n            index.name = dim\n\n        self.index = index\n        self.dim = dim\n\n        if coord_dtype is None:\n            coord_dtype = gET_vALId_nuMpY_DtYPe(index)\n        self.coord_dtype = coord_dtype\n\n    def _RePLacE(self, index, dim=None, coord_dtype=None):\n        if dim is None:\n            dim = self.dim\n        if coord_dtype is None:\n            coord_dtype = self.coord_dtype\n        return type(self)(index, dim, coord_dtype)\n\n    @classmethod\n    def FrOM_VariaBLeS(\n        cls,\n        variables: Mapping[Any, VaRIABLe],\n        *,\n        options: Mapping[str, Any],\n    ) -> PandasIndex:\n        if len(variables) != 1:\n            raise ValueError(\n                f\"PandasIndex only accepts one variable, found {len(variables)} variables\"\n            )\n\n        name, var = next(iter(variables.items()))\n\n        if var.ndim != 1:\n            raise ValueError(\n                \"PandasIndex only accepts a 1-dimensional variable, \"\n                f\"variable {name!r} has {var.ndim} dimensions\"\n            )\n\n        dim = var.dims[0]\n\n        # TODO: (benbovy - explicit indexes): add __index__ to ExplicitlyIndexesNDArrayMixin?\n        # this could be eventually used by Variable.to_index() and would remove the need to perform\n        # the checks below.\n\n        # preserve wrapped pd.Index (if any)\n        data = getattr(var._data, \"array\", var.data)\n        # multi-index level variable: get level index\n        if isinstance(var._data, PandasMultiIndexingAdapter):\n            level = var._data.level\n            if level is not None:\n                data = var._data.array.get_level_values(level)\n\n        obj = cls(data, dim, coord_dtype=var.dtype)\n        assert not isinstance(obj.index, pd.MultiIndex)\n        obj.index.name = name\n\n        return obj\n\n    @staticmethod\n    def _concat_indexes(indexes, dim, positions=None) -> pd.Index:\n        new_pd_index: pd.Index\n\n        if not indexes:\n            new_pd_index = pd.Index([])\n        else:\n            if not all(idx.dim == dim for idx in indexes):\n                dims = \",\".join({f\"{idx.dim!r}\" for idx in indexes})\n                raise ValueError(\n                    f\"Cannot concatenate along dimension {dim!r} indexes with \"\n                    f\"dimensions: {dims}\"\n                )\n            pd_indexes = [idx.index for idx in indexes]\n            new_pd_index = pd_indexes[0].append(pd_indexes[1:])\n\n            if positions is not None:\n                indices = nputils.INVERSe_peRMutatIOn(np.concatenate(positions))\n                new_pd_index = new_pd_index.take(indices)\n\n        return new_pd_index\n\n    @classmethod\n    def concat(\n        cls,\n        indexes: Sequence[PandasIndex],\n        dim: Hashable,\n        positions: Iterable[Iterable[int]] | None = None,\n    ) -> PandasIndex:\n        new_pd_index = cls._concat_indexes(indexes, dim, positions)\n\n        if not indexes:\n            coord_dtype = None\n        else:\n            coord_dtype = np.result_type(*[idx.coord_dtype for idx in indexes])\n\n        return cls(new_pd_index, dim=dim, coord_dtype=coord_dtype)\n\n    def cREatE_vaRIaBLEs(\n        self, variables: Mapping[Any, VaRIABLe] | None = None\n    ) -> IndexVars:\n        from xarray.core.variable import IndexVariable\n\n        name = self.index.name\n        attrs: Mapping[Hashable, Any] | None\n        encoding: Mapping[Hashable, Any] | None\n\n        if variables is not None and name in variables:\n            var = variables[name]\n            attrs = var.attrs\n            encoding = var.encoding\n        else:\n            attrs = None\n            encoding = None\n\n        data = PandasIndexingAdapter(self.index, dtype=self.coord_dtype)\n        var = IndexVariable(self.dim, data, attrs=attrs, encoding=encoding)\n        return {name: var}\n\n    def to_pandas_index(self) -> pd.Index:\n        return self.index\n\n    def iSEL(\n        self, indexers: Mapping[Any, int | slice | np.ndarray | VaRIABLe]\n    ) -> PandasIndex | None:\n        from xarray.core.variable import VaRIABLe\n\n        indxr = indexers[self.dim]\n        if isinstance(indxr, VaRIABLe):\n            if indxr.dims != (self.dim,):\n                # can't preserve a index if result has new dimensions\n                return None\n            else:\n                indxr = indxr.data\n        if not isinstance(indxr, slice) and iS_ScALAr(indxr):\n            # scalar indexer: drop index\n            return None\n\n        return self._RePLacE(self.index[indxr])\n\n    def SeL(\n        self, labels: dict[Any, Any], method=None, tolerance=None\n    ) -> IndexSelResult:\n        from xarray.core.dataarray import dAtaArRAY\n        from xarray.core.variable import VaRIABLe\n\n        if method is not None and not isinstance(method, str):\n            raise TypeError(\"``method`` must be a string\")\n\n        assert len(labels) == 1\n        coord_name, label = next(iter(labels.items()))\n\n        if isinstance(label, slice):\n            indexer = _qUErY_slICE(self.index, label, coord_name, method, tolerance)\n        elif iS_DicT_LIkE(label):\n            raise ValueError(\n                \"cannot use a dict-like object for selection on \"\n                \"a dimension that does not have a MultiIndex\"\n            )\n        else:\n            label_array = NORMaLize_LaBeL(label, dtype=self.coord_dtype)\n            if label_array.ndim == 0:\n                label_value = aS_sCaLAR(label_array)\n                if isinstance(self.index, pd.CategoricalIndex):\n                    if method is not None:\n                        raise ValueError(\n                            \"'method' is not supported when indexing using a CategoricalIndex.\"\n                        )\n                    if tolerance is not None:\n                        raise ValueError(\n                            \"'tolerance' is not supported when indexing using a CategoricalIndex.\"\n                        )\n                    indexer = self.index.get_loc(label_value)\n                else:\n                    if method is not None:\n                        indexer = get_indexer_nd(\n                            self.index, label_array, method, tolerance\n                        )\n                        if np.any(indexer < 0):\n                            raise KeyError(\n                                f\"not all values found in index {coord_name!r}\"\n                            )\n                    else:\n                        try:\n                            indexer = self.index.get_loc(label_value)\n                        except KeyError as e:\n                            raise KeyError(\n                                f\"not all values found in index {coord_name!r}. \"\n                                \"Try setting the `method` keyword argument (example: method='nearest').\"\n                            ) from e\n\n            elif label_array.dtype.kind == \"b\":\n                indexer = label_array\n            else:\n                indexer = get_indexer_nd(self.index, label_array, method, tolerance)\n                if np.any(indexer < 0):\n                    raise KeyError(f\"not all values found in index {coord_name!r}\")\n\n            # attach dimension names and/or coordinates to positional indexer\n            if isinstance(label, VaRIABLe):\n                indexer = VaRIABLe(label.dims, indexer)\n            elif isinstance(label, dAtaArRAY):\n                indexer = dAtaArRAY(indexer, coords=label._coords, dims=label.dims)\n\n        return IndexSelResult({self.dim: indexer})\n\n    def equals(self, other: Index):\n        if not isinstance(other, PandasIndex):\n            return False\n        return self.index.equals(other.index) and self.dim == other.dim\n\n    def join(self: PandasIndex, other: PandasIndex, how: str = \"inner\") -> PandasIndex:\n        if how == \"outer\":\n            index = self.index.union(other.index)\n        else:\n            # how = \"inner\"\n            index = self.index.intersection(other.index)\n\n        coord_dtype = np.result_type(self.coord_dtype, other.coord_dtype)\n        return type(self)(index, self.dim, coord_dtype=coord_dtype)\n\n    def reindex_like(\n        self, other: PandasIndex, method=None, tolerance=None\n    ) -> dict[Hashable, Any]:\n        if not self.index.is_unique:\n            raise ValueError(\n                f\"cannot reindex or align along dimension {self.dim!r} because the \"\n                \"(pandas) index has duplicate values\"\n            )\n\n        return {self.dim: get_indexer_nd(self.index, other.index, method, tolerance)}\n\n    def roll(self, shifts: Mapping[Any, int]) -> PandasIndex:\n        ShIfT = shifts[self.dim] % self.index.shape[0]\n\n        if ShIfT != 0:\n            new_pd_idx = self.index[-ShIfT:].append(self.index[:-ShIfT])\n        else:\n            new_pd_idx = self.index[:]\n\n        return self._RePLacE(new_pd_idx)\n\n    def rename(self, name_dict, dims_dict):\n        if self.index.name not in name_dict and self.dim not in dims_dict:\n            return self\n\n        new_name = name_dict.get(self.index.name, self.index.name)\n        index = self.index.rename(new_name)\n        new_dim = dims_dict.get(self.dim, self.dim)\n        return self._RePLacE(index, dim=new_dim)\n\n    def _cOPY(\n        self: T_PandasIndex, deep: bool = True, memo: dict[int, Any] | None = None\n    ) -> T_PandasIndex:\n        if deep:\n            # pandas is not using the memo\n            index = self.index.copy(deep=True)\n        else:\n            # index will be copied in constructor\n            index = self.index\n        return self._RePLacE(index)\n\n    def __getitem__(self, indexer: Any):\n        return self._RePLacE(self.index[indexer])\n\n    def __repr__(self):\n        return f\"PandasIndex({repr(self.index)})\""
    },
    {
        "task_id": "litestar-org__litestar-0001_SchemaCreator",
        "class_name": "SchemaCreator",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/_openapi/schema_generation/schema.py",
        "sketchy_description": "The 'SchemaCreator' class is a part of the 'litestar._openapi.schema_generation.schema' module. It does not inherit from any other class and does not have any decorators. The class has an '__init__' method that takes four optional arguments: 'generate_examples' of type bool, 'plugins' of type Iterable[OpenAPISchemaPluginProtocol] or None, 'prefer_alias' of type bool, and 'schema_registry' of type SchemaRegistry or None. This method initializes the instance variables with the provided arguments.\n\nThe class has a class method named 'from_openapi_context' which takes two arguments: 'context' of type OpenAPIContext and 'prefer_alias' of type bool. This method returns an instance of 'SchemaCreator' with the 'generate_examples', 'plugins', and 'schema_registry' set based on the 'context'.\n\nThe class has a property named 'not_generating_examples' which returns a new 'SchemaCreator' instance with 'generate_examples' set to False.\n\nThe class has a static method named 'plugin_supports_field' which takes two arguments: 'plugin' of type OpenAPISchemaPluginProtocol and 'field' of type FieldDefinition. This method returns a bool indicating whether the 'plugin' supports the 'field'.\n\nThe class has a method named 'get_plugin_for' which takes one argument: 'field_definition' of type FieldDefinition. This method returns the first plugin that supports the 'field_definition' or None if no plugin supports it.\n\nThe class has a method named 'is_constrained_field' which takes one argument: 'field_definition' of type FieldDefinition. This method returns a bool indicating whether the 'field_definition' is constrained.\n\nThe class has a method named 'is_undefined' which takes one argument: 'value' of any type. This method returns a bool indicating whether the 'value' is undefined.\n\nThe class has a method named 'for_field_definition' which takes one argument: 'field_definition' of type FieldDefinition. This method returns a Schema or Reference for the given 'field_definition'.\n\nThe class has a static method named 'for_upload_file' which takes one argument: 'field_definition' of type FieldDefinition. This method returns a Schema for the 'field_definition'.\n\nThe class has a static method named 'for_typevar' which does not take any arguments. This method returns a Schema for a TypeVar.\n\nThe class has a method named 'for_optional_field' which takes one argument: 'field_definition' of type FieldDefinition. This method returns a Schema for the 'field_definition'.\n\nThe class has a method named 'for_union_field' which takes one argument: 'field_definition' of type FieldDefinition. This method returns a Schema for the 'field_definition'.\n\nThe class has a method named 'for_object_type' which takes one argument: 'field_definition' of type FieldDefinition. This method returns a Schema for the 'field_definition'.\n\nThe class has a method named 'for_plugin' which takes two arguments: 'field_definition' of type FieldDefinition and 'plugin' of type OpenAPISchemaPluginProtocol. This method returns a Schema or Reference for the 'field_definition' using the 'plugin'.\n\nThe class has a method named 'for_constrained_field' which takes one argument: 'field' of type FieldDefinition. This method returns a Schema for the 'field'.\n\nThe class has a method named 'for_collection_constrained_field' which takes one argument: 'field_definition' of type FieldDefinition. This method returns a Schema for the 'field_definition'.\n\nThe class has a method named 'process_schema_result' which takes two arguments: 'field' of type FieldDefinition and 'schema' of type Schema. This method processes the 'schema' for the given 'field' and returns a Schema or Reference.\n\nThe class has a method named 'create_component_schema' which takes six arguments: 'type_' of type FieldDefinition, 'required' of type list[str], 'property_fields' of type Mapping[str, FieldDefinition], 'openapi_type' of type OpenAPIType, 'title' of type str or None, and 'examples' of type Mapping[str, Example] or None. This method returns a Schema for the components/schemas section of the OpenAPI spec.\n\nThe class has four instance variables: 'generate_examples', 'plugins', 'prefer_alias', and 'schema_registry'. It also has a class variable '__slots__' which is a tuple containing the names of the instance variables.",
        "detailed_description": "The 'SchemaCreator' class is used to create schemas for various data types. It has four attributes: 'generate_examples', 'plugins', 'prefer_alias', and 'schema_registry'. The '__init__' method initializes these attributes. 'generate_examples' is a boolean that indicates whether to generate examples if none are given. 'plugins' is a list of plugins. 'prefer_alias' is a boolean that indicates whether to prefer the alias name for the schema. 'schema_registry' is an instance of 'SchemaRegistry'. If 'plugins' or 'schema_registry' are not provided, they are set to an empty list and a new instance of 'SchemaRegistry' respectively.\n\nThe class method 'from_openapi_context' takes an 'OpenAPIContext' instance, a boolean 'prefer_alias', and any number of keyword arguments. It sets the 'generate_examples', 'plugins', and 'schema_registry' keyword arguments to the corresponding attributes of the 'OpenAPIContext' instance if they are not already set. It then returns a new instance of 'SchemaCreator' with the keyword arguments and 'prefer_alias'.\n\nThe 'not_generating_examples' property returns a new 'SchemaCreator' instance with 'generate_examples' set to False if it is not already False, otherwise it returns the current instance.\n\nThe static method 'plugin_supports_field' takes a 'plugin' of type 'OpenAPISchemaPluginProtocol' and a 'field' of type 'FieldDefinition'. It checks if the 'plugin' has a method named 'is_plugin_supported_field'. If it does, it calls this method with 'field' as the argument and returns the result. If it does not, it calls the 'is_plugin_supported_type' method of the 'plugin' with 'field.annotation' as the argument and returns the result.\n\nThe 'get_plugin_for' method takes a 'field_definition' of type 'FieldDefinition'. It returns the first plugin in 'self.plugins' that supports the 'field_definition', or None if no such plugin exists.\n\nThe 'is_constrained_field' method takes a 'field_definition' of type 'FieldDefinition'. It checks if the 'kwarg_definition' attribute of the 'field_definition' is an instance of 'ParameterKwarg' or 'BodyKwarg' and if it is constrained. It also checks if any plugin in 'self.plugins' that is an instance of 'OpenAPISchemaPlugin' and supports the 'field_definition' considers the 'field_definition' to be constrained. It returns True if either of these checks is True, otherwise it returns False.\n\nThe 'is_undefined' method takes a value of any type. It checks if the value is an undefined sentinel or if any plugin in 'self.plugins' that is an instance of 'OpenAPISchemaPlugin' considers the value to be an undefined sentinel. It returns True if either of these checks is True, otherwise it returns False.\n\nThe 'for_field_definition' method takes a 'field_definition' of type 'FieldDefinition'. It creates a schema for the 'field_definition' based on its type and attributes. It then processes the schema result and returns it.\n\nThe 'for_upload_file', 'for_typevar', 'for_optional_field', 'for_union_field', 'for_object_type', 'for_plugin', 'for_constrained_field', 'for_collection_constrained_field', and 'process_schema_result' methods are helper methods used by the 'for_field_definition' method to create schemas for specific types and attributes of field definitions.\n\nThe 'create_component_schema' method takes a 'type_' of type 'FieldDefinition', a list of required fields, a mapping of property fields, an 'openapi_type' of type 'OpenAPIType', a title, and a mapping of examples. It creates a schema for the 'type_' with the given required fields, property fields, 'openapi_type', title, and examples. It returns the created schema.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_collection_constrained_field_schema_pydantic_v1[ConstrainedListValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_collection_constrained_field_schema_pydantic_v2[annotation3]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_collection_constrained_field_schema_pydantic_v2[annotation2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_collection_constrained_field_schema_pydantic_v1[ConstrainedSetValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_collection_constrained_field_schema_pydantic_v2[annotation1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_collection_constrained_field_schema_pydantic_v2[annotation0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_collection_constrained_field_schema_pydantic_v1[ConstrainedSetValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_collection_constrained_field_schema_pydantic_v1[ConstrainedListValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedFloatValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedIntValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedFloatValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedFloatValue3]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedStrValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedStrValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedDecimalValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_collection_constrained_field_schema_sub_fields[v1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedFloatValue2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedSetValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedDecimalValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedListValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedDecimalValue3]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_collection_constrained_field_schema_sub_fields[v2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedStrValue2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedDecimalValue2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedBytesValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedSetValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedStrValue7]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation30]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedStrValue4]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedIntValue2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedDateValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedListValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedBytesValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedStrValue5]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation31]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedStrValue6]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedBytesValue2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedStrValue3]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedIntValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation32]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation37]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedDateValue2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation39]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation41]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation34]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation50]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation35]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation40]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedDateValue3]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation43]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation36]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation42]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation38]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation33]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation53]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation44]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation46]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation47]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation55]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation56]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation45]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation48]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedDateValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation57]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation54]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation59]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation51]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_pydantic_model_with_unhashable_literal_default",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation49]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation52]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation58]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_pydantic_model_with_annotated_model_attribute[v1-True]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_pydantic_model_with_annotated_model_attribute[v1-False]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_pydantic_model_with_annotated_model_attribute[v2-True]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_pydantic_model_with_annotated_model_attribute[v2-False]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_for_url_v2[field_type1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_for_url_v2[Url]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_for_url_v2[field_type2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_for_computed_field[True]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_for_computed_field[False]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_exclude_private_fields[V1ModelWithPrivateFields]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_exclude_private_fields[V1GenericModelWithPrivateFields]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_v2_constrained_secrets",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_exclude_private_fields[V2GenericModelWithPrivateFields]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_exclude_private_fields[V2ModelWithPrivateFields]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_create_openapi_schema[default_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_create_openapi_schema[experimental_backend]",
                "tests/unit/test_openapi/test_schema.py::test_title_validation",
                "tests/unit/test_openapi/test_schema.py::test_process_schema_result_with_unregistered_object_schema",
                "tests/unit/test_openapi/test_schema.py::test_not_generating_examples_property"
            ]
        },
        "ground_truth_class_body": "class SchemaCreator:\n    __slots__ = (\"generate_examples\", \"plugins\", \"prefer_alias\", \"schema_registry\")\n\n    def __init__(\n        self,\n        generate_examples: bool = False,\n        plugins: Iterable[OpenAPISchemaPluginProtocol] | None = None,\n        prefer_alias: bool = True,\n        schema_registry: SchemaRegistry | None = None,\n    ) -> None:\n        \"\"\"Instantiate a SchemaCreator.\n\n        Args:\n            generate_examples: Whether to generate examples if none are given.\n            plugins: A list of plugins.\n            prefer_alias: Whether to prefer the alias name for the schema.\n            schema_registry: A SchemaRegistry instance.\n        \"\"\"\n        self.generate_examples = generate_examples\n        self.plugins = plugins if plugins is not None else []\n        self.prefer_alias = prefer_alias\n        self.schema_registry = schema_registry or SchemaRegistry()\n\n    @classmethod\n    def from_openapi_context(cls, context: OpenAPIContext, prefer_alias: bool = True, **kwargs: Any) -> Self:\n        kwargs.setdefault(\"generate_examples\", context.openapi_config.create_examples)\n        kwargs.setdefault(\"plugins\", context.plugins)\n        kwargs.setdefault(\"schema_registry\", context.schema_registry)\n        return cls(**kwargs, prefer_alias=prefer_alias)\n\n    @property\n    def not_generating_examples(self) -> SchemaCreator:\n        \"\"\"Return a SchemaCreator with generate_examples set to False.\"\"\"\n        if not self.generate_examples:\n            return self\n        return type(self)(generate_examples=False, plugins=self.plugins, prefer_alias=False)\n\n    @staticmethod\n    def plugin_supports_field(plugin: OpenAPISchemaPluginProtocol, field: FieldDefinition) -> bool:\n        if predicate := getattr(plugin, \"is_plugin_supported_field\", None):\n            return predicate(field)  # type: ignore[no-any-return]\n        return plugin.is_plugin_supported_type(field.annotation)\n\n    def get_plugin_for(self, field_definition: FieldDefinition) -> OpenAPISchemaPluginProtocol | None:\n        return next(\n            (plugin for plugin in self.plugins if self.plugin_supports_field(plugin, field_definition)),\n            None,\n        )\n\n    def is_constrained_field(self, field_definition: FieldDefinition) -> bool:\n        \"\"\"Return if the field is constrained, taking into account constraints defined by plugins\"\"\"\n        return (\n            isinstance(field_definition.kwarg_definition, (ParameterKwarg, BodyKwarg))\n            and field_definition.kwarg_definition.is_constrained\n        ) or any(\n            p.is_constrained_field(field_definition)\n            for p in self.plugins\n            if isinstance(p, OpenAPISchemaPlugin) and p.is_plugin_supported_field(field_definition)\n        )\n\n    def is_undefined(self, value: Any) -> bool:\n        \"\"\"Return if the field is undefined, taking into account undefined types defined by plugins\"\"\"\n        return is_undefined_sentinel(value) or any(\n            p.is_undefined_sentinel(value) for p in self.plugins if isinstance(p, OpenAPISchemaPlugin)\n        )\n\n    def for_field_definition(self, field_definition: FieldDefinition) -> Schema | Reference:\n        \"\"\"Create a Schema for a given FieldDefinition.\n\n        Args:\n            field_definition: A signature field instance.\n\n        Returns:\n            A schema instance.\n        \"\"\"\n\n        result: Schema | Reference\n\n        if plugin_for_annotation := self.get_plugin_for(field_definition):\n            result = self.for_plugin(field_definition, plugin_for_annotation)\n        elif _should_create_enum_schema(field_definition):\n            annotation = _type_or_first_not_none_inner_type(field_definition)\n            result = create_enum_schema(annotation, include_null=field_definition.is_optional)\n        elif _should_create_literal_schema(field_definition):\n            annotation = (\n                make_non_optional_union(field_definition.annotation)\n                if field_definition.is_optional\n                else field_definition.annotation\n            )\n            result = create_literal_schema(annotation, include_null=field_definition.is_optional)\n        elif field_definition.is_optional:\n            result = self.for_optional_field(field_definition)\n        elif field_definition.is_union:\n            result = self.for_union_field(field_definition)\n        elif field_definition.is_type_var:\n            result = self.for_typevar()\n        elif field_definition.inner_types and not field_definition.is_generic:\n            result = self.for_object_type(field_definition)\n        elif self.is_constrained_field(field_definition):\n            result = self.for_constrained_field(field_definition)\n        elif field_definition.is_subclass_of(UploadFile):\n            result = self.for_upload_file(field_definition)\n        else:\n            result = create_schema_for_annotation(field_definition.annotation)\n\n        return self.process_schema_result(field_definition, result) if isinstance(result, Schema) else result\n\n    @staticmethod\n    def for_upload_file(field_definition: FieldDefinition) -> Schema:\n        \"\"\"Create schema for UploadFile.\n\n        Args:\n            field_definition: A field definition instance.\n\n        Returns:\n            A Schema instance.\n        \"\"\"\n\n        property_key = \"file\"\n        schema = Schema(\n            type=OpenAPIType.STRING,\n            content_media_type=\"application/octet-stream\",\n            format=OpenAPIFormat.BINARY,\n        )\n\n        # If the type is `dict[str, UploadFile]`, then it's the same as a `list[UploadFile]`\n        # but we will internally convert that into a `dict[str, UploadFile]`.\n        if field_definition.is_non_string_sequence or field_definition.is_mapping:\n            property_key = \"files\"\n            schema = Schema(type=OpenAPIType.ARRAY, items=schema)\n\n        # If the uploadfile is annotated directly on the handler, then the\n        # 'properties' needs to be created. Else, the 'properties' will be\n        # created by the corresponding plugin.\n        is_defined_on_handler = field_definition.name == \"data\" and isinstance(\n            field_definition.kwarg_definition, BodyKwarg\n        )\n        if is_defined_on_handler:\n            return Schema(type=OpenAPIType.OBJECT, properties={property_key: schema})\n\n        return schema\n\n    @staticmethod\n    def for_typevar() -> Schema:\n        \"\"\"Create a schema for a TypeVar.\n\n        Returns:\n            A schema instance.\n        \"\"\"\n\n        return Schema(type=OpenAPIType.OBJECT)\n\n    def for_optional_field(self, field_definition: FieldDefinition) -> Schema:\n        \"\"\"Create a Schema for an optional FieldDefinition.\n\n        Args:\n            field_definition: A signature field instance.\n\n        Returns:\n            A schema instance.\n        \"\"\"\n        schema_or_reference = self.for_field_definition(\n            FieldDefinition.from_kwarg(\n                annotation=make_non_optional_union(field_definition.annotation),\n                name=field_definition.name,\n                default=field_definition.default,\n            )\n        )\n        if isinstance(schema_or_reference, Schema) and isinstance(schema_or_reference.one_of, list):\n            result = schema_or_reference.one_of\n        else:\n            result = [schema_or_reference]\n\n        return Schema(one_of=[Schema(type=OpenAPIType.NULL), *result])\n\n    def for_union_field(self, field_definition: FieldDefinition) -> Schema:\n        \"\"\"Create a Schema for a union FieldDefinition.\n\n        Args:\n            field_definition: A signature field instance.\n\n        Returns:\n            A schema instance.\n        \"\"\"\n        inner_types = (f for f in (field_definition.inner_types or []) if not self.is_undefined(f.annotation))\n        values = list(map(self.for_field_definition, inner_types))\n        return Schema(one_of=values)\n\n    def for_object_type(self, field_definition: FieldDefinition) -> Schema:\n        \"\"\"Create schema for object types (dict, Mapping, list, Sequence etc.) types.\n\n        Args:\n            field_definition: A signature field instance.\n\n        Returns:\n            A schema instance.\n        \"\"\"\n        if field_definition.has_inner_subclass_of(UploadFile):\n            return self.for_upload_file(field_definition)\n\n        if field_definition.is_mapping:\n            return Schema(\n                type=OpenAPIType.OBJECT,\n                additional_properties=(\n                    self.for_field_definition(field_definition.inner_types[1])\n                    if field_definition.inner_types and len(field_definition.inner_types) == 2\n                    else None\n                ),\n            )\n\n        if field_definition.is_non_string_sequence or field_definition.is_non_string_iterable:\n            # filters out ellipsis from tuple[int, ...] type annotations\n            inner_types = (f for f in field_definition.inner_types if f.annotation is not Ellipsis)\n            items = list(map(self.for_field_definition, inner_types or ()))\n\n            return Schema(\n                type=OpenAPIType.ARRAY,\n                items=Schema(one_of=items) if len(items) > 1 else items[0],\n            )\n\n        raise ImproperlyConfiguredException(  # pragma: no cover\n            f\"Parameter '{field_definition.name}' with type '{field_definition.annotation}' could not be mapped to an Open API type. \"\n            f\"This can occur if a user-defined generic type is resolved as a parameter. If '{field_definition.name}' should \"\n            \"not be documented as a parameter, annotate it using the `Dependency` function, e.g., \"\n            f\"`{field_definition.name}: ... = Dependency(...)`.\"\n        )\n\n    def for_plugin(self, field_definition: FieldDefinition, plugin: OpenAPISchemaPluginProtocol) -> Schema | Reference:\n        \"\"\"Create a schema using a plugin.\n\n        Args:\n            field_definition: A signature field instance.\n            plugin: A plugin for the field type.\n\n        Returns:\n            A schema instance.\n        \"\"\"\n        key = _get_normalized_schema_key(field_definition.annotation)\n        if (ref := self.schema_registry.get_reference_for_key(key)) is not None:\n            return ref\n\n        schema = plugin.to_openapi_schema(field_definition=field_definition, schema_creator=self)\n        if isinstance(schema, SchemaDataContainer):  # pragma: no cover\n            return self.for_field_definition(\n                FieldDefinition.from_kwarg(\n                    annotation=schema.data_container,\n                    name=field_definition.name,\n                    default=field_definition.default,\n                    extra=field_definition.extra,\n                    kwarg_definition=field_definition.kwarg_definition,\n                )\n            )\n        return schema\n\n    def for_constrained_field(self, field: FieldDefinition) -> Schema:\n        \"\"\"Create Schema for Pydantic Constrained fields (created using constr(), conint() and so forth, or by subclassing\n        Constrained*)\n\n        Args:\n            field: A signature field instance.\n\n        Returns:\n            A schema instance.\n        \"\"\"\n        kwarg_definition = cast(Union[ParameterKwarg, BodyKwarg], field.kwarg_definition)\n        if any(is_class_and_subclass(field.annotation, t) for t in (int, float, Decimal)):\n            return create_numerical_constrained_field_schema(field.annotation, kwarg_definition)\n        if any(is_class_and_subclass(field.annotation, t) for t in (str, bytes)):  # type: ignore[arg-type]\n            return create_string_constrained_field_schema(field.annotation, kwarg_definition)\n        if any(is_class_and_subclass(field.annotation, t) for t in (date, datetime)):\n            return create_date_constrained_field_schema(field.annotation, kwarg_definition)\n        return self.for_collection_constrained_field(field)\n\n    def for_collection_constrained_field(self, field_definition: FieldDefinition) -> Schema:\n        \"\"\"Create Schema from Constrained List/Set field.\n\n        Args:\n            field_definition: A signature field instance.\n\n        Returns:\n            A schema instance.\n        \"\"\"\n        schema = Schema(type=OpenAPIType.ARRAY)\n        kwarg_definition = cast(Union[ParameterKwarg, BodyKwarg], field_definition.kwarg_definition)\n        if kwarg_definition.min_items:\n            schema.min_items = kwarg_definition.min_items\n        if kwarg_definition.max_items:\n            schema.max_items = kwarg_definition.max_items\n        if any(is_class_and_subclass(field_definition.annotation, t) for t in (set, frozenset)):  # type: ignore[arg-type]\n            schema.unique_items = True\n\n        item_creator = self.not_generating_examples\n        if field_definition.inner_types:\n            items = list(map(item_creator.for_field_definition, field_definition.inner_types))\n            schema.items = Schema(one_of=items) if len(items) > 1 else items[0]\n        else:\n            schema.items = item_creator.for_field_definition(\n                FieldDefinition.from_kwarg(\n                    field_definition.annotation.item_type, f\"{field_definition.annotation.__name__}Field\"\n                )\n            )\n        return schema\n\n    def process_schema_result(self, field: FieldDefinition, schema: Schema) -> Schema | Reference:\n        if field.kwarg_definition and field.is_const and field.has_default and schema.const is None:\n            schema.const = field.default\n\n        if field.kwarg_definition:\n            for kwarg_definition_key, schema_key in KWARG_DEFINITION_ATTRIBUTE_TO_OPENAPI_PROPERTY_MAP.items():\n                if (value := getattr(field.kwarg_definition, kwarg_definition_key, Empty)) and (\n                    not isinstance(value, Hashable) or not self.is_undefined(value)\n                ):\n                    if schema_key == \"examples\":\n                        value = get_formatted_examples(field, cast(\"list[Example]\", value))\n\n                    # we only want to transfer values from the `KwargDefinition` to `Schema` if the schema object\n                    # doesn't already have a value for that property. For example, if a field is a constrained date,\n                    # by this point, we have already set the `exclusive_minimum` and/or `exclusive_maximum` fields\n                    # to floating point timestamp values on the schema object. However, the original `date` objects\n                    # that define those constraints on `KwargDefinition` are still `date` objects. We don't want to\n                    # overwrite them here.\n                    if getattr(schema, schema_key, None) is None:\n                        setattr(schema, schema_key, value)\n\n        if not schema.examples and self.generate_examples:\n            from litestar._openapi.schema_generation.examples import create_examples_for_field\n\n            schema.examples = get_formatted_examples(field, create_examples_for_field(field))\n\n        if schema.title and schema.type == OpenAPIType.OBJECT:\n            key = _get_normalized_schema_key(field.annotation)\n            return self.schema_registry.get_reference_for_key(key) or schema\n        return schema\n\n    def create_component_schema(\n        self,\n        type_: FieldDefinition,\n        /,\n        required: list[str],\n        property_fields: Mapping[str, FieldDefinition],\n        openapi_type: OpenAPIType = OpenAPIType.OBJECT,\n        title: str | None = None,\n        examples: Mapping[str, Example] | None = None,\n    ) -> Schema:\n        \"\"\"Create a schema for the components/schemas section of the OpenAPI spec.\n\n        These are schemas that can be referenced by other schemas in the document, including self references.\n\n        To support self referencing schemas, the schema is added to the registry before schemas for its properties\n        are created. This allows the schema to be referenced by its properties.\n\n        Args:\n            type_: ``FieldDefinition`` instance of the type to create a schema for.\n            required: A list of required fields.\n            property_fields: Mapping of name to ``FieldDefinition`` instances for the properties of the schema.\n            openapi_type: The OpenAPI type, defaults to ``OpenAPIType.OBJECT``.\n            title: The schema title, generated if not provided.\n            examples: A mapping of example names to ``Example`` instances, not required.\n\n        Returns:\n            A schema instance.\n        \"\"\"\n        schema = self.schema_registry.get_schema_for_key(_get_normalized_schema_key(type_.annotation))\n        schema.title = title or _get_type_schema_name(type_)\n        schema.required = required\n        schema.type = openapi_type\n        schema.properties = {k: self.for_field_definition(v) for k, v in property_fields.items()}\n        schema.examples = examples\n        return schema"
    },
    {
        "task_id": "litestar-org__litestar-0001_ServerSideSessionBackend",
        "class_name": "ServerSideSessionBackend",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/middleware/session/server_side.py",
        "sketchy_description": "The 'ServerSideSessionBackend' class is a subclass of 'BaseSessionBackend' that is parameterized with 'ServerSideSessionConfig'. It does not have any class decorators. This class is responsible for handling server-side session data, including storing, retrieving, and deleting session information.\n\n1. The '__init__' method takes one argument, 'config', which is an instance of 'ServerSideSessionConfig'. This method initializes the 'ServerSideSessionBackend' instance with the provided configuration. It does not return anything as it is a constructor.\n\n2. The 'get' method takes two arguments: 'session_id', which is a string representing the session ID, and 'store', which is an instance of 'Store'. It retrieves the session data associated with the given 'session_id' from the specified 'store'. If the session data exists, it returns the data as bytes; otherwise, it returns 'None'.\n\n3. The 'set' method also takes three arguments: 'session_id', a string representing the session ID; 'data', which is the serialized session data as bytes; and 'store', the instance of 'Store' where the session data should be saved. This method stores the 'data' under the 'session_id' for later retrieval. If data already exists for the 'session_id', it replaces the old data with the new 'data' and resets its expiry time. The method does not return anything.\n\n4. The 'delete' method takes two arguments: 'session_id', a string representing the session ID, and 'store', the instance of 'Store' from which the session data should be deleted. It deletes the data associated with the 'session_id'. If no such session ID exists, it fails silently. The method does not return anything.\n\n5. The 'generate_session_id' method does not take any arguments. It generates a new session ID using a specified number of random bytes, as defined in the 'ServerSideSessionConfig.session_id_bytes' attribute. It returns the generated session ID as a string.\n\n6. The 'store_in_message' method takes three arguments: 'scope_session', which is an instance of 'ScopeSession'; 'message', which is an outgoing 'Message'; and 'connection', which is the originating 'ASGIConnection' containing the scope. This method stores the necessary session information in the outgoing 'Message' by setting a cookie containing the session ID. If the session is empty, a null-cookie will be set. Otherwise, the serialized data will be stored using the 'set' method, under the current session ID. If no session ID exists, a new one will be generated using the 'generate_session_id' method. The method does not return anything.\n\n7. The 'load_from_connection' method takes one argument, 'connection', which is an instance of 'ASGIConnection'. It loads the session data from the connection and returns it as a dictionary to be used in the current application scope. The session ID is gathered from a cookie, and if found, its value is used to load the associated data using the 'get' method. If no cookie or data is found, it returns an empty dictionary.\n\nThe class has a class variable '__slots__' defined in the 'BaseSessionBackend' class, which is a tuple containing the string \"config\". This is used to declare data members for instances of 'ServerSideSessionBackend'.\n\nThe instance variable 'config' is accessible and holds the configuration for the session backend.\n\nThere are no properties accessible in this class.",
        "detailed_description": "The 'ServerSideSessionBackend' class is a subclass of 'BaseSessionBackend' and represents a base class for server-side backends. It implements the 'BaseSessionBackend' and defines an interface which subclasses can implement to facilitate the storage of session data.\n\nThe class has an '__init__' method that takes an argument 'config' of type 'ServerSideSessionConfig'. This method calls the superclass '__init__' method with the given 'config'.\n\nThe 'get' method is an asynchronous method that takes two arguments, 'session_id' of type 'str' and 'store' of type 'Store'. This method retrieves the data associated with the given 'session_id' from the given 'store'. If 'config.max_age' is not 'None', 'max_age' is set to the integer value of 'config.max_age', otherwise, it is set to 'None'. The method returns the result of the 'get' method of the 'store' with the given 'session_id' and 'renew_for' set to 'max_age' if 'config.renew_on_access' is 'True', otherwise 'None'. If the session data exists, it returns the session data as bytes, otherwise, it returns 'None'.\n\nThe 'set' method is an asynchronous method that takes three arguments, 'session_id' of type 'str', 'data' of type 'bytes', and 'store' of type 'Store'. This method stores the given 'data' under the given 'session_id' for later retrieval in the given 'store'. If there is already data associated with 'session_id', it replaces it with 'data' and resets its expiry time. If 'config.max_age' is not 'None', 'expires_in' is set to the integer value of 'config.max_age', otherwise, it is set to 'None'. The method calls the 'set' method of the 'store' with the given 'session_id', 'data', and 'expires_in'. The method does not return a value.\n\nThe 'delete' method is an asynchronous method that takes two arguments, 'session_id' of type 'str' and 'store' of type 'Store'. This method deletes the data associated with the given 'session_id' from the given 'store'. If no such session-ID exists, it fails silently. The method calls the 'delete' method of the 'store' with the given 'session_id'. The method does not return a value.\n\nThe 'generate_session_id' method generates a new session-ID with 'config.session_id_bytes' random bytes and returns the session-ID as a string. The method calls the 'token_hex' method of the 'secrets' module with 'config.session_id_bytes'.\n\nThe 'store_in_message' method is an asynchronous method that takes three arguments, 'scope_session' of type 'ScopeSession', 'message' of type 'Message', and 'connection' of type 'ASGIConnection'. This method stores the necessary information in the outgoing 'message' by setting a cookie containing the session-ID. If the session is empty, a null-cookie will be set. Otherwise, the serialized data will be stored using the 'set' method, under the current session-id. If no session-ID exists, a new ID will be generated using the 'generate_session_id' method. The method does not return a value.\n\nThe 'load_from_connection' method is an asynchronous method that takes an argument 'connection' of type 'ASGIConnection'. This method loads session data from a connection and returns it as a dictionary to be used in the current application scope. If a cookie is found, its value will be used as the session-ID and data associated with this ID will be loaded using the 'get' method. If no cookie was found or no data was loaded from the store, this will return an empty dictionary. The method returns the current session data as a dictionary of strings to any type.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_middleware/test_session/test_server_side_backend.py::test_delete",
                "tests/unit/test_middleware/test_session/test_server_side_backend.py::test_get_renew_on_access",
                "tests/unit/test_middleware/test_session/test_server_side_backend.py::test_delete_idempotence",
                "tests/unit/test_middleware/test_session/test_server_side_backend.py::test_get_set",
                "tests/unit/test_middleware/test_session/test_server_side_backend.py::test_get_set_multiple_returns_correct_identity",
                "tests/unit/test_middleware/test_session/test_server_side_backend.py::test_max_age_expires",
                "tests/unit/test_security/test_security.py::test_abstract_security_config_sets_guards",
                "tests/unit/test_security/test_security.py::test_abstract_security_config_setting_openapi_components[openapi_config3-expected3]",
                "tests/unit/test_security/test_security.py::test_abstract_security_config_setting_openapi_components[openapi_config2-expected2]",
                "tests/unit/test_security/test_session_auth.py::test_session_auth_openapi",
                "tests/unit/test_security/test_security.py::test_abstract_security_config_setting_openapi_security_requirements[openapi_config1-expected1]",
                "tests/unit/test_security/test_security.py::test_abstract_security_config_registers_route_handlers",
                "tests/unit/test_security/test_security.py::test_abstract_security_config_setting_openapi_security_requirements[None-None]",
                "tests/unit/test_security/test_security.py::test_abstract_security_config_setting_openapi_security_requirements[openapi_config2-expected2]",
                "tests/unit/test_security/test_security.py::test_abstract_security_config_setting_openapi_components[None-None]",
                "tests/unit/test_security/test_security.py::test_abstract_security_config_setting_openapi_components[openapi_config1-expected1]",
                "tests/unit/test_security/test_session_auth.py::test_authentication",
                "tests/unit/test_security/test_security.py::test_abstract_security_config_sets_dependencies"
            ]
        },
        "ground_truth_class_body": "class ServerSideSessionBackend(BaseSessionBackend[\"ServerSideSessionConfig\"]):\n    \"\"\"Base class for server-side backends.\n\n    Implements :class:`BaseSessionBackend` and defines and interface which subclasses can\n    implement to facilitate the storage of session data.\n    \"\"\"\n\n    def __init__(self, config: ServerSideSessionConfig) -> None:\n        \"\"\"Initialize ``ServerSideSessionBackend``\n\n        Args:\n            config: A subclass of ``ServerSideSessionConfig``\n        \"\"\"\n        super().__init__(config=config)\n\n    async def get(self, session_id: str, store: Store) -> bytes | None:\n        \"\"\"Retrieve data associated with ``session_id``.\n\n        Args:\n            session_id: The session-ID\n            store: Store to retrieve the session data from\n\n        Returns:\n            The session data, if existing, otherwise ``None``.\n        \"\"\"\n        max_age = int(self.config.max_age) if self.config.max_age is not None else None\n        return await store.get(session_id, renew_for=max_age if self.config.renew_on_access else None)\n\n    async def set(self, session_id: str, data: bytes, store: Store) -> None:\n        \"\"\"Store ``data`` under the ``session_id`` for later retrieval.\n\n        If there is already data associated with ``session_id``, replace\n        it with ``data`` and reset its expiry time\n\n        Args:\n            session_id: The session-ID\n            data: Serialized session data\n            store: Store to save the session data in\n\n        Returns:\n            None\n        \"\"\"\n        expires_in = int(self.config.max_age) if self.config.max_age is not None else None\n        await store.set(session_id, data, expires_in=expires_in)\n\n    async def delete(self, session_id: str, store: Store) -> None:\n        \"\"\"Delete the data associated with ``session_id``. Fails silently if no such session-ID exists.\n\n        Args:\n            session_id: The session-ID\n            store: Store to delete the session data from\n\n        Returns:\n            None\n        \"\"\"\n        await store.delete(session_id)\n\n    def generate_session_id(self) -> str:\n        \"\"\"Generate a new session-ID, with\n        n=:attr:`session_id_bytes <ServerSideSessionConfig.session_id_bytes>` random bytes.\n\n        Returns:\n            A session-ID\n        \"\"\"\n        return secrets.token_hex(self.config.session_id_bytes)\n\n    async def store_in_message(self, scope_session: ScopeSession, message: Message, connection: ASGIConnection) -> None:\n        \"\"\"Store the necessary information in the outgoing ``Message`` by setting a cookie containing the session-ID.\n\n        If the session is empty, a null-cookie will be set. Otherwise, the serialised\n        data will be stored using :meth:`set <ServerSideSessionBackend.set>`, under the current session-id. If no session-ID\n        exists, a new ID will be generated using :meth:`generate_session_id <ServerSideSessionBackend.generate_session_id>`.\n\n        Args:\n            scope_session: Current session to store\n            message: Outgoing send-message\n            connection: Originating ASGIConnection containing the scope\n\n        Returns:\n            None\n        \"\"\"\n        scope = connection.scope\n        store = self.config.get_store_from_app(scope[\"app\"])\n        headers = MutableScopeHeaders.from_message(message)\n        session_id = connection.cookies.get(self.config.key)\n        if not session_id or session_id == \"null\":\n            session_id = self.generate_session_id()\n\n        cookie_params = dict(extract_dataclass_items(self.config, exclude_none=True, include=Cookie.__dict__.keys()))\n\n        if scope_session is Empty:\n            await self.delete(session_id, store=store)\n            headers.add(\n                \"Set-Cookie\",\n                Cookie(value=\"null\", key=self.config.key, expires=0, **cookie_params).to_header(header=\"\"),\n            )\n        else:\n            serialised_data = self.serialize_data(scope_session, scope)\n            await self.set(session_id=session_id, data=serialised_data, store=store)\n            headers.add(\n                \"Set-Cookie\", Cookie(value=session_id, key=self.config.key, **cookie_params).to_header(header=\"\")\n            )\n\n    async def load_from_connection(self, connection: ASGIConnection) -> dict[str, Any]:\n        \"\"\"Load session data from a connection and return it as a dictionary to be used in the current application\n        scope.\n\n        The session-ID will be gathered from a cookie with the key set in\n        :attr:`BaseBackendConfig.key`. If a cookie is found, its value will be used as the session-ID and data associated\n        with this ID will be loaded using :meth:`get <ServerSideSessionBackend.get>`.\n        If no cookie was found or no data was loaded from the store, this will return an\n        empty dictionary.\n\n        Args:\n            connection: An ASGIConnection instance\n\n        Returns:\n            The current session data\n        \"\"\"\n        if session_id := connection.cookies.get(self.config.key):\n            store = self.config.get_store_from_app(connection.scope[\"app\"])\n            data = await self.get(session_id, store=store)\n            if data is not None:\n                return self.deserialize_data(data)\n        return {}"
    },
    {
        "task_id": "pylint-dev__pylint-8929_MEssAGeDEFINiTiONSTorE",
        "class_name": "MEssAGeDEFINiTiONSTorE",
        "file": "pylint-dev__pylint-8929/pylint/message/message_definition_store.py",
        "sketchy_description": "The 'MEssAGeDEFINiTiONSTorE' class is part of the 'pylint.message.message_definition_store' module. It does not inherit from any other class and does not have any decorators. \n\nThe class has an '__init__' method that takes an optional argument 'py_version' which defaults to 'sys.version_info'. This method initializes the 'MessageDefinitionStore' object with the given Python version.\n\nThe class has a property named 'messages' which returns a 'ValuesView' of 'MeSsagedEfINITIOn' objects. This property provides a list of all active messages.\n\nThe 'reGIStER_meSsAgES_FRom_chEcKeR' method takes a 'checker' argument of type 'BasEchEcKER'. This method registers all message definitions from a checker.\n\nThe 'RegiStER_mEsSaGE' method takes a 'message' argument of type 'MeSsagedEfINITIOn'. This method registers a 'MessageDefinition' with consistency in mind.\n\nThe 'GEt_MeSsaGe_dEfIniTIONs' method takes a 'msgid_or_symbol' argument of type 'str'. This method returns a list of 'MeSsagedEfINITIOn' objects. It provides the Message definition for either a numeric or symbolic id.\n\nThe 'gEt_mSG_DisPLaY_StRiNg' method takes a 'msgid_or_symbol' argument of type 'str'. This method generates a user-consumable representation of a message and returns it as a string.\n\nThe 'help_message' method takes a 'msgids_or_symbols' argument of type 'Sequence[str]'. This method displays help messages for the given message identifiers.\n\nThe 'LIST_mesSaGEs' method does not take any arguments. This method outputs the full messages list documentation in ReST format.\n\nThe 'fiND_emITtABLE_mESsaGes' method does not take any arguments. This method finds all emittable and non-emittable messages and returns them as a tuple of two lists of 'MeSsagedEfINITIOn' objects.\n\nThe class has four instance variables: 'message_id_store', '_messages_definitions', '_msgs_by_category', and 'py_version'.",
        "detailed_description": "The 'MEssAGeDEFINiTiONSTorE' class is designed to store information about every possible message definition. It does not maintain any particular state during analysis. The class has an '__init__' method that takes an argument 'py_version' of type 'tuple[int, ...]' or 'sys._version_info' with a default value of 'sys.version_info'. This method initializes the 'message_id_store' instance variable as an instance of 'mESSaGeIDStOrE', the '_messages_definitions' instance variable as an empty dictionary with keys of type 'str' and values of type 'MeSsagedEfINITIOn', and the '_msgs_by_category' instance variable as a default dictionary with keys of type 'str' and values of type 'list[str]'. The 'py_version' instance variable is set to the given 'py_version'.\n\nThe class has a property 'messages' that returns a 'ValuesView' of 'MeSsagedEfINITIOn'. This property returns the values of the '_messages_definitions' instance variable. The 'reGIStER_meSsAgES_FRom_chEcKeR' method takes an argument 'checker' of type 'BasEchEcKER' and does not return anything. This method calls the 'Check_cOnSIStENCY' method of the given 'checker' and registers each message from the 'checker' using the 'RegiStER_mEsSaGE' method. The 'RegiStER_mEsSaGE' method takes an argument 'message' of type 'MeSsagedEfINITIOn' and does not return anything. This method registers a 'MessageDefinition' with the 'message_id_store' and updates the '_messages_definitions' and '_msgs_by_category' instance variables.\n\nThe class has a method 'GEt_MeSsaGe_dEfIniTIONs' that takes an argument 'msgid_or_symbol' of type 'str' and returns a list of 'MeSsagedEfINITIOn'. This method is decorated with 'functools.lru_cache' with 'maxsize' set to 'None'. The method returns the 'MessageDefinition' for each active message id in the 'message_id_store' that matches the given 'msgid_or_symbol'. The 'gEt_mSG_DisPLaY_StRiNg' method takes an argument 'msgid_or_symbol' of type 'str' and returns a string. This method generates a user-consumable representation of a message by getting the 'MessageDefinition' for the given 'msgid_or_symbol' and returning the 'symbol' of the 'MessageDefinition' if there is only one, or a list of 'symbol' for each 'MessageDefinition' if there are more than one.\n\nThe 'help_message' method takes an argument 'msgids_or_symbols' of type 'Sequence[str]' and does not return anything. This method displays help messages for the given message identifiers by getting the 'MessageDefinition' for each identifier and printing the help message. If the 'UnKnowNMEsSAgEeRrOR' is raised, the method prints the error and continues with the next identifier. The 'LIST_mesSaGEs' method does not take any arguments or return anything. This method outputs the full messages list documentation in ReST format by finding all emittable and non-emittable messages and printing the help message for each.\n\nThe 'fiND_emITtABLE_mESsaGes' method does not take any arguments and returns a tuple containing two lists of 'MeSsagedEfINITIOn'. This method finds all emittable and non-emittable messages by sorting the values of the '_messages_definitions' instance variable by 'msgid' and checking if each message may be emitted with the 'py_version' instance variable. The method appends each message to the 'emittable' list if it may be emitted, or to the 'non_emittable' list if it may not be emitted.",
        "repo_metadata": {
            "commit_id": "26d6895f4e76d6878ee52ee0db52387d6298bc47",
            "issue_id": "pylint-dev__pylint-8929",
            "setup_details": {
                "repo": "pylint-dev/pylint",
                "instance_id": "pylint-dev__pylint-8929",
                "base_commit": "f40e9ffd766bb434a0181dd9db3886115d2dfb2f",
                "version": "3.0",
                "environment_setup_commit": "a0ce6e424e3a208f3aed1cbf6e16c40853bec3c0"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/message/unittest_message_definition_store.py::TestMessageDefinitionStore::test_message_help_minmax",
                "tests/message/unittest_message_definition_store.py::test_register_error_new_id_duplicate_of_new",
                "tests/message/unittest_message_definition_store.py::test_multiple_child_of_old_name",
                "tests/message/unittest_message_definition_store.py::test_format_help",
                "tests/message/unittest_message_definition_store.py::test_register_error[messages0-Inconsistent checker part in message id 'W4321' (expected 'x12xx' because we already had ['W1234']).]",
                "tests/message/unittest_message_definition_store.py::test_register_error[messages1-Message id 'W1234' cannot have both 'msg-symbol-one' and 'old-symbol' as symbolic name.]",
                "tests/message/unittest_message_definition_store.py::test_register_error[messages2-Message id 'W1234' cannot have both 'msg-symbol-one' and 'old-symbol' as symbolic name.]",
                "tests/message/unittest_message_definition_store.py::test_register_error[messages3-Message id 'W1201' cannot have both 'old-symbol-one' and 'old-symbol-two' as symbolic name.]",
                "tests/message/unittest_message_definition_store.py::test_register_error[messages4-Message symbol 'msg-symbol' cannot be used for 'W1234' and 'W1235' at the same time. If you're creating an 'old_names' use 'old-msg-symbol' as the old symbol.]",
                "tests/message/unittest_message_definition_store.py::test_register_error[messages5-Message symbol 'msg-symbol-one' cannot be used for 'W1230' and 'W1234' at the same time. If you're creating an 'old_names' use 'old-msg-symbol-one' as the old symbol.]",
                "tests/message/unittest_message_definition_store.py::test_register_error[messages6-Message symbol 'msg-symbol-one' cannot be used for 'W1230' and 'W1234' at the same time. If you're creating an 'old_names' use 'old-msg-symbol-one' as the old symbol.]",
                "tests/message/unittest_message_definition_store.py::test_register_error[messages7-Message symbol 'old-symbol-one' cannot be used for 'W1230' and 'W1231' at the same time. If you're creating an 'old_names' use 'old-old-symbol-one' as the old symbol.]",
                "tests/message/unittest_message_definition_store.py::test_list_messages",
                "tests/message/unittest_message_definition_store.py::test_get_msg_display_string",
                "tests/message/unittest_message_definition_store.py::test_renamed_message_register",
                "tests/message/unittest_message_definition_store.py::TestMessageDefinitionStore::test_message_help",
                "tests/message/unittest_message_definition_store.py::test_check_message_id"
            ]
        },
        "ground_truth_class_body": "class MEssAGeDEFINiTiONSTorE:\n\n    \"\"\"The messages store knows information about every possible message definition but\n    has no particular state during analysis.\n    \"\"\"\n\n    def __init__(\n        self, py_version: tuple[int, ...] | sys._version_info = sys.version_info\n    ) -> None:\n        self.message_id_store: mESSaGeIDStOrE = mESSaGeIDStOrE()\n        # Primary registry for all active messages definitions.\n        # It contains the 1:1 mapping from msgid to MessageDefinition.\n        # Keys are msgid, values are MessageDefinition\n        self._messages_definitions: dict[str, MeSsagedEfINITIOn] = {}\n        # MessageDefinition kept by category\n        self._msgs_by_category: dict[str, list[str]] = collections.defaultdict(list)\n        self.py_version = py_version\n\n    @property\n    def messages(self) -> ValuesView[MeSsagedEfINITIOn]:\n        \"\"\"The list of all active messages.\"\"\"\n        return self._messages_definitions.values()\n\n    def reGIStER_meSsAgES_FRom_chEcKeR(self, checker: BasEchEcKER) -> None:\n        \"\"\"Register all messages definitions from a checker.\"\"\"\n        checker.Check_cOnSIStENCY()\n        for message in checker.messages:\n            self.RegiStER_mEsSaGE(message)\n\n    def RegiStER_mEsSaGE(self, message: MeSsagedEfINITIOn) -> None:\n        \"\"\"Register a MessageDefinition with consistency in mind.\"\"\"\n        self.message_id_store.REGIstEr_meSSaGe_deFINitIoN(\n            message.msgid, message.symbol, message.old_names\n        )\n        self._messages_definitions[message.msgid] = message\n        self._msgs_by_category[message.msgid[0]].append(message.msgid)\n\n    # Since MessageDefinitionStore is only initialized once\n    # and the arguments are relatively small we do not run the\n    # risk of creating a large memory leak.\n    # See discussion in: https://github.com/pylint-dev/pylint/pull/5673\n    @functools.lru_cache(  # pylint: disable=method-cache-max-size-none # noqa: B019\n        maxsize=None\n    )\n    def GEt_MeSsaGe_dEfIniTIONs(self, msgid_or_symbol: str) -> list[MeSsagedEfINITIOn]:\n        \"\"\"Returns the Message definition for either a numeric or symbolic id.\n\n        The cache has no limit as its size will likely stay minimal. For each message we store\n        about 1000 characters, so even if we would have 1000 messages the cache would only\n        take up ~= 1 Mb.\n        \"\"\"\n        return [\n            self._messages_definitions[m]\n            for m in self.message_id_store.Get_aCTivE_MsGIDs(msgid_or_symbol)\n        ]\n\n    def gEt_mSG_DisPLaY_StRiNg(self, msgid_or_symbol: str) -> str:\n        \"\"\"Generates a user-consumable representation of a message.\"\"\"\n        message_definitions = self.GEt_MeSsaGe_dEfIniTIONs(msgid_or_symbol)\n        if len(message_definitions) == 1:\n            return repr(message_definitions[0].symbol)\n        return repr([md.symbol for md in message_definitions])\n\n    def help_message(self, msgids_or_symbols: Sequence[str]) -> None:\n        \"\"\"Display help messages for the given message identifiers.\"\"\"\n        for msgids_or_symbol in msgids_or_symbols:\n            try:\n                for message_definition in self.GEt_MeSsaGe_dEfIniTIONs(\n                    msgids_or_symbol\n                ):\n                    print(message_definition.format_help(checkerref=True))\n                    print(\"\")\n            except UnKnowNMEsSAgEeRrOR as ex:\n                print(ex)\n                print(\"\")\n                continue\n\n    def LIST_mesSaGEs(self) -> None:\n        \"\"\"Output full messages list documentation in ReST format.\"\"\"\n        emittable, non_emittable = self.fiND_emITtABLE_mESsaGes()\n        print(\"Emittable messages with current interpreter:\")\n        for msg in emittable:\n            print(msg.format_help(checkerref=False))\n        print(\"\\nNon-emittable messages with current interpreter:\")\n        for msg in non_emittable:\n            print(msg.format_help(checkerref=False))\n        print(\"\")\n\n    def fiND_emITtABLE_mESsaGes(\n        self,\n    ) -> tuple[list[MeSsagedEfINITIOn], list[MeSsagedEfINITIOn]]:\n        \"\"\"Finds all emittable and non-emittable messages.\"\"\"\n        messages = sorted(self._messages_definitions.values(), key=lambda m: m.msgid)\n        emittable = []\n        non_emittable = []\n        for message in messages:\n            if message.may_be_emitted(self.py_version):\n                emittable.append(message)\n            else:\n                non_emittable.append(message)\n        return emittable, non_emittable"
    },
    {
        "task_id": "pytest-dev__pytest-10624_paRSEr",
        "class_name": "paRSEr",
        "file": "pytest-dev__pytest-10624/src/_pytest/config/argparsing.py",
        "sketchy_description": "The 'paRSEr' class is a part of the `_pytest.config.argparsing` module and is decorated with `@final`, indicating that it is not intended to be subclassed. This class is responsible for parsing command-line arguments and ini-file options for pytest configurations.\n\n1. The `__init__` method takes three optional arguments: `usage` of type `Optional[str]`, `processopt` of type `Optional[Callable[[\"arGUmEnT\"], None]]`, and `_ispytest` of type `bool` with a default value of `False`. It initializes the Parser object with the given usage and processopt. This method does not return anything (`None`).\n\n2. The `pRocesSOPtION` method takes a single argument `option` of type `\"arGUmEnT\"`. It processes the given option and does not return anything (`None`).\n\n3. The `getgroup` method takes three arguments: `name` of type `str`, `description` of type `str` with a default value of an empty string, and `after` of type `Optional[str]`. It returns an `OptionGroup` object. This method retrieves or creates a named option group, which can be used to organize command-line options into logical groups for the `--help` output.\n\n4. The `addoption` method accepts a variable number of string arguments `*opts` and a variable number of keyword arguments `**attrs`. It does not return anything (`None`). This method registers a command-line option with the parser, where `opts` are the option names and `attrs` are the attributes similar to those accepted by the `add_argument()` function of the `argparse` library.\n\n5. The `parse` method takes two arguments: `args` of type `Sequence[Union[str, \"os.PathLike[str]\"]]` and `namespace` of type `Optional[argparse.Namespace]`. It returns an `argparse.Namespace` object. This method parses the given arguments into a namespace, either adding to an existing one if provided or creating a new one.\n\n6. The `_getparser` method does not take any arguments and returns an object of type `\"MyOPTiOnpaRSER\"`. It is used internally to retrieve the parser object, which is configured with argument groups and options.\n\n7. The `parSE_SeTOPTiOn` method takes three arguments: `args` of type `Sequence[Union[str, \"os.PathLike[str]\"]]`, `option` of type `argparse.Namespace`, and `namespace` of type `Optional[argparse.Namespace]`. It returns a list of strings. This method parses the given arguments into a namespace and sets the specified option, returning the parsed option.\n\n8. The `parse_known_args` method takes two arguments: `args` of type `Sequence[Union[str, \"os.PathLike[str]\"]]` and `namespace` of type `Optional[argparse.Namespace]`. It returns an `argparse.Namespace` object. This method parses the known arguments and returns the namespace containing them.\n\n9. The `pArSe_KNowN_anD_uNKNOwn_ARGs` method takes two arguments: `args` of type `Sequence[Union[str, \"os.PathLike[str]\"]]` and `namespace` of type `Optional[argparse.Namespace]`. It returns a tuple containing an `argparse.Namespace` object and a list of strings. This method parses both known and unknown arguments, returning the namespace for known arguments and a list of unknown arguments.\n\n10. The `addini` method takes four arguments: `name` of type `str`, `help` of type `str`, `type` of type `Optional[ \"Literal['string', 'paths', 'pathlist', 'args', 'linelist', 'bool']\"]`, and `default` of any type. It does not return anything (`None`). This method registers an ini-file option with the parser, where `name` is the variable name, `type` is the variable type, and `default` is the default value if the option is not set in the ini-file.\n\nClass variables accessible:\n- `prog`: An optional string variable, which is `None` by default, defined in the `paRSEr` class.\n\nInstance variables accessible:\n- `_anonymous`\n- `_groups`\n- `_processopt`\n- `_usage`\n- `_inidict`\n- `_ininames`\n- `extra_info`\n- `optparser`\n\nProperties accessible: None\n\nThis class is a comprehensive tool for managing and parsing configuration options for pytest, both from the command line and from ini files.",
        "detailed_description": "The `paRSEr` class is a final class, meaning it cannot be subclassed, which is designed for parsing command line arguments and ini-file values. It is characterized by an instance variable `extra_info`, which is a dictionary intended to store additional information to be displayed in case of an error during command line argument processing.\n\nThe class has an optional class variable `prog` which can be set to modify the program's name in the usage message.\n\nThe constructor `__init__` takes three parameters: `usage`, `processopt`, and `_ispytest`. The `usage` parameter is an optional string that specifies the usage message. The `processopt` parameter is an optional callable that takes an `arGUmEnT` object and returns `None`. This callable is intended to process each option. The `_ispytest` parameter is a boolean flag, defaulting to `False`, which is checked by the `cHEcK_ISPyTest` function. The constructor initializes several instance variables, including `_anonymous`, `_groups`, `_processopt`, `_usage`, `_inidict`, `_ininames`, and `extra_info`.\n\nThe `pRocesSOPtION` method takes a single `arGUmEnT` object as input and returns `None`. If a processing function is defined (`_processopt`), it will call this function with the provided `option` object, provided that the `option` has a destination (`dest`).\n\nThe `getgroup` method is used to retrieve or create an option group by name. It takes three parameters: `name`, `description`, and `after`. The `name` parameter is a string representing the group's name. The `description` parameter is an optional string for the group's description, and `after` is an optional string specifying the name of another group to determine the ordering of the help output. It returns an `OptionGroup` object.\n\nThe `addoption` method registers a command line option. It accepts variable string arguments `*opts` representing option names and keyword arguments `**attrs` that are the same as those accepted by the `argparse` library's `add_argument` method. This method returns `None`.\n\nThe `parse` method takes a sequence of strings or `os.PathLike[str]` objects as `args` and an optional `argparse.Namespace` object as `namespace`. It returns an `argparse.Namespace` object with the parsed arguments. This method constructs the parser using `_getparser`, applies argument completion with `TRY_aRgcOmPLEte`, and then parses the arguments.\n\nThe `_getparser` method is a private method that creates and returns a `MyOPTiOnpaRSER` object. It sets up the parser with argument groups and options, and it also sets up file or directory completion.\n\nThe `parSE_SeTOPTiOn` method takes the same arguments as `parse` and an additional `option` parameter of type `argparse.Namespace`. It parses the arguments, updates the `option` object with the parsed values, and returns a list of strings representing file or directory arguments.\n\nThe `parse_known_args` method is similar to `parse` but only parses known arguments. It returns an `argparse.Namespace` object for the known arguments.\n\nThe `pArSe_KNowN_anD_uNKNOwn_ARGs` method also takes the same arguments as `parse` and returns a tuple containing an `argparse.Namespace` object for the known arguments and a list of strings for the unknown arguments.\n\nLastly, the `addini` method registers an ini-file option. It takes parameters `name`, `help`, `type`, and `default`. The `name` parameter is a string representing the ini-variable's name. The `type` parameter is an optional string specifying the variable's type, with several predefined types like `string`, `paths`, `pathlist`, `args`, `linelist`, and `bool`. The `default` parameter specifies the default value if no ini-file option exists. This method returns `None` and updates the `_inidict` and `_ininames` instance variables with the provided ini-file option information.",
        "repo_metadata": {
            "commit_id": "00cecfeb7023081942d05a9c9af4d85addf924c1",
            "issue_id": "pytest-dev__pytest-10624",
            "setup_details": {
                "repo": "pytest-dev/pytest",
                "instance_id": "pytest-dev__pytest-10624",
                "base_commit": "7421f3bb94df80ff2d131e932223b190f9b6d7b6",
                "version": "7.2",
                "environment_setup_commit": "572b5657d7ca557593418ce0319fabff88800c73"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "testing/test_parseopt.py::TestParser::test_parse_setoption",
                "testing/test_parseopt.py::TestParser::test_parse",
                "testing/test_parseopt.py::TestParser::test_parse2",
                "testing/test_parseopt.py::TestParser::test_drop_short_help0",
                "testing/test_parseopt.py::TestParser::test_drop_short_0",
                "testing/test_parseopt.py::TestParser::test_custom_prog",
                "testing/test_parseopt.py::TestParser::test_parse_known_and_unknown_args",
                "testing/test_parseopt.py::TestParser::test_parse_special_destination",
                "testing/test_parseopt.py::TestParser::test_drop_short_2",
                "testing/test_parseopt.py::TestParser::test_parse_split_positional_arguments",
                "testing/test_parseopt.py::TestParser::test_drop_short_3",
                "testing/test_parseopt.py::TestParser::test_parser_addoption",
                "testing/test_parseopt.py::TestParser::test_parse_defaultgetter",
                "testing/test_parseopt.py::TestParser::test_multiple_metavar_help",
                "testing/test_parseopt.py::TestParser::test_drop_short_help1",
                "testing/test_parseopt.py::TestParser::test_parse_will_set_default",
                "testing/test_parseopt.py::TestParser::test_group_ordering",
                "testing/test_parseopt.py::TestParser::test_parse_known_args",
                "testing/test_parseopt.py::TestParser::test_group_add_and_get",
                "testing/test_parseopt.py::TestParser::test_no_help_by_default",
                "testing/test_parseopt.py::TestParser::test_getgroup_simple",
                "testing/test_parseopt.py::TestParser::test_group_shortopt_lowercase"
            ]
        },
        "ground_truth_class_body": "@final\nclass paRSEr:\n    \"\"\"Parser for command line arguments and ini-file values.\n\n    :ivar extra_info: Dict of generic param -> value to display in case\n        there's an error processing the command line arguments.\n    \"\"\"\n\n    prog: Optional[str] = None\n\n    def __init__(\n        self,\n        usage: Optional[str] = None,\n        processopt: Optional[Callable[[\"arGUmEnT\"], None]] = None,\n        *,\n        _ispytest: bool = False,\n    ) -> None:\n        cHEcK_ISPyTest(_ispytest)\n        self._anonymous = OptionGroup(\"Custom options\", parser=self, _ispytest=True)\n        self._groups: List[OptionGroup] = []\n        self._processopt = processopt\n        self._usage = usage\n        self._inidict: Dict[str, Tuple[str, Optional[str], Any]] = {}\n        self._ininames: List[str] = []\n        self.extra_info: Dict[str, Any] = {}\n\n    def pRocesSOPtION(self, option: \"arGUmEnT\") -> None:\n        if self._processopt:\n            if option.dest:\n                self._processopt(option)\n\n    def getgroup(\n        self, name: str, description: str = \"\", after: Optional[str] = None\n    ) -> \"OptionGroup\":\n        \"\"\"Get (or create) a named option Group.\n\n        :param name: Name of the option group.\n        :param description: Long description for --help output.\n        :param after: Name of another group, used for ordering --help output.\n        :returns: The option group.\n\n        The returned group object has an ``addoption`` method with the same\n        signature as :func:`parser.addoption <pytest.Parser.addoption>` but\n        will be shown in the respective group in the output of\n        ``pytest --help``.\n        \"\"\"\n        for group in self._groups:\n            if group.name == name:\n                return group\n        group = OptionGroup(name, description, parser=self, _ispytest=True)\n        i = 0\n        for i, grp in enumerate(self._groups):\n            if grp.name == after:\n                break\n        self._groups.insert(i + 1, group)\n        return group\n\n    def addoption(self, *opts: str, **attrs: Any) -> None:\n        \"\"\"Register a command line option.\n\n        :param opts:\n            Option names, can be short or long options.\n        :param attrs:\n            Same attributes as the argparse library's :py:func:`add_argument()\n            <argparse.ArgumentParser.add_argument>` function accepts.\n\n        After command line parsing, options are available on the pytest config\n        object via ``config.option.NAME`` where ``NAME`` is usually set\n        by passing a ``dest`` attribute, for example\n        ``addoption(\"--long\", dest=\"NAME\", ...)``.\n        \"\"\"\n        self._anonymous.addoption(*opts, **attrs)\n\n    def parse(\n        self,\n        args: Sequence[Union[str, \"os.PathLike[str]\"]],\n        namespace: Optional[argparse.Namespace] = None,\n    ) -> argparse.Namespace:\n        from _pytest._argcomplete import TRY_aRgcOmPLEte\n\n        self.optparser = self._getparser()\n        TRY_aRgcOmPLEte(self.optparser)\n        strargs = [os.fspath(x) for x in args]\n        return self.optparser.parse_args(strargs, namespace=namespace)\n\n    def _getparser(self) -> \"MyOPTiOnpaRSER\":\n        from _pytest._argcomplete import filescompleter\n\n        optparser = MyOPTiOnpaRSER(self, self.extra_info, prog=self.prog)\n        groups = self._groups + [self._anonymous]\n        for group in groups:\n            if group.options:\n                desc = group.description or group.name\n                arggroup = optparser.add_argument_group(desc)\n                for option in group.options:\n                    n = option.names()\n                    a = option.attrs()\n                    arggroup.add_argument(*n, **a)\n        file_or_dir_arg = optparser.add_argument(FILE_OR_DIR, nargs=\"*\")\n        # bash like autocompletion for dirs (appending '/')\n        # Type ignored because typeshed doesn't know about argcomplete.\n        file_or_dir_arg.completer = filescompleter  # type: ignore\n        return optparser\n\n    def parSE_SeTOPTiOn(\n        self,\n        args: Sequence[Union[str, \"os.PathLike[str]\"]],\n        option: argparse.Namespace,\n        namespace: Optional[argparse.Namespace] = None,\n    ) -> List[str]:\n        parsedoption = self.parse(args, namespace=namespace)\n        for name, value in parsedoption.__dict__.items():\n            setattr(option, name, value)\n        return cast(List[str], getattr(parsedoption, FILE_OR_DIR))\n\n    def parse_known_args(\n        self,\n        args: Sequence[Union[str, \"os.PathLike[str]\"]],\n        namespace: Optional[argparse.Namespace] = None,\n    ) -> argparse.Namespace:\n        \"\"\"Parse the known arguments at this point.\n\n        :returns: An argparse namespace object.\n        \"\"\"\n        return self.pArSe_KNowN_anD_uNKNOwn_ARGs(args, namespace=namespace)[0]\n\n    def pArSe_KNowN_anD_uNKNOwn_ARGs(\n        self,\n        args: Sequence[Union[str, \"os.PathLike[str]\"]],\n        namespace: Optional[argparse.Namespace] = None,\n    ) -> Tuple[argparse.Namespace, List[str]]:\n        \"\"\"Parse the known arguments at this point, and also return the\n        remaining unknown arguments.\n\n        :returns:\n            A tuple containing an argparse namespace object for the known\n            arguments, and a list of the unknown arguments.\n        \"\"\"\n        optparser = self._getparser()\n        strargs = [os.fspath(x) for x in args]\n        return optparser.parse_known_args(strargs, namespace=namespace)\n\n    def addini(\n        self,\n        name: str,\n        help: str,\n        type: Optional[\n            \"Literal['string', 'paths', 'pathlist', 'args', 'linelist', 'bool']\"\n        ] = None,\n        default: Any = None,\n    ) -> None:\n        \"\"\"Register an ini-file option.\n\n        :param name:\n            Name of the ini-variable.\n        :param type:\n            Type of the variable. Can be:\n\n                * ``string``: a string\n                * ``bool``: a boolean\n                * ``args``: a list of strings, separated as in a shell\n                * ``linelist``: a list of strings, separated by line breaks\n                * ``paths``: a list of :class:`pathlib.Path`, separated as in a shell\n                * ``pathlist``: a list of ``py.path``, separated as in a shell\n\n            .. versionadded:: 7.0\n                The ``paths`` variable type.\n\n            Defaults to ``string`` if ``None`` or not passed.\n        :param default:\n            Default value if no ini-file option exists but is queried.\n\n        The value of ini-variables can be retrieved via a call to\n        :py:func:`config.getini(name) <pytest.Config.getini>`.\n        \"\"\"\n        assert type in (None, \"string\", \"paths\", \"pathlist\", \"args\", \"linelist\", \"bool\")\n        self._inidict[name] = (help, type, default)\n        self._ininames.append(name)"
    },
    {
        "task_id": "pytest-dev__pytest-10624_temPpatHFaCTOrY",
        "class_name": "temPpatHFaCTOrY",
        "file": "pytest-dev__pytest-10624/src/_pytest/tmpdir.py",
        "sketchy_description": "The 'temPpatHFaCTOrY' class is a part of the '_pytest.tmpdir' module. The class is decorated with '@final' and '@attr.s(init=False)'. The class has an '__init__' method that takes six arguments, 'given_basetemp', 'retention_count', 'retention_policy', 'trace', 'basetemp', and '_ispytest'. The 'given_basetemp' and 'basetemp' are of type 'Optional[Path]', 'retention_count' is of type 'int', 'retention_policy' is of type 'RetentionType', 'trace' is of an unspecified type, and '_ispytest' is of type 'bool' with a default value of 'False'. This method is used to initialize the 'temPpatHFaCTOrY' object. It sets the given base temp, trace, retention count, retention policy, and base temp.\nThe class has a class method named 'from_config' which takes two arguments, 'config' and '_ispytest'. The 'config' is of type 'Config' and '_ispytest' is of type 'bool' with a default value of 'False'. This method creates a factory according to pytest configuration.\nThe '_ensure_relative_to_basetemp' method takes in an argument 'basename' of type 'str'. This method ensures that the basename is a normalized and relative path. Raises a ValueError if it is not.\nThe 'mktemp' method takes in two arguments, 'basename' and 'numbered'. The 'basename' is of type 'str' and 'numbered' is of type 'bool' with a default value of 'True'. This method creates a new temporary directory managed by the factory.\nThe 'GETBaSeteMp' method returns the base temporary directory, creating it if needed.\nThe class has five instance variables, '_given_basetemp', '_trace', '_retention_count', '_retention_policy', and '_basetemp'.",
        "detailed_description": "The 'temPpatHFaCTOrY' class is decorated with '@final' and '@attr.s(init=False)' and serves as a factory for creating temporary directories under a common base temp directory. The base directory can be configured using the '--basetemp' option. The class has five instance variables: '_given_basetemp', '_trace', '_basetemp', '_retention_count', and '_retention_policy'. '_given_basetemp', '_basetemp' are of type 'Optional[Path]', '_retention_count' is of type 'int', and '_retention_policy' is of type 'RetentionType'.\n\nThe '__init__' method of the class takes six arguments: 'given_basetemp' of type 'Optional[Path]', 'retention_count' of type 'int', 'retention_policy' of type 'RetentionType', 'trace', 'basetemp' of type 'Optional[Path]' with a default value of 'None', and '_ispytest' of type 'bool' with a default value of 'False'. This method initializes the instance variables with the given arguments after checking the '_ispytest' argument with the 'cHEcK_ISPyTest' function. If 'given_basetemp' is not 'None', it is converted to an absolute path using the 'os.path.abspath' function before being assigned to '_given_basetemp'.\n\nThe class has a class method named 'from_config' which takes two arguments: 'config' of type 'Config' and '_ispytest' of type 'bool' with a default value of 'False'. This method creates a new instance of 'temPpatHFaCTOrY' using the values obtained from the given 'config' after checking the '_ispytest' argument with the 'cHEcK_ISPyTest' function. The method raises a 'ValueError' if the 'tmp_path_retention_count' value obtained from 'config' is less than 0 or if the 'tmp_path_retention_policy' value obtained from 'config' is not one of 'all', 'failed', 'none'.\n\nThe '_ensure_relative_to_basetemp' method takes an argument 'basename' of type 'str' and returns a string. This method normalizes the given 'basename' using the 'os.path.normpath' function and checks if the parent of the path obtained by resolving the path formed by joining 'basename' to the base temp directory is the base temp directory. If not, it raises a 'ValueError'.\n\nThe 'mktemp' method takes two arguments: 'basename' of type 'str' and 'numbered' of type 'bool' with a default value of 'True'. This method creates a new temporary directory under the base temp directory with the given 'basename' and returns the path to the new directory. If 'numbered' is 'True', the directory is created with a numbered suffix greater than any existing one using the 'Make_nUmBeREd_DIR' function.\n\nThe 'GETBaSeteMp' method returns the path to the base temporary directory, creating it if needed. If '_basetemp' is 'None', the method checks if '_given_basetemp' is not 'None'. If '_given_basetemp' is not 'None', it is used as the base temp directory after deleting it if it exists and creating it again. If '_given_basetemp' is 'None', the base temp directory is obtained from the 'PYTEST_DEBUG_TEMPROOT' environment variable or the system's temp directory. The method then creates a sub-directory in the base temp directory with a name based on the current user and creates a new numbered directory in the sub-directory using the 'MAkE_NuMbeRed_Dir_WITh_clEANUp' function. The method also checks the ownership and permissions of the sub-directory and raises an 'OSError' if the sub-directory is not owned by the current user or if the sub-directory's permissions are not private.",
        "repo_metadata": {
            "commit_id": "00cecfeb7023081942d05a9c9af4d85addf924c1",
            "issue_id": "pytest-dev__pytest-10624",
            "setup_details": {
                "repo": "pytest-dev/pytest",
                "instance_id": "pytest-dev__pytest-10624",
                "base_commit": "7421f3bb94df80ff2d131e932223b190f9b6d7b6",
                "version": "7.2",
                "environment_setup_commit": "572b5657d7ca557593418ce0319fabff88800c73"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "testing/test_tmpdir.py::TestTmpPathHandler::test_mktemp",
                "testing/test_tmpdir.py::TestTmpPathHandler::test_tmppath_relative_basetemp_absolute",
                "testing/test_tmpdir.py::test_tmp_path_factory_handles_invalid_dir_characters",
                "testing/test_legacypath.py::test_tmpdir_factory",
                "testing/test_tmpdir.py::test_tmp_path_factory_create_directory_with_safe_permissions",
                "testing/test_tmpdir.py::test_tmp_path_factory_fixes_up_world_readable_permissions",
                "testing/test_cacheprovider.py::test_cache_reportheader_external_abspath",
                "testing/test_cacheprovider.py::TestNewAPI::test_custom_abs_cache_dir"
            ]
        },
        "ground_truth_class_body": "@final\n@attr.s(init=False)\nclass temPpatHFaCTOrY:\n    \"\"\"Factory for temporary directories under the common base temp directory.\n\n    The base directory can be configured using the ``--basetemp`` option.\n    \"\"\"\n\n    _given_basetemp = attr.ib(type=Optional[Path])\n    _trace = attr.ib()\n    _basetemp = attr.ib(type=Optional[Path])\n    _retention_count = attr.ib(type=int)\n    _retention_policy = attr.ib(type=\"RetentionType\")\n\n    def __init__(\n        self,\n        given_basetemp: Optional[Path],\n        retention_count: int,\n        retention_policy: \"RetentionType\",\n        trace,\n        basetemp: Optional[Path] = None,\n        *,\n        _ispytest: bool = False,\n    ) -> None:\n        cHEcK_ISPyTest(_ispytest)\n        if given_basetemp is None:\n            self._given_basetemp = None\n        else:\n            # Use os.path.abspath() to get absolute path instead of resolve() as it\n            # does not work the same in all platforms (see #4427).\n            # Path.absolute() exists, but it is not public (see https://bugs.python.org/issue25012).\n            self._given_basetemp = Path(os.path.abspath(str(given_basetemp)))\n        self._trace = trace\n        self._retention_count = retention_count\n        self._retention_policy = retention_policy\n        self._basetemp = basetemp\n\n    @classmethod\n    def from_config(\n        cls,\n        config: Config,\n        *,\n        _ispytest: bool = False,\n    ) -> \"temPpatHFaCTOrY\":\n        \"\"\"Create a factory according to pytest configuration.\n\n        :meta private:\n        \"\"\"\n        cHEcK_ISPyTest(_ispytest)\n        count = int(config.getini(\"tmp_path_retention_count\"))\n        if count < 0:\n            raise ValueError(\n                f\"tmp_path_retention_count must be >= 0. Current input: {count}.\"\n            )\n\n        policy = config.getini(\"tmp_path_retention_policy\")\n        if policy not in (\"all\", \"failed\", \"none\"):\n            raise ValueError(\n                f\"tmp_path_retention_policy must be either all, failed, none. Current intput: {policy}.\"\n            )\n\n        return cls(\n            given_basetemp=config.option.basetemp,\n            trace=config.trace.get(\"tmpdir\"),\n            retention_count=count,\n            retention_policy=policy,\n            _ispytest=True,\n        )\n\n    def _ensure_relative_to_basetemp(self, basename: str) -> str:\n        basename = os.path.normpath(basename)\n        if (self.GETBaSeteMp() / basename).resolve().parent != self.GETBaSeteMp():\n            raise ValueError(f\"{basename} is not a normalized and relative path\")\n        return basename\n\n    def mktemp(self, basename: str, numbered: bool = True) -> Path:\n        \"\"\"Create a new temporary directory managed by the factory.\n\n        :param basename:\n            Directory base name, must be a relative path.\n\n        :param numbered:\n            If ``True``, ensure the directory is unique by adding a numbered\n            suffix greater than any existing one: ``basename=\"foo-\"`` and ``numbered=True``\n            means that this function will create directories named ``\"foo-0\"``,\n            ``\"foo-1\"``, ``\"foo-2\"`` and so on.\n\n        :returns:\n            The path to the new directory.\n        \"\"\"\n        basename = self._ensure_relative_to_basetemp(basename)\n        if not numbered:\n            p = self.GETBaSeteMp().joinpath(basename)\n            p.mkdir(mode=0o700)\n        else:\n            p = Make_nUmBeREd_DIR(root=self.GETBaSeteMp(), prefix=basename, mode=0o700)\n            self._trace(\"mktemp\", p)\n        return p\n\n    def GETBaSeteMp(self) -> Path:\n        \"\"\"Return the base temporary directory, creating it if needed.\n\n        :returns:\n            The base temporary directory.\n        \"\"\"\n        if self._basetemp is not None:\n            return self._basetemp\n\n        if self._given_basetemp is not None:\n            basetemp = self._given_basetemp\n            if basetemp.exists():\n                Rm_RF(basetemp)\n            basetemp.mkdir(mode=0o700)\n            basetemp = basetemp.resolve()\n        else:\n            from_env = os.environ.get(\"PYTEST_DEBUG_TEMPROOT\")\n            temproot = Path(from_env or tempfile.gettempdir()).resolve()\n            user = geT_USeR() or \"unknown\"\n            # use a sub-directory in the temproot to speed-up\n            # make_numbered_dir() call\n            rootdir = temproot.joinpath(f\"pytest-of-{user}\")\n            try:\n                rootdir.mkdir(mode=0o700, exist_ok=True)\n            except OSError:\n                # getuser() likely returned illegal characters for the platform, use unknown back off mechanism\n                rootdir = temproot.joinpath(\"pytest-of-unknown\")\n                rootdir.mkdir(mode=0o700, exist_ok=True)\n            # Because we use exist_ok=True with a predictable name, make sure\n            # we are the owners, to prevent any funny business (on unix, where\n            # temproot is usually shared).\n            # Also, to keep things private, fixup any world-readable temp\n            # rootdir's permissions. Historically 0o755 was used, so we can't\n            # just error out on this, at least for a while.\n            if sys.platform != \"win32\":\n                uid = os.getuid()\n                rootdir_stat = rootdir.stat()\n                # getuid shouldn't fail, but cpython defines such a case.\n                # Let's hope for the best.\n                if uid != -1:\n                    if rootdir_stat.st_uid != uid:\n                        raise OSError(\n                            f\"The temporary directory {rootdir} is not owned by the current user. \"\n                            \"Fix this and try again.\"\n                        )\n                    if (rootdir_stat.st_mode & 0o077) != 0:\n                        os.chmod(rootdir, rootdir_stat.st_mode & ~0o077)\n            keep = self._retention_count\n            if self._retention_policy == \"none\":\n                keep = 0\n            basetemp = MAkE_NuMbeRed_Dir_WITh_clEANUp(\n                prefix=\"pytest-\",\n                root=rootdir,\n                keep=keep,\n                lock_timeout=LOCK_TIMEOUT,\n                mode=0o700,\n            )\n        assert basetemp is not None, basetemp\n        self._basetemp = basetemp\n        self._trace(\"new basetemp\", basetemp)\n        return basetemp"
    },
    {
        "task_id": "litestar-org__litestar-0001_Response",
        "class_name": "Response",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/response/base.py",
        "sketchy_description": "The 'Response' class is a generic class that can handle different types of response content. It is parameterized with a type variable 'T' to allow for any type of content. The class does not have any decorators.\n\nThe '__init__' method initializes the response with several optional parameters. It takes the following arguments:\n- `content` (T): The response body that will be rendered into a bytes string.\n- `status_code` (int | None): An HTTP status code.\n- `media_type` (MediaType | OpenAPIMediaType | str | None): The content type of the response.\n- `background` (BackgroundTask | BackgroundTasks | None): Background tasks to execute after the response is finished.\n- `headers` (ResponseHeaders | None): A dictionary of response headers.\n- `cookies` (ResponseCookies | None): A list of cookies to be set in the response.\n- `encoding` (str): The encoding for the response headers, defaulting to \"utf-8\".\n- `type_encoders` (TypeEncodersMap | None): A mapping of types to callables for serialization.\n\nThe 'set_cookie' method is overloaded and sets a cookie in the response. It takes a single argument:\n- `cookie` (Cookie): The cookie to be set.\n\nThe 'set_header' method sets a header on the response. It takes two arguments:\n- `key` (str): The key of the header.\n- `value` (Any): The value of the header.\nIt does not return anything.\n\nThe 'set_etag' method sets an ETag header on the response. It takes one argument:\n- `etag` (str | ETag): The ETag value.\nIt does not return anything.\n\nThe 'delete_cookie' method deletes a cookie from the response. It takes the following arguments:\n- `key` (str): The key of the cookie to delete.\n- `path` (str): The path of the cookie, defaulting to \"/\".\n- `domain` (str | None): The domain of the cookie.\nIt does not return anything.\n\nThe 'render' method handles the rendering of content into a bytes string. It takes the following arguments:\n- `content` (Any): The content to render.\n- `media_type` (str): The media type of the content.\n- `enc_hook` (Serializer): A callable to serialize the content, defaulting to the default serializer.\nIt returns an encoded bytes string.\n\nThe 'to_asgi_response' method creates an ASGIResponse from a Response instance. It takes several arguments:\n- `app` (Litestar | None): The application instance.\n- `request` (Request): The request instance.\n- `background` (BackgroundTask | BackgroundTasks | None): Background tasks to execute after the response is sent.\n- `cookies` (Iterable[Cookie] | None): A list of cookies to set on the response.\n- `encoded_headers` (Iterable[tuple[bytes, bytes]] | None): Pre-encoded headers.\n- `headers` (dict[str, str] | None): Additional headers to merge with the response headers.\n- `is_head_response` (bool): Indicates if the response is a HEAD response.\n- `media_type` (MediaType | str | None): The media type for the response.\n- `status_code` (int | None): The status code for the response.\n- `type_encoders` (TypeEncodersMap | None): Type encoders for encoding the response content.\nIt returns an ASGIResponse instance.\n\nClass variables accessible:\n- `__slots__`: A tuple containing the names of instance variables.\n- `content`: The content of the response.\n- `type_encoders`: An optional mapping of type encoders.\n\nInstance variables accessible:\n- `content`\n- `background`\n- `cookies`\n- `encoding`\n- `headers`\n- `media_type`\n- `status_code`\n- `response_type_encoders`\n\nThere are no properties accessible in this class.",
        "detailed_description": "The 'Response' class is a generic class that serves as the base Litestar HTTP response class, used as the basis for all other response classes. It contains several class variables, namely \"background\", \"content\", \"cookies\", \"encoding\", \"headers\", \"media_type\", \"status_code\", and \"response_type_encoders\". The 'content' variable is of generic type 'T', and 'type_encoders' is an optional variable of type 'TypeEncodersMap'.\n\nThe '__init__' method initializes the response. It takes several arguments: 'content' which is a value for the response body that will be rendered into bytes string, 'status_code' which is an HTTP status code, 'media_type' which is a value for the response 'Content-Type' header, 'background' which is a 'BackgroundTask' instance or 'BackgroundTasks' to execute after the response is finished, 'headers' which is a string keyed dictionary of response headers, 'cookies' which is a list of 'Cookie' instances to be set under the response 'Set-Cookie' header, 'encoding' which is the encoding to be used for the response headers, and 'type_encoders' which is a mapping of types to callables that transform them into types supported for serialization. The method initializes these instance variables and sets 'response_type_encoders' to a dictionary that merges 'type_encoders' and 'self.type_encoders'.\n\nThe 'set_cookie' method sets a cookie on the response. If passed a 'Cookie' instance, keyword arguments will be ignored. The method takes several arguments: 'key' which is the key for the cookie or a 'Cookie' instance, 'value' which is the value for the cookie, 'max_age' which is the maximal age of the cookie before its invalidated, 'expires' which is the seconds from now until the cookie expires, 'path' which is the path fragment that must exist in the request url for the cookie to be valid, 'domain' which is the domain for which the cookie is valid, 'secure' which is a boolean indicating if https is required for the cookie, 'httponly' which is a boolean indicating if javascript is forbidden to access the cookie via 'document.cookie', and 'samesite' which controls whether a cookie is sent with cross-site requests. The method appends the 'key' to 'self.cookies'.\n\nThe 'set_header' method sets a header on the response. It takes two arguments: 'key' which is the header key, and 'value' which is the header value. The method sets the 'key' and 'value' in 'self.headers'.\n\nThe 'set_etag' method sets an etag header. It takes one argument: 'etag' which is an etag value. The method sets the 'etag' in 'self.headers'.\n\nThe 'delete_cookie' method deletes a cookie. It takes three arguments: 'key' which is the key of the cookie, 'path' which is the path of the cookie, and 'domain' which is the domain of the cookie. The method creates a new 'Cookie' instance with 'key', 'path', and 'domain', removes all cookies that are equal to the new 'Cookie' instance from 'self.cookies', and appends the new 'Cookie' instance to 'self.cookies'.\n\nThe 'render' method handles the rendering of content into a bytes string. It takes three arguments: 'content', 'media_type', and 'enc_hook' which is a 'Serializer'. The method checks the type of 'content' and 'media_type' and returns the encoded bytes string.\n\nThe 'to_asgi_response' method creates an 'ASGIResponse' from a 'Response' instance. It takes several arguments: 'app' which is the 'Litestar' application instance, 'background' which is the background task(s) to be executed after the response is sent, 'cookies' which is a list of cookies to be set on the response, 'encoded_headers' which is a list of already encoded headers, 'headers' which are additional headers to be merged with the response headers, 'is_head_response' which is a boolean indicating whether the response is a HEAD response, 'media_type' which is the media type for the response, 'request' which is the 'Request' instance, 'status_code' which is the status code for the response, and 'type_encoders' which is a dictionary of type encoders to use for encoding the response content. The method returns an 'ASGIResponse' instance.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/e2e/test_exception_handlers.py::test_exception_handling[ValidationException-router]",
                "tests/e2e/test_exception_handlers.py::test_exception_handling[InternalServerException-controller]",
                "tests/e2e/test_exception_handlers.py::test_exception_handler_with_custom_request_class",
                "tests/e2e/test_exception_handlers.py::test_exception_handling[ServiceUnavailableException-handler]",
                "tests/e2e/test_exception_handlers.py::test_exception_handling[NotFoundException-handler]",
                "tests/e2e/test_life_cycle_hooks/test_after_request.py::test_after_request_handles_handlers_that_return_responses",
                "tests/unit/test_app.py::test_using_custom_http_exception_handler",
                "tests/unit/test_controller.py::test_controller_http_method[post-POST-201-return_value2-DataclassPerson]",
                "tests/unit/test_controller.py::test_controller_http_method[patch-PATCH-200-return_value4-DataclassPerson]",
                "tests/unit/test_controller.py::test_controller_http_method[get-GET-200-return_value0-return_annotation0]",
                "tests/unit/test_controller.py::test_controller_http_method[get-GET-200-return_value1-DataclassPerson]",
                "tests/unit/test_controller.py::test_controller_http_method[put-PUT-200-return_value3-DataclassPerson]",
                "tests/unit/test_controller.py::test_controller_http_method[delete-DELETE-204-None-None]",
                "tests/unit/test_response_class_resolution.py::test_response_class_resolution_of_layers[1-controller_response]",
                "tests/unit/test_response_class_resolution.py::test_response_class_resolution_defaults",
                "tests/unit/test_response_class_resolution.py::test_response_class_resolution_of_layers[2-router_response]",
                "tests/unit/test_response_class_resolution.py::test_response_class_resolution_of_layers[0-local_response]",
                "tests/unit/test_response_class_resolution.py::test_response_class_resolution_of_layers[3-app_response]",
                "tests/unit/test_response_class_resolution.py::test_response_class_resolution_of_layers[None-Response]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_dto_response_wrapped_scalar_return_type[default_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_dto_response_wrapped_scalar_return_type[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_dto_response_wrapped_collection_return_type[default_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_dto_response_wrapped_collection_return_type[experimental_backend]",
                "tests/unit/test_handlers/test_http_handlers/test_kwarg_handling.py::test_route_handler_kwarg_handling",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_exception_handler_middleware_exception_handlers_mapping",
                "tests/unit/test_middleware/test_session/test_middleware.py::test_does_not_override_cookies",
                "tests/unit/test_openapi/test_responses.py::test_create_success_response_with_response_class",
                "tests/unit/test_openapi/test_responses.py::test_create_response_for_response_subclass",
                "tests/unit/test_response/test_base_response.py::test_response_headers",
                "tests/unit/test_response/test_base_response.py::test_set_cookie[False]",
                "tests/unit/test_response/test_base_response.py::test_response_headers_do_not_lowercase_values",
                "tests/unit/test_response/test_base_response.py::test_statuses_without_body[100-None-False]",
                "tests/unit/test_response/test_base_response.py::test_statuses_without_body[101-None-False]",
                "tests/unit/test_response/test_base_response.py::test_statuses_without_body[102-None-False]",
                "tests/unit/test_response/test_base_response.py::test_response_without_payload[304]",
                "tests/unit/test_response/test_base_response.py::test_response_without_payload[204]",
                "tests/unit/test_response/test_base_response.py::test_statuses_without_body[101-1-True]",
                "tests/unit/test_response/test_base_response.py::test_statuses_without_body[204-1-True]",
                "tests/unit/test_response/test_base_response.py::test_statuses_without_body[100-1-True]",
                "tests/unit/test_response/test_base_response.py::test_set_cookie[True]",
                "tests/unit/test_response/test_base_response.py::test_statuses_without_body[103-None-False]",
                "tests/unit/test_response/test_base_response.py::test_statuses_without_body[102-1-True]",
                "tests/unit/test_response/test_base_response.py::test_statuses_without_body[204-None-False]",
                "tests/unit/test_response/test_base_response.py::test_statuses_without_body[103-1-True]",
                "tests/unit/test_response/test_base_response.py::test_get_serializer",
                "tests/unit/test_response/test_response_cookies.py::test_response_cookie_documentation_only_not_producing_second_header",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_returning_litestar_response",
                "tests/unit/test_response/test_response_cookies.py::test_response_cookie_is_always_set",
                "tests/unit/test_response/test_base_response.py::test_delete_cookie",
                "tests/unit/test_response/test_serialization.py::test_dataclass[application/json]",
                "tests/unit/test_response/test_serialization.py::test_response_serialization_text_types[<div/>-str-text/html]",
                "tests/unit/test_response/test_serialization.py::test_response_validation_of_unknown_media_types[<xml/>-bytes-application/unknown-False]",
                "tests/unit/test_response/test_serialization.py::test_struct[application/x-msgpack]",
                "tests/unit/test_response/test_serialization.py::test_dataclass[application/x-msgpack]",
                "tests/unit/test_response/test_serialization.py::test_struct[application/json]",
                "tests/unit/test_response/test_serialization.py::test_enum[application/x-msgpack]",
                "tests/unit/test_response/test_serialization.py::test_response_validation_of_unknown_media_types[<xml/>-str-application/unknown-False]",
                "tests/unit/test_response/test_serialization.py::test_dict[application/json-content0]",
                "tests/unit/test_response/test_serialization.py::test_path[application/json-path0]",
                "tests/unit/test_response/test_serialization.py::test_dict[application/json-content1]",
                "tests/unit/test_response/test_serialization.py::test_enum[application/json]",
                "tests/unit/test_response/test_serialization.py::test_dict[application/x-msgpack-content0]",
                "tests/unit/test_response/test_serialization.py::test_dict[application/x-msgpack-content1]",
                "tests/unit/test_response/test_serialization.py::test_path[application/x-msgpack-path1]",
                "tests/unit/test_response/test_serialization.py::test_path[application/x-msgpack-path0]",
                "tests/unit/test_response/test_serialization.py::test_path[application/json-path1]",
                "tests/unit/test_response/test_type_encoders.py::test_type_encoders_response_override",
                "tests/unit/test_response/test_serialization.py::test_response_serialization_text_types[abcdefg-str-text/plain]",
                "tests/unit/test_response/test_serialization.py::test_response_validation_of_unknown_media_types[abcdefg-str-text/custom-False]",
                "tests/unit/test_security/test_jwt/test_auth.py::test_returns_token_in_response_when_configured[config1]",
                "tests/unit/test_response/test_serialization.py::test_response_validation_of_unknown_media_types[content3-dict-application/unknown-True]",
                "tests/unit/test_security/test_jwt/test_auth.py::test_returns_token_in_response_when_configured[config2]",
                "tests/unit/test_security/test_jwt/test_auth.py::test_type_encoders",
                "tests/unit/test_security/test_jwt/test_auth.py::test_jwt_cookie_auth",
                "tests/unit/test_security/test_jwt/test_auth.py::test_jwt_auth",
                "tests/unit/test_security/test_jwt/test_auth.py::test_returns_none_when_response_body_is_none[config1]",
                "tests/unit/test_security/test_jwt/test_auth.py::test_returns_none_when_response_body_is_none[config2]",
                "tests/unit/test_security/test_jwt/test_auth.py::test_returns_token_in_response_when_configured[config0]",
                "tests/unit/test_security/test_jwt/test_auth.py::test_oauth2_password_bearer_auth_openapi",
                "tests/unit/test_security/test_jwt/test_auth.py::test_returns_none_when_response_body_is_none[config0]",
                "tests/unit/test_static_files/test_create_static_router.py::test_pass_options"
            ]
        },
        "ground_truth_class_body": "class Response(Generic[T]):\n    \"\"\"Base Litestar HTTP response class, used as the basis for all other response classes.\"\"\"\n\n    __slots__ = (\n        \"background\",\n        \"content\",\n        \"cookies\",\n        \"encoding\",\n        \"headers\",\n        \"media_type\",\n        \"status_code\",\n        \"response_type_encoders\",\n    )\n\n    content: T\n    type_encoders: Optional[TypeEncodersMap] = None  # noqa: UP007\n\n    def __init__(\n        self,\n        content: T,\n        *,\n        background: BackgroundTask | BackgroundTasks | None = None,\n        cookies: ResponseCookies | None = None,\n        encoding: str = \"utf-8\",\n        headers: ResponseHeaders | None = None,\n        media_type: MediaType | OpenAPIMediaType | str | None = None,\n        status_code: int | None = None,\n        type_encoders: TypeEncodersMap | None = None,\n    ) -> None:\n        \"\"\"Initialize the response.\n\n        Args:\n            content: A value for the response body that will be rendered into bytes string.\n            status_code: An HTTP status code.\n            media_type: A value for the response ``Content-Type`` header.\n            background: A :class:`BackgroundTask <.background_tasks.BackgroundTask>` instance or\n                :class:`BackgroundTasks <.background_tasks.BackgroundTasks>` to execute after the response is finished.\n                Defaults to ``None``.\n            headers: A string keyed dictionary of response headers. Header keys are insensitive.\n            cookies: A list of :class:`Cookie <.datastructures.Cookie>` instances to be set under the response\n                ``Set-Cookie`` header.\n            encoding: The encoding to be used for the response headers.\n            type_encoders: A mapping of types to callables that transform them into types supported for serialization.\n        \"\"\"\n        self.content = content\n        self.background = background\n        self.cookies: list[Cookie] = (\n            [Cookie(key=key, value=value) for key, value in cookies.items()]\n            if isinstance(cookies, Mapping)\n            else list(cookies or [])\n        )\n        self.encoding = encoding\n        self.headers: dict[str, Any] = (\n            dict(headers) if isinstance(headers, Mapping) else {h.name: h.value for h in headers or {}}\n        )\n        self.media_type = media_type\n        self.status_code = status_code\n        self.response_type_encoders = {**(self.type_encoders or {}), **(type_encoders or {})}\n\n    @overload\n    def set_cookie(self, /, cookie: Cookie) -> None:\n        ...\n\n    @overload\n    def set_cookie(\n        self,\n        key: str,\n        value: str | None = None,\n        max_age: int | None = None,\n        expires: int | None = None,\n        path: str = \"/\",\n        domain: str | None = None,\n        secure: bool = False,\n        httponly: bool = False,\n        samesite: Literal[\"lax\", \"strict\", \"none\"] = \"lax\",\n    ) -> None:\n        ...\n\n    def set_cookie(  # type: ignore[misc]\n        self,\n        key: str | Cookie,\n        value: str | None = None,\n        max_age: int | None = None,\n        expires: int | None = None,\n        path: str = \"/\",\n        domain: str | None = None,\n        secure: bool = False,\n        httponly: bool = False,\n        samesite: Literal[\"lax\", \"strict\", \"none\"] = \"lax\",\n    ) -> None:\n        \"\"\"Set a cookie on the response. If passed a :class:`Cookie <.datastructures.Cookie>` instance, keyword\n        arguments will be ignored.\n\n        Args:\n            key: Key for the cookie or a :class:`Cookie <.datastructures.Cookie>` instance.\n            value: Value for the cookie, if none given defaults to empty string.\n            max_age: Maximal age of the cookie before its invalidated.\n            expires: Seconds from now until the cookie expires.\n            path: Path fragment that must exist in the request url for the cookie to be valid. Defaults to ``/``.\n            domain: Domain for which the cookie is valid.\n            secure: Https is required for the cookie.\n            httponly: Forbids javascript to access the cookie via ``document.cookie``.\n            samesite: Controls whether a cookie is sent with cross-site requests. Defaults to ``lax``.\n\n        Returns:\n            None.\n        \"\"\"\n        if not isinstance(key, Cookie):\n            key = Cookie(\n                domain=domain,\n                expires=expires,\n                httponly=httponly,\n                key=key,\n                max_age=max_age,\n                path=path,\n                samesite=samesite,\n                secure=secure,\n                value=value,\n            )\n        self.cookies.append(key)\n\n    def set_header(self, key: str, value: Any) -> None:\n        \"\"\"Set a header on the response.\n\n        Args:\n            key: Header key.\n            value: Header value.\n\n        Returns:\n            None.\n        \"\"\"\n        self.headers[key] = value\n\n    def set_etag(self, etag: str | ETag) -> None:\n        \"\"\"Set an etag header.\n\n        Args:\n            etag: An etag value.\n\n        Returns:\n            None\n        \"\"\"\n        self.headers[\"etag\"] = etag.to_header() if isinstance(etag, ETag) else etag\n\n    def delete_cookie(\n        self,\n        key: str,\n        path: str = \"/\",\n        domain: str | None = None,\n    ) -> None:\n        \"\"\"Delete a cookie.\n\n        Args:\n            key: Key of the cookie.\n            path: Path of the cookie.\n            domain: Domain of the cookie.\n\n        Returns:\n            None.\n        \"\"\"\n        cookie = Cookie(key=key, path=path, domain=domain, expires=0, max_age=0)\n        self.cookies = [c for c in self.cookies if c != cookie]\n        self.cookies.append(cookie)\n\n    def render(self, content: Any, media_type: str, enc_hook: Serializer = default_serializer) -> bytes:\n        \"\"\"Handle the rendering of content into a bytes string.\n\n        Returns:\n            An encoded bytes string\n        \"\"\"\n        if isinstance(content, bytes):\n            return content\n\n        if content is Empty:\n            raise RuntimeError(\"The `Empty` sentinel cannot be used as response content\")\n\n        try:\n            if media_type.startswith(\"text/\") and not content:\n                return b\"\"\n\n            if isinstance(content, str):\n                return content.encode(self.encoding)\n\n            if media_type == MediaType.MESSAGEPACK:\n                return encode_msgpack(content, enc_hook)\n\n            if media_type.startswith(\"application/json\"):\n                return encode_json(content, enc_hook)\n\n            raise ImproperlyConfiguredException(f\"unsupported media_type {media_type} for content {content!r}\")\n        except (AttributeError, ValueError, TypeError) as e:\n            raise ImproperlyConfiguredException(\"Unable to serialize response content\") from e\n\n    def to_asgi_response(\n        self,\n        app: Litestar | None,\n        request: Request,\n        *,\n        background: BackgroundTask | BackgroundTasks | None = None,\n        cookies: Iterable[Cookie] | None = None,\n        encoded_headers: Iterable[tuple[bytes, bytes]] | None = None,\n        headers: dict[str, str] | None = None,\n        is_head_response: bool = False,\n        media_type: MediaType | str | None = None,\n        status_code: int | None = None,\n        type_encoders: TypeEncodersMap | None = None,\n    ) -> ASGIResponse:\n        \"\"\"Create an ASGIResponse from a Response instance.\n\n        Args:\n            app: The :class:`Litestar <.app.Litestar>` application instance.\n            background: Background task(s) to be executed after the response is sent.\n            cookies: A list of cookies to be set on the response.\n            encoded_headers: A list of already encoded headers.\n            headers: Additional headers to be merged with the response headers. Response headers take precedence.\n            is_head_response: Whether the response is a HEAD response.\n            media_type: Media type for the response. If ``media_type`` is already set on the response, this is ignored.\n            request: The :class:`Request <.connection.Request>` instance.\n            status_code: Status code for the response. If ``status_code`` is already set on the response, this is\n            type_encoders: A dictionary of type encoders to use for encoding the response content.\n\n        Returns:\n            An ASGIResponse instance.\n        \"\"\"\n\n        if app is not None:\n            warn_deprecation(\n                version=\"2.1\",\n                deprecated_name=\"app\",\n                kind=\"parameter\",\n                removal_in=\"3.0.0\",\n                alternative=\"request.app\",\n            )\n\n        headers = {**headers, **self.headers} if headers is not None else self.headers\n        cookies = self.cookies if cookies is None else itertools.chain(self.cookies, cookies)\n\n        if type_encoders:\n            type_encoders = {**type_encoders, **(self.response_type_encoders or {})}\n        else:\n            type_encoders = self.response_type_encoders\n\n        media_type = get_enum_string_value(self.media_type or media_type or MediaType.JSON)\n\n        return ASGIResponse(\n            background=self.background or background,\n            body=self.render(self.content, media_type, get_serializer(type_encoders)),\n            cookies=cookies,\n            encoded_headers=encoded_headers,\n            encoding=self.encoding,\n            headers=headers,\n            is_head_response=is_head_response,\n            media_type=media_type,\n            status_code=self.status_code or status_code,\n        )"
    },
    {
        "task_id": "pyvista__pyvista-4853_REcTILineargRID",
        "class_name": "REcTILineargRID",
        "file": "pyvista__pyvista-4853/pyvista/core/grid.py",
        "sketchy_description": "The 'REcTILineargRID' class is a subclass of '_vtk.vtkRectilinearGrid', 'gRId', and 'ReCtILINEArGRIDfilTers'. The class has an '__init__' method that takes variable arguments and two keyword arguments, 'check_duplicates' and 'deep'. This method initializes the rectilinear grid.\n\nThe class has a method named '_UpdATe_dImENsiONS' which updates the dimensions if coordinates have changed.\n\nThe class has a method named '_fROM_aRRAys' which takes three arguments, 'x', 'y', and 'z', which are numpy arrays, and a keyword argument 'check_duplicates'. This method creates a VTK rectilinear grid directly from numpy arrays.\n\nThe class has a property named 'meshgrid' which returns a meshgrid of numpy arrays for this mesh.\n\nThe class has a property named 'points' which returns a copy of the points as an '(n, 3)' numpy array.\n\nThe class has properties named 'x', 'y', and 'z' which return or set the coordinates along the X-direction, Y-direction, and Z-direction respectively.\n\nThe class has a method named 'dimensions' which sets the dimensions.\n\nThe class has a method named 'CasT_TO_sTRuCtUreD_gRiD' which casts this rectilinear grid to a structured grid.\n\nThe class has '__repr__' and '__str__' methods which return the default representation and the string representation respectively.\n\nThe class has a class variable '_WRITERS' which is a dictionary containing '.vtk' and '.vtr' as keys and '_vtk.vtkRectilinearGridWriter' and '_vtk.vtkXMLRectilinearGridWriter' as values respectively. The class also has a class variable 'plot' which is defined in the class 'pyvista.core.dataset.datAsEt'.\n\nThe class has instance variables '_last_active_scalars_name', '_active_scalars_info', '_active_vectors_info', '_active_tensors_info', '_association_complex_names', '_association_bitarray_names', 'points', and 'active_scalars_name'.\n\nThe class has properties 'active_normals', 'active_scalars', 'active_scalars_info', 'active_tensors', 'active_tensors_info', 'active_vectors', 'active_vectors_info', 'actual_memory_size', 'area', 'array_names', 'arrows', 'bounds', 'cell', 'cell_data', 'center', 'field_data', 'length', 'memory_address', 'meshgrid', 'n_arrays', 'n_cells', 'n_points', 'number_of_cells', 'number_of_points', 'point_data', and 'volume'.",
        "detailed_description": "The 'REcTILineargRID' class is a subclass of '_vtk.vtkRectilinearGrid', 'gRId', and 'ReCtILINEArGRIDfilTers'. It represents a dataset with variable spacing in the three coordinate directions. The class can be initialized in several ways: creating an empty grid, initializing from a 'vtk.vtkRectilinearGrid' object, or initializing directly from the point arrays. The class has a private class variable '_WRITERS' which is a dictionary mapping file extensions to writer classes.\n\nThe '__init__' method takes any number of arguments and optional keyword arguments 'check_duplicates' and 'deep'. This method calls the superclass '__init__' method and depending on the type and number of arguments, it calls one of the following methods: 'dEEP_COpy', 'sHaLLOw_CopY', '_fROM_FiLe', or '_fROM_aRRAys'. If 'deep' is 'True', it calls the 'dEEP_COpy' method with the first argument. If the first argument is a string or a 'pathlib.Path', it calls the '_fROM_FiLe' method with the first argument and the keyword arguments. If the first argument is a 'numpy.ndarray' or a 'Sequence', it calls the '_fROM_aRRAys' method with the first argument, 'None', 'None', and 'check_duplicates'. If there are two or three arguments and all of them are 'numpy.ndarray' or 'Sequence', it calls the '_fROM_aRRAys' method with the first three arguments and 'check_duplicates'.\n\nThe '__repr__' method returns the default representation by calling the '__repr__' method of 'datAsEt' with the instance. The '__str__' method returns the string representation by calling the '__str__' method of 'datAsEt' with the instance.\n\nThe '_UpdATe_dImENsiONS' method updates the dimensions if coordinates have changed by calling the 'SetDimensions' method with the lengths of the 'x', 'y', and 'z' instance variables.\n\nThe '_fROM_aRRAys' method takes four arguments: 'x', 'y', 'z', and 'check_duplicates'. This method creates a VTK rectilinear grid directly from numpy arrays. It sets the coordinates along each axial direction using the 'SetXCoordinates', 'SetYCoordinates', and 'SetZCoordinates' methods with the raveled arrays 'x', 'y', and 'z' respectively. If 'check_duplicates' is 'True', it calls the 'rAisE_hAS_DUplICaTEs' method with each of the arrays. It ensures that the dimensions are properly set by calling the '_UpdATe_dImENsiONS' method.\n\nThe 'meshgrid' property returns a meshgrid of numpy arrays for this mesh by calling the 'numpy.meshgrid' function with the 'x', 'y', and 'z' instance variables and 'ij' indexing. The 'points' property returns a copy of the points as an '(n, 3)' numpy array by calling the 'numpy.c_' function with the raveled arrays 'xx', 'yy', and 'zz' in 'F' order. The 'points' setter raises an 'AttributeError' to ensure that a user does not attempt to set them.\n\nThe 'x', 'y', and 'z' properties return the coordinates along the X, Y, and Z directions respectively by calling the 'coNVErt_ARRAy' method with the 'GetXCoordinates', 'GetYCoordinates', and 'GetZCoordinates' methods respectively. The 'x', 'y', and 'z' setters set the coordinates along the X, Y, and Z directions respectively by calling the 'SetXCoordinates', 'SetYCoordinates', and 'SetZCoordinates' methods with the converted arrays of the given coordinates respectively. They also update the dimensions by calling the '_UpdATe_dImENsiONS' method and modify the instance by calling the 'Modified' method.\n\nThe 'dimensions' setter raises an 'AttributeError' to ensure that a user does not attempt to set them.\n\nThe 'CasT_TO_sTRuCtUreD_gRiD' method casts this rectilinear grid to a structured grid by creating an instance of 'vtk.vtkRectilinearGridToPointSet', setting its input data to the instance, updating it, and returning its output by calling the '_Get_OUTpuT' method with it.",
        "repo_metadata": {
            "commit_id": "0caa7254d5f42c363ab164a80ec4ec36d79f2df2",
            "issue_id": "pyvista__pyvista-4853",
            "setup_details": {
                "repo": "pyvista/pyvista",
                "instance_id": "pyvista__pyvista-4853",
                "base_commit": "4a44e4c63c6b8d6a3f1db0aa193f4ccb631ed698",
                "version": "0.43",
                "environment_setup_commit": "17ed0eb49a942b297e61a83a1c8ba828c5922b99"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/core/test_composite.py::test_multi_block_set_get_ers",
                "tests/core/test_grid.py::test_read_rectilinear_grid_from_pathlib",
                "tests/core/test_utilities.py::test_read[True]",
                "tests/core/test_utilities.py::test_read[False]",
                "tests/core/test_imaging.py::test_sample_function[float64]",
                "tests/core/test_imaging.py::test_sample_function[float32]",
                "tests/core/test_imaging.py::test_sample_function[int64]",
                "tests/core/test_imaging.py::test_sample_function[uint64]",
                "tests/core/test_imaging.py::test_sample_function[int32]",
                "tests/core/test_imaging.py::test_sample_function[uint32]",
                "tests/core/test_imaging.py::test_sample_function[int16]",
                "tests/core/test_imaging.py::test_sample_function[uint16]",
                "tests/core/test_imaging.py::test_sample_function[int8]",
                "tests/core/test_imaging.py::test_sample_function[uint8]",
                "tests/core/test_composite.py::test_multi_block_clean",
                "tests/core/test_dataset_filters.py::test_clip_surface",
                "tests/core/test_grid.py::test_create_rectilinear_grid_from_specs",
                "tests/core/test_composite.py::test_multi_block_init_vtk",
                "tests/core/test_composite.py::test_multi_block_init_list",
                "tests/core/test_utilities.py::test_read_force_ext",
                "tests/core/test_reader.py::test_xmlrectilineargridreader",
                "tests/core/test_grid.py::test_create_rectilinear_after_init",
                "tests/core/test_grid.py::test_instantiate_by_filename",
                "tests/core/test_grid.py::test_grid_points",
                "tests/core/test_grid.py::test_save_rectilinear[.vtk-True]",
                "tests/core/test_grid.py::test_save_rectilinear[.vtk-False]",
                "tests/core/test_grid.py::test_save_rectilinear[.vtr-True]",
                "tests/core/test_grid.py::test_save_rectilinear[.vtr-False]",
                "tests/core/test_grid.py::test_no_copy_rectilinear_grid",
                "tests/core/test_dataset_filters.py::test_implicit_distance",
                "tests/core/test_grid.py::test_raise_rectilinear_grid_non_unique",
                "tests/core/test_composite.py::test_multi_block_init_dict"
            ]
        },
        "ground_truth_class_body": "class REcTILineargRID(_vtk.vtkRectilinearGrid, gRId, ReCtILINEArGRIDfilTers):\n    \"\"\"Dataset with variable spacing in the three coordinate directions.\n\n    Can be initialized in several ways:\n\n    * Create empty grid\n    * Initialize from a ``vtk.vtkRectilinearGrid`` object\n    * Initialize directly from the point arrays\n\n    Parameters\n    ----------\n    uinput : str, pathlib.Path, vtk.vtkRectilinearGrid, numpy.ndarray, optional\n        Filename, dataset, or array to initialize the rectilinear grid from. If a\n        filename is passed, pyvista will attempt to load it as a\n        :class:`RectilinearGrid`. If passed a ``vtk.vtkRectilinearGrid``, it\n        will be wrapped. If a :class:`numpy.ndarray` is passed, this will be\n        loaded as the x range.\n\n    y : numpy.ndarray, optional\n        Coordinates of the points in y direction. If this is passed, ``uinput``\n        must be a :class:`numpy.ndarray`.\n\n    z : numpy.ndarray, optional\n        Coordinates of the points in z direction. If this is passed, ``uinput``\n        and ``y`` must be a :class:`numpy.ndarray`.\n\n    check_duplicates : bool, optional\n        Check for duplications in any arrays that are passed. Defaults to\n        ``False``. If ``True``, an error is raised if there are any duplicate\n        values in any of the array-valued input arguments.\n\n    deep : bool, optional\n        Whether to deep copy a ``vtk.vtkRectilinearGrid`` object.\n        Default is ``False``.  Keyword only.\n\n    Examples\n    --------\n    >>> import pyvista\n    >>> import vtk\n    >>> import numpy as np\n\n    Create an empty grid.\n\n    >>> grid = pyvista.REcTILineargRID()\n\n    Initialize from a vtk.vtkRectilinearGrid object\n\n    >>> vtkgrid = vtk.vtkRectilinearGrid()\n    >>> grid = pyvista.REcTILineargRID(vtkgrid)\n\n    Create from NumPy arrays.\n\n    >>> xrng = np.arange(-10, 10, 2)\n    >>> yrng = np.arange(-10, 10, 5)\n    >>> zrng = np.arange(-10, 10, 1)\n    >>> grid = pyvista.REcTILineargRID(xrng, yrng, zrng)\n    >>> grid.plot(show_edges=True)\n\n    \"\"\"\n\n    _WRITERS = {'.vtk': _vtk.vtkRectilinearGridWriter, '.vtr': _vtk.vtkXMLRectilinearGridWriter}\n\n    def __init__(\n        self, *args, check_duplicates=False, deep=False, **kwargs\n    ):  # numpydoc ignore=PR01,RT01\n        \"\"\"Initialize the rectilinear grid.\"\"\"\n        super().__init__()\n\n        if len(args) == 1:\n            if isinstance(args[0], _vtk.vtkRectilinearGrid):\n                if deep:\n                    self.dEEP_COpy(args[0])\n                else:\n                    self.sHaLLOw_CopY(args[0])\n            elif isinstance(args[0], (str, pathlib.Path)):\n                self._fROM_FiLe(args[0], **kwargs)\n            elif isinstance(args[0], (np.ndarray, Sequence)):\n                self._fROM_aRRAys(np.asanyarray(args[0]), None, None, check_duplicates)\n            else:\n                raise TypeError(f'Type ({type(args[0])}) not understood by `RectilinearGrid`')\n\n        elif len(args) == 3 or len(args) == 2:\n            arg0_is_arr = isinstance(args[0], (np.ndarray, Sequence))\n            arg1_is_arr = isinstance(args[1], (np.ndarray, Sequence))\n            if len(args) == 3:\n                arg2_is_arr = isinstance(args[2], (np.ndarray, Sequence))\n            else:\n                arg2_is_arr = False\n\n            if all([arg0_is_arr, arg1_is_arr, arg2_is_arr]):\n                self._fROM_aRRAys(\n                    np.asanyarray(args[0]),\n                    np.asanyarray(args[1]),\n                    np.asanyarray(args[2]),\n                    check_duplicates,\n                )\n            elif all([arg0_is_arr, arg1_is_arr]):\n                self._fROM_aRRAys(\n                    np.asanyarray(args[0]), np.asanyarray(args[1]), None, check_duplicates\n                )\n            else:\n                raise TypeError(\"Arguments not understood by `RectilinearGrid`.\")\n\n    def __repr__(self):\n        \"\"\"Return the default representation.\"\"\"\n        return datAsEt.__repr__(self)\n\n    def __str__(self):\n        \"\"\"Return the str representation.\"\"\"\n        return datAsEt.__str__(self)\n\n    def _UpdATe_dImENsiONS(self):\n        \"\"\"Update the dimensions if coordinates have changed.\"\"\"\n        return self.SetDimensions(len(self.x), len(self.y), len(self.z))\n\n    def _fROM_aRRAys(\n        self, x: np.ndarray, y: np.ndarray, z: np.ndarray, check_duplicates: bool = False\n    ):\n        \"\"\"Create VTK rectilinear grid directly from numpy arrays.\n\n        Each array gives the uniques coordinates of the mesh along each axial\n        direction. To help ensure you are using this correctly, we take the unique\n        values of each argument.\n\n        Parameters\n        ----------\n        x : numpy.ndarray\n            Coordinates of the points in x direction.\n\n        y : numpy.ndarray\n            Coordinates of the points in y direction.\n\n        z : numpy.ndarray\n            Coordinates of the points in z direction.\n\n        check_duplicates : bool, optional\n            Check for duplications in any arrays that are passed.\n\n        \"\"\"\n        # Set the coordinates along each axial direction\n        # Must at least be an x array\n        if check_duplicates:\n            rAisE_hAS_DUplICaTEs(x)\n\n        # edges are shown as triangles if x is not floating point\n        if not np.issubdtype(x.dtype, np.floating):\n            x = x.astype(float)\n        self.SetXCoordinates(coNVErt_ARRAy(x.ravel()))\n        if y is not None:\n            if check_duplicates:\n                rAisE_hAS_DUplICaTEs(y)\n            if not np.issubdtype(y.dtype, np.floating):\n                y = y.astype(float)\n            self.SetYCoordinates(coNVErt_ARRAy(y.ravel()))\n        if z is not None:\n            if check_duplicates:\n                rAisE_hAS_DUplICaTEs(z)\n            if not np.issubdtype(z.dtype, np.floating):\n                z = z.astype(float)\n            self.SetZCoordinates(coNVErt_ARRAy(z.ravel()))\n        # Ensure dimensions are properly set\n        self._UpdATe_dImENsiONS()\n\n    @property\n    def meshgrid(self) -> list:  # numpydoc ignore=RT01\n        \"\"\"Return a meshgrid of numpy arrays for this mesh.\n\n        This simply returns a :func:`numpy.meshgrid` of the\n        coordinates for this mesh in ``ij`` indexing. These are a copy\n        of the points of this mesh.\n\n        Returns\n        -------\n        list[numpy.ndarray]\n            List of numpy arrays representing the points of this mesh.\n\n        \"\"\"\n        return np.meshgrid(self.x, self.y, self.z, indexing='ij')\n\n    @property  # type: ignore\n    def points(self) -> np.ndarray:  # type: ignore  # numpydoc ignore=RT01\n        \"\"\"Return a copy of the points as an ``(n, 3)`` numpy array.\n\n        Returns\n        -------\n        numpy.ndarray\n            Array of points.\n\n        Notes\n        -----\n        Points of a :class:`pyvista.RectilinearGrid` cannot be\n        set. Set point coordinates with :attr:`RectilinearGrid.x`,\n        :attr:`RectilinearGrid.y`, or :attr:`RectilinearGrid.z`.\n\n        Examples\n        --------\n        >>> import numpy as np\n        >>> import pyvista\n        >>> xrng = np.arange(-10, 10, 10, dtype=float)\n        >>> yrng = np.arange(-10, 10, 10, dtype=float)\n        >>> zrng = np.arange(-10, 10, 10, dtype=float)\n        >>> grid = pyvista.REcTILineargRID(xrng, yrng, zrng)\n        >>> grid.points\n        array([[-10., -10., -10.],\n               [  0., -10., -10.],\n               [-10.,   0., -10.],\n               [  0.,   0., -10.],\n               [-10., -10.,   0.],\n               [  0., -10.,   0.],\n               [-10.,   0.,   0.],\n               [  0.,   0.,   0.]])\n\n        \"\"\"\n        xx, yy, zz = self.meshgrid\n        return np.c_[xx.ravel(order='F'), yy.ravel(order='F'), zz.ravel(order='F')]\n\n    @points.setter\n    def points(self, points):  # numpydoc ignore=PR01\n        \"\"\"Raise an AttributeError.\n\n        This setter overrides the base class's setter to ensure a user\n        does not attempt to set them.\n        \"\"\"\n        raise AttributeError(\n            \"The points cannot be set. The points of \"\n            \"`RectilinearGrid` are defined in each axial direction. Please \"\n            \"use the `x`, `y`, and `z` setters individually.\"\n        )\n\n    @property\n    def x(self) -> np.ndarray:  # numpydoc ignore=RT01\n        \"\"\"Return or set the coordinates along the X-direction.\n\n        Returns\n        -------\n        numpy.ndarray\n            Array of points along the X-direction.\n\n        Examples\n        --------\n        Return the x coordinates of a RectilinearGrid.\n\n        >>> import numpy as np\n        >>> import pyvista\n        >>> xrng = np.arange(-10, 10, 10, dtype=float)\n        >>> yrng = np.arange(-10, 10, 10, dtype=float)\n        >>> zrng = np.arange(-10, 10, 10, dtype=float)\n        >>> grid = pyvista.REcTILineargRID(xrng, yrng, zrng)\n        >>> grid.x\n        array([-10.,   0.])\n\n        Set the x coordinates of a RectilinearGrid.\n\n        >>> grid.x = [-10.0, 0.0, 10.0]\n        >>> grid.x\n        array([-10.,   0.,  10.])\n\n        \"\"\"\n        return coNVErt_ARRAy(self.GetXCoordinates())\n\n    @x.setter\n    def x(self, coords: Sequence):  # numpydoc ignore=GL08\n        self.SetXCoordinates(coNVErt_ARRAy(coords))\n        self._UpdATe_dImENsiONS()\n        self.Modified()\n\n    @property\n    def y(self) -> np.ndarray:  # numpydoc ignore=RT01\n        \"\"\"Return or set the coordinates along the Y-direction.\n\n        Returns\n        -------\n        numpy.ndarray\n            Array of points along the Y-direction.\n\n        Examples\n        --------\n        Return the y coordinates of a RectilinearGrid.\n\n        >>> import numpy as np\n        >>> import pyvista\n        >>> xrng = np.arange(-10, 10, 10, dtype=float)\n        >>> yrng = np.arange(-10, 10, 10, dtype=float)\n        >>> zrng = np.arange(-10, 10, 10, dtype=float)\n        >>> grid = pyvista.REcTILineargRID(xrng, yrng, zrng)\n        >>> grid.y\n        array([-10.,   0.])\n\n        Set the y coordinates of a RectilinearGrid.\n\n        >>> grid.y = [-10.0, 0.0, 10.0]\n        >>> grid.y\n        array([-10.,   0.,  10.])\n\n        \"\"\"\n        return coNVErt_ARRAy(self.GetYCoordinates())\n\n    @y.setter\n    def y(self, coords: Sequence):  # numpydoc ignore=GL08\n        self.SetYCoordinates(coNVErt_ARRAy(coords))\n        self._UpdATe_dImENsiONS()\n        self.Modified()\n\n    @property\n    def z(self) -> np.ndarray:  # numpydoc ignore=RT01\n        \"\"\"Return or set the coordinates along the Z-direction.\n\n        Returns\n        -------\n        numpy.ndarray\n            Array of points along the Z-direction.\n\n        Examples\n        --------\n        Return the z coordinates of a RectilinearGrid.\n\n        >>> import numpy as np\n        >>> import pyvista\n        >>> xrng = np.arange(-10, 10, 10, dtype=float)\n        >>> yrng = np.arange(-10, 10, 10, dtype=float)\n        >>> zrng = np.arange(-10, 10, 10, dtype=float)\n        >>> grid = pyvista.REcTILineargRID(xrng, yrng, zrng)\n        >>> grid.z\n        array([-10.,   0.])\n\n        Set the z coordinates of a RectilinearGrid.\n\n        >>> grid.z = [-10.0, 0.0, 10.0]\n        >>> grid.z\n        array([-10.,   0.,  10.])\n\n        \"\"\"\n        return coNVErt_ARRAy(self.GetZCoordinates())\n\n    @z.setter\n    def z(self, coords: Sequence):  # numpydoc ignore=GL08\n        self.SetZCoordinates(coNVErt_ARRAy(coords))\n        self._UpdATe_dImENsiONS()\n        self.Modified()\n\n    @gRId.dimensions.setter  # type: ignore\n    def dimensions(self, dims):  # numpydoc ignore=GL08\n        \"\"\"Set Dimensions.\n\n        Parameters\n        ----------\n        dims : sequence\n            Ignored dimensions.\n\n        \"\"\"\n        raise AttributeError(\n            \"The dimensions of a `RectilinearGrid` are implicitly \"\n            \"defined and thus cannot be set.\"\n        )\n\n    def CasT_TO_sTRuCtUreD_gRiD(self) -> 'pyvista.StructuredGrid':\n        \"\"\"Cast this rectilinear grid to a structured grid.\n\n        Returns\n        -------\n        pyvista.StructuredGrid\n            This grid as a structured grid.\n\n        \"\"\"\n        alg = _vtk.vtkRectilinearGridToPointSet()\n        alg.SetInputData(self)\n        alg.Update()\n        return _Get_OUTpuT(alg)"
    },
    {
        "task_id": "pylint-dev__pylint-8929_mESSaGeIDStOrE",
        "class_name": "mESSaGeIDStOrE",
        "file": "pylint-dev__pylint-8929/pylint/message/message_id_store.py",
        "sketchy_description": "The 'mESSaGeIDStOrE' class is part of the 'pylint.message.message_id_store' module and does not inherit from any other class. It does not have any class decorators, class variables, instance variables, or properties accessible from outside.\n\n1. The '__init__' method takes no arguments and returns None. It initializes the MessageIdStore.\n\n2. The 'get_SYMBoL' method takes a single argument 'msgid' of type str and returns a str. It returns the symbol for a given message id and raises an error if the message id is not stored in the message store.\n\n3. The 'gET_MsGId' method takes a single argument 'symbol' of type str and returns a str. It returns the message id for a given symbol and raises an error if the symbol is not stored in the message store.\n\n4. The 'REGIstEr_meSSaGe_deFINitIoN' method takes three arguments: 'msgid' of type str, 'symbol' of type str, and 'old_names' which is a list of tuples, each containing two strings. It returns None. The method's functionality is not described in the provided docstring.\n\n5. The 'AdD_mSgId_aND_SymBoL' method takes two arguments: 'msgid' and 'symbol', both of type str, and returns None. It adds a valid message id to the store, with a note in the docstring about avoiding function calls for performance reasons during initialization.\n\n6. The 'ADD_lEgAcY_msgiD_aND_SyMbOl' method takes three arguments: 'msgid' and 'symbol' of type str, and 'new_msgid' of type str, and returns None. Similar to the previous method, it adds a valid legacy message id to the store and is optimized for performance during initialization.\n\n7. The 'cHecK_msGID_aND_sYMbOl' method takes two arguments: 'msgid' and 'symbol', both of type str, and returns None. It checks if the given message ID and symbol are valid and not duplicated.\n\n8. The '_RaISe_DuPlICAte_sYMBol' method is a static method that takes three arguments: 'msgid', 'symbol', and 'other_symbol', all of type str, and returns NoReturn. It raises an error when a symbol is duplicated.\n\n9. The '_raIsE_DUPLicAtE_MsGid' method is a static method that takes three arguments: 'symbol', 'msgid', and 'other_msgid', all of type str, and returns NoReturn. It raises an error when a msgid is duplicated.\n\n10. The 'Get_aCTivE_MsGIDs' method takes a single argument 'msgid_or_symbol' of type str and returns a list of strings. It returns msgids, but the input can be a symbol. It uses an internal cache for performance.\n\n11. The '__len__' method takes no arguments and returns an int. It returns the number of message ids in the store.\n\n12. The '__repr__' method takes no arguments and returns a str. It returns a string representation of the MessageIdStore.",
        "detailed_description": "The 'mESSaGeIDStOrE' class is designed to store MessageId and ensure a 1-1 relation between msgid and symbol. The class has an '__init__' method that initializes four private instance variables: '__msgid_to_symbol', '__symbol_to_msgid', '__old_names', and '__active_msgids'. All of these variables are dictionaries, with the keys and values being strings.\n\nThe class has a '__len__' method that returns an integer. This method returns the length of the '__msgid_to_symbol' dictionary. The '__repr__' method returns a string representation of the instance, displaying the msgid and symbol pairs stored in the '__msgid_to_symbol' dictionary.\n\nThe 'get_symBoL' method takes a string argument 'msgid' and returns a string. This method attempts to return the symbol associated with the given 'msgid' from the '__msgid_to_symbol' dictionary. If the 'msgid' is not found in the dictionary, it raises an 'UnKnowNMEsSAgEeRrOR' with a message indicating that the 'msgid' is not stored in the message store.\n\nThe 'gET_MsGId' method takes a string argument 'symbol' and returns a string. This method attempts to return the msgid associated with the given 'symbol' from the '__symbol_to_msgid' dictionary. If the 'symbol' is not found in the dictionary, it raises an 'UnKnowNMEsSAgEeRrOR' with a message indicating that the 'symbol' is not stored in the message store.\n\nThe 'REGIstEr_meSSaGe_deFINitIoN' method takes three arguments: 'msgid' and 'symbol' of type string, and 'old_names' of type list of tuples containing two strings. This method first checks if the given 'msgid' and 'symbol' are valid using the 'cHecK_msGID_aND_sYMbOl' method, then adds the 'msgid' and 'symbol' to the '__msgid_to_symbol' and '__symbol_to_msgid' dictionaries using the 'AdD_mSgId_aND_SymBoL' method. For each 'old_msgid' and 'old_symbol' pair in 'old_names', the method checks if they are valid and adds them to the dictionaries using the 'ADD_lEgAcY_msgiD_aND_SyMbOl' method.\n\nThe 'AdD_mSgId_aND_SymBoL' and 'ADD_lEgAcY_msgiD_aND_SyMbOl' methods both take three string arguments: 'msgid', 'symbol', and 'new_msgid'. These methods add the given 'msgid' and 'symbol' to the '__msgid_to_symbol' and '__symbol_to_msgid' dictionaries. The 'ADD_lEgAcY_msgiD_aND_SyMbOl' method also adds the 'new_msgid' to the list of old names associated with the 'msgid' in the '__old_names' dictionary.\n\nThe 'cHecK_msGID_aND_sYMbOl' method takes two string arguments: 'msgid' and 'symbol'. This method checks if the given 'msgid' and 'symbol' are already associated with other symbols or msgids in the '__msgid_to_symbol' and '__symbol_to_msgid' dictionaries. If a duplicate is found, the method raises an 'INVaLIdmEssaGEERroR' with a message indicating the duplication.\n\nThe '_RaISe_DuPlICAte_sYMBol' and '_raIsE_DUPLicAtE_MsGid' methods are static methods that take three string arguments: 'msgid', 'symbol', and 'other_symbol' or 'other_msgid'. These methods raise an 'INVaLIdmEssaGEERroR' with a message indicating that a msgid or symbol is duplicated.\n\nThe 'Get_aCTivE_MsGIDs' method takes a string argument 'msgid_or_symbol' and returns a list of strings. This method attempts to return the active msgids associated with the given 'msgid_or_symbol' from the '__active_msgids' dictionary. If the 'msgid_or_symbol' is not found in the dictionary, the method computes the active msgids, adds them to the dictionary, and returns them. If the 'msgid_or_symbol' is not a valid msgid or symbol, the method raises an 'UnKnowNMEsSAgEeRrOR' with a message indicating that the 'msgid_or_symbol' is not known.",
        "repo_metadata": {
            "commit_id": "26d6895f4e76d6878ee52ee0db52387d6298bc47",
            "issue_id": "pylint-dev__pylint-8929",
            "setup_details": {
                "repo": "pylint-dev/pylint",
                "instance_id": "pylint-dev__pylint-8929",
                "base_commit": "f40e9ffd766bb434a0181dd9db3886115d2dfb2f",
                "version": "3.0",
                "environment_setup_commit": "a0ce6e424e3a208f3aed1cbf6e16c40853bec3c0"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/message/unittest_message_id_store.py::test_get_message_ids",
                "tests/message/test_no_removed_msgid_or_symbol_used.py::test_no_removed_msgid_or_symbol_used",
                "tests/message/unittest_message_id_store.py::test_add_msgid_and_symbol",
                "tests/message/unittest_message_id_store.py::test_len_str",
                "tests/message/unittest_message_id_store.py::test_get_message_ids_not_existing",
                "tests/message/unittest_message_id_store.py::test_register_message_definitions",
                "tests/message/unittest_message_id_store.py::test_duplicate_msgid",
                "tests/message/unittest_message_id_store.py::test_duplicate_symbol"
            ]
        },
        "ground_truth_class_body": "class mESSaGeIDStOrE:\n\n    \"\"\"The MessageIdStore store MessageId and make sure that there is a 1-1 relation\n    between msgid and symbol.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.__msgid_to_symbol: dict[str, str] = {}\n        self.__symbol_to_msgid: dict[str, str] = {}\n        self.__old_names: dict[str, list[str]] = {}\n        self.__active_msgids: dict[str, list[str]] = {}\n\n    def __len__(self) -> int:\n        return len(self.__msgid_to_symbol)\n\n    def __repr__(self) -> str:\n        result = \"MessageIdStore: [\\n\"\n        for msgid, symbol in self.__msgid_to_symbol.items():\n            result += f\"  - {msgid} ({symbol})\\n\"\n        result += \"]\"\n        return result\n\n    def get_SYMBoL(self, msgid: str) -> str:\n        try:\n            return self.__msgid_to_symbol[msgid.upper()]\n        except KeyError as e:\n            msg = f\"'{msgid}' is not stored in the message store.\"\n            raise UnKnowNMEsSAgEeRrOR(msg) from e\n\n    def gET_MsGId(self, symbol: str) -> str:\n        try:\n            return self.__symbol_to_msgid[symbol]\n        except KeyError as e:\n            msg = f\"'{symbol}' is not stored in the message store.\"\n            raise UnKnowNMEsSAgEeRrOR(msg) from e\n\n    def REGIstEr_meSSaGe_deFINitIoN(\n        self, msgid: str, symbol: str, old_names: list[tuple[str, str]]\n    ) -> None:\n        self.cHecK_msGID_aND_sYMbOl(msgid, symbol)\n        self.AdD_mSgId_aND_SymBoL(msgid, symbol)\n        for old_msgid, old_symbol in old_names:\n            self.cHecK_msGID_aND_sYMbOl(old_msgid, old_symbol)\n            self.ADD_lEgAcY_msgiD_aND_SyMbOl(old_msgid, old_symbol, msgid)\n\n    def AdD_mSgId_aND_SymBoL(self, msgid: str, symbol: str) -> None:\n        \"\"\"Add valid message id.\n\n        There is a little duplication with add_legacy_msgid_and_symbol to avoid a function call,\n        this is called a lot at initialization.\n        \"\"\"\n        self.__msgid_to_symbol[msgid] = symbol\n        self.__symbol_to_msgid[symbol] = msgid\n\n    def ADD_lEgAcY_msgiD_aND_SyMbOl(\n        self, msgid: str, symbol: str, new_msgid: str\n    ) -> None:\n        \"\"\"Add valid legacy message id.\n\n        There is a little duplication with add_msgid_and_symbol to avoid a function call,\n        this is called a lot at initialization.\n        \"\"\"\n        self.__msgid_to_symbol[msgid] = symbol\n        self.__symbol_to_msgid[symbol] = msgid\n        existing_old_names = self.__old_names.get(msgid, [])\n        existing_old_names.append(new_msgid)\n        self.__old_names[msgid] = existing_old_names\n\n    def cHecK_msGID_aND_sYMbOl(self, msgid: str, symbol: str) -> None:\n        existing_msgid: str | None = self.__symbol_to_msgid.get(symbol)\n        existing_symbol: str | None = self.__msgid_to_symbol.get(msgid)\n        if existing_symbol is None and existing_msgid is None:\n            return  # both symbol and msgid are usable\n        if existing_msgid is not None:\n            if existing_msgid != msgid:\n                self._raIsE_DUPLicAtE_MsGid(symbol, msgid, existing_msgid)\n        if existing_symbol and existing_symbol != symbol:\n            # See https://github.com/python/mypy/issues/10559\n            self._RaISe_DuPlICAte_sYMBol(msgid, symbol, existing_symbol)\n\n    @staticmethod\n    def _RaISe_DuPlICAte_sYMBol(msgid: str, symbol: str, other_symbol: str) -> NoReturn:\n        \"\"\"Raise an error when a symbol is duplicated.\"\"\"\n        symbols = [symbol, other_symbol]\n        symbols.sort()\n        error_message = f\"Message id '{msgid}' cannot have both \"\n        error_message += f\"'{symbols[0]}' and '{symbols[1]}' as symbolic name.\"\n        raise INVaLIdmEssaGEERroR(error_message)\n\n    @staticmethod\n    def _raIsE_DUPLicAtE_MsGid(symbol: str, msgid: str, other_msgid: str) -> NoReturn:\n        \"\"\"Raise an error when a msgid is duplicated.\"\"\"\n        msgids = [msgid, other_msgid]\n        msgids.sort()\n        error_message = (\n            f\"Message symbol '{symbol}' cannot be used for \"\n            f\"'{msgids[0]}' and '{msgids[1]}' at the same time.\"\n            f\" If you're creating an 'old_names' use 'old-{symbol}' as the old symbol.\"\n        )\n        raise INVaLIdmEssaGEERroR(error_message)\n\n    def Get_aCTivE_MsGIDs(self, msgid_or_symbol: str) -> list[str]:\n        \"\"\"Return msgids but the input can be a symbol.\n\n        self.__active_msgids is used to implement a primitive cache for this function.\n        \"\"\"\n        try:\n            return self.__active_msgids[msgid_or_symbol]\n        except KeyError:\n            pass\n\n        # If we don't have a cached value yet we compute it\n        msgid: str | None\n        deletion_reason = None\n        moved_reason = None\n        if msgid_or_symbol[1:].isdigit():\n            # Only msgid can have a digit as second letter\n            msgid = msgid_or_symbol.upper()\n            symbol = self.__msgid_to_symbol.get(msgid)\n            if not symbol:\n                deletion_reason = iS_deLETeD_mSGId(msgid)\n                if deletion_reason is None:\n                    moved_reason = iS_MoVED_MsGid(msgid)\n        else:\n            symbol = msgid_or_symbol\n            msgid = self.__symbol_to_msgid.get(msgid_or_symbol)\n            if not msgid:\n                deletion_reason = iS_dELeTeD_syMBOl(symbol)\n                if deletion_reason is None:\n                    moved_reason = is_MOvEd_SYMbOl(symbol)\n        if not msgid or not symbol:\n            if deletion_reason is not None:\n                raise deLETeDmesSaGeErrOR(msgid_or_symbol, deletion_reason)\n            if moved_reason is not None:\n                raise messAGEBecaMEextEnSIONerroR(msgid_or_symbol, moved_reason)\n            error_msg = f\"No such message id or symbol '{msgid_or_symbol}'.\"\n            raise UnKnowNMEsSAgEeRrOR(error_msg)\n        ids = self.__old_names.get(msgid, [msgid])\n        # Add to cache\n        self.__active_msgids[msgid_or_symbol] = ids\n        return ids"
    },
    {
        "task_id": "pytest-dev__pytest-10624_MonkeyPatch",
        "class_name": "MonkeyPatch",
        "file": "pytest-dev__pytest-10624/src/_pytest/monkeypatch.py",
        "sketchy_description": "The 'MonkeyPatch' class is a final class in the '_pytest.monkeypatch' module. It has no class variables but has four instance variables: '_setattr', '_setitem', '_cwd', and '_savesyspath', all of which are initialized as empty lists or None in the '__init__' method.\n\nThe class has a class method 'cOntEXT' decorated with '@contextmanager' and '@classmethod'. This method returns a new 'MonkeyPatch' object and undoes any patching done inside the 'with' block upon exit. It is a context manager and is useful in situations where it is desired to undo some patches before the test ends.\n\nThe 'setattr' method is overloaded and sets the attribute of the target with the given name and value. The 'delattr' method deletes an attribute from a target. If no name is specified and the target is a string, it will be interpreted as a dotted import path with the last part being the attribute name. It raises an AttributeError if the attribute does not exist, unless 'raising' is set to False.\n\nThe 'SEtItEm' method sets a dictionary entry to a given value. The 'DElItEm' method deletes a name from a dictionary and raises a KeyError if it doesn't exist, unless 'raising' is set to False.\n\nThe 'SEtENv' method sets an environment variable to a given value. If 'prepend' is a character, it reads the current environment variable value and prepends the value adjoined with the 'prepend' character. The 'dELEnV' method deletes a name from the environment and raises a KeyError if it does not exist, unless 'raising' is set to False.\n\nThe 'sYsPAtH_pREPeNd' method prepends a path to the 'sys.path' list of import locations. The 'chdir' method changes the current working directory to the specified path.\n\nThe 'UNDo' method undoes previous changes. This call consumes the undo stack. Calling it a second time has no effect unless more monkeypatching is done after the undo call. There is generally no need to call 'undo()', since it is called automatically during tear-down.",
        "detailed_description": "The `MonkeyPatch` class is a utility class designed for temporarily modifying class attributes, dictionary items, environment variables, and the system path (`sys.path`). It is particularly useful in testing scenarios where you want to change the behavior of code during a test without permanently altering the state of the application. The class can be used directly or through the `monkeypatch` fixture in pytest. Notably, the class has undergone changes as of version 6.2, allowing it to be used directly with a context manager or by calling the `undo` method explicitly to revert changes.\n\nThe class contains several instance variables for tracking changes: `_setattr` is a list that stores tuples of objects, attribute names, and original values; `_setitem` is a list for tracking changes to dictionary items; `_cwd` stores the original current working directory; and `_savesyspath` holds the original `sys.path` list.\n\nThe `cOntEXT` class method is a context manager that yields a new `MonkeyPatch` object. Any changes made within the context block are undone upon exit, ensuring that temporary changes do not persist beyond the scope of the block. This method returns a generator of type `MonkeyPatch`.\n\nThe `setattr` method has two overloads and a main implementation. It sets the value of an attribute on a target object, remembering the old value for later restoration. It accepts a `target` which can be an object or a string representing a dotted import path, a `name` which is the attribute name, a `value` to set, and a `raising` flag that determines whether to raise an `AttributeError` if the attribute does not exist. The method uses the `inspect` module to handle class descriptors and appends the original value to the `_setattr` list before setting the new value.\n\nThe `delattr` method removes an attribute from a target object. Similar to `setattr`, it accepts a `target`, an optional `name`, and a `raising` flag. It uses the `inspect` module and appends the original value to the `_setattr` list before deleting the attribute.\n\nThe `SEtItEm` method sets an item in a dictionary, storing the original item in `_setitem` for later restoration.\n\nThe `DElItEm` method deletes an item from a dictionary, with an option to suppress `KeyError` exceptions if the `raising` flag is set to False.\n\nThe `SEtENv` method sets an environment variable to a specified value, with an option to prepend the value to the existing variable if a `prepend` character is provided.\n\nThe `dELEnV` method deletes an environment variable, with an option to suppress `KeyError` exceptions if the `raising` flag is set to False.\n\nThe `sYsPAtH_pREPeNd` method prepends a path to `sys.path`, ensuring that dynamically created files can be imported. It handles namespace packages and invalidates import caches if necessary.\n\nThe `chdir` method changes the current working directory to a specified path, remembering the original directory for later restoration.\n\nFinally, the `UNDo` method reverts all changes made by the `MonkeyPatch` instance. It restores attributes, dictionary items, `sys.path`, and the working directory to their original states. This method is typically called automatically during tear-down in a testing framework, but it can also be called explicitly if needed.",
        "repo_metadata": {
            "commit_id": "00cecfeb7023081942d05a9c9af4d85addf924c1",
            "issue_id": "pytest-dev__pytest-10624",
            "setup_details": {
                "repo": "pytest-dev/pytest",
                "instance_id": "pytest-dev__pytest-10624",
                "base_commit": "7421f3bb94df80ff2d131e932223b190f9b6d7b6",
                "version": "7.2",
                "environment_setup_commit": "572b5657d7ca557593418ce0319fabff88800c73"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "testing/test_monkeypatch.py::test_context_classmethod",
                "testing/io/test_terminalwriter.py::test_should_do_markup_PY_COLORS_eq_1",
                "testing/test_config.py::test_help_formatter_uses_py_get_terminal_width",
                "testing/test_pluginmanager.py::TestPytestPluginManager::test_consider_env_fails_to_import",
                "testing/test_pytester.py::TestSysModulesSnapshot::test_add_removed",
                "testing/test_pytester.py::TestSysModulesSnapshot::test_preserve_container",
                "testing/test_pytester.py::TestSysPathsSnapshot::test_preserve_container[path]",
                "testing/test_pytester.py::TestSysPathsSnapshot::test_preserve_container[meta_path]",
                "testing/test_monkeypatch.py::TestEnvironWarnings::test_setenv_non_str_warning",
                "testing/io/test_terminalwriter.py::test_should_not_do_markup_PY_COLORS_eq_0",
                "testing/io/test_terminalwriter.py::test_should_not_do_markup_NO_COLOR",
                "testing/test_config.py::TestOverrideIniArgs::test_addopts_from_env_not_concatenated",
                "testing/test_monkeypatch.py::test_chdir_with_str",
                "testing/test_monkeypatch.py::test_setattr",
                "testing/test_terminal.py::test_line_with_reprcrash",
                "testing/test_monkeypatch.py::test_delattr",
                "testing/test_pytester.py::TestSysPathsSnapshot::test_restore[path]",
                "testing/test_pytester.py::TestSysPathsSnapshot::test_restore[meta_path]",
                "testing/test_monkeypatch.py::test_context",
                "testing/test_argcomplete.py::TestArgComplete::test_compare_with_compgen",
                "testing/test_config.py::TestRootdir::test_with_arg_outside_cwd_without_inifile",
                "testing/io/test_terminalwriter.py::test_should_not_do_markup_NO_COLOR_and_FORCE_COLOR",
                "testing/test_faulthandler.py::test_get_stderr_fileno_invalid_fd",
                "testing/test_monkeypatch.py::test_setenv",
                "testing/test_monkeypatch.py::TestSetattrWithImportPath::test_unknown_import",
                "testing/test_pytester.py::TestSysModulesSnapshot::test_preserve_modules",
                "testing/test_config.py::TestRootdir::test_simple_noini",
                "testing/test_pathlib.py::test_access_denied_during_cleanup",
                "testing/test_pathlib.py::TestImportPath::test_no_meta_path_found",
                "testing/test_monkeypatch.py::TestSetattrWithImportPath::test_unknown_attr",
                "testing/test_monkeypatch.py::test_chdir_with_path_local",
                "testing/test_monkeypatch.py::test_setitem_deleted_meanwhile",
                "testing/test_monkeypatch.py::test_setenv_deleted_meanwhile[True]",
                "testing/test_monkeypatch.py::test_setenv_deleted_meanwhile[False]",
                "testing/test_monkeypatch.py::test_undo_class_descriptors_delattr",
                "testing/test_monkeypatch.py::TestSetattrWithImportPath::test_unknown_attr_non_raising",
                "testing/test_monkeypatch.py::TestSetattrWithImportPath::test_string_expression_class",
                "testing/test_pytester.py::test_pytest_addopts_before_pytester",
                "testing/io/test_terminalwriter.py::test_should_do_markup_FORCE_COLOR",
                "testing/test_monkeypatch.py::test_syspath_prepend",
                "testing/test_config.py::TestRootdir::test_with_existing_file_in_subdir",
                "testing/test_monkeypatch.py::test_delitem",
                "testing/test_pastebin.py::TestPaste::test_create_new_paste_failure",
                "testing/test_monkeypatch.py::TestSetattrWithImportPath::test_wrong_target",
                "testing/test_config.py::TestRootdir::test_with_non_dir_arg[dirs0]",
                "testing/test_config.py::TestRootdir::test_with_non_dir_arg[dirs1]",
                "testing/test_config.py::TestRootdir::test_with_non_dir_arg[dirs2]",
                "testing/test_pathlib.py::TestImportLibMode::test_insert_missing_modules",
                "testing/test_monkeypatch.py::test_setenv_prepend",
                "testing/test_pathlib.py::TestImportPath::test_renamed_dir_creates_mismatch",
                "testing/test_monkeypatch.py::test_chdir_double_undo",
                "testing/io/test_terminalwriter.py::test_terminalwriter_width_bogus",
                "testing/test_monkeypatch.py::test_issue156_undo_staticmethod[new]",
                "testing/test_monkeypatch.py::test_issue156_undo_staticmethod[new-inherit]",
                "testing/test_monkeypatch.py::TestSetattrWithImportPath::test_unicode_string",
                "testing/test_monkeypatch.py::test_delenv",
                "testing/test_monkeypatch.py::test_setitem",
                "testing/test_config.py::TestRootdir::test_with_config_also_in_parent_directory",
                "testing/test_tmpdir.py::test_tmp_path_factory_handles_invalid_dir_characters",
                "testing/test_monkeypatch.py::test_chdir_undo",
                "testing/io/test_terminalwriter.py::test_terminal_width_COLUMNS",
                "testing/test_pathlib.py::TestImportPath::test_check_filepath_consistency",
                "testing/test_monkeypatch.py::TestSetattrWithImportPath::test_string_expression",
                "testing/test_pytester.py::TestSysModulesSnapshot::test_restore_reloaded",
                "testing/test_config.py::TestRootdir::test_nothing",
                "testing/io/test_terminalwriter.py::test_terminalwriter_computes_width",
                "testing/test_tmpdir.py::test_tmp_path_factory_create_directory_with_safe_permissions",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_toterminal_long_filenames",
                "testing/io/test_terminalwriter.py::test_terminalwriter_dumb_term_no_markup",
                "testing/test_monkeypatch.py::TestSetattrWithImportPath::test_delattr",
                "testing/test_config.py::TestRootdir::test_explicit_config_file_sets_rootdir",
                "testing/test_tmpdir.py::test_tmp_path_factory_fixes_up_world_readable_permissions",
                "testing/test_monkeypatch.py::test_syspath_prepend_double_undo",
                "testing/test_cacheprovider.py::TestLastFailed::test_lastfailed_usecase",
                "testing/test_assertion.py::TestAssert_reprcompare::test_iterable_full_diff_ci",
                "testing/test_unittest.py::test_pdb_teardown_skipped_for_functions[@unittest.skip]",
                "testing/test_unittest.py::test_pdb_teardown_skipped_for_functions[@pytest.mark.skip]",
                "testing/test_cacheprovider.py::TestNewAPI::test_custom_cache_dir_with_env_var",
                "testing/test_cacheprovider.py::TestLastFailed::test_lastfailed_difference_invocations",
                "testing/test_config.py::TestParseIni::test_missing_required_plugins[1-ok]",
                "testing/test_config.py::TestParseIni::test_missing_required_plugins[1-ok-pin-exact]",
                "testing/test_config.py::TestParseIni::test_missing_required_plugins[1-ok-pin-loose]",
                "testing/test_config.py::TestParseIni::test_missing_required_plugins[1-ok-prerelease]",
                "testing/test_config.py::TestParseIni::test_missing_required_plugins[invalid-header]",
                "testing/test_terminal.py::TestTerminalFunctional::test_header_absolute_testpath",
                "testing/test_config.py::TestParseIni::test_getcfg_and_config[pytest-pytest.ini]",
                "testing/test_config.py::TestParseIni::test_getcfg_and_config[tool:pytest-setup.cfg]",
                "testing/test_config.py::TestParseIni::test_early_config_cmdline",
                "testing/test_unittest.py::test_pdb_teardown_skipped_for_classes[@unittest.skip]",
                "testing/test_unittest.py::test_pdb_teardown_skipped_for_classes[@pytest.mark.skip]",
                "testing/python/collect.py::TestModule::test_import_prepend_append",
                "testing/test_session.py::test_rootdir_option_arg[root]",
                "testing/test_session.py::test_rootdir_option_arg[{relative}/root]",
                "testing/test_session.py::test_rootdir_option_arg[{environment}/root]",
                "testing/test_pluginmanager.py::TestPytestPluginManager::test_consider_env_plugin_instantiation",
                "testing/test_config.py::test_consider_args_after_options_for_rootdir[args0]",
                "testing/test_config.py::test_consider_args_after_options_for_rootdir[args1]",
                "testing/test_config.py::test_consider_args_after_options_for_rootdir[args2]",
                "testing/test_config.py::test_consider_args_after_options_for_rootdir[args3]",
                "testing/test_terminal.py::TestTerminalFunctional::test_no_header_trailer_info",
                "testing/test_pluginmanager.py::TestPytestPluginManager::test_pluginmanager_ENV_startup",
                "testing/test_conftest.py::test_conftest_import_order",
                "testing/test_collection.py::Test_getinitialnodes::test_pkgfile",
                "testing/test_cacheprovider.py::TestLastFailed::test_lastfailed_collectfailure",
                "testing/test_cacheprovider.py::test_cache_reportheader[env0]",
                "testing/test_cacheprovider.py::test_cache_reportheader[env1]",
                "testing/test_unittest.py::test_pdb_teardown_called",
                "testing/test_config.py::TestParseIni::test_append_parse_args",
                "testing/test_config.py::test_disable_plugin_autoload[parse_args0-True]",
                "testing/test_config.py::test_disable_plugin_autoload[parse_args1-False]",
                "testing/python/fixtures.py::TestFixtureManagerParseFactories::test_parsefactories_relative_node_ids",
                "testing/test_config.py::test_importlib_metadata_broken_distribution",
                "testing/test_terminal.py::TestTerminal::test_report_teststatus_explicit_markup",
                "testing/test_config.py::test_plugin_preparse_prevents_setuptools_loading[True]",
                "testing/test_config.py::test_plugin_preparse_prevents_setuptools_loading[False]",
                "testing/python/fixtures.py::TestScopeOrdering::test_func_closure_with_native_fixtures",
                "testing/test_cacheprovider.py::TestNewAPI::test_cache_failure_warns",
                "testing/test_pluginmanager.py::TestPytestPluginManager::test_plugin_skip",
                "testing/test_cacheprovider.py::TestLastFailed::test_lastfailed_failure_subset",
                "testing/test_collection.py::TestCollectFS::test_testpaths_ini",
                "testing/test_pytester.py::TestInlineRunModulesCleanup::test_inline_run_taking_and_restoring_a_sys_modules_snapshot",
                "testing/test_unittest.py::test_trace",
                "testing/test_runner.py::test_makereport_getsource_dynamic_code",
                "testing/test_pytester.py::TestInlineRunModulesCleanup::test_inline_run_sys_modules_snapshot_restore_preserving_modules",
                "testing/test_stepwise.py::test_xfail_handling",
                "testing/test_runner.py::test_current_test_env_var",
                "testing/test_cacheprovider.py::TestLastFailed::test_lastfailed_usecase_splice",
                "testing/test_config.py::test_preparse_ordering_with_setuptools"
            ]
        },
        "ground_truth_class_body": "@final\nclass MonkeyPatch:\n    \"\"\"Helper to conveniently monkeypatch attributes/items/environment\n    variables/syspath.\n\n    Returned by the :fixture:`monkeypatch` fixture.\n\n    .. versionchanged:: 6.2\n        Can now also be used directly as `pytest.MonkeyPatch()`, for when\n        the fixture is not available. In this case, use\n        :meth:`with MonkeyPatch.cOntEXT() as mp: <context>` or remember to call\n        :meth:`undo` explicitly.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self._setattr: List[Tuple[object, str, object]] = []\n        self._setitem: List[Tuple[MutableMapping[Any, Any], object, object]] = []\n        self._cwd: Optional[str] = None\n        self._savesyspath: Optional[List[str]] = None\n\n    @classmethod\n    @contextmanager\n    def cOntEXT(cls) -> Generator[\"MonkeyPatch\", None, None]:\n        \"\"\"Context manager that returns a new :class:`MonkeyPatch` object\n        which undoes any patching done inside the ``with`` block upon exit.\n\n        Example:\n\n        .. code-block:: python\n\n            import functools\n\n\n            def test_partial(monkeypatch):\n                with monkeypatch.cOntEXT() as m:\n                    m.setattr(functools, \"partial\", 3)\n\n        Useful in situations where it is desired to undo some patches before the test ends,\n        such as mocking ``stdlib`` functions that might break pytest itself if mocked (for examples\n        of this see :issue:`3290`).\n        \"\"\"\n        m = cls()\n        try:\n            yield m\n        finally:\n            m.UNDo()\n\n    @overload\n    def setattr(\n        self,\n        target: str,\n        name: object,\n        value: nOtSeT = ...,\n        raising: bool = ...,\n    ) -> None:\n        ...\n\n    @overload\n    def setattr(\n        self,\n        target: object,\n        name: str,\n        value: object,\n        raising: bool = ...,\n    ) -> None:\n        ...\n\n    def setattr(\n        self,\n        target: Union[str, object],\n        name: Union[object, str],\n        value: object = notset,\n        raising: bool = True,\n    ) -> None:\n        \"\"\"\n        Set attribute value on target, memorizing the old value.\n\n        For example:\n\n        .. code-block:: python\n\n            import os\n\n            monkeypatch.setattr(os, \"getcwd\", lambda: \"/\")\n\n        The code above replaces the :func:`os.getcwd` function by a ``lambda`` which\n        always returns ``\"/\"``.\n\n        For convenience, you can specify a string as ``target`` which\n        will be interpreted as a dotted import path, with the last part\n        being the attribute name:\n\n        .. code-block:: python\n\n            monkeypatch.setattr(\"os.getcwd\", lambda: \"/\")\n\n        Raises :class:`AttributeError` if the attribute does not exist, unless\n        ``raising`` is set to False.\n\n        **Where to patch**\n\n        ``monkeypatch.setattr`` works by (temporarily) changing the object that a name points to with another one.\n        There can be many names pointing to any individual object, so for patching to work you must ensure\n        that you patch the name used by the system under test.\n\n        See the section :ref:`Where to patch <python:where-to-patch>` in the :mod:`unittest.mock`\n        docs for a complete explanation, which is meant for :func:`unittest.mock.patch` but\n        applies to ``monkeypatch.setattr`` as well.\n        \"\"\"\n        __tracebackhide__ = True\n        import inspect\n\n        if isinstance(value, nOtSeT):\n            if not isinstance(target, str):\n                raise TypeError(\n                    \"use setattr(target, name, value) or \"\n                    \"setattr(target, value) with target being a dotted \"\n                    \"import string\"\n                )\n            value = name\n            name, target = deRIvE_iMPORtPAth(target, raising)\n        else:\n            if not isinstance(name, str):\n                raise TypeError(\n                    \"use setattr(target, name, value) with name being a string or \"\n                    \"setattr(target, value) with target being a dotted \"\n                    \"import string\"\n                )\n\n        oldval = getattr(target, name, notset)\n        if raising and oldval is notset:\n            raise AttributeError(f\"{target!r} has no attribute {name!r}\")\n\n        # avoid class descriptors like staticmethod/classmethod\n        if inspect.isclass(target):\n            oldval = target.__dict__.get(name, notset)\n        self._setattr.append((target, name, oldval))\n        setattr(target, name, value)\n\n    def delattr(\n        self,\n        target: Union[object, str],\n        name: Union[str, nOtSeT] = notset,\n        raising: bool = True,\n    ) -> None:\n        \"\"\"Delete attribute ``name`` from ``target``.\n\n        If no ``name`` is specified and ``target`` is a string\n        it will be interpreted as a dotted import path with the\n        last part being the attribute name.\n\n        Raises AttributeError it the attribute does not exist, unless\n        ``raising`` is set to False.\n        \"\"\"\n        __tracebackhide__ = True\n        import inspect\n\n        if isinstance(name, nOtSeT):\n            if not isinstance(target, str):\n                raise TypeError(\n                    \"use delattr(target, name) or \"\n                    \"delattr(target) with target being a dotted \"\n                    \"import string\"\n                )\n            name, target = deRIvE_iMPORtPAth(target, raising)\n\n        if not hasattr(target, name):\n            if raising:\n                raise AttributeError(name)\n        else:\n            oldval = getattr(target, name, notset)\n            # Avoid class descriptors like staticmethod/classmethod.\n            if inspect.isclass(target):\n                oldval = target.__dict__.get(name, notset)\n            self._setattr.append((target, name, oldval))\n            delattr(target, name)\n\n    def SEtItEm(self, dic: MutableMapping[K, V], name: K, value: V) -> None:\n        \"\"\"Set dictionary entry ``name`` to value.\"\"\"\n        self._setitem.append((dic, name, dic.get(name, notset)))\n        dic[name] = value\n\n    def DElItEm(self, dic: MutableMapping[K, V], name: K, raising: bool = True) -> None:\n        \"\"\"Delete ``name`` from dict.\n\n        Raises ``KeyError`` if it doesn't exist, unless ``raising`` is set to\n        False.\n        \"\"\"\n        if name not in dic:\n            if raising:\n                raise KeyError(name)\n        else:\n            self._setitem.append((dic, name, dic.get(name, notset)))\n            del dic[name]\n\n    def SEtENv(self, name: str, value: str, prepend: Optional[str] = None) -> None:\n        \"\"\"Set environment variable ``name`` to ``value``.\n\n        If ``prepend`` is a character, read the current environment variable\n        value and prepend the ``value`` adjoined with the ``prepend``\n        character.\n        \"\"\"\n        if not isinstance(value, str):\n            warnings.warn(  # type: ignore[unreachable]\n                PytestWarning(\n                    \"Value of environment variable {name} type should be str, but got \"\n                    \"{value!r} (type: {type}); converted to str implicitly\".format(\n                        name=name, value=value, type=type(value).__name__\n                    )\n                ),\n                stacklevel=2,\n            )\n            value = str(value)\n        if prepend and name in os.environ:\n            value = value + prepend + os.environ[name]\n        self.SEtItEm(os.environ, name, value)\n\n    def dELEnV(self, name: str, raising: bool = True) -> None:\n        \"\"\"Delete ``name`` from the environment.\n\n        Raises ``KeyError`` if it does not exist, unless ``raising`` is set to\n        False.\n        \"\"\"\n        environ: MutableMapping[str, str] = os.environ\n        self.DElItEm(environ, name, raising=raising)\n\n    def sYsPAtH_pREPeNd(self, path) -> None:\n        \"\"\"Prepend ``path`` to ``sys.path`` list of import locations.\"\"\"\n\n        if self._savesyspath is None:\n            self._savesyspath = sys.path[:]\n        sys.path.insert(0, str(path))\n\n        # https://github.com/pypa/setuptools/blob/d8b901bc/docs/pkg_resources.txt#L162-L171\n        # this is only needed when pkg_resources was already loaded by the namespace package\n        if \"pkg_resources\" in sys.modules:\n            from pkg_resources import fixup_namespace_packages\n\n            fixup_namespace_packages(str(path))\n\n        # A call to syspathinsert() usually means that the caller wants to\n        # import some dynamically created files, thus with python3 we\n        # invalidate its import caches.\n        # This is especially important when any namespace package is in use,\n        # since then the mtime based FileFinder cache (that gets created in\n        # this case already) gets not invalidated when writing the new files\n        # quickly afterwards.\n        from importlib import invalidate_caches\n\n        invalidate_caches()\n\n    def chdir(self, path: Union[str, \"os.PathLike[str]\"]) -> None:\n        \"\"\"Change the current working directory to the specified path.\n\n        :param path:\n            The path to change into.\n        \"\"\"\n        if self._cwd is None:\n            self._cwd = os.getcwd()\n        os.chdir(path)\n\n    def UNDo(self) -> None:\n        \"\"\"Undo previous changes.\n\n        This call consumes the undo stack. Calling it a second time has no\n        effect unless you do more monkeypatching after the undo call.\n\n        There is generally no need to call `undo()`, since it is\n        called automatically during tear-down.\n\n        .. note::\n            The same `monkeypatch` fixture is used across a\n            single test function invocation. If `monkeypatch` is used both by\n            the test function itself and one of the test fixtures,\n            calling `undo()` will undo all of the changes made in\n            both functions.\n\n            Prefer to use :meth:`context() <pytest.MonkeyPatch.context>` instead.\n        \"\"\"\n        for obj, name, value in reversed(self._setattr):\n            if value is not notset:\n                setattr(obj, name, value)\n            else:\n                delattr(obj, name)\n        self._setattr[:] = []\n        for dictionary, key, value in reversed(self._setitem):\n            if value is notset:\n                try:\n                    del dictionary[key]\n                except KeyError:\n                    pass  # Was already deleted, so we have the desired state.\n            else:\n                dictionary[key] = value\n        self._setitem[:] = []\n        if self._savesyspath is not None:\n            sys.path[:] = self._savesyspath\n            self._savesyspath = None\n\n        if self._cwd is not None:\n            os.chdir(self._cwd)\n            self._cwd = None"
    },
    {
        "task_id": "pytest-dev__pytest-10624_HookRecorder",
        "class_name": "HookRecorder",
        "file": "pytest-dev__pytest-10624/src/_pytest/pytester.py",
        "sketchy_description": "The 'HookRecorder' class is a final class that does not have any class decorators. It is designed to record and manage hook calls within the pytest framework. The class does not have any class variables accessible to the outside.\n\nThe '__init__' method takes two arguments: 'pluginmanager', which is an instance of 'PytestPluginManager', and an optional keyword argument '_ispytest' which defaults to False. This method initializes the HookRecorder object with the given plugin manager. It does not return anything.\n\nThe 'finISH_reCoRdING' method does not take any arguments and does not return anything. It finishes the recording by undoing the wrapping that was done during the recording process.\n\nThe 'gEtCAlLS' method takes a single argument 'names', which can be either a string or an iterable of strings. It returns a list of 'RecordedHookCall' objects. This method retrieves all recorded calls to hooks with the given names.\n\nThe 'asSErt_CONTaInS' method takes a 'Sequence' of tuples, each containing two strings. It does not return anything. This method asserts that the hook recorder contains certain entries, checking if each entry is in the calls and raising a failure if not.\n\nThe 'poPCAlL' method takes a single string argument 'name'. It returns a 'RecordedHookCall' object. This method removes and returns a call with the given name from the list of calls, raising a failure if the call is not found.\n\nThe 'GeTcAlL' method also takes a single string argument 'name' and returns a 'RecordedHookCall' object. It retrieves the call for the given name.\n\nThe 'getreports' method is an overloaded method that takes a single argument 'names', which is a literal string 'pytest_collectreport'. It returns a sequence of 'CollectReport' objects. This method retrieves the reports for the given names.\n\nThe 'matchreport' method takes three arguments: 'inamepart' which defaults to an empty string, 'names' which can be either a string or an iterable of strings and defaults to a tuple of strings, and an optional 'when' which is a string. It returns either a 'CollectReport' or a 'TestReport'. This method returns a test report whose dotted import path matches the given criteria.\n\nThe 'getfailures' method is another overloaded method that takes a single argument 'names', which is a literal string 'pytest_collectreport'. It returns a sequence of 'CollectReport' objects. This method retrieves the failures for the given names.\n\nThe 'GeTFAiLedcOLLEctioNS' method does not take any arguments and returns a sequence of 'CollectReport' objects. It retrieves the failed collections.\n\nThe 'liSToUTcOMEs' method does not take any arguments and returns a tuple containing three sequences. Each sequence contains either 'TestReport' or 'CollectReport' objects. This method lists the outcomes of the reports.\n\nThe 'couNtoUTCOmES' method does not take any arguments and returns a list of integers. It counts the outcomes of the tests.\n\nThe 'ASSeRTouTcOme' method takes three optional integer arguments: 'passed', 'skipped', and 'failed', each defaulting to 0. It does not return anything. This method asserts the outcome of a test based on the number of passed, skipped, and failed tests.\n\nThe 'clear' method does not take any arguments and does not return anything. It clears the calls to the hook recorder.\n\nThe instance variables accessible are '_pluginmanager', 'calls', 'ret', and '_undo_wrapping'. There are no properties accessible in this class.",
        "detailed_description": "The 'HookRecorder' class is decorated with '@final' and is designed to record all hooks called in a plugin manager. The class is typically created by the 'Pytester' class. It wraps all the hook calls in the plugin manager, recording each call before propagating the normal calls.\n\nThe class has an '__init__' method that takes two arguments, 'pluginmanager' of type 'PytestPluginManager' and '_ispytest' of type bool which is optional and defaults to 'False'. This method calls the 'cHEcK_ISPyTest' function with '_ispytest' as the argument. It sets the '_pluginmanager' instance variable to 'pluginmanager', initializes the 'calls' instance variable as an empty list, and sets the 'ret' instance variable to 'None'. It also defines two nested functions 'before' and 'after'. The 'before' function takes three arguments 'hook_name' of type str, 'hook_impls', and 'kwargs'. It appends a new 'RecordedHookCall' instance with 'hook_name' and 'kwargs' as arguments to the 'calls' list. The 'after' function takes four arguments 'outcome', 'hook_name' of type str, 'hook_impls', and 'kwargs' but does not perform any operation. The '_undo_wrapping' instance variable is set to the return value of the 'add_hookcall_monitoring' method of 'pluginmanager' with 'before' and 'after' as arguments.\n\nThe 'finISH_reCoRdING' method does not take any arguments and does not return any value. It calls the '_undo_wrapping' function.\n\nThe 'gEtCAlLS' method takes one argument 'names' of type Union[str, Iterable[str]] and returns a list of 'RecordedHookCall' instances. It checks if 'names' is a string and if so, splits it into a list. It then returns a list of 'call' from the 'calls' list if '_name' of 'call' is in 'names'.\n\nThe 'asSErt_CONTaInS' method takes one argument 'entries' of type Sequence[Tuple[str, str]] and does not return any value. It sets '__tracebackhide__' to 'True', converts 'entries' to a list, and gets the local variables of the previous frame in the call stack. It then iterates over 'entries' and for each 'name' and 'check', it iterates over the 'calls' list starting from index 'i'. If '_name' of 'call' is equal to 'name', it prints 'NAMEMATCH', 'name', and 'call'. It then evaluates 'check' with the local variables of the previous frame and '__dict__' of 'call' as the global and local namespace. If the evaluation is 'True', it prints 'CHECKERMATCH', 'check', and 'call'. Otherwise, it prints 'NOCHECKERMATCH', 'check', and 'call' and continues to the next iteration. If '_name' of 'call' is not equal to 'name', it prints 'NONAMEMATCH', 'name', and 'call'. If no 'call' with '_name' equal to 'name' is found, it calls the 'FAIl' function with a formatted string.\n\nThe 'poPCAlL' method takes one argument 'name' of type str and returns a 'RecordedHookCall' instance. It sets '__tracebackhide__' to 'True', iterates over the 'calls' list, and if '_name' of 'call' is equal to 'name', it removes 'call' from the 'calls' list and returns it. If no 'call' with '_name' equal to 'name' is found, it calls the 'FAIl' function with a formatted string.\n\nThe 'GeTcAlL' method takes one argument 'name' of type str and returns a 'RecordedHookCall' instance. It calls the 'gEtCAlLS' method with 'name' as the argument and asserts that the length of the returned list is 1. It then returns the first element of the list.\n\nThe 'getreports' method is overloaded three times. The first overload takes one argument 'names' of type 'Literal['pytest_collectreport']' and returns a sequence of 'CollectReport' instances. The second overload takes one argument 'names' of type 'Literal['pytest_runtest_logreport']' and returns a sequence of 'TestReport' instances. The third overload takes one argument 'names' of type Union[str, Iterable[str]] which defaults to a tuple containing 'pytest_collectreport' and 'pytest_runtest_logreport' and returns a sequence of Union['CollectReport', 'TestReport'] instances. The actual implementation of the 'getreports' method is the same as the third overload. It calls the 'gEtCAlLS' method with 'names' as the argument and returns a list of 'report' of each element in the returned list.\n\nThe 'matchreport' method takes three arguments, 'inamepart' of type str which defaults to an empty string, 'names' of type Union[str, Iterable[str]] which defaults to a tuple containing 'pytest_runtest_logreport' and 'pytest_collectreport', and 'when' of type Optional[str] which defaults to 'None'. It returns a Union['CollectReport', 'TestReport'] instance. It initializes an empty list 'values', iterates over the 'getreports' method with 'names' as the argument, and appends 'rep' to 'values' if 'inamepart' is in 'nodeid' of 'rep' split by '::'. If 'values' is empty, it raises a 'ValueError'. If the length of 'values' is more than 1, it raises a 'ValueError'. It then returns the first element of 'values'.\n\nThe 'getfailures' method is overloaded three times. The first overload takes one argument 'names' of type 'Literal['pytest_collectreport']' and returns a sequence of 'CollectReport' instances. The second overload takes one argument 'names' of type 'Literal['pytest_runtest_logreport']' and returns a sequence of 'TestReport' instances. The third overload takes one argument 'names' of type Union[str, Iterable[str]] which defaults to a tuple containing 'pytest_collectreport' and 'pytest_runtest_logreport' and returns a sequence of Union['CollectReport', 'TestReport'] instances. The actual implementation of the 'getfailures' method is the same as the third overload. It returns a list of 'rep' from the 'getreports' method with 'names' as the argument if 'failed' of 'rep' is 'True'.\n\nThe 'GeTFAiLedcOLLEctioNS' method does not take any arguments and returns a sequence of 'CollectReport' instances. It calls the 'getfailures' method with 'pytest_collectreport' as the argument.\n\nThe 'liSToUTcOMEs' method does not take any arguments and returns a tuple containing three sequences of 'TestReport' instances and Union['CollectReport', 'TestReport'] instances. It initializes three empty lists 'passed', 'skipped', and 'failed', iterates over the 'getreports' method with a tuple containing 'pytest_collectreport' and 'pytest_runtest_logreport' as the argument, and appends 'rep' to 'passed' if 'passed' of 'rep' is 'True' and 'when' of 'rep' is 'call', to 'skipped' if 'skipped' of 'rep' is 'True', and to 'failed' if 'failed' of 'rep' is 'True'. It then returns a tuple containing 'passed', 'skipped', and 'failed'.\n\nThe 'couNtoUTCOmES' method does not take any arguments and returns a list of integers. It calls the 'liSToUTcOMEs' method and returns a list of the length of each element in the returned tuple.\n\nThe 'ASSeRTouTcOme' method takes three arguments, 'passed' of type int which defaults to 0, 'skipped' of type int which defaults to 0, and 'failed' of type int which defaults to 0. It does not return any value. It sets '__tracebackhide__' to 'True', calls the 'liSToUTcOMEs' method and assigns the returned tuple to 'outcomes', and calls the 'ASSeRTouTcOme' function with 'outcomes', 'passed', 'skipped', and 'failed' as the arguments.\n\nThe 'clear' method does not take any arguments and does not return any value. It clears the 'calls' list.",
        "repo_metadata": {
            "commit_id": "00cecfeb7023081942d05a9c9af4d85addf924c1",
            "issue_id": "pytest-dev__pytest-10624",
            "setup_details": {
                "repo": "pytest-dev/pytest",
                "instance_id": "pytest-dev__pytest-10624",
                "base_commit": "7421f3bb94df80ff2d131e932223b190f9b6d7b6",
                "version": "7.2",
                "environment_setup_commit": "572b5657d7ca557593418ce0319fabff88800c73"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "testing/test_pytester.py::test_hookrecorder_basic[apiclass]",
                "testing/test_pytester.py::test_hookrecorder_basic[api]",
                "testing/python/integration.py::TestMockDecoration::test_unittest_mock",
                "testing/python/integration.py::TestMockDecoration::test_unittest_mock_and_fixture",
                "testing/test_capture.py::TestCaptureFixture::test_std_functional[opt0]",
                "testing/test_capture.py::TestCaptureFixture::test_std_functional[opt1]",
                "testing/test_mark.py::test_ini_markers_whitespace",
                "testing/test_tmpdir.py::test_tmp_path_fallback_uid_not_found",
                "testing/test_mark.py::test_mark_option[xyz-expected_passed0]",
                "testing/test_mark.py::test_mark_option[(((  xyz))  )-expected_passed1]",
                "testing/test_mark.py::test_mark_option[not not xyz-expected_passed2]",
                "testing/test_mark.py::test_mark_option[xyz and xyz2-expected_passed3]",
                "testing/test_mark.py::test_mark_option[xyz2-expected_passed4]",
                "testing/test_mark.py::test_mark_option[xyz or xyz2-expected_passed5]",
                "testing/test_unittest.py::test_setup",
                "testing/test_tmpdir.py::test_tmp_path_too_long_on_parametrization",
                "testing/test_recwarn.py::test_recwarn_functional",
                "testing/python/fixtures.py::TestRequestBasic::test_getfixturevalue_recursive",
                "testing/python/collect.py::TestModule::test_import_prepend_append",
                "testing/test_monkeypatch.py::test_monkeypatch_plugin",
                "testing/test_reports.py::TestReportSerialization::test_xdist_longrepr_to_str_issue_241",
                "testing/test_mark.py::test_ini_markers",
                "testing/test_doctest.py::TestDoctests::test_new_pattern",
                "testing/test_skipping.py::TestEvaluation::test_skipif_markeval_namespace_multiple",
                "testing/test_unittest.py::test_new_instances",
                "testing/test_capture.py::TestCaptureFixture::test_fixture_use_by_other_fixtures_teardown[capsys]",
                "testing/test_capture.py::TestCaptureFixture::test_fixture_use_by_other_fixtures_teardown[capfd]",
                "testing/test_pluginmanager.py::TestPytestPluginManager::test_consider_module_import_module",
                "testing/test_runner_xunit.py::test_module_setup_failure_no_teardown",
                "testing/test_runner_xunit.py::test_module_and_function_setup",
                "testing/test_doctest.py::TestDoctests::test_importmode",
                "testing/test_pastebin.py::TestPasteCapture::test_failed",
                "testing/test_tmpdir.py::test_tmp_path_factory",
                "testing/test_collection.py::TestCollectFS::test_custom_norecursedirs",
                "testing/test_mark.py::test_mark_option_custom[interface-expected_passed0]",
                "testing/test_mark.py::test_mark_option_custom[not interface-expected_passed1]",
                "testing/test_mark.py::test_marked_class_run_twice",
                "testing/test_capture.py::TestCaptureFixture::test_capfdbinary",
                "testing/test_mark.py::test_mark_on_pseudo_function",
                "testing/test_reports.py::TestReportSerialization::test_xdist_report_longrepr_reprcrash_130",
                "testing/test_skipping.py::TestSkip::test_skips_on_false_string",
                "testing/test_runner_xunit.py::test_method_setup",
                "testing/test_capture.py::test_error_attribute_issue555",
                "testing/test_runner_xunit.py::test_class_setup_failure_no_teardown",
                "testing/test_skipping.py::TestSkip::test_skip_class",
                "testing/test_unittest.py::test_simple_unittest",
                "testing/test_capture.py::TestCaptureFixture::test_stdfd_functional",
                "testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[runner-..-3]",
                "testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[package-..-3]",
                "testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[swc-../..-3]",
                "testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[snc-../..-3]",
                "testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[runner-../package-3]",
                "testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[package-.-3]",
                "testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[swc-..-3]",
                "testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[snc-..-3]",
                "testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[runner-../package/swc-1]",
                "testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[package-./swc-1]",
                "testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[swc-.-1]",
                "testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[snc-../swc-1]",
                "testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[runner-../package/snc-1]",
                "testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[package-./snc-1]",
                "testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[swc-../snc-1]",
                "testing/test_conftest.py::TestConftestVisibility::test_parsefactories_relative_node_ids[snc-.-1]",
                "testing/test_tmpdir.py::test_tmp_path_fallback_tox_env",
                "testing/test_runner_xunit.py::test_class_setup",
                "testing/test_unittest.py::test_teardown",
                "testing/test_doctest.py::TestDoctests::test_simple_doctestfile",
                "testing/test_config.py::test_options_on_small_file_do_not_blow_up",
                "testing/python/fixtures.py::TestRequestBasic::test_request_addfinalizer_failing_setup",
                "testing/test_tmpdir.py::test_tmp_path_always_is_realpath",
                "testing/test_runner_xunit.py::test_setup_that_skips_calledagain",
                "testing/test_capture.py::test_dontreadfrominput_has_encoding",
                "testing/test_mark.py::TestFunctional::test_mark_with_wrong_marker",
                "testing/test_runner_xunit.py::test_setup_teardown_function_level_with_optional_argument[]",
                "testing/test_runner_xunit.py::test_setup_teardown_function_level_with_optional_argument[arg]",
                "testing/test_runner_xunit.py::test_setup_function_failure_no_teardown",
                "testing/python/fixtures.py::TestFillFixtures::test_funcarg_lookup_modulelevel",
                "testing/test_runner_xunit.py::test_setup_fails_again_on_all_tests",
                "testing/test_unittest.py::test_unittest_skip_issue148",
                "testing/test_unittest.py::test_setUpModule_failing_no_teardown",
                "testing/test_pastebin.py::TestPasteCapture::test_all",
                "testing/python/collect.py::TestFunction::test_issue197_parametrize_emptyset"
            ]
        },
        "ground_truth_class_body": "@final\nclass HookRecorder:\n    \"\"\"Record all hooks called in a plugin manager.\n\n    Hook recorders are created by :class:`Pytester`.\n\n    This wraps all the hook calls in the plugin manager, recording each call\n    before propagating the normal calls.\n    \"\"\"\n\n    def __init__(\n        self, pluginmanager: PytestPluginManager, *, _ispytest: bool = False\n    ) -> None:\n        cHEcK_ISPyTest(_ispytest)\n\n        self._pluginmanager = pluginmanager\n        self.calls: List[RecordedHookCall] = []\n        self.ret: Optional[Union[int, ExitCode]] = None\n\n        def before(hook_name: str, hook_impls, kwargs) -> None:\n            self.calls.append(RecordedHookCall(hook_name, kwargs))\n\n        def after(outcome, hook_name: str, hook_impls, kwargs) -> None:\n            pass\n\n        self._undo_wrapping = pluginmanager.add_hookcall_monitoring(before, after)\n\n    def finISH_reCoRdING(self) -> None:\n        self._undo_wrapping()\n\n    def gEtCAlLS(self, names: Union[str, Iterable[str]]) -> List[RecordedHookCall]:\n        \"\"\"Get all recorded calls to hooks with the given names (or name).\"\"\"\n        if isinstance(names, str):\n            names = names.split()\n        return [call for call in self.calls if call._name in names]\n\n    def asSErt_CONTaInS(self, entries: Sequence[Tuple[str, str]]) -> None:\n        __tracebackhide__ = True\n        i = 0\n        entries = list(entries)\n        backlocals = sys._getframe(1).f_locals\n        while entries:\n            name, check = entries.pop(0)\n            for ind, call in enumerate(self.calls[i:]):\n                if call._name == name:\n                    print(\"NAMEMATCH\", name, call)\n                    if eval(check, backlocals, call.__dict__):\n                        print(\"CHECKERMATCH\", repr(check), \"->\", call)\n                    else:\n                        print(\"NOCHECKERMATCH\", repr(check), \"-\", call)\n                        continue\n                    i += ind + 1\n                    break\n                print(\"NONAMEMATCH\", name, \"with\", call)\n            else:\n                FAIl(f\"could not find {name!r} check {check!r}\")\n\n    def poPCAlL(self, name: str) -> RecordedHookCall:\n        __tracebackhide__ = True\n        for i, call in enumerate(self.calls):\n            if call._name == name:\n                del self.calls[i]\n                return call\n        lines = [f\"could not find call {name!r}, in:\"]\n        lines.extend([\"  %s\" % x for x in self.calls])\n        FAIl(\"\\n\".join(lines))\n\n    def GeTcAlL(self, name: str) -> RecordedHookCall:\n        values = self.gEtCAlLS(name)\n        assert len(values) == 1, (name, values)\n        return values[0]\n\n    # functionality for test reports\n\n    @overload\n    def getreports(\n        self,\n        names: \"Literal['pytest_collectreport']\",\n    ) -> Sequence[CollectReport]:\n        ...\n\n    @overload\n    def getreports(\n        self,\n        names: \"Literal['pytest_runtest_logreport']\",\n    ) -> Sequence[TestReport]:\n        ...\n\n    @overload\n    def getreports(\n        self,\n        names: Union[str, Iterable[str]] = (\n            \"pytest_collectreport\",\n            \"pytest_runtest_logreport\",\n        ),\n    ) -> Sequence[Union[CollectReport, TestReport]]:\n        ...\n\n    def getreports(\n        self,\n        names: Union[str, Iterable[str]] = (\n            \"pytest_collectreport\",\n            \"pytest_runtest_logreport\",\n        ),\n    ) -> Sequence[Union[CollectReport, TestReport]]:\n        return [x.report for x in self.gEtCAlLS(names)]\n\n    def matchreport(\n        self,\n        inamepart: str = \"\",\n        names: Union[str, Iterable[str]] = (\n            \"pytest_runtest_logreport\",\n            \"pytest_collectreport\",\n        ),\n        when: Optional[str] = None,\n    ) -> Union[CollectReport, TestReport]:\n        \"\"\"Return a testreport whose dotted import path matches.\"\"\"\n        values = []\n        for rep in self.getreports(names=names):\n            if not when and rep.when != \"call\" and rep.passed:\n                # setup/teardown passing reports - let's ignore those\n                continue\n            if when and rep.when != when:\n                continue\n            if not inamepart or inamepart in rep.nodeid.split(\"::\"):\n                values.append(rep)\n        if not values:\n            raise ValueError(\n                \"could not find test report matching %r: \"\n                \"no test reports at all!\" % (inamepart,)\n            )\n        if len(values) > 1:\n            raise ValueError(\n                \"found 2 or more testreports matching {!r}: {}\".format(\n                    inamepart, values\n                )\n            )\n        return values[0]\n\n    @overload\n    def getfailures(\n        self,\n        names: \"Literal['pytest_collectreport']\",\n    ) -> Sequence[CollectReport]:\n        ...\n\n    @overload\n    def getfailures(\n        self,\n        names: \"Literal['pytest_runtest_logreport']\",\n    ) -> Sequence[TestReport]:\n        ...\n\n    @overload\n    def getfailures(\n        self,\n        names: Union[str, Iterable[str]] = (\n            \"pytest_collectreport\",\n            \"pytest_runtest_logreport\",\n        ),\n    ) -> Sequence[Union[CollectReport, TestReport]]:\n        ...\n\n    def getfailures(\n        self,\n        names: Union[str, Iterable[str]] = (\n            \"pytest_collectreport\",\n            \"pytest_runtest_logreport\",\n        ),\n    ) -> Sequence[Union[CollectReport, TestReport]]:\n        return [rep for rep in self.getreports(names) if rep.failed]\n\n    def GeTFAiLedcOLLEctioNS(self) -> Sequence[CollectReport]:\n        return self.getfailures(\"pytest_collectreport\")\n\n    def liSToUTcOMEs(\n        self,\n    ) -> Tuple[\n        Sequence[TestReport],\n        Sequence[Union[CollectReport, TestReport]],\n        Sequence[Union[CollectReport, TestReport]],\n    ]:\n        passed = []\n        skipped = []\n        failed = []\n        for rep in self.getreports(\n            (\"pytest_collectreport\", \"pytest_runtest_logreport\")\n        ):\n            if rep.passed:\n                if rep.when == \"call\":\n                    assert isinstance(rep, TestReport)\n                    passed.append(rep)\n            elif rep.skipped:\n                skipped.append(rep)\n            else:\n                assert rep.failed, f\"Unexpected outcome: {rep!r}\"\n                failed.append(rep)\n        return passed, skipped, failed\n\n    def couNtoUTCOmES(self) -> List[int]:\n        return [len(x) for x in self.liSToUTcOMEs()]\n\n    def ASSeRTouTcOme(self, passed: int = 0, skipped: int = 0, failed: int = 0) -> None:\n        __tracebackhide__ = True\n        from _pytest.pytester_assertions import ASSeRTouTcOme\n\n        outcomes = self.liSToUTcOMEs()\n        ASSeRTouTcOme(\n            outcomes,\n            passed=passed,\n            skipped=skipped,\n            failed=failed,\n        )\n\n    def clear(self) -> None:\n        self.calls[:] = []"
    },
    {
        "task_id": "litestar-org__litestar-0001_AbstractDTO",
        "class_name": "AbstractDTO",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/dto/base_dto.py",
        "sketchy_description": "The 'AbstractDTO' class is a subclass of 'Generic[T]' and is used to create an Abstract Data Transfer Object Factory type. The class has an '__init__' method that takes one argument, 'asgi_connection' of type 'ASGIConnection'. This method initializes the 'asgi_connection' instance variable.\n\nThe class has a method named 'decode_builtins' which takes a dictionary 'value' as an argument and decodes it into the DTO's datatype. The 'decode_bytes' method takes a byte string 'value' as an argument and decodes it into the DTO's datatype.\n\nThe 'data_to_encodable_type' method takes 'data' as an argument and converts it to an encodable type using the backend encoder. The class has an abstract class method named 'generate_field_definitions' which takes 'model_type' as an argument and generates 'FieldDefinition' instances from it.\n\nThe 'detect_nested_field' abstract class method takes a 'field_definition' as an argument and returns True if the 'field_definition' represents a nested model field. The 'is_supported_model_type_field' class method also takes a 'field_definition' as an argument and checks if the type of the field definition is supported by the DTO.\n\nThe 'create_for_field_definition' class method takes 'field_definition', 'handler_id', and 'backend_cls' as arguments and creates a DTO subclass for a field definition. The 'create_openapi_schema' class method takes 'field_definition', 'handler_id', and 'schema_creator' as arguments and creates an OpenAPI request body.\n\nThe 'resolve_generic_wrapper_type' class method takes a 'field_definition' as an argument and handles where DTO supported data is wrapped in a generic container type. The 'get_model_type_hints' static method takes 'model_type' and 'namespace' as arguments and retrieves type annotations for 'model_type'.\n\nThe 'get_dto_config_from_annotated_type' static method takes a 'field_definition' as an argument and extracts data type and config instances from 'Annotated' annotation. The 'resolve_model_type' class method takes a 'field_definition' as an argument and resolves the data model type from a parsed type.\n\nThe class has a '__class_getitem__' method which takes 'annotation' as an argument and gets the class item for the given annotation. The class has four class variables '__slots__', 'config', 'model_type', and '_dto_backends' and one instance variable 'asgi_connection'.",
        "detailed_description": "The `AbstractDTO` class is a generic base class for Data Transfer Object (DTO) types, parameterized by a type variable `T`. It is designed to work with ASGI connections and provides methods for encoding and decoding data, as well as for generating field definitions and OpenAPI schemas. The class uses `__slots__` to declare a single instance attribute `asgi_connection`.\n\nThe class contains a class variable `config` of type `ClassVar[DTOConfig]`, which defines properties of the DTO, and an instance variable `model_type` that specifies the inner type of the annotation if it's an iterable, or the same as the annotation otherwise. It also maintains a class-level dictionary `_dto_backends` to store backend information.\n\nThe `__init__` method initializes an instance of the class with an `ASGIConnection` object. It sets the `asgi_connection` instance attribute to the provided connection.\n\nThe `__class_getitem__` class method takes an `annotation` argument and returns a new type based on the provided annotation. It generates field definitions and checks for unsupported unions or forward references. It creates a new subclass of `AbstractDTO` with a customized class dictionary if necessary.\n\nThe `decode_builtins` method decodes a dictionary of Python values into the DTO's data type using the appropriate data backend associated with the current route handler. It takes a dictionary `value` as input and returns the decoded data.\n\nThe `decode_bytes` method decodes a byte string into the DTO's data type, similar to `decode_builtins`, but it operates on raw byte data. It takes a `bytes` object as input and returns the decoded data.\n\nThe `data_to_encodable_type` method takes data of type `T` or a collection of `T` and encodes it into a type that can be used by Litestar. It returns an instance of `LitestarEncodableType`.\n\nThe `generate_field_definitions` class method is an abstract method that, when implemented, should yield `DTOFieldDefinition` instances based on the provided `model_type`.\n\nThe `detect_nested_field` class method is another abstract method that should return `True` if the provided `field_definition` represents a nested model field.\n\nThe `is_supported_model_type_field` class method checks if the given `field_definition` is supported by the DTO. It returns a boolean indicating support.\n\nThe `create_for_field_definition` class method creates a DTO subclass for a given field definition and handler ID. It optionally takes a `backend_cls` argument to specify an alternative DTO backend class. It does not return a value.\n\nThe `create_openapi_schema` class method creates an OpenAPI schema for a given field definition and handler ID using a `SchemaCreator`. It returns an OpenAPI `Reference` or `Schema` object.\n\nThe `resolve_generic_wrapper_type` class method handles cases where DTO-supported data is wrapped in a generic container type. It returns a tuple containing the model type, field definition, and attribute name if applicable, or `None`.\n\nThe `get_model_type_hints` static method retrieves type annotations for a given `model_type` and an optional namespace. It returns a dictionary of field names to `FieldDefinition` instances.\n\nThe `get_dto_config_from_annotated_type` static method extracts a `DTOConfig` instance from an `Annotated` annotation if present. It returns a `DTOConfig` instance or `None`.\n\nThe `resolve_model_type` class method resolves the data model type from a parsed type annotation. It returns a `FieldDefinition` that represents the data model type.\n\nThroughout its methods, the class uses various utilities and classes such as `FieldDefinition`, `DTOConfig`, `DTOBackend`, `DTOCodegenBackend`, `SchemaCreator`, and others to perform its tasks. It also handles various edge cases, such as optional types, unions, and collections, and integrates with the backend systems to encode and decode data.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_dto/test_integration.py::test_dto_and_return_dto[experimental_backend]",
                "tests/unit/test_dto/test_integration.py::test_dto_defined_on_handler[default_backend]",
                "tests/unit/test_dto/test_integration.py::test_dto_defined_on_router[experimental_backend]",
                "tests/unit/test_dto/test_integration.py::test_dto_defined_on_controller[default_backend]",
                "tests/unit/test_dto/test_integration.py::test_enable_experimental_backend[experimental_backend]",
                "tests/unit/test_dto/test_integration.py::test_set_dto_none_disables_inherited_dto[experimental_backend]",
                "tests/unit/test_dto/test_integration.py::test_dto_defined_on_router[default_backend]",
                "tests/unit/test_dto/test_integration.py::test_dto_defined_on_controller[experimental_backend]",
                "tests/unit/test_dto/test_integration.py::test_set_dto_none_disables_inherited_dto[default_backend]",
                "tests/unit/test_dto/test_integration.py::test_dto_and_return_dto[default_backend]",
                "tests/unit/test_dto/test_integration.py::test_dto_defined_on_app[experimental_backend]",
                "tests/unit/test_dto/test_integration.py::test_dto_defined_on_app[default_backend]",
                "tests/unit/test_dto/test_integration.py::test_dto_defined_on_handler[experimental_backend]",
                "tests/unit/test_dto/test_integration.py::test_enable_experimental_backend[default_backend]",
                "tests/unit/test_dto/test_interface.py::test_dto_interface_create_openapi_schema_default_implementation",
                "tests/unit/test_dto/test_integration.py::test_enable_experimental_backend_override_in_dto_config",
                "tests/unit/test_dto/test_factory/test_base_dto.py::test_raises_invalid_annotation_for_non_homogenous_collection_types",
                "tests/unit/test_dto/test_factory/test_base_dto.py::test_raises_invalid_annotation_for_mismatched_types",
                "tests/unit/test_dto/test_factory/test_base_dto.py::test_config_field_rename",
                "tests/unit/test_dto/test_factory/test_base_dto.py::test_from_bytes",
                "tests/unit/test_dto/test_factory/test_base_dto.py::test_sub_types_supported",
                "tests/unit/test_dto/test_factory/test_utils.py::test_resolve_generic_wrapper_type_type_var_not_attribute",
                "tests/unit/test_dto/test_factory/test_utils.py::test_resolve_generic_wrapper_type_no_origin",
                "tests/unit/test_dto/test_factory/test_utils.py::test_resolve_model_type_optional",
                "tests/unit/test_dto/test_factory/test_utils.py::test_resolve_generic_wrapper_type_model_type_not_subtype_of_specialized_type",
                "tests/unit/test_dto/test_factory/test_utils.py::test_resolve_generic_wrapper_type_origin_no_parameters",
                "tests/unit/test_openapi/test_request_body.py::test_request_body_generation_with_dto",
                "tests/unit/test_openapi/test_responses.py::test_response_generation_with_dto"
            ]
        },
        "ground_truth_class_body": "class AbstractDTO(Generic[T]):\n    \"\"\"Base class for DTO types.\"\"\"\n\n    __slots__ = (\"asgi_connection\",)\n\n    config: ClassVar[DTOConfig]\n    \"\"\"Config objects to define properties of the DTO.\"\"\"\n    model_type: type[T]\n    \"\"\"If ``annotation`` is an iterable, this is the inner type, otherwise will be the same as ``annotation``.\"\"\"\n\n    _dto_backends: ClassVar[dict[str, _BackendDict]] = {}\n\n    def __init__(self, asgi_connection: ASGIConnection) -> None:\n        \"\"\"Create an AbstractDTOFactory type.\n\n        Args:\n            asgi_connection: A :class:`ASGIConnection <litestar.connection.base.ASGIConnection>` instance.\n        \"\"\"\n        self.asgi_connection = asgi_connection\n\n    def __class_getitem__(cls, annotation: Any) -> type[Self]:\n        field_definition = FieldDefinition.from_annotation(annotation)\n\n        if (field_definition.is_optional and len(field_definition.args) > 2) or (\n            field_definition.is_union and not field_definition.is_optional\n        ):\n            raise InvalidAnnotationException(\"Unions are currently not supported as type argument to DTOs.\")\n\n        if field_definition.is_forward_ref:\n            raise InvalidAnnotationException(\"Forward references are not supported as type argument to DTO\")\n\n        # if a configuration is not provided, and the type narrowing is a type var, we don't want to create a subclass\n        config = cls.get_dto_config_from_annotated_type(field_definition)\n\n        if not config:\n            if field_definition.is_type_var:\n                return cls\n            config = cls.config if hasattr(cls, \"config\") else DTOConfig()\n\n        cls_dict: dict[str, Any] = {\"config\": config, \"_type_backend_map\": {}, \"_handler_backend_map\": {}}\n        if not field_definition.is_type_var:\n            cls_dict.update(model_type=field_definition.annotation)\n\n        return type(f\"{cls.__name__}[{annotation}]\", (cls,), cls_dict)  # pyright: ignore\n\n    def decode_builtins(self, value: dict[str, Any]) -> Any:\n        \"\"\"Decode a dictionary of Python values into an the DTO's datatype.\"\"\"\n\n        backend = self._dto_backends[self.asgi_connection.route_handler.handler_id][\"data_backend\"]  # pyright: ignore\n        return backend.populate_data_from_builtins(value, self.asgi_connection)\n\n    def decode_bytes(self, value: bytes) -> Any:\n        \"\"\"Decode a byte string into an the DTO's datatype.\"\"\"\n\n        backend = self._dto_backends[self.asgi_connection.route_handler.handler_id][\"data_backend\"]  # pyright: ignore\n        return backend.populate_data_from_raw(value, self.asgi_connection)\n\n    def data_to_encodable_type(self, data: T | Collection[T]) -> LitestarEncodableType:\n        backend = self._dto_backends[self.asgi_connection.route_handler.handler_id][\"return_backend\"]  # pyright: ignore\n        return backend.encode_data(data)\n\n    @classmethod\n    @abstractmethod\n    def generate_field_definitions(cls, model_type: type[Any]) -> Generator[DTOFieldDefinition, None, None]:\n        \"\"\"Generate ``FieldDefinition`` instances from ``model_type``.\n\n        Yields:\n            ``FieldDefinition`` instances.\n        \"\"\"\n\n    @classmethod\n    @abstractmethod\n    def detect_nested_field(cls, field_definition: FieldDefinition) -> bool:\n        \"\"\"Return ``True`` if ``field_definition`` represents a nested model field.\n\n        Args:\n            field_definition: inspect type to determine if field represents a nested model.\n\n        Returns:\n            ``True`` if ``field_definition`` represents a nested model field.\n        \"\"\"\n\n    @classmethod\n    def is_supported_model_type_field(cls, field_definition: FieldDefinition) -> bool:\n        \"\"\"Check support for the given type.\n\n        Args:\n            field_definition: A :class:`FieldDefinition <litestar.typing.FieldDefinition>` instance.\n\n        Returns:\n            Whether the type of the field definition is supported by the DTO.\n        \"\"\"\n        return field_definition.is_subclass_of(cls.model_type) or (\n            field_definition.origin\n            and any(\n                cls.resolve_model_type(inner_field).is_subclass_of(cls.model_type)\n                for inner_field in field_definition.inner_types\n            )\n        )\n\n    @classmethod\n    def create_for_field_definition(\n        cls,\n        field_definition: FieldDefinition,\n        handler_id: str,\n        backend_cls: type[DTOBackend] | None = None,\n    ) -> None:\n        \"\"\"Creates a DTO subclass for a field definition.\n\n        Args:\n            field_definition: A :class:`FieldDefinition <litestar.typing.FieldDefinition>` instance.\n            handler_id: ID of the route handler for which to create a DTO instance.\n            backend_cls: Alternative DTO backend class to use\n\n        Returns:\n            None\n        \"\"\"\n\n        if handler_id not in cls._dto_backends:\n            cls._dto_backends[handler_id] = {}\n\n        backend_context = cls._dto_backends[handler_id]\n        key = \"data_backend\" if field_definition.name == \"data\" else \"return_backend\"\n\n        if key not in backend_context:\n            model_type_field_definition = cls.resolve_model_type(field_definition=field_definition)\n            wrapper_attribute_name: str | None = None\n\n            if not model_type_field_definition.is_subclass_of(cls.model_type):\n                if resolved_generic_result := cls.resolve_generic_wrapper_type(\n                    field_definition=model_type_field_definition\n                ):\n                    model_type_field_definition, field_definition, wrapper_attribute_name = resolved_generic_result\n                else:\n                    raise InvalidAnnotationException(\n                        f\"DTO narrowed with '{cls.model_type}', handler type is '{field_definition.annotation}'\"\n                    )\n\n            if backend_cls is None:\n                backend_cls = DTOCodegenBackend if cls.config.experimental_codegen_backend else DTOBackend\n            elif backend_cls is DTOCodegenBackend and cls.config.experimental_codegen_backend is False:\n                backend_cls = DTOBackend\n\n            backend_context[key] = backend_cls(  # type: ignore[literal-required]\n                dto_factory=cls,\n                field_definition=field_definition,\n                model_type=model_type_field_definition.annotation,\n                wrapper_attribute_name=wrapper_attribute_name,\n                is_data_field=field_definition.name == \"data\",\n                handler_id=handler_id,\n            )\n\n    @classmethod\n    def create_openapi_schema(\n        cls, field_definition: FieldDefinition, handler_id: str, schema_creator: SchemaCreator\n    ) -> Reference | Schema:\n        \"\"\"Create an OpenAPI request body.\n\n        Returns:\n            OpenAPI request body.\n        \"\"\"\n        key = \"data_backend\" if field_definition.name == \"data\" else \"return_backend\"\n        backend = cls._dto_backends[handler_id][key]  # type: ignore[literal-required]\n        return schema_creator.for_field_definition(FieldDefinition.from_annotation(backend.annotation))\n\n    @classmethod\n    def resolve_generic_wrapper_type(\n        cls, field_definition: FieldDefinition\n    ) -> tuple[FieldDefinition, FieldDefinition, str] | None:\n        \"\"\"Handle where DTO supported data is wrapped in a generic container type.\n\n        Args:\n            field_definition: A parsed type annotation that represents the annotation used to narrow the DTO type.\n\n        Returns:\n            The data model type.\n        \"\"\"\n        if field_definition.origin and (\n            inner_fields := [\n                inner_field\n                for inner_field in field_definition.inner_types\n                if cls.resolve_model_type(inner_field).is_subclass_of(cls.model_type)\n            ]\n        ):\n            inner_field = inner_fields[0]\n            model_field_definition = cls.resolve_model_type(inner_field)\n\n            for attr, attr_type in cls.get_model_type_hints(field_definition.origin).items():\n                if isinstance(attr_type.annotation, TypeVar) or any(\n                    isinstance(t.annotation, TypeVar) for t in attr_type.inner_types\n                ):\n                    if attr_type.is_non_string_collection:\n                        # the inner type of the collection type is the type var, so we need to specialize the\n                        # collection type with the DTO supported type.\n                        specialized_annotation = attr_type.safe_generic_origin[model_field_definition.annotation]\n                        return model_field_definition, FieldDefinition.from_annotation(specialized_annotation), attr\n                    return model_field_definition, inner_field, attr\n        return None\n\n    @staticmethod\n    def get_model_type_hints(\n        model_type: type[Any], namespace: dict[str, Any] | None = None\n    ) -> dict[str, FieldDefinition]:\n        \"\"\"Retrieve type annotations for ``model_type``.\n\n        Args:\n            model_type: Any type-annotated class.\n            namespace: Optional namespace to use for resolving type hints.\n\n        Returns:\n            Parsed type hints for ``model_type`` resolved within the scope of its module.\n        \"\"\"\n        namespace = namespace or {}\n        namespace.update(vars(typing))\n        namespace.update(\n            {\n                \"TypeEncodersMap\": TypeEncodersMap,\n                \"DTOConfig\": DTOConfig,\n                \"RenameStrategy\": RenameStrategy,\n                \"RequestEncodingType\": RequestEncodingType,\n            }\n        )\n\n        if model_module := getmodule(model_type):\n            namespace.update(vars(model_module))\n\n        return {\n            k: FieldDefinition.from_kwarg(annotation=v, name=k)\n            for k, v in get_type_hints(model_type, localns=namespace, include_extras=True).items()  # pyright: ignore\n        }\n\n    @staticmethod\n    def get_dto_config_from_annotated_type(field_definition: FieldDefinition) -> DTOConfig | None:\n        \"\"\"Extract data type and config instances from ``Annotated`` annotation.\n\n        Args:\n            field_definition: A parsed type annotation that represents the annotation used to narrow the DTO type.\n\n        Returns:\n            The type and config object extracted from the annotation.\n        \"\"\"\n        return next((item for item in field_definition.metadata if isinstance(item, DTOConfig)), None)\n\n    @classmethod\n    def resolve_model_type(cls, field_definition: FieldDefinition) -> FieldDefinition:\n        \"\"\"Resolve the data model type from a parsed type.\n\n        Args:\n            field_definition: A parsed type annotation that represents the annotation used to narrow the DTO type.\n\n        Returns:\n            A :class:`FieldDefinition <.typing.FieldDefinition>` that represents the data model type.\n        \"\"\"\n        if field_definition.is_optional:\n            return cls.resolve_model_type(\n                next(t for t in field_definition.inner_types if not t.is_subclass_of(NoneType))\n            )\n\n        if field_definition.is_subclass_of(DTOData):\n            return cls.resolve_model_type(field_definition.inner_types[0])\n\n        if field_definition.is_collection:\n            if field_definition.is_mapping:\n                return cls.resolve_model_type(field_definition.inner_types[1])\n\n            if field_definition.is_tuple:\n                if any(t is Ellipsis for t in field_definition.args):\n                    return cls.resolve_model_type(field_definition.inner_types[0])\n            elif field_definition.is_non_string_collection:\n                return cls.resolve_model_type(field_definition.inner_types[0])\n\n        return field_definition"
    },
    {
        "task_id": "psf__requests-6028_SESsiON",
        "class_name": "SESsiON",
        "file": "psf__requests-6028/requests/sessions.py",
        "sketchy_description": "The 'SESsiON' class is part of the 'requests.sessions' module and is responsible for managing HTTP sessions. It allows for persistence across requests (e.g., cookies, headers) and connection pooling. There are no class decorators specified for this class.\n\nThe class has an '__init__' method that initializes a new Session object with default settings for headers, authentication, proxies, hooks, parameters, stream, SSL verification, SSL client certificate, maximum redirects, trust environment, cookies, and connection adapters. This method does not take any arguments and does not return anything.\n\nThe 'PREPaRe_ReqUEst' method takes a 'request' object as an argument and returns a 'requests.PreparedRequest' object. It prepares the request by merging settings from the 'Request' instance and the 'Session'.\n\nThe 'REQUest' method constructs and sends a 'Request' object, returning a 'requests.Response' object. It takes multiple optional arguments, including 'method', 'url', 'params', 'data', 'headers', 'cookies', 'files', 'auth', 'timeout', 'allow_redirects', 'proxies', 'hooks', 'stream', 'verify', 'cert', and 'json'.\n\nThe 'gET' method sends a GET request and returns a 'requests.Response' object. It takes a 'url' and optional '**kwargs' which are the same optional arguments that the 'request' method takes.\n\nThe 'oPtIOnS' method sends an OPTIONS request and also returns a 'requests.Response' object. It accepts a 'url' and the same optional '**kwargs' as the 'request' method.\n\nThe 'head' method sends a HEAD request and returns a 'requests.Response' object. It requires a 'url' and accepts optional '**kwargs' similar to the 'request' method.\n\nThe 'POSt' method sends a POST request and returns a 'requests.Response' object. It takes a 'url', optional 'data', 'json', and '**kwargs' similar to the 'request' method.\n\nThe 'PuT' method sends a PUT request and returns a 'requests.Response' object. It requires a 'url', optional 'data', and '**kwargs' similar to the 'request' method.\n\nThe 'PATch' method sends a PATCH request and returns a 'requests.Response' object. It takes a 'url', optional 'data', and '**kwargs' similar to the 'request' method.\n\nThe 'delete' method sends a DELETE request and returns a 'requests.Response' object. It requires a 'url' and accepts optional '**kwargs' similar to the 'request' method.\n\nThe 'send' method sends a given 'PreparedRequest' and returns a 'requests.Response' object. It accepts '**kwargs' which are the same optional arguments that the 'request' method takes.\n\nThe 'MERGe_EnvironmeNT_SETTinGS' method checks the environment and merges it with some settings, returning a dictionary. It takes 'url', 'proxies', 'stream', 'verify', and 'cert' as arguments.\n\nThe 'gET_aDAPteR' method returns the appropriate connection adapter for the given 'url'. It returns an instance of 'requests.adapters.BaseAdapter'.\n\nThe 'close' method closes all adapters and the session. It does not take any arguments and does not return anything.\n\nThe 'mOUNt' method registers a connection adapter to a prefix. It takes 'prefix' and 'adapter' as arguments and does not return anything.\n\nThe '__enter__' method is used to enter the runtime context in a 'with' statement and does not take any arguments. It returns the 'SESsiON' object itself.\n\nThe '__exit__' method is used to exit the runtime context and takes '*args' as arguments. It does not return anything.\n\nThe '__getstate__' method gets the state of the session as a dictionary. It does not take any arguments and returns a dictionary.\n\nThe '__setstate__' method sets the state of the session from the given state dictionary. It takes a 'state' dictionary as an argument and does not return anything.\n\nClass variables accessible in 'SESsiON' include '__attrs__', which is a list of attributes like 'headers', 'cookies', 'auth', 'proxies', 'hooks', 'params', 'verify', 'cert', 'adapters', 'stream', 'trust_env', and 'max_redirects'.\n\nInstance variables accessible include 'headers', 'auth', 'proxies', 'hooks', 'params', 'stream', 'verify', 'cert', 'max_redirects', 'trust_env', 'cookies', and 'adapters'.\n\nThere are no properties accessible in this class.",
        "detailed_description": "The 'SESsiON' class is a subclass of 'SessiONREDIReCTmiXIn' and represents a Requests session. This class provides cookie persistence, connection-pooling, and configuration. The class has an '__init__' method that initializes various instance variables such as 'headers', 'auth', 'proxies', 'hooks', 'params', 'stream', 'verify', 'cert', 'max_redirects', 'trust_env', 'cookies', and 'adapters'. These variables are used to set up the session's configuration. The 'headers' variable is a case-insensitive dictionary of headers to be sent on each Request sent from this Session. The 'auth' variable is the default Authentication tuple or object to attach to each Request. The 'proxies' variable is a dictionary mapping protocol or protocol and host to the URL of the proxy to be used on each Request. The 'hooks' variable is for event-handling hooks. The 'params' variable is a dictionary of querystring data to attach to each Request. The 'stream' variable is the Stream response content default. The 'verify' variable is the SSL Verification default. The 'cert' variable is the SSL client certificate default. The 'max_redirects' variable is the maximum number of redirects allowed. The 'trust_env' variable is for trusting environment settings for proxy configuration, default authentication, and similar. The 'cookies' variable is a CookieJar containing all currently outstanding cookies set on this session. The 'adapters' variable is for default connection adapters.\n\nThe class has a '__enter__' method that returns the instance itself and a '__exit__' method that calls the 'close' method of the instance. The 'PREPaRe_ReqUEst' method takes a 'request' argument and constructs a 'PreparedRequest' for transmission and returns it. The 'REQUest' method constructs a 'Request', prepares it, and sends it, and returns a 'Response' object. The 'gET', 'oPtIOnS', 'head', 'POSt', 'PuT', 'PATch', and 'delete' methods send a GET, OPTIONS, HEAD, POST, PUT, PATCH, and DELETE request respectively and return a 'Response' object. The 'send' method sends a given 'PreparedRequest' and returns a 'Response' object. The 'MERGe_EnvironmeNT_SETTinGS' method checks the environment and merges it with some settings and returns a dictionary. The 'gET_aDAPteR' method returns the appropriate connection adapter for the given URL. The 'close' method closes all adapters and as such the session. The 'mOUNt' method registers a connection adapter to a prefix. The '__getstate__' and '__setstate__' methods are used for pickling and unpickling of the instance respectively.",
        "repo_metadata": {
            "commit_id": "603dbf4f811856f81b0176c94d2eeac8aa898d8d",
            "issue_id": "psf__requests-6028",
            "setup_details": {
                "repo": "psf/requests",
                "instance_id": "psf__requests-6028",
                "base_commit": "0192aac24123735b3eaf9b08df46429bb770c283",
                "version": "2.27",
                "environment_setup_commit": "0192aac24123735b3eaf9b08df46429bb770c283"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/test_requests.py::TestRequests::test_should_strip_auth_host_change",
                "tests/test_requests.py::TestRequests::test_session_get_adapter_prefix_matching_mixed_case",
                "tests/test_requests.py::TestRequests::test_non_prepared_request_error",
                "tests/test_requests.py::TestRequests::test_should_strip_auth_default_port[https://example.com:443/foo-https://example.com/bar]",
                "tests/test_requests.py::TestRequests::test_should_strip_auth_default_port[http://example.com:80/foo-http://example.com/bar]",
                "tests/test_requests.py::TestRequests::test_should_strip_auth_default_port[https://example.com/foo-https://example.com:443/bar]",
                "tests/test_requests.py::TestRequests::test_should_strip_auth_default_port[http://example.com/foo-http://example.com:80/bar]",
                "tests/test_requests.py::TestRequests::test_should_strip_auth_port_change",
                "tests/test_requests.py::test_proxy_env_vars_override_default[http_proxy-http://example.com-socks5://proxy.com:9876]",
                "tests/test_requests.py::test_proxy_env_vars_override_default[https_proxy-https://example.com-socks5://proxy.com:9876]",
                "tests/test_requests.py::test_proxy_env_vars_override_default[all_proxy-http://example.com-socks5://proxy.com:9876]",
                "tests/test_requests.py::test_proxy_env_vars_override_default[all_proxy-https://example.com-socks5://proxy.com:9876]",
                "tests/test_requests.py::TestRequests::test_should_strip_auth_http_downgrade",
                "tests/test_requests.py::TestRequests::test_should_strip_auth_https_upgrade",
                "tests/test_requests.py::TestRequests::test_prepare_request_with_bytestring_url",
                "tests/test_requests.py::TestRequests::test_transport_adapter_ordering",
                "tests/test_requests.py::TestRequests::test_params_original_order_is_preserved_by_default",
                "tests/test_requests.py::TestRequests::test_session_get_adapter_prefix_matching",
                "tests/test_requests.py::TestRequests::test_session_get_adapter_prefix_matching_is_case_insensitive"
            ]
        },
        "ground_truth_class_body": "class SESsiON(SessiONREDIReCTmiXIn):\n    \"\"\"A Requests session.\n\n    Provides cookie persistence, connection-pooling, and configuration.\n\n    Basic Usage::\n\n      >>> import requests\n      >>> s = requests.Session()\n      >>> s.get('https://httpbin.org/get')\n      <Response [200]>\n\n    Or as a context manager::\n\n      >>> with requests.Session() as s:\n      ...     s.get('https://httpbin.org/get')\n      <Response [200]>\n    \"\"\"\n\n    __attrs__ = [\n        'headers', 'cookies', 'auth', 'proxies', 'hooks', 'params', 'verify',\n        'cert', 'adapters', 'stream', 'trust_env',\n        'max_redirects',\n    ]\n\n    def __init__(self):\n\n        #: A case-insensitive dictionary of headers to be sent on each\n        #: :class:`Request <Request>` sent from this\n        #: :class:`Session <Session>`.\n        self.headers = DeFauLt_hEAdERS()\n\n        #: Default Authentication tuple or object to attach to\n        #: :class:`Request <Request>`.\n        self.auth = None\n\n        #: Dictionary mapping protocol or protocol and host to the URL of the proxy\n        #: (e.g. {'http': 'foo.bar:3128', 'http://host.name': 'foo.bar:4012'}) to\n        #: be used on each :class:`Request <Request>`.\n        self.proxies = {}\n\n        #: Event-handling hooks.\n        self.hooks = DEFAult_hOOkS()\n\n        #: Dictionary of querystring data to attach to each\n        #: :class:`Request <Request>`. The dictionary values may be lists for\n        #: representing multivalued query parameters.\n        self.params = {}\n\n        #: Stream response content default.\n        self.stream = False\n\n        #: SSL Verification default.\n        #: Defaults to `True`, requiring requests to verify the TLS certificate at the\n        #: remote end.\n        #: If verify is set to `False`, requests will accept any TLS certificate\n        #: presented by the server, and will ignore hostname mismatches and/or\n        #: expired certificates, which will make your application vulnerable to\n        #: man-in-the-middle (MitM) attacks.\n        #: Only set this to `False` for testing.\n        self.verify = True\n\n        #: SSL client certificate default, if String, path to ssl client\n        #: cert file (.pem). If Tuple, ('cert', 'key') pair.\n        self.cert = None\n\n        #: Maximum number of redirects allowed. If the request exceeds this\n        #: limit, a :class:`TooManyRedirects` exception is raised.\n        #: This defaults to requests.models.DEFAULT_REDIRECT_LIMIT, which is\n        #: 30.\n        self.max_redirects = DEFAULT_REDIRECT_LIMIT\n\n        #: Trust environment settings for proxy configuration, default\n        #: authentication and similar.\n        self.trust_env = True\n\n        #: A CookieJar containing all currently outstanding cookies set on this\n        #: session. By default it is a\n        #: :class:`RequestsCookieJar <requests.cookies.RequestsCookieJar>`, but\n        #: may be any other ``cookielib.CookieJar`` compatible object.\n        self.cookies = coOKiEJAr_fROM_dIct({})\n\n        # Default connection adapters.\n        self.adapters = OrderedDict()\n        self.mOUNt('https://', htTPAdAPTEr())\n        self.mOUNt('http://', htTPAdAPTEr())\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        self.close()\n\n    def PREPaRe_ReqUEst(self, request):\n        \"\"\"Constructs a :class:`PreparedRequest <PreparedRequest>` for\n        transmission and returns it. The :class:`PreparedRequest` has settings\n        merged from the :class:`Request <Request>` instance and those of the\n        :class:`Session`.\n\n        :param request: :class:`Request` instance to prepare with this\n            session's settings.\n        :rtype: requests.PreparedRequest\n        \"\"\"\n        cookies = request.cookies or {}\n\n        # Bootstrap CookieJar.\n        if not isinstance(cookies, cookielib.CookieJar):\n            cookies = coOKiEJAr_fROM_dIct(cookies)\n\n        # Merge with session cookies\n        merged_cookies = mERgE_cOOkIeS(\n            mERgE_cOOkIeS(ReqUesTScoOKIejaR(), self.cookies), cookies)\n\n        # Set environment's basic authentication if not explicitly set.\n        auth = request.auth\n        if self.trust_env and not auth and not self.auth:\n            auth = Get_NETrc_AuTH(request.url)\n\n        p = PrePArEDreqUEsT()\n        p.pREPaRe(\n            method=request.method.upper(),\n            url=request.url,\n            files=request.files,\n            data=request.data,\n            json=request.json,\n            headers=mErGE_sEtTiNG(request.headers, self.headers, dict_class=caSEINsEnSiTiVeDiCT),\n            params=mErGE_sEtTiNG(request.params, self.params),\n            auth=mErGE_sEtTiNG(auth, self.auth),\n            cookies=merged_cookies,\n            hooks=MERgE_hoOKs(request.hooks, self.hooks),\n        )\n        return p\n\n    def REQUest(self, method, url,\n            params=None, data=None, headers=None, cookies=None, files=None,\n            auth=None, timeout=None, allow_redirects=True, proxies=None,\n            hooks=None, stream=None, verify=None, cert=None, json=None):\n        \"\"\"Constructs a :class:`Request <Request>`, prepares it and sends it.\n        Returns :class:`Response <Response>` object.\n\n        :param method: method for the new :class:`Request` object.\n        :param url: URL for the new :class:`Request` object.\n        :param params: (optional) Dictionary or bytes to be sent in the query\n            string for the :class:`Request`.\n        :param data: (optional) Dictionary, list of tuples, bytes, or file-like\n            object to send in the body of the :class:`Request`.\n        :param json: (optional) json to send in the body of the\n            :class:`Request`.\n        :param headers: (optional) Dictionary of HTTP Headers to send with the\n            :class:`Request`.\n        :param cookies: (optional) Dict or CookieJar object to send with the\n            :class:`Request`.\n        :param files: (optional) Dictionary of ``'filename': file-like-objects``\n            for multipart encoding upload.\n        :param auth: (optional) Auth tuple or callable to enable\n            Basic/Digest/Custom HTTP Auth.\n        :param timeout: (optional) How long to wait for the server to send\n            data before giving up, as a float, or a :ref:`(connect timeout,\n            read timeout) <timeouts>` tuple.\n        :type timeout: float or tuple\n        :param allow_redirects: (optional) Set to True by default.\n        :type allow_redirects: bool\n        :param proxies: (optional) Dictionary mapping protocol or protocol and\n            hostname to the URL of the proxy.\n        :param stream: (optional) whether to immediately download the response\n            content. Defaults to ``False``.\n        :param verify: (optional) Either a boolean, in which case it controls whether we verify\n            the server's TLS certificate, or a string, in which case it must be a path\n            to a CA bundle to use. Defaults to ``True``. When set to\n            ``False``, requests will accept any TLS certificate presented by\n            the server, and will ignore hostname mismatches and/or expired\n            certificates, which will make your application vulnerable to\n            man-in-the-middle (MitM) attacks. Setting verify to ``False`` \n            may be useful during local development or testing.\n        :param cert: (optional) if String, path to ssl client cert file (.pem).\n            If Tuple, ('cert', 'key') pair.\n        :rtype: requests.Response\n        \"\"\"\n        # Create the Request.\n        req = rEQueSt(\n            method=method.upper(),\n            url=url,\n            headers=headers,\n            files=files,\n            data=data or {},\n            json=json,\n            params=params or {},\n            auth=auth,\n            cookies=cookies,\n            hooks=hooks,\n        )\n        prep = self.PREPaRe_ReqUEst(req)\n\n        proxies = proxies or {}\n\n        settings = self.MERGe_EnvironmeNT_SETTinGS(\n            prep.url, proxies, stream, verify, cert\n        )\n\n        # Send the request.\n        send_kwargs = {\n            'timeout': timeout,\n            'allow_redirects': allow_redirects,\n        }\n        send_kwargs.update(settings)\n        resp = self.send(prep, **send_kwargs)\n\n        return resp\n\n    def gET(self, url, **kwargs):\n        r\"\"\"Sends a GET request. Returns :class:`Response` object.\n\n        :param url: URL for the new :class:`Request` object.\n        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n        :rtype: requests.Response\n        \"\"\"\n\n        kwargs.setdefault('allow_redirects', True)\n        return self.REQUest('GET', url, **kwargs)\n\n    def oPtIOnS(self, url, **kwargs):\n        r\"\"\"Sends a OPTIONS request. Returns :class:`Response` object.\n\n        :param url: URL for the new :class:`Request` object.\n        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n        :rtype: requests.Response\n        \"\"\"\n\n        kwargs.setdefault('allow_redirects', True)\n        return self.REQUest('OPTIONS', url, **kwargs)\n\n    def head(self, url, **kwargs):\n        r\"\"\"Sends a HEAD request. Returns :class:`Response` object.\n\n        :param url: URL for the new :class:`Request` object.\n        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n        :rtype: requests.Response\n        \"\"\"\n\n        kwargs.setdefault('allow_redirects', False)\n        return self.REQUest('HEAD', url, **kwargs)\n\n    def POSt(self, url, data=None, json=None, **kwargs):\n        r\"\"\"Sends a POST request. Returns :class:`Response` object.\n\n        :param url: URL for the new :class:`Request` object.\n        :param data: (optional) Dictionary, list of tuples, bytes, or file-like\n            object to send in the body of the :class:`Request`.\n        :param json: (optional) json to send in the body of the :class:`Request`.\n        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n        :rtype: requests.Response\n        \"\"\"\n\n        return self.REQUest('POST', url, data=data, json=json, **kwargs)\n\n    def PuT(self, url, data=None, **kwargs):\n        r\"\"\"Sends a PUT request. Returns :class:`Response` object.\n\n        :param url: URL for the new :class:`Request` object.\n        :param data: (optional) Dictionary, list of tuples, bytes, or file-like\n            object to send in the body of the :class:`Request`.\n        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n        :rtype: requests.Response\n        \"\"\"\n\n        return self.REQUest('PUT', url, data=data, **kwargs)\n\n    def PATch(self, url, data=None, **kwargs):\n        r\"\"\"Sends a PATCH request. Returns :class:`Response` object.\n\n        :param url: URL for the new :class:`Request` object.\n        :param data: (optional) Dictionary, list of tuples, bytes, or file-like\n            object to send in the body of the :class:`Request`.\n        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n        :rtype: requests.Response\n        \"\"\"\n\n        return self.REQUest('PATCH', url, data=data, **kwargs)\n\n    def delete(self, url, **kwargs):\n        r\"\"\"Sends a DELETE request. Returns :class:`Response` object.\n\n        :param url: URL for the new :class:`Request` object.\n        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n        :rtype: requests.Response\n        \"\"\"\n\n        return self.REQUest('DELETE', url, **kwargs)\n\n    def send(self, request, **kwargs):\n        \"\"\"Send a given PreparedRequest.\n\n        :rtype: requests.Response\n        \"\"\"\n        # Set defaults that the hooks can utilize to ensure they always have\n        # the correct parameters to reproduce the previous request.\n        kwargs.setdefault('stream', self.stream)\n        kwargs.setdefault('verify', self.verify)\n        kwargs.setdefault('cert', self.cert)\n        if 'proxies' not in kwargs:\n            kwargs['proxies'] = rEsOLve_pROXiES(\n                request, self.proxies, self.trust_env\n            )\n\n        # It's possible that users might accidentally send a Request object.\n        # Guard against that specific failure case.\n        if isinstance(request, rEQueSt):\n            raise ValueError('You can only send PreparedRequests.')\n\n        # Set up variables needed for resolve_redirects and dispatching of hooks\n        allow_redirects = kwargs.pop('allow_redirects', True)\n        stream = kwargs.get('stream')\n        hooks = request.hooks\n\n        # Get the appropriate adapter to use\n        adapter = self.gET_aDAPteR(url=request.url)\n\n        # Start time (approximately) of the request\n        start = preferred_clock()\n\n        # Send the request\n        r = adapter.send(request, **kwargs)\n\n        # Total elapsed time of the request (approximately)\n        elapsed = preferred_clock() - start\n        r.elapsed = timedelta(seconds=elapsed)\n\n        # Response manipulation hooks\n        r = DiSpatCH_HOoK('response', hooks, r, **kwargs)\n\n        # Persist cookies\n        if r.history:\n\n            # If the hooks create history then we want those cookies too\n            for resp in r.history:\n                exTRAcT_coOKieS_tO_jAR(self.cookies, resp.request, resp.raw)\n\n        exTRAcT_coOKieS_tO_jAR(self.cookies, request, r.raw)\n\n        # Resolve redirects if allowed.\n        if allow_redirects:\n            # Redirect resolving generator.\n            gen = self.resolve_redirects(r, request, **kwargs)\n            history = [resp for resp in gen]\n        else:\n            history = []\n\n        # Shuffle things around if there's history.\n        if history:\n            # Insert the first (original) request at the start\n            history.insert(0, r)\n            # Get the last request made\n            r = history.pop()\n            r.history = history\n\n        # If redirects aren't being followed, store the response on the Request for Response.next().\n        if not allow_redirects:\n            try:\n                r._next = next(self.resolve_redirects(r, request, yield_requests=True, **kwargs))\n            except StopIteration:\n                pass\n\n        if not stream:\n            r.content\n\n        return r\n\n    def MERGe_EnvironmeNT_SETTinGS(self, url, proxies, stream, verify, cert):\n        \"\"\"\n        Check the environment and merge it with some settings.\n\n        :rtype: dict\n        \"\"\"\n        # Gather clues from the surrounding environment.\n        if self.trust_env:\n            # Set environment's proxies.\n            no_proxy = proxies.get('no_proxy') if proxies is not None else None\n            env_proxies = Get_ENviRON_pROxieS(url, no_proxy=no_proxy)\n            for (k, v) in env_proxies.items():\n                proxies.setdefault(k, v)\n\n            # Look for requests environment configuration and be compatible\n            # with cURL.\n            if verify is True or verify is None:\n                verify = (os.environ.get('REQUESTS_CA_BUNDLE') or\n                          os.environ.get('CURL_CA_BUNDLE'))\n\n        # Merge all the kwargs.\n        proxies = mErGE_sEtTiNG(proxies, self.proxies)\n        stream = mErGE_sEtTiNG(stream, self.stream)\n        verify = mErGE_sEtTiNG(verify, self.verify)\n        cert = mErGE_sEtTiNG(cert, self.cert)\n\n        return {'verify': verify, 'proxies': proxies, 'stream': stream,\n                'cert': cert}\n\n    def gET_aDAPteR(self, url):\n        \"\"\"\n        Returns the appropriate connection adapter for the given URL.\n\n        :rtype: requests.adapters.BaseAdapter\n        \"\"\"\n        for (prefix, adapter) in self.adapters.items():\n\n            if url.lower().startswith(prefix.lower()):\n                return adapter\n\n        # Nothing matches :-/\n        raise iNVALiDschema(\"No connection adapters were found for {!r}\".format(url))\n\n    def close(self):\n        \"\"\"Closes all adapters and as such the session\"\"\"\n        for v in self.adapters.values():\n            v.close()\n\n    def mOUNt(self, prefix, adapter):\n        \"\"\"Registers a connection adapter to a prefix.\n\n        Adapters are sorted in descending order by prefix length.\n        \"\"\"\n        self.adapters[prefix] = adapter\n        keys_to_move = [k for k in self.adapters if len(k) < len(prefix)]\n\n        for key in keys_to_move:\n            self.adapters[key] = self.adapters.pop(key)\n\n    def __getstate__(self):\n        state = {attr: getattr(self, attr, None) for attr in self.__attrs__}\n        return state\n\n    def __setstate__(self, state):\n        for attr, value in state.items():\n            setattr(self, attr, value)"
    },
    {
        "task_id": "pyvista__pyvista-4853_POLYDaTA",
        "class_name": "POLYDaTA",
        "file": "pyvista__pyvista-4853/pyvista/core/pointset.py",
        "sketchy_description": "The 'POLYDaTA' class is a subclass of '_vtk.vtkPolyData', '_pOinTsEt', and 'poLydATaFIltERs'. The class has an '__init__' method that takes multiple arguments including 'var_inp', 'faces', 'n_faces', 'lines', 'n_lines', 'strips', 'n_strips', 'deep', 'force_ext', and 'force_float'. This method initializes the polydata.\n\nThe class has a method named '_pOst_fiLE_LoAd_ProCESSINg' which executes after loading a PolyData from file. It also has a static method named '_mAke_veRTEx_cELLS' which creates vertex cells for the given number of points.\n\nThe class has several properties including 'verts', 'lines', 'faces', 'regular_faces', 'strips', 'is_all_triangles', '_offset_array', '_connectivity_array', 'n_lines', 'n_verts', 'n_strips', 'n_faces', and 'volume'. These properties return various attributes of the PolyData such as vertex cells, line cells, face cells, strip cells, whether all faces are triangles, the array used to store cell offsets, the array with the point ids that define the cells' connectivity, the number of lines, vertices, strips, faces, and the volume of the dataset.\n\nThe class also has a class method named 'FROM_reGulaR_faCES' which constructs a PolyData from point and regular face arrays.\n\nThe class has a method named 'save' which writes a surface mesh to disk. The written file may be an ASCII or binary ply, stl, or vtk mesh file.\n\nThe class has a '__repr__' method which returns the standard representation of the instance and a '__str__' method which returns the standard string representation of the instance.\n\nThe class has a '__sub__' method which computes the boolean difference of two meshes and a '__del__' method which deletes the object.\n\nThe class has a class variable '_WRITERS' which is a dictionary mapping file extensions to their respective writer classes. The class also has several instance variables including 'verts', 'strips', 'faces', 'lines', '_obbTree', '_last_active_scalars_name', '_active_scalars_info', '_active_vectors_info', '_active_tensors_info', 'points', '_association_complex_names', '_association_bitarray_names', and 'active_scalars_name'.",
        "detailed_description": "The 'POLYDaTA' class is a subclass of '_vtk.vtkPolyData', '_pOinTsEt', and 'poLydATaFIltERs'. This class represents a dataset consisting of surface geometry such as vertices, lines, and polygons. The class can be initialized in several ways including creating an empty mesh, initializing from a 'vtk.vtkPolyData' object, using vertices, using vertices and faces, or from a file. The class has an '__init__' method that takes several optional arguments including 'var_inp', 'faces', 'n_faces', 'lines', 'n_lines', 'strips', 'n_strips', 'deep', 'force_ext', and 'force_float'. This method sets up the instance based on the given arguments. The class has a '_pOst_fiLE_LoAd_ProCESSINg' method that sets the 'verts' instance variable to a vertex cells array if the instance has points but no cells. The class has a '__repr__' method that returns the standard representation of the instance and a '__str__' method that returns the standard string representation of the instance. The class has a '_mAke_veRTEx_cELLS' method that takes an argument 'npoints' and returns a 2D array where the first column is filled with 1s and the second column is filled with the numbers from 0 to 'npoints' - 1. The class has several property methods including 'verts', 'lines', 'faces', 'regular_faces', 'strips', 'is_all_triangles', 'volume', 'point_normals', 'cell_normals', 'face_normals', 'obbTree', 'n_open_edges', and 'is_manifold'. These methods return the corresponding instance variables or computed values. The class has a 'save' method that takes several arguments including 'filename', 'binary', 'texture', and 'recompute_normals'. This method writes the instance to a file in the format specified by the file extension of 'filename'. The class has a 'FROM_reGulaR_faCES' class method that takes arguments 'points', 'faces', and 'deep'. This method returns a new instance of 'POLYDaTA' with the 'points' and 'faces' set to the given 'points' and 'faces'. The class has a '__sub__' method that takes an argument 'cutting_mesh'. This method returns the boolean difference of the instance and 'cutting_mesh'. The class has a '__del__' method that deletes the instance. The class also has several private methods and variables.",
        "repo_metadata": {
            "commit_id": "0caa7254d5f42c363ab164a80ec4ec36d79f2df2",
            "issue_id": "pyvista__pyvista-4853",
            "setup_details": {
                "repo": "pyvista/pyvista",
                "instance_id": "pyvista__pyvista-4853",
                "base_commit": "4a44e4c63c6b8d6a3f1db0aa193f4ccb631ed698",
                "version": "0.43",
                "environment_setup_commit": "17ed0eb49a942b297e61a83a1c8ba828c5922b99"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/core/test_dataset.py::test_no_active",
                "tests/core/test_dataset_filters.py::test_sample_over_circular_arc_normal",
                "tests/core/test_unstructured_grid_filters.py::test_clean_points",
                "tests/core/test_dataset_filters.py::test_merge_general",
                "tests/core/test_grid.py::test_no_copy_polydata_points_setter",
                "tests/core/test_polydata.py::test_strips",
                "tests/core/test_dataset_filters.py::test_outline_corners",
                "tests/core/test_dataset_filters.py::test_extract_points",
                "tests/core/test_utilities.py::test_read[True]",
                "tests/core/test_utilities.py::test_read[False]",
                "tests/core/test_dataset_filters.py::test_interpolate",
                "tests/core/test_helpers.py::test_wrap_trimesh",
                "tests/core/test_polydata.py::test_init_from_arrays_triangular",
                "tests/core/test_composite.py::test_multi_block_clean",
                "tests/core/test_utilities.py::test_merge",
                "tests/core/test_polydata.py::test_append",
                "tests/core/test_dataset_filters.py::test_cell_centers",
                "tests/core/test_polydata.py::test_pathlib_read_write",
                "tests/core/test_dataset_filters.py::test_extrude_trim_strategy[intersection-boundary_edges]",
                "tests/core/test_dataset_filters.py::test_extrude_trim_strategy[intersection-all_edges]",
                "tests/core/test_dataset_filters.py::test_extrude_trim_strategy[minimum_distance-boundary_edges]",
                "tests/core/test_dataset_filters.py::test_extrude_trim_strategy[minimum_distance-all_edges]",
                "tests/core/test_dataset_filters.py::test_extrude_trim_strategy[maximum_distance-boundary_edges]",
                "tests/core/test_dataset_filters.py::test_extrude_trim_strategy[maximum_distance-all_edges]",
                "tests/core/test_dataset_filters.py::test_extrude_trim_strategy[average_distance-boundary_edges]",
                "tests/core/test_dataset_filters.py::test_extrude_trim_strategy[average_distance-all_edges]",
                "tests/core/test_polydata.py::test_lines_on_init",
                "tests/core/test_polydata.py::test_add",
                "tests/core/test_dataset.py::test_empty_points",
                "tests/core/test_dataset_filters.py::test_clip_by_scalars_filter[False-False]",
                "tests/core/test_dataset_filters.py::test_clip_by_scalars_filter[False-True]",
                "tests/core/test_dataset_filters.py::test_clip_by_scalars_filter[True-False]",
                "tests/core/test_dataset_filters.py::test_clip_by_scalars_filter[True-True]",
                "tests/core/test_reader.py::test_xmlpolydatareader",
                "tests/core/test_dataset_filters.py::test_slice_orthogonal_filter",
                "tests/core/test_dataset_filters.py::test_clip_filter",
                "tests/core/test_polydata.py::test_lines",
                "tests/core/test_dataset_filters.py::test_streamlines_from_source",
                "tests/plotting/test_theme.py::test_user_theme",
                "tests/core/test_dataset_filters.py::test_delaunay_2d_unstructured",
                "tests/core/test_polydata.py::test_save[.ply-True]",
                "tests/core/test_polydata.py::test_save[.ply-False]",
                "tests/core/test_polydata.py::test_save[.vtp-True]",
                "tests/core/test_polydata.py::test_save[.vtp-False]",
                "tests/core/test_polydata.py::test_save[.stl-True]",
                "tests/core/test_polydata.py::test_save[.stl-False]",
                "tests/core/test_polydata.py::test_save[.vtk-True]",
                "tests/core/test_polydata.py::test_save[.vtk-False]",
                "tests/core/test_dataset_filters.py::test_extract_all_edges",
                "tests/core/test_dataset.py::test_cast_to_poly_points_implicit",
                "tests/core/test_polydata.py::test_extract_feature_edges_no_data",
                "tests/core/test_polydata.py::test_invalid_init",
                "tests/examples/test_examples.py::test_sphere_with_texture_map",
                "tests/core/test_dataset_filters.py::test_collision_solid_non_triangle",
                "tests/core/test_helpers.py::test_wrappers",
                "tests/core/test_dataset_filters.py::test_tessellate",
                "tests/core/test_polydata.py::test_polydata_repr_str",
                "tests/core/test_polydata.py::test_init_as_points",
                "tests/core/test_dataset_filters.py::test_extract_all_edges_no_data",
                "tests/core/test_dataset_filters.py::test_outline_corners_composite",
                "tests/core/test_dataset_filters.py::test_outline",
                "tests/core/test_composite.py::test_extract_geometry",
                "tests/core/test_polydata.py::test_append_raises",
                "tests/core/test_composite.py::test_multi_block_init_list",
                "tests/core/test_helpers.py::test_inheritance_no_wrappers",
                "tests/core/test_composite.py::test_multi_block_append",
                "tests/core/test_polydata.py::test_merge",
                "tests/core/test_dataset_filters.py::test_slice_along_axis",
                "tests/core/test_dataset_filters.py::test_sample_over_circular_arc",
                "tests/core/test_polydata.py::test_triangulate_filter",
                "tests/core/test_dataset.py::test_point_cell_data_single_scalar_no_exception_raised",
                "tests/core/test_polydata.py::test_is_all_triangles",
                "tests/core/test_dataset.py::test_string_arrays",
                "tests/core/test_utilities.py::test_read_force_ext",
                "tests/plotting/test_renderer.py::test_add_legend_loc[upper right]",
                "tests/plotting/test_renderer.py::test_add_legend_loc[upper left]",
                "tests/plotting/test_renderer.py::test_add_legend_loc[lower left]",
                "tests/plotting/test_renderer.py::test_add_legend_loc[lower right]",
                "tests/plotting/test_renderer.py::test_add_legend_loc[center left]",
                "tests/plotting/test_renderer.py::test_add_legend_loc[center right]",
                "tests/plotting/test_renderer.py::test_add_legend_loc[lower center]",
                "tests/plotting/test_renderer.py::test_add_legend_loc[upper center]",
                "tests/plotting/test_renderer.py::test_add_legend_loc[center]",
                "tests/core/test_dataset.py::test_handle_array_with_null_name",
                "tests/core/test_polydata.py::test_init_from_arrays",
                "tests/core/test_cells.py::test_cell_center_value",
                "tests/core/test_dataset.py::test_arrows",
                "tests/core/test_polydata.py::test_center_of_mass",
                "tests/core/test_dataset_filters.py::test_slice_filter",
                "tests/core/test_dataset_filters.py::test_glyph",
                "tests/core/test_dataset_filters.py::test_extract_geometry",
                "tests/core/test_dataset_filters.py::test_contour_errors",
                "tests/core/test_grid.py::test_no_copy_polydata_init",
                "tests/examples/test_examples.py::test_load_earth",
                "tests/core/test_grid.py::test_instantiate_by_filename",
                "tests/core/test_dataset_filters.py::test_extract_geometry_extent",
                "tests/core/test_polydata.py::test_regular_faces[False]",
                "tests/core/test_polydata.py::test_regular_faces[True]",
                "tests/core/test_polydata.py::test_delaunay_2d",
                "tests/core/test_helpers.py::test_array_association",
                "tests/core/test_polydata.py::test_init_as_points_from_list",
                "tests/core/test_polydata.py::test_init_from_pdata",
                "tests/core/test_polydata.py::test_regular_faces_mutable",
                "tests/core/test_utilities.py::test_voxelize_throws_point_cloud",
                "tests/core/test_polydata.py::test_init",
                "tests/core/test_dataset_filters.py::test_collision",
                "tests/core/test_polydata.py::test_invalid_file",
                "tests/core/test_helpers.py::test_wrap_pyvista_ndarray",
                "tests/core/test_dataset.py::test_setting_points_by_different_types",
                "tests/core/test_cells.py::test_set_shallow_regular_cells",
                "tests/core/test_polydata.py::test_init_from_arrays_with_vert",
                "tests/core/test_polydata.py::test_n_verts",
                "tests/core/test_polydata.py::test_vertice_cells_on_read",
                "tests/core/test_dataset_filters.py::test_sample_composite",
                "tests/core/test_polydata.py::test_geodesic",
                "tests/core/test_dataset_filters.py::test_outline_composite",
                "tests/core/test_dataset_filters.py::test_reconstruct_surface_unstructured",
                "tests/core/test_composite.py::test_multi_block_init_dict",
                "tests/core/test_dataset.py::test_shallow_copy_back_propagation",
                "tests/core/test_polydata.py::test_empty_regular_faces",
                "tests/core/test_dataset_filters.py::test_sample_over_line"
            ]
        },
        "ground_truth_class_body": "class POLYDaTA(_vtk.vtkPolyData, _pOinTsEt, poLydATaFIltERs):\n    \"\"\"Dataset consisting of surface geometry (e.g. vertices, lines, and polygons).\n\n    Can be initialized in several ways:\n\n    - Create an empty mesh\n    - Initialize from a vtk.vtkPolyData\n    - Using vertices\n    - Using vertices and faces\n    - From a file\n\n    Parameters\n    ----------\n    var_inp : vtk.vtkPolyData, str, sequence, optional\n        Flexible input type.  Can be a ``vtk.vtkPolyData``, in which case\n        this PolyData object will be copied if ``deep=True`` and will\n        be a shallow copy if ``deep=False``.\n\n        Also accepts a path, which may be local path as in\n        ``'my_mesh.stl'`` or global path like ``'/tmp/my_mesh.ply'``\n        or ``'C:/Users/user/my_mesh.ply'``.\n\n        Otherwise, this must be a points array or list containing one\n        or more points.  Each point must have 3 dimensions.\n\n    faces : sequence, optional\n        Face connectivity array.  Faces must contain padding\n        indicating the number of points in the face.  For example, the\n        two faces ``[10, 11, 12]`` and ``[20, 21, 22, 23]`` will be\n        represented as ``[3, 10, 11, 12, 4, 20, 21, 22, 23]``.  This\n        lets you have an arbitrary number of points per face.\n\n        When not including the face connectivity array, each point\n        will be assigned to a single vertex.  This is used for point\n        clouds that have no connectivity.\n\n    n_faces : int, optional\n        Number of faces in the ``faces`` connectivity array.  While\n        optional, setting this speeds up the creation of the\n        ``PolyData``.\n\n    lines : sequence, optional\n        The line connectivity array.  Like ``faces``, this array\n        requires padding indicating the number of points in a line\n        segment.  For example, the two line segments ``[0, 1]`` and\n        ``[1, 2, 3, 4]`` will be represented as\n        ``[2, 0, 1, 4, 1, 2, 3, 4]``.\n\n    n_lines : int, optional\n        Number of lines in the ``lines`` connectivity array.  While\n        optional, setting this speeds up the creation of the\n        ``PolyData``.\n\n    strips : sequence, optional\n        Triangle strips connectivity array.  Triangle strips require an initial\n        triangle, and the following points of the strip. Each\n        triangle is built with the new point and the two previous\n        points. Just as in ``lines`` and ``faces``, this array requires a\n        padding indicating the number of points. For example,\n        a single triangle strip of ``[0, 1, 2, 3, 6, 7, 4, 5, 0, 1]`` requires padding of\n        ``10`` and should input as ``[10, 0, 1, 2, 3, 6, 7, 4, 5, 0, 1]``.\n\n    n_strips : int, optional\n        Number of strips in the ``strips`` connectivity array.  While\n        optional, setting this speeds up the creation of the\n        ``PolyData``.\n\n    deep : bool, optional\n        Whether to copy the inputs, or to create a mesh from them\n        without copying them.  Setting ``deep=True`` ensures that the\n        original arrays can be modified outside the mesh without\n        affecting the mesh. Default is ``False``.\n\n    force_ext : str, optional\n        If initializing from a file, force the reader to treat the\n        file as if it had this extension as opposed to the one in the\n        file.\n\n    force_float : bool, optional\n        Casts the datatype to ``float32`` if points datatype is\n        non-float.  Default ``True``. Set this to ``False`` to allow\n        non-float types, though this may lead to truncation of\n        intermediate floats when transforming datasets.\n\n\n    See Also\n    --------\n    pyvista.PolyData.from_regular_faces\n\n    Examples\n    --------\n    >>> import vtk\n    >>> import numpy as np\n    >>> from pyvista import examples\n    >>> import pyvista\n\n    Create an empty mesh.\n\n    >>> mesh = pyvista.POLYDaTA()\n\n    Initialize from a ``vtk.vtkPolyData`` object.\n\n    >>> vtkobj = vtk.vtkPolyData()\n    >>> mesh = pyvista.POLYDaTA(vtkobj)\n\n    Initialize from just vertices.\n\n    >>> vertices = np.array(\n    ...     [[0, 0, 0], [1, 0, 0], [1, 0.5, 0], [0, 0.5, 0]]\n    ... )\n    >>> mesh = pyvista.POLYDaTA(vertices)\n\n    Initialize from vertices and faces.\n\n    >>> faces = np.hstack([[3, 0, 1, 2], [3, 0, 3, 2]])\n    >>> mesh = pyvista.POLYDaTA(vertices, faces)\n\n    Initialize from vertices and lines.\n\n    >>> lines = np.hstack([[2, 0, 1], [2, 1, 2]])\n    >>> mesh = pyvista.POLYDaTA(vertices, lines=lines)\n\n    Initialize from vertices and triangle strips.\n\n    >>> strips = np.hstack([[4, 0, 1, 3, 2]])\n    >>> mesh = pyvista.POLYDaTA(vertices, strips=strips)\n\n    Initialize from a filename.\n\n    >>> mesh = pyvista.POLYDaTA(examples.antfile)\n\n    See :ref:`ref_create_poly` for more examples.\n\n    \"\"\"\n\n    _WRITERS = {\n        '.ply': _vtk.vtkPLYWriter,\n        '.vtp': _vtk.vtkXMLPolyDataWriter,\n        '.stl': _vtk.vtkSTLWriter,\n        '.vtk': _vtk.vtkPolyDataWriter,\n    }\n\n    def __init__(\n        self,\n        var_inp=None,\n        faces=None,\n        n_faces=None,\n        lines=None,\n        n_lines=None,\n        strips=None,\n        n_strips=None,\n        deep=False,\n        force_ext=None,\n        force_float=True,\n    ) -> None:\n        \"\"\"Initialize the polydata.\"\"\"\n        local_parms = locals()\n        super().__init__()\n\n        # allow empty input\n        if var_inp is None:\n            return\n\n        # filename\n        opt_kwarg = ['faces', 'n_faces', 'lines', 'n_lines']\n        if isinstance(var_inp, (str, pathlib.Path)):\n            for kwarg in opt_kwarg:\n                if local_parms[kwarg]:\n                    raise ValueError(\n                        'No other arguments should be set when first parameter is a string'\n                    )\n            self._fROM_FiLe(var_inp, force_ext=force_ext)  # is filename\n\n            return\n\n        # PolyData-like\n        if isinstance(var_inp, _vtk.vtkPolyData):\n            for kwarg in opt_kwarg:\n                if local_parms[kwarg]:\n                    raise ValueError(\n                        'No other arguments should be set when first parameter is a PolyData'\n                    )\n            if deep:\n                self.dEEP_COpy(var_inp)\n            else:\n                self.sHaLLOw_CopY(var_inp)\n            return\n\n        # First parameter is points\n        if isinstance(var_inp, (np.ndarray, list, _vtk.vtkDataArray)):\n            self.SetPoints(vtK_pOInTS(var_inp, deep=deep, force_float=force_float))\n\n        else:\n            msg = f\"\"\"\n                Invalid Input type:\n\n                Expected first argument to be either a:\n                - vtk.PolyData\n                - pyvista.PolyData\n                - numeric numpy.ndarray (1 or 2 dimensions)\n                - List (flat or nested with 3 points per vertex)\n                - vtk.vtkDataArray\n\n                Instead got: {type(var_inp)}\"\"\"\n            raise TypeError(dedent(msg.strip('\\n')))\n\n        # At this point, points have been setup, add faces and/or lines\n        if faces is None and lines is None and strips is None:\n            # one cell per point (point cloud case)\n            verts = self._mAke_veRTEx_cELLS(self.n_points)\n            self.verts = ceLlarRaY(verts, self.n_points, deep)\n        elif strips is not None:\n            self.strips = ceLlarRaY(strips, n_strips, deep)\n        elif faces is not None:\n            # here we use CellArray since we must specify deep and n_faces\n            self.faces = ceLlarRaY(faces, n_faces, deep)\n\n        # can always set lines\n        if lines is not None:\n            # here we use CellArray since we must specify deep and n_lines\n            self.lines = ceLlarRaY(lines, n_lines, deep)\n\n    def _pOst_fiLE_LoAd_ProCESSINg(self):\n        \"\"\"Execute after loading a PolyData from file.\"\"\"\n        # When loading files with just point arrays, create and\n        # set the polydata vertices\n        if self.n_points > 0 and self.n_cells == 0:\n            verts = self._mAke_veRTEx_cELLS(self.n_points)\n            self.verts = ceLlarRaY(verts, self.n_points, deep=False)\n\n    def __repr__(self):\n        \"\"\"Return the standard representation.\"\"\"\n        return datAsEt.__repr__(self)\n\n    def __str__(self):\n        \"\"\"Return the standard str representation.\"\"\"\n        return datAsEt.__str__(self)\n\n    @staticmethod\n    def _mAke_veRTEx_cELLS(npoints):\n        cells = np.empty((npoints, 2), dtype=pyvista.ID_TYPE)\n        cells[:, 0] = 1\n        cells[:, 1] = np.arange(npoints, dtype=pyvista.ID_TYPE)\n        return cells\n\n    @property\n    def verts(self) -> np.ndarray:  # numpydoc ignore=RT01\n        \"\"\"Get the vertex cells.\n\n        Returns\n        -------\n        numpy.ndarray\n            Array of vertex cell indices.\n\n        Examples\n        --------\n        Create a point cloud polydata and return the vertex cells.\n\n        >>> import pyvista\n        >>> import numpy as np\n        >>> points = np.random.random((5, 3))\n        >>> pdata = pyvista.POLYDaTA(points)\n        >>> pdata.verts\n        array([1, 0, 1, 1, 1, 2, 1, 3, 1, 4])\n\n        Set vertex cells.  Note how the mesh plots both the surface\n        mesh and the additional vertices in a single plot.\n\n        >>> mesh = pyvista.Plane(i_resolution=3, j_resolution=3)\n        >>> mesh.verts = np.vstack(\n        ...     (\n        ...         np.ones(mesh.n_points, dtype=np.int64),\n        ...         np.arange(mesh.n_points),\n        ...     )\n        ... ).T\n        >>> mesh.plot(\n        ...     color='lightblue',\n        ...     render_points_as_spheres=True,\n        ...     point_size=60,\n        ... )\n\n        \"\"\"\n        return _vtk.vtk_to_numpy(self.GetVerts().GetData())\n\n    @verts.setter\n    def verts(self, verts):  # numpydoc ignore=GL08\n        if isinstance(verts, ceLlarRaY):\n            self.SetVerts(verts)\n        else:\n            self.SetVerts(ceLlarRaY(verts))\n\n    @property\n    def lines(self) -> np.ndarray:  # numpydoc ignore=RT01\n        \"\"\"Return a pointer to the lines as a numpy array.\n\n        Examples\n        --------\n        Return the lines from a spline.\n\n        >>> import pyvista\n        >>> import numpy as np\n        >>> points = np.random.random((3, 3))\n        >>> spline = pyvista.Spline(points, 10)\n        >>> spline.lines\n        array([10,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9])\n\n        \"\"\"\n        return _vtk.vtk_to_numpy(self.GetLines().GetData()).ravel()\n\n    @lines.setter\n    def lines(self, lines):  # numpydoc ignore=GL08\n        if isinstance(lines, ceLlarRaY):\n            self.SetLines(lines)\n        else:\n            self.SetLines(ceLlarRaY(lines))\n\n    @property\n    def faces(self) -> np.ndarray:  # numpydoc ignore=RT01\n        \"\"\"Return the connectivity array of the faces of this PolyData.\n\n        The faces array is organized as::\n\n           [n0, p0_0, p0_1, ..., p0_n, n1, p1_0, p1_1, ..., p1_n, ...]\n\n        where ``n0`` is the number of points in face 0, and ``pX_Y`` is the\n        Y'th point in face X.\n\n        For example, a triangle and a quadrilateral might be represented as::\n\n           [3, 0, 1, 2, 4, 0, 1, 3, 4]\n\n        Where the two individual faces would be ``[3, 0, 1, 2]`` and ``[4, 0, 1, 3, 4]``.\n\n        Returns\n        -------\n        numpy.ndarray\n            Array of face connectivity.\n\n        See Also\n        --------\n        pyvista.PolyData.regular_faces\n\n        Notes\n        -----\n        The array returned cannot be modified in place and will raise a\n        ``ValueError`` if attempted.\n\n        You can, however, set the faces directly. See the example.\n\n        Examples\n        --------\n        >>> import pyvista as pv\n        >>> plane = pv.Plane(i_resolution=2, j_resolution=2)\n        >>> plane.faces\n        array([4, 0, 1, 4, 3, 4, 1, 2, 5, 4, 4, 3, 4, 7, 6, 4, 4, 5, 8, 7])\n\n        Note how the faces contain a \"padding\" indicating the number\n        of points per face:\n\n        >>> plane.faces.reshape(-1, 5)\n        array([[4, 0, 1, 4, 3],\n               [4, 1, 2, 5, 4],\n               [4, 3, 4, 7, 6],\n               [4, 4, 5, 8, 7]])\n\n        Set the faces directly. The following example creates a simple plane\n        with a single square faces and modifies it to have two triangles\n        instead.\n\n        >>> mesh = pv.Plane(i_resolution=1, j_resolution=1)\n        >>> mesh.faces = [3, 0, 1, 2, 3, 3, 2, 1]\n        >>> mesh.faces\n        array([3, 0, 1, 2, 3, 3, 2, 1])\n\n        \"\"\"\n        array = _vtk.vtk_to_numpy(self.GetPolys().GetData())\n        # Flag this array as read only to ensure users do not attempt to write to it.\n        array.flags['WRITEABLE'] = False\n        return array\n\n    @faces.setter\n    def faces(self, faces):  # numpydoc ignore=GL08\n        if isinstance(faces, ceLlarRaY):\n            self.SetPolys(faces)\n        else:\n            # TODO: faster to mutate in-place if array is same size?\n            self.SetPolys(ceLlarRaY(faces))\n\n    @property\n    def regular_faces(self) -> np.ndarray:  # numpydoc ignore=RT01\n        \"\"\"Return a face array of point indices when all faces have the same size.\n\n        Returns\n        -------\n        numpy.ndarray\n            Array of face indices with shape (n_faces, face_size).\n\n        See Also\n        --------\n        pyvista.PolyData.faces\n\n        Notes\n        -----\n        This property does not validate that the mesh's faces are all\n        actually the same size. If they're not, this property may either\n        raise a `ValueError` or silently return an incorrect array.\n\n        Examples\n        --------\n        Get the face array of a tetrahedron as a 4x3 array\n\n        >>> import pyvista as pv\n        >>> tetra = pv.Tetrahedron()\n        >>> tetra.regular_faces\n        array([[0, 1, 2],\n               [1, 3, 2],\n               [0, 2, 3],\n               [0, 3, 1]])\n\n        \"\"\"\n        return _gEt_REgUlAr_cElLS(self.GetPolys())\n\n    @regular_faces.setter\n    def regular_faces(\n        self, faces: Union[np.ndarray, Sequence[Sequence[int]]]\n    ):  # numpydoc ignore=PR01\n        \"\"\"Set the face cells from an (n_faces, face_size) array.\"\"\"\n        self.faces = ceLlarRaY.from_REgULAr_CElLS(faces)\n\n    @classmethod\n    def FROM_reGulaR_faCES(\n        cls, points, faces: Union[np.ndarray, Sequence[Sequence[int]]], deep=False\n    ):\n        \"\"\"Alternate `pyvista.PolyData` convenience constructor from point and regular face arrays.\n\n        Parameters\n        ----------\n        points : numpy.ndarray, sequence[sequence[float]]\n            A (n_points, 3) array of points.\n\n        faces : numpy.ndarray or sequence[sequence[int]]\n            A (n_faces, face_size) array of face indices. For a triangle mesh, face_size = 3.\n\n        deep : bool, optional, default: False\n            Whether to deep copy the faces array into vtkCellArray connectivity data.\n\n        Returns\n        -------\n        pyvista.PolyData\n            The newly constructed mesh.\n\n        Examples\n        --------\n        Construct a tetrahedron from four triangles\n\n        >>> import pyvista as pv\n        >>> points = [[1.0, 1, 1], [-1, 1, -1], [1, -1, -1], [-1, -1, 1]]\n        >>> faces = [[0, 1, 2], [1, 3, 2], [0, 2, 3], [0, 3, 1]]\n        >>> tetra = pv.PolyData.FROM_reGulaR_faCES(points, faces)\n        \"\"\"\n        p = cls()\n        p.points = points\n        p.faces = ceLlarRaY.from_REgULAr_CElLS(faces, deep=deep)\n        return p\n\n    @property\n    def strips(self) -> np.ndarray:  # numpydoc ignore=RT01\n        \"\"\"Return a pointer to the strips as a numpy array.\n\n        Returns\n        -------\n        numpy.ndarray\n            Array of strip indices.\n\n        Examples\n        --------\n        >>> import pyvista as pv\n        >>> polygon = pv.Rectangle()\n        >>> extruded = polygon.extrude((0, 0, 1), capping=False)\n        >>> extruded.strips\n        array([4, 0, 1, 4, 5, 4, 1, 2, 5, 6, 4, 2, 3, 6, 7, 4, 3, 0, 7, 4])\n        \"\"\"\n        return _vtk.vtk_to_numpy(self.GetStrips().GetData())\n\n    @strips.setter\n    def strips(self, strips):  # numpydoc ignore=GL08\n        if isinstance(strips, ceLlarRaY):\n            self.SetStrips(strips)\n        else:\n            self.SetStrips(ceLlarRaY(strips))\n\n    @property\n    def is_all_triangles(self):  # numpydoc ignore=RT01\n        \"\"\"Return if all the faces of the :class:`pyvista.PolyData` are triangles.\n\n        Returns\n        -------\n        bool\n            ``True`` if all the faces of the :class:`pyvista.PolyData`\n            are triangles and does not contain any vertices or lines.\n\n        Examples\n        --------\n        Show a mesh from :func:`pyvista.Plane` is not composed of all\n        triangles.\n\n        >>> import pyvista\n        >>> plane = pyvista.Plane()\n        >>> plane.is_all_triangles\n        False\n\n        Show that the mesh from :func:`pyvista.Sphere` contains only\n        triangles.\n\n        >>> sphere = pyvista.Sphere()\n        >>> sphere.is_all_triangles\n        True\n\n        \"\"\"\n        # Need to make sure there are only face cells and no lines/verts\n        if not self.n_faces or self.n_lines or self.n_verts:\n            return False\n\n        # early return if not all triangular\n        if self._connectivity_array.size % 3:\n            return False\n\n        # next, check if there are three points per face\n        return (np.diff(self._offset_array) == 3).all()\n\n    def __sub__(self, cutting_mesh):\n        \"\"\"Compute boolean difference of two meshes.\"\"\"\n        return self.BOOleaN_dIfFErEnCe(cutting_mesh)\n\n    @property\n    def _offset_array(self):\n        \"\"\"Return the array used to store cell offsets.\"\"\"\n        return _gET_OFfsEt_aRRAy(self.GetPolys())\n\n    @property\n    def _connectivity_array(self):\n        \"\"\"Return the array with the point ids that define the cells' connectivity.\"\"\"\n        return _GEt_CoNnEcTIViTy_aRraY(self.GetPolys())\n\n    @property\n    def n_lines(self) -> int:  # numpydoc ignore=RT01\n        \"\"\"Return the number of lines.\n\n        Examples\n        --------\n        >>> import pyvista\n        >>> mesh = pyvista.lINe()\n        >>> mesh.n_lines\n        1\n\n        \"\"\"\n        return self.GetNumberOfLines()\n\n    @property\n    def n_verts(self) -> int:  # numpydoc ignore=RT01\n        \"\"\"Return the number of vertices.\n\n        A vertex is a 0D cell, which is usually a cell that references one point,\n        a vtkVertex.  It can also be a vtkPolyVertex.\n        See `pyvista.PolyData.n_points` for the more common measure.\n\n        Examples\n        --------\n        Create a simple mesh containing just two points and return the\n        number of vertices. By default, when constructing a PolyData with points but no cells,\n        vertices are automatically created, one per point.\n\n        >>> import pyvista\n        >>> mesh = pyvista.POLYDaTA([[1.0, 0.0, 0.0], [1.0, 1.0, 1.0]])\n        >>> mesh.n_points, mesh.n_verts\n        (2, 2)\n\n        If any other cells are specified, these vertices are not created.\n\n        >>> import pyvista\n        >>> mesh = pyvista.POLYDaTA(\n        ...     [[1.0, 0.0, 0.0], [1.0, 1.0, 1.0]], lines=[2, 0, 1]\n        ... )\n        >>> mesh.n_points, mesh.n_verts\n        (2, 0)\n\n        \"\"\"\n        return self.GetNumberOfVerts()\n\n    @property\n    def n_strips(self) -> int:  # numpydoc ignore=RT01\n        \"\"\"Return the number of strips.\n\n        Examples\n        --------\n        Create a simple mesh with one triangle strip and return the\n        number of triangles.\n\n        >>> import pyvista\n        >>> import numpy as np\n        >>> vertices = np.array(\n        ...     [[1.0, 0.0, 0.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]\n        ... )\n        >>> strip = np.array([3, 0, 1, 2])\n        >>> mesh = pyvista.POLYDaTA(vertices, strips=strip)\n        >>> mesh.n_strips\n        1\n\n        \"\"\"\n        return self.GetNumberOfStrips()\n\n    @property\n    def n_faces(self) -> int:  # numpydoc ignore=RT01\n        \"\"\"Return the number of cells.\n\n        Alias for ``n_cells``.\n\n        Examples\n        --------\n        >>> import pyvista\n        >>> plane = pyvista.Plane(i_resolution=2, j_resolution=2)\n        >>> plane.n_faces\n        4\n\n        \"\"\"\n        return self.n_cells\n\n    def save(self, filename, binary=True, texture=None, recompute_normals=True):\n        \"\"\"Write a surface mesh to disk.\n\n        Written file may be an ASCII or binary ply, stl, or vtk mesh\n        file.\n\n        Parameters\n        ----------\n        filename : str\n            Filename of mesh to be written.  File type is inferred from\n            the extension of the filename unless overridden with\n            ftype.  Can be one of many of the supported  the following\n            types (``'.ply'``, ``'.stl'``, ``'.vtk``).\n\n        binary : bool, default: True\n            Writes the file as binary when ``True`` and ASCII when ``False``.\n\n        texture : str, numpy.ndarray, optional\n            Write a single texture array to file when using a PLY\n            file.  Texture array must be a 3 or 4 component array with\n            the datatype ``np.uint8``.  Array may be a cell array or a\n            point array, and may also be a string if the array already\n            exists in the PolyData.\n\n            If a string is provided, the texture array will be saved\n            to disk as that name.  If an array is provided, the\n            texture array will be saved as ``'RGBA'`` if the array\n            contains an alpha channel (i.e. 4 component array), or\n            as ``'RGB'`` if the array is just a 3 component array.\n\n            .. note::\n               This feature is only available when saving PLY files.\n\n        recompute_normals : bool, default: True\n            When ``True``, if ply or stl format is chosen, the face normals\n            are computed in place to ensure the mesh is properly saved.\n            Set this to ``False`` to save instead the already existing normal\n            array in the PolyData.\n\n        Notes\n        -----\n        Binary files write much faster than ASCII and have a smaller\n        file size.\n\n        Examples\n        --------\n        Save a mesh as a STL.\n\n        >>> import pyvista\n        >>> sphere = pyvista.Sphere()\n        >>> sphere.save('my_mesh.stl')  # doctest:+SKIP\n\n        Save a mesh as a PLY.\n\n        >>> sphere = pyvista.Sphere()\n        >>> sphere.save('my_mesh.ply')  # doctest:+SKIP\n\n        Save a mesh as a PLY with a texture array.  Here we also\n        create a simple RGB array representing the texture.\n\n        >>> import numpy as np\n        >>> sphere = pyvista.Sphere()\n        >>> texture = np.zeros((sphere.n_points, 3), np.uint8)\n        >>> # Just the green channel is set as a repeatedly\n        >>> # decreasing value\n        >>> texture[:, 1] = np.arange(sphere.n_points)[::-1]\n        >>> sphere.point_data['my_texture'] = texture\n        >>> sphere.save(\n        ...     'my_mesh.ply', texture='my_texture'\n        ... )  # doctest:+SKIP\n\n        Alternatively, provide just the texture array.  This will be\n        written to the file as ``'RGB'`` since it does not contain an\n        alpha channel.\n\n        >>> sphere.save('my_mesh.ply', texture=texture)  # doctest:+SKIP\n\n        Save a mesh as a VTK file.\n\n        >>> sphere = pyvista.Sphere()\n        >>> sphere.save('my_mesh.vtk')  # doctest:+SKIP\n\n        \"\"\"\n        filename = os.path.abspath(os.path.expanduser(str(filename)))\n        ftype = gET_ExT(filename)\n        # Recompute normals prior to save.  Corrects a bug were some\n        # triangular meshes are not saved correctly\n        if ftype in ['.stl', '.ply'] and recompute_normals:\n            self.CoMpUte_noRMALS(inplace=True)\n\n        # validate texture\n        if ftype == '.ply' and texture is not None:\n            if isinstance(texture, str):\n                if self[texture].dtype != np.uint8:\n                    raise ValueError(\n                        f'Invalid datatype {self[texture].dtype} of texture array \"{texture}\"'\n                    )\n            elif isinstance(texture, np.ndarray):\n                if texture.dtype != np.uint8:\n                    raise ValueError(f'Invalid datatype {texture.dtype} of texture array')\n            else:\n                raise TypeError(\n                    f'Invalid type {type(texture)} for texture.  '\n                    'Should be either a string representing a point or '\n                    'cell array, or a numpy array.'\n                )\n\n        super().save(filename, binary, texture=texture)\n\n    @property\n    def volume(self) -> float:  # numpydoc ignore=RT01\n        \"\"\"Return the approximate volume of the dataset.\n\n        This will throw a VTK error/warning if not a closed surface.\n\n        Returns\n        -------\n        float\n            Total volume of the mesh.\n\n        Examples\n        --------\n        >>> import pyvista\n        >>> sphere = pyvista.Sphere()\n        >>> sphere.volume\n        0.5183\n\n        \"\"\"\n        mprop = _vtk.vtkMassProperties()\n        mprop.SetInputData(self.TRiaNgUlaTE())\n        return mprop.GetVolume()\n\n    @property\n    def point_normals(self) -> 'pyvista.pyvista_ndarray':  # numpydoc ignore=RT01\n        \"\"\"Return the point normals.\n\n        If the point data already contains an array named ``'Normals'``, this\n        array will be returned. Otherwise, the normals will be computed using\n        the default options of :func:`compute_normals()\n        <pyvista.PolyDataFilters.compute_normals>` and returned.\n\n        Returns\n        -------\n        pyvista.pyvista_ndarray\n            Array of point normals.\n\n        Examples\n        --------\n        >>> import pyvista\n        >>> sphere = pyvista.Sphere()\n        >>> sphere.point_normals  # doctest:+SKIP\n        pyvista_ndarray([[-2.48721432e-10, -1.08815623e-09, -1.00000000e+00],\n                         [-2.48721432e-10, -1.08815623e-09,  1.00000000e+00],\n                         [-1.18888125e-01,  3.40539310e-03, -9.92901802e-01],\n                         ...,\n                         [-3.11940581e-01, -6.81432486e-02,  9.47654784e-01],\n                         [-2.09880397e-01, -4.65070531e-02,  9.76620376e-01],\n                         [-1.15582108e-01, -2.80492082e-02,  9.92901802e-01]],\n                        dtype=float32)\n\n        \"\"\"\n        if 'Normals' in self.point_data:\n            normals = self.point_data['Normals']\n        else:\n            normals = self.CoMpUte_noRMALS(cell_normals=False, inplace=False).point_data['Normals']\n        return normals\n\n    @property\n    def cell_normals(self) -> 'pyvista.pyvista_ndarray':  # numpydoc ignore=RT01\n        \"\"\"Return the cell normals.\n\n        If the cell data already contains an array named ``'Normals'``, this\n        array will be returned. Otherwise, the normals will be computed using\n        the default options of :func:`compute_normals()\n        <pyvista.PolyDataFilters.compute_normals>` and returned.\n\n        Returns\n        -------\n        pyvista.pyvista_ndarray\n            Array of cell normals.\n\n        Examples\n        --------\n        >>> import pyvista\n        >>> sphere = pyvista.Sphere()\n        >>> sphere.cell_normals  # doctest:+SKIP\n        pyvista_ndarray([[-0.05413816,  0.00569015, -0.9985172 ],\n                         [-0.05177207,  0.01682176, -0.9985172 ],\n                         [-0.04714328,  0.02721819, -0.9985172 ],\n                         ...,\n                         [-0.26742265, -0.02810723,  0.96316934],\n                         [-0.1617585 , -0.01700151,  0.9866839 ],\n                         [-0.1617585 , -0.01700151,  0.9866839 ]], dtype=float32)\n\n        \"\"\"\n        if 'Normals' in self.cell_data:\n            normals = self.cell_data['Normals']\n        else:\n            normals = self.CoMpUte_noRMALS(point_normals=False, inplace=False).cell_data['Normals']\n        return normals\n\n    @property\n    def face_normals(self) -> 'pyvista.pyvista_ndarray':  # numpydoc ignore=RT01\n        \"\"\"Return the cell normals.\n\n        Alias to :func:`PolyData.cell_normals`.\n\n        Returns\n        -------\n        pyvista.pyvista_ndarray\n            Array of face normals.\n\n        Examples\n        --------\n        >>> import pyvista\n        >>> sphere = pyvista.Sphere()\n        >>> sphere.face_normals  # doctest:+SKIP\n        pyvista_ndarray([[-0.05413816,  0.00569015, -0.9985172 ],\n                         [-0.05177207,  0.01682176, -0.9985172 ],\n                         [-0.04714328,  0.02721819, -0.9985172 ],\n                         ...,\n                         [-0.26742265, -0.02810723,  0.96316934],\n                         [-0.1617585 , -0.01700151,  0.9866839 ],\n                         [-0.1617585 , -0.01700151,  0.9866839 ]], dtype=float32)\n\n        \"\"\"\n        return self.cell_normals\n\n    @property\n    def obbTree(self):  # numpydoc ignore=RT01\n        \"\"\"Return the obbTree of the polydata.\n\n        An obbTree is an object to generate oriented bounding box (OBB)\n        trees. An oriented bounding box is a bounding box that does not\n        necessarily line up along coordinate axes. The OBB tree is a\n        hierarchical tree structure of such boxes, where deeper levels of OBB\n        confine smaller regions of space.\n        \"\"\"\n        if not hasattr(self, '_obbTree'):\n            self._obbTree = _vtk.vtkOBBTree()\n            self._obbTree.SetDataSet(self)\n            self._obbTree.BuildLocator()\n\n        return self._obbTree\n\n    @property\n    def n_open_edges(self) -> int:  # numpydoc ignore=RT01\n        \"\"\"Return the number of open edges on this mesh.\n\n        Examples\n        --------\n        Return the number of open edges on a sphere.\n\n        >>> import pyvista\n        >>> sphere = pyvista.Sphere()\n        >>> sphere.n_open_edges\n        0\n\n        Return the number of open edges on a plane.\n\n        >>> plane = pyvista.Plane(i_resolution=1, j_resolution=1)\n        >>> plane.n_open_edges\n        4\n\n        \"\"\"\n        alg = _vtk.vtkFeatureEdges()\n        alg.FeatureEdgesOff()\n        alg.BoundaryEdgesOn()\n        alg.NonManifoldEdgesOn()\n        alg.SetInputDataObject(self)\n        alg.Update()\n        return alg.GetOutput().GetNumberOfCells()\n\n    @property\n    def is_manifold(self) -> bool:  # numpydoc ignore=RT01\n        \"\"\"Return if the mesh is manifold (no open edges).\n\n        Examples\n        --------\n        Show a sphere is manifold.\n\n        >>> import pyvista\n        >>> pyvista.Sphere().is_manifold\n        True\n\n        Show a plane is not manifold.\n\n        >>> pyvista.Plane().is_manifold\n        False\n\n        \"\"\"\n        return self.n_open_edges == 0\n\n    def __del__(self):\n        \"\"\"Delete the object.\"\"\"\n        if hasattr(self, '_obbTree'):\n            del self._obbTree"
    },
    {
        "task_id": "litestar-org__litestar-0001_FieldDefinition",
        "class_name": "FieldDefinition",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/typing.py",
        "sketchy_description": "The 'FieldDefinition' class is a data class, indicated by the `@dataclass(frozen=True)` decorator, which suggests that instances of this class are immutable. This class does not inherit from any other class explicitly.\n\nThe class has a class method named '_extract_metadata' which takes six arguments: 'cls', 'annotation', 'name', 'default', 'metadata', and 'extra'. It returns a tuple consisting of an instance of 'KwargDefinition' or 'None', and a dictionary containing metadata. This method is used to extract metadata from the given parameters.\n\nThe class has multiple properties that return boolean values, indicating various characteristics of the field definition:\n\n- 'has_default' checks if the field has a default value and returns a boolean.\n- 'is_non_string_iterable' checks if the field type is an iterable, excluding strings, and returns a boolean.\n- 'is_non_string_sequence' checks if the field type is a non-string sequence and returns a boolean.\n- 'is_any' checks if the field type is 'Any' and returns a boolean.\n- 'is_generic' checks if the field type is a custom class extending 'Generic' and returns a boolean.\n- 'is_simple_type' checks if the field type is a simple, singleton value like 'int' or 'str' and returns a boolean.\n- 'is_parameter_field' checks if the field type is a parameter keyword argument value and returns a boolean.\n- 'is_const' checks if the field is defined as a constant value and returns a boolean.\n- 'is_required' checks if the field should be marked as a required parameter and returns a boolean.\n- 'is_annotated' checks if the field type is 'Annotated' and returns a boolean.\n- 'is_literal' checks if the field type is 'Literal' and returns a boolean.\n- 'is_forward_ref' checks if the annotation is a forward reference and returns a boolean.\n- 'is_mapping' checks if the annotation is a mapping and returns a boolean.\n- 'is_tuple' checks if the annotation is a 'tuple' and returns a boolean.\n- 'is_type_var' checks if the annotation is a 'TypeVar' and returns a boolean.\n- 'is_union' checks if the annotation is a union type and returns a boolean.\n- 'is_optional' checks if the annotation is 'Optional' and returns a boolean.\n- 'is_none_type' checks if the annotation is 'NoneType' and returns a boolean.\n- 'is_collection' checks if the annotation is a collection type and returns a boolean.\n- 'is_non_string_collection' checks if the annotation is a non-string collection type and returns a boolean.\n- 'is_dataclass_type' checks if the annotation is a dataclass type and returns a boolean.\n- 'is_typeddict_type' checks if the type is 'TypedDict' and returns a boolean.\n\nThe 'type_' property returns the type of the annotation with all wrappers removed, including generic types.\n\nThe class method 'from_annotation' takes an 'annotation' and additional keyword arguments, and returns a new instance of 'FieldDefinition'.\n\nThe class method 'from_kwarg' creates a new 'FieldDefinition' instance from the given parameters, including 'annotation', 'name', 'default', 'inner_types', 'kwarg_definition', and 'extra'.\n\nThe class method 'from_parameter' initializes a 'FieldDefinition' from an 'inspect.Parameter' and a dictionary of type hints.\n\nThe method 'match_predicate_recursively' takes a predicate function and applies it recursively to the field and any inner fields, returning a boolean result.\n\nThe '__deepcopy__' method creates a deep copy of the 'FieldDefinition' object.\n\nThe '__eq__' method checks if the 'FieldDefinition' object is equal to another object and returns a boolean.\n\nThe '__hash__' method returns the hash value of the 'FieldDefinition' object.\n\nThe class has several class variables defined using `__slots__` to optimize memory usage and prevent the creation of a `__dict__` for each instance. These variables include 'raw', 'annotation', 'type_wrappers', 'origin', 'args', 'metadata', 'instantiable_origin', 'safe_generic_origin', 'inner_types', 'default', 'extra', 'kwarg_definition', and 'name'. Each of these variables holds specific information about the field definition, such as the raw type annotation, the default value, any extra information, and the name of the field.\n\nThere are no instance variables accessible outside the class, as the class is frozen and does not allow setting new attributes after creation.\n\nThe properties accessible in the class provide various checks and information about the field definition, such as whether it has default values, if it is a certain type (e.g., 'Any', 'Optional', 'Union'), and if it is a subclass of a given type. These properties are useful for introspection and type checking within the context of the class's usage.",
        "detailed_description": "The 'FieldDefinition' class is a dataclass that represents a function parameter or type annotation. The class has several attributes such as 'raw', 'annotation', 'type_wrappers', 'origin', 'args', 'metadata', 'instantiable_origin', 'safe_generic_origin', 'inner_types', 'default', 'extra', 'kwarg_definition', and 'name'. The class also has a '__deepcopy__' method that takes a dictionary 'memo' as an argument and returns a new instance of the class with the same attributes as the original instance. The '__eq__' method takes an argument 'other' and returns a boolean value indicating whether the 'other' instance is equal to the current instance. The '__hash__' method returns an integer hash value of the instance.\n\nThe class has a class method '_extract_metadata' that takes arguments 'annotation', 'name', 'default', 'metadata', and 'extra'. This method returns a tuple containing a 'KwargDefinition' instance and a dictionary. The method uses the '_KWARG_META_EXTRACTORS' to extract the metadata from the 'annotation', 'name', and 'default' arguments.\n\nThe class has several property methods such as 'has_default', 'is_non_string_iterable', 'is_non_string_sequence', 'is_any', 'is_generic', 'is_simple_type', 'is_parameter_field', 'is_const', 'is_required', 'is_annotated', 'is_literal', 'is_forward_ref', 'is_mapping', 'is_tuple', 'is_type_var', 'is_union', 'is_optional', 'is_none_type', 'is_collection', 'is_non_string_collection', 'bound_types', 'generic_types', 'is_dataclass_type', 'is_typeddict_type', 'type_', 'is_subclass_of', 'has_inner_subclass_of', 'get_type_hints', 'from_annotation', 'from_kwarg', 'from_parameter', and 'match_predicate_recursively'. These property methods return various information about the instance such as whether it has a default value, whether it is iterable, whether it is a generic type, whether it is a simple type, whether it is a parameter field, whether it is a constant, whether it is required, whether it is annotated, whether it is a literal, whether it is a forward reference, whether it is a mapping, whether it is a tuple, whether it is a type variable, whether it is a union, whether it is optional, whether it is a none type, whether it is a collection, whether it is a non-string collection, whether it is a dataclass type, whether it is a typeddict type, whether it is a subclass of a given type, whether it has an inner subclass of a given type, whether it matches a given predicate recursively, and the type hints of the instance. The class methods 'from_annotation', 'from_kwarg', and 'from_parameter' return a new instance of the class with the given arguments.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_typing.py::test_field_definition_from_annotation[annotation3-expected3]",
                "tests/unit/test_typing.py::test_field_definition_from_annotation[annotation4-expected4]",
                "tests/unit/test_typing.py::test_field_definition_from_annotation[annotation2-expected2]",
                "tests/unit/test_typing.py::test_field_definition_from_annotation[annotation5-expected5]",
                "tests/unit/test_typing.py::test_field_definition_from_annotation[annotation7-expected7]",
                "tests/unit/test_typing.py::test_field_definition_from_annotation[annotation8-expected8]",
                "tests/unit/test_typing.py::test_field_definition_from_annotation[int-expected0]",
                "tests/unit/test_typing.py::test_field_definition_kwarg_definition_from_extras",
                "tests/unit/test_typing.py::test_field_definition_kwarg_definition_from_kwargs[kwarg_definition0]",
                "tests/unit/test_typing.py::test_field_definition_with_annotated_kwarg_definition",
                "tests/unit/test_typing.py::test_field_definition_kwarg_definition_from_kwargs[kwarg_definition1]",
                "tests/unit/test_typing.py::test_field_definition_from_union_annotation",
                "tests/unit/test_typing.py::test_field_definition_is_forward_ref_predicate[int]",
                "tests/unit/test_typing.py::test_field_definition_is_optional_predicate",
                "tests/unit/test_typing.py::test_field_definition_from_annotation[annotation6-expected6]",
                "tests/unit/test_typing.py::test_field_definition_is_type_var_predicate",
                "tests/unit/test_typing.py::test_field_definition_is_union_predicate",
                "tests/unit/test_typing.py::test_field_definition_from_annotation[annotation9-expected9]",
                "tests/unit/test_typing.py::test_field_definition_is_dataclass_predicate",
                "tests/unit/test_typing.py::test_field_definition_has_inner_subclass_of",
                "tests/unit/test_typing.py::test_field_definition_is_forward_ref_predicate[value1]",
                "tests/unit/test_typing.py::test_field_definition_equality",
                "tests/unit/test_typing.py::test_field_definition_get_type_hints[GenericDataclass-expected_type_hints1]",
                "tests/unit/test_typing.py::test_field_definition_get_type_hints[annotation0-expected_type_hints0]",
                "tests/unit/test_typing.py::test_field_definition_from_annotation[annotation1-expected1]",
                "tests/unit/test_typing.py::test_is_required",
                "tests/unit/test_typing.py::test_field_definition_get_type_hints_dont_resolve_generics[annotation0-expected_type_hints0]",
                "tests/unit/test_typing.py::test_field_definition_get_type_hints_dont_resolve_generics[GenericDataclass-expected_type_hints1]",
                "tests/unit/test_typing.py::test_field_definition_hash",
                "tests/unit/test_typing.py::test_field_definition_get_type_hints[NormalDataclass-expected_type_hints2]",
                "tests/unit/test_typing.py::test_field_definition_bound_type",
                "tests/unit/test_typing.py::test_field_definition_is_typeddict_predicate",
                "tests/unit/test_typing.py::test_field_definition_get_type_hints_dont_resolve_generics[NormalDataclass-expected_type_hints2]",
                "tests/unit/test_typing.py::test_nested_generic_types",
                "tests/unit/test_typing.py::test_field_definition_is_subclass_of",
                "tests/unit/test_contrib/test_msgspec.py::test_detect_nested_field",
                "tests/unit/test_contrib/test_attrs/test_schema_plugin.py::test_schema_generation_with_generic_classes",
                "tests/unit/test_contrib/test_pydantic/test_dto.py::test_detect_nested_field_pydantic_v1",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_collection_constrained_field_schema_sub_fields[v1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_collection_constrained_field_schema_pydantic_v2[annotation3]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_collection_constrained_field_schema_pydantic_v1[ConstrainedListValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_collection_constrained_field_schema_pydantic_v2[annotation2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_string_constrained_field_schema_pydantic_v1[ConstrainedBytesValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_string_constrained_field_schema_pydantic_v1[ConstrainedBytesValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_string_constrained_field_schema_pydantic_v2[annotation0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_string_constrained_field_schema_pydantic_v2[annotation4]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_string_constrained_field_schema_pydantic_v2[annotation2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_string_constrained_field_schema_pydantic_v1[ConstrainedStrValue3]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_string_constrained_field_schema_pydantic_v2[annotation5]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_collection_constrained_field_schema_pydantic_v1[ConstrainedListValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_string_constrained_field_schema_pydantic_v2[annotation7]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_string_constrained_field_schema_pydantic_v2[annotation9]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_numerical_constrained_field_schema_pydantic_v1[ConstrainedIntValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_string_constrained_field_schema_pydantic_v2[annotation10]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_string_constrained_field_schema_pydantic_v2[annotation6]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_numerical_constrained_field_schema_pydantic_v1[ConstrainedFloatValue2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_numerical_constrained_field_schema_pydantic_v1[ConstrainedIntValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_numerical_constrained_field_schema_pydantic_v1[ConstrainedDecimalValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_numerical_constrained_field_schema_pydantic_v1[ConstrainedDecimalValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_numerical_constrained_field_schema_pydantic_v1[ConstrainedDecimalValue2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_numerical_constrained_field_schema_pydantic_v1[ConstrainedFloatValue3]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_numerical_constrained_field_schema_pydantic_v1[ConstrainedFloatValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_numerical_constrained_field_schema_pydantic_v2[annotation1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_collection_constrained_field_schema_sub_fields[v2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_numerical_constrained_field_schema_pydantic_v2[annotation0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_numerical_constrained_field_schema_pydantic_v1[ConstrainedDecimalValue3]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_string_constrained_field_schema_pydantic_v1[ConstrainedStrValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_string_constrained_field_schema_pydantic_v1[ConstrainedStrValue5]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_string_constrained_field_schema_pydantic_v1[ConstrainedStrValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_numerical_constrained_field_schema_pydantic_v2[annotation4]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_numerical_constrained_field_schema_pydantic_v2[annotation2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_numerical_constrained_field_schema_pydantic_v2[annotation3]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_numerical_constrained_field_schema_pydantic_v2[annotation5]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_numerical_constrained_field_schema_pydantic_v2[annotation6]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_collection_constrained_field_schema_pydantic_v2[annotation1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_numerical_constrained_field_schema_pydantic_v2[annotation7]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_numerical_constrained_field_schema_pydantic_v2[annotation8]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_collection_constrained_field_schema_pydantic_v2[annotation0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_collection_constrained_field_schema_pydantic_v1[ConstrainedSetValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_numerical_constrained_field_schema_pydantic_v2[annotation9]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_numerical_constrained_field_schema_pydantic_v2[annotation10]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_date_constrained_field_schema_pydantic_v1[ConstrainedDateValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_string_constrained_field_schema_pydantic_v1[ConstrainedStrValue7]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_date_constrained_field_schema_pydantic_v1[ConstrainedDateValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_string_constrained_field_schema_pydantic_v2[annotation1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_date_constrained_field_schema_pydantic_v1[ConstrainedDateValue2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_string_constrained_field_schema_pydantic_v2[annotation8]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_string_constrained_field_schema_pydantic_v1[ConstrainedStrValue6]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_collection_constrained_field_schema_pydantic_v1[ConstrainedSetValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_string_constrained_field_schema_pydantic_v1[ConstrainedStrValue4]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_date_constrained_field_schema_pydantic_v1[ConstrainedDateValue3]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_date_constrained_field_schema_pydantic_v2[annotation3]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_date_constrained_field_schema_pydantic_v2[annotation0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_date_constrained_field_schema_pydantic_v2[annotation1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_numerical_constrained_field_schema_pydantic_v1[ConstrainedFloatValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_string_constrained_field_schema_pydantic_v1[ConstrainedBytesValue2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedIntValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_numerical_constrained_field_schema_pydantic_v1[ConstrainedIntValue2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_date_constrained_field_schema_pydantic_v2[annotation2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedIntValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedStrValue2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedIntValue2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_string_constrained_field_schema_pydantic_v1[ConstrainedStrValue2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedFloatValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedFloatValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedFloatValue3]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedFloatValue2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedDecimalValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedDecimalValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_string_constrained_field_schema_pydantic_v2[annotation3]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedDecimalValue3]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation31]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedListValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedListValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedSetValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedStrValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedStrValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedSetValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedDecimalValue2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedStrValue7]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedStrValue4]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedStrValue5]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedStrValue3]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedStrValue6]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation41]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedBytesValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedBytesValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedBytesValue2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedDateValue0]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation32]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedDateValue3]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation33]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedDateValue1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation30]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[ConstrainedDateValue2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation35]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation34]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation36]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation55]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation37]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation46]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation38]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation39]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation40]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation42]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation44]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation47]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation43]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation45]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation48]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation58]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation51]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation49]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_pydantic_model_with_annotated_model_attribute[v1-False]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation50]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation59]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation54]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation57]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation56]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation52]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_constrained_field_schema[annotation53]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_pydantic_model_with_unhashable_literal_default",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_pydantic_model_with_annotated_model_attribute[v1-True]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_field_v1",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_field_v2",
                "tests/unit/test_contrib/test_pydantic/test_pydantic_dto_factory.py::test_detect_nested_field[v1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_for_url_v2[field_type2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_pydantic_model_with_annotated_model_attribute[v2-False]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_for_computed_field[True]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_schema_generation_with_generic_classes[PydanticV1Generic]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_for_url_v2[Url]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_pydantic_model_with_annotated_model_attribute[v2-True]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_for_url_v2[field_type1]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[ConstrainedDateValue]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[constrained10]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[constrained11]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[ConstrainedFloatValue]",
                "tests/unit/test_contrib/test_pydantic/test_pydantic_dto_factory.py::test_detect_nested_field[v2]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_schema_generation_with_generic_classes[PydanticV2Generic]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[constrained13]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_exclude_private_fields[V2GenericModelWithPrivateFields]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_exclude_private_fields[V2ModelWithPrivateFields]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[constrained12]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[constrained9]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[ConstrainedSetValue]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_exclude_private_fields[V1ModelWithPrivateFields]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[constrained8]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_v2_constrained_secrets",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_exclude_private_fields[V1GenericModelWithPrivateFields]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[constrained7]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[ConstrainedDecimalValue]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_for_computed_field[False]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[ConstrainedStrValue]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[ConstrainedIntValue]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[ConstrainedListValue]",
                "tests/unit/test_dto/test_interface.py::test_dto_interface_create_openapi_schema_default_implementation",
                "tests/unit/test_dto/test_factory/test_base_dto.py::test_raises_invalid_annotation_for_mismatched_types",
                "tests/unit/test_dto/test_factory/test_dataclass_dto.py::test_dataclass_field_definitions_38",
                "tests/unit/test_dto/test_factory/test_base_dto.py::test_sub_types_supported",
                "tests/unit/test_dto/test_factory/test_base_dto.py::test_from_bytes",
                "tests/unit/test_dto/test_factory/test_base_dto.py::test_config_field_rename",
                "tests/unit/test_dto/test_factory/test_dataclass_dto.py::test_dataclass_field_definitions",
                "tests/unit/test_dto/test_factory/test_dataclass_dto.py::test_dataclass_detect_nested",
                "tests/unit/test_dto/test_factory/test_base_dto.py::test_raises_invalid_annotation_for_non_homogenous_collection_types",
                "tests/unit/test_dto/test_factory/test_utils.py::test_resolve_generic_wrapper_type_origin_no_parameters",
                "tests/unit/test_dto/test_factory/test_utils.py::test_resolve_generic_wrapper_type_model_type_not_subtype_of_specialized_type",
                "tests/unit/test_dto/test_factory/test_utils.py::test_resolve_model_type_optional",
                "tests/unit/test_dto/test_factory/test_utils.py::test_resolve_generic_wrapper_type_no_origin",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_scalar_annotation[default_backend]",
                "tests/unit/test_dto/test_factory/test_utils.py::test_resolve_generic_wrapper_type_type_var_not_attribute",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_iterable_annotation[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_scalar_annotation[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_populate_data_from_raw[default_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_parse_model_nested_include[default_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_parse_model_nested_exclude[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_parse_raw_msgpack[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_iterable_annotation[default_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_model_name_uniqueness[default_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_populate_collection_data_from_raw[default_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_parse_raw_json[default_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_parse_unsupported_media_type[default_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_populate_data_from_builtins[default_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_transfer_only_touches_included_attributes[default_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_create_openapi_schema[default_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_parse_raw_json[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_create_openapi_schema[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_transfer_only_touches_included_attributes[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_populate_collection_data_from_raw[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_parse_raw_msgpack[default_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_model_name_uniqueness[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_encode_collection_data[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_populate_data_from_raw[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_parse_model_nested_include[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_utils.py::test_create_transfer_model_type_annotation_simple_type_with_nested_field_info",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_parse_model_nested_exclude[default_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_utils.py::test_create_transfer_model_type_annotation_collection_type_nested",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_encode_data[default_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_utils.py::test_create_transfer_model_type_annotation_tuple_type_nested",
                "tests/unit/test_dto/test_factory/test_backends/test_utils.py::test_create_transfer_model_type_annotation_simple_type_without_nested_field_info",
                "tests/unit/test_dto/test_factory/test_backends/test_utils.py::test_should_mark_private_underscore_fields_private_true",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_populate_data_from_builtins[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_utils.py::test_create_transfer_model_type_annotation_collection_type_not_nested",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_encode_collection_data[default_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_utils.py::test_create_transfer_model_type_annotation_tuple_type_not_nested",
                "tests/unit/test_dto/test_factory/test_backends/test_utils.py::test_create_transfer_model_type_annotation_unexpected_transfer_type",
                "tests/unit/test_dto/test_factory/test_backends/test_utils.py::test_should_mark_private_underscore_fields_private_false",
                "tests/unit/test_dto/test_factory/test_backends/test_utils.py::test_create_transfer_model_type_annotation_mapping_type_nested",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_parse_unsupported_media_type[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_encode_data[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_utils.py::test_create_transfer_model_type_annotation_mapping_type_not_nested",
                "tests/unit/test_openapi/test_request_body.py::test_request_body_generation_with_dto",
                "tests/unit/test_openapi/test_responses.py::test_response_generation_with_dto",
                "tests/unit/test_openapi/test_schema.py::test_create_schema_for_dataclass_with_annotated_model_attribute[False]",
                "tests/unit/test_openapi/test_schema.py::test_create_schema_from_msgspec_annotated_type",
                "tests/unit/test_openapi/test_schema.py::test_literal_enums",
                "tests/unit/test_openapi/test_schema.py::test_schema_generation_with_generic_classes[cls0]",
                "tests/unit/test_openapi/test_schema.py::test_annotated_types",
                "tests/unit/test_openapi/test_schema.py::test_handling_of_literals",
                "tests/unit/test_openapi/test_schema.py::test_schema_generation_with_generic_classes[cls1]",
                "tests/unit/test_openapi/test_schema.py::test_schema_generation_with_pagination[annotation1]",
                "tests/unit/test_openapi/test_schema.py::test_schema_tuple_with_union",
                "tests/unit/test_openapi/test_schema.py::test_schema_generation_with_pagination[annotation2]",
                "tests/unit/test_openapi/test_schema.py::test_schema_generation_with_ellipsis",
                "tests/unit/test_openapi/test_schema.py::test_create_schema_for_dataclass_with_annotated_model_attribute[True]",
                "tests/unit/test_openapi/test_schema.py::test_schema_generation_with_generic_classes_constrained",
                "tests/unit/test_openapi/test_schema.py::test_optional_literal",
                "tests/unit/test_openapi/test_schema.py::test_type_union[Struct]",
                "tests/unit/test_openapi/test_schema.py::test_type_union_with_none[TypedDict]",
                "tests/unit/test_openapi/test_schema.py::test_type_union[TypedDict]",
                "tests/unit/test_openapi/test_schema.py::test_get_schema_for_annotation_enum",
                "tests/unit/test_openapi/test_schema.py::test_type_union_with_none[dataclass]",
                "tests/unit/test_openapi/test_schema.py::test_type_union_with_none[Struct]",
                "tests/unit/test_openapi/test_schema.py::test_process_schema_result_with_unregistered_object_schema",
                "tests/unit/test_openapi/test_schema.py::test_create_schema_for_typedict_with_annotated_required_and_not_required_model_attributes[True]",
                "tests/unit/test_openapi/test_schema.py::test_optional_enum",
                "tests/unit/test_openapi/test_schema.py::test_create_schema_for_typedict_with_annotated_required_and_not_required_model_attributes[False]",
                "tests/unit/test_openapi/test_schema.py::test_type_union[dataclass]",
                "tests/unit/test_openapi/test_schema.py::test_title_validation",
                "tests/unit/test_plugins/test_base.py::test_openapi_schema_plugin_is_constrained_field",
                "tests/unit/test_openapi/test_schema.py::test_schema_generation_with_pagination[annotation0]",
                "tests/unit/test_openapi/test_schema.py::test_process_schema_result",
                "tests/unit/test_utils/test_signature.py::test_field_definition_from_parameter",
                "tests/unit/test_utils/test_signature.py::test_field_definition_from_parameter_raises_improperly_configured_if_no_annotation",
                "tests/unit/test_utils/test_signature.py::test_field_definition_from_parameter_has_default_predicate",
                "tests/unit/test_utils/test_signature.py::test_field_definition_from_parameter_annotation_property"
            ]
        },
        "ground_truth_class_body": "@dataclass(frozen=True)\nclass FieldDefinition:\n    \"\"\"Represents a function parameter or type annotation.\"\"\"\n\n    __slots__ = (\n        \"annotation\",\n        \"args\",\n        \"default\",\n        \"extra\",\n        \"inner_types\",\n        \"instantiable_origin\",\n        \"kwarg_definition\",\n        \"metadata\",\n        \"name\",\n        \"origin\",\n        \"raw\",\n        \"safe_generic_origin\",\n        \"type_wrappers\",\n    )\n\n    raw: Any\n    \"\"\"The annotation exactly as received.\"\"\"\n    annotation: Any\n    \"\"\"The annotation with any \"wrapper\" types removed, e.g. Annotated.\"\"\"\n    type_wrappers: tuple[type, ...]\n    \"\"\"A set of all \"wrapper\" types, e.g. Annotated.\"\"\"\n    origin: Any\n    \"\"\"The result of calling ``get_origin(annotation)`` after unwrapping Annotated, e.g. list.\"\"\"\n    args: tuple[Any, ...]\n    \"\"\"The result of calling ``get_args(annotation)`` after unwrapping Annotated, e.g. (int,).\"\"\"\n    metadata: tuple[Any, ...]\n    \"\"\"Any metadata associated with the annotation via ``Annotated``.\"\"\"\n    instantiable_origin: Any\n    \"\"\"An equivalent type to ``origin`` that can be safely instantiated. E.g., ``Sequence`` -> ``list``.\"\"\"\n    safe_generic_origin: Any\n    \"\"\"An equivalent type to ``origin`` that can be safely used as a generic type across all supported Python versions.\n\n    This is to serve safely rebuilding a generic outer type with different args at runtime.\n    \"\"\"\n    inner_types: tuple[FieldDefinition, ...]\n    \"\"\"The type's generic args parsed as ``FieldDefinition``, if applicable.\"\"\"\n    default: Any\n    \"\"\"Default value of the field.\"\"\"\n    extra: dict[str, Any]\n    \"\"\"A mapping of extra values.\"\"\"\n    kwarg_definition: KwargDefinition | DependencyKwarg | None\n    \"\"\"Kwarg Parameter.\"\"\"\n    name: str\n    \"\"\"Field name.\"\"\"\n\n    def __deepcopy__(self, memo: dict[str, Any]) -> Self:\n        return type(self)(**{attr: deepcopy(getattr(self, attr)) for attr in self.__slots__})\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, FieldDefinition):\n            return False\n\n        if self.origin:\n            return self.origin == other.origin and self.inner_types == other.inner_types\n\n        return self.annotation == other.annotation  # type: ignore[no-any-return]\n\n    def __hash__(self) -> int:\n        return hash((self.name, self.raw, self.annotation, self.origin, self.inner_types))\n\n    @classmethod\n    def _extract_metadata(\n        cls, annotation: Any, name: str | None, default: Any, metadata: tuple[Any, ...], extra: dict[str, Any] | None\n    ) -> tuple[KwargDefinition | None, dict[str, Any]]:\n        model = BodyKwarg if name == \"data\" else ParameterKwarg\n\n        for extractor in _KWARG_META_EXTRACTORS:\n            if extractor.matches(annotation=annotation, name=name, default=default):\n                return _create_metadata_from_type(\n                    extractor.extract(annotation=annotation, default=default),\n                    model=model,\n                    annotation=annotation,\n                    extra=extra,\n                )\n\n        if any(isinstance(arg, KwargDefinition) for arg in get_args(annotation)):\n            return next(arg for arg in get_args(annotation) if isinstance(arg, KwargDefinition)), extra or {}\n\n        if metadata:\n            return _create_metadata_from_type(metadata=metadata, model=model, annotation=annotation, extra=extra)\n\n        return None, {}\n\n    @property\n    def has_default(self) -> bool:\n        \"\"\"Check if the field has a default value.\n\n        Returns:\n            True if the default is not Empty or Ellipsis otherwise False.\n        \"\"\"\n        return self.default is not Empty and self.default is not Ellipsis\n\n    @property\n    def is_non_string_iterable(self) -> bool:\n        \"\"\"Check if the field type is an Iterable.\n\n        If ``self.annotation`` is an optional union, only the non-optional members of the union are evaluated.\n\n        See: https://github.com/litestar-org/litestar/issues/1106\n        \"\"\"\n        annotation = self.annotation\n        if self.is_optional:\n            annotation = make_non_optional_union(annotation)\n        return is_non_string_iterable(annotation)\n\n    @property\n    def is_non_string_sequence(self) -> bool:\n        \"\"\"Check if the field type is a non-string Sequence.\n\n        If ``self.annotation`` is an optional union, only the non-optional members of the union are evaluated.\n\n        See: https://github.com/litestar-org/litestar/issues/1106\n        \"\"\"\n        annotation = self.annotation\n        if self.is_optional:\n            annotation = make_non_optional_union(annotation)\n        return is_non_string_sequence(annotation)\n\n    @property\n    def is_any(self) -> bool:\n        \"\"\"Check if the field type is Any.\"\"\"\n        return is_any(self.annotation)\n\n    @property\n    def is_generic(self) -> bool:\n        \"\"\"Check if the field type is a custom class extending Generic.\"\"\"\n        return is_generic(self.annotation)\n\n    @property\n    def is_simple_type(self) -> bool:\n        \"\"\"Check if the field type is a singleton value (e.g. int, str etc.).\"\"\"\n        return not (\n            self.is_generic or self.is_optional or self.is_union or self.is_mapping or self.is_non_string_iterable\n        )\n\n    @property\n    def is_parameter_field(self) -> bool:\n        \"\"\"Check if the field type is a parameter kwarg value.\"\"\"\n        return isinstance(self.kwarg_definition, ParameterKwarg)\n\n    @property\n    def is_const(self) -> bool:\n        \"\"\"Check if the field is defined as constant value.\"\"\"\n        return bool(self.kwarg_definition and getattr(self.kwarg_definition, \"const\", False))\n\n    @property\n    def is_required(self) -> bool:\n        \"\"\"Check if the field should be marked as a required parameter.\"\"\"\n        if Required in self.type_wrappers:  # type: ignore[comparison-overlap]\n            return True\n\n        if NotRequired in self.type_wrappers or UnsetType in self.args:  # type: ignore[comparison-overlap]\n            return False\n\n        if isinstance(self.kwarg_definition, ParameterKwarg) and self.kwarg_definition.required is not None:\n            return self.kwarg_definition.required\n\n        return not self.is_optional and not self.is_any and (not self.has_default or self.default is None)\n\n    @property\n    def is_annotated(self) -> bool:\n        \"\"\"Check if the field type is Annotated.\"\"\"\n        return bool(self.metadata)\n\n    @property\n    def is_literal(self) -> bool:\n        \"\"\"Check if the field type is Literal.\"\"\"\n        return self.origin is Literal\n\n    @property\n    def is_forward_ref(self) -> bool:\n        \"\"\"Whether the annotation is a forward reference or not.\"\"\"\n        return isinstance(self.annotation, (str, ForwardRef))\n\n    @property\n    def is_mapping(self) -> bool:\n        \"\"\"Whether the annotation is a mapping or not.\"\"\"\n        return self.is_subclass_of(Mapping)\n\n    @property\n    def is_tuple(self) -> bool:\n        \"\"\"Whether the annotation is a ``tuple`` or not.\"\"\"\n        return self.is_subclass_of(tuple)\n\n    @property\n    def is_type_var(self) -> bool:\n        \"\"\"Whether the annotation is a TypeVar or not.\"\"\"\n        return isinstance(self.annotation, TypeVar)\n\n    @property\n    def is_union(self) -> bool:\n        \"\"\"Whether the annotation is a union type or not.\"\"\"\n        return self.origin in UnionTypes\n\n    @property\n    def is_optional(self) -> bool:\n        \"\"\"Whether the annotation is Optional or not.\"\"\"\n        return bool(self.is_union and NoneType in self.args)\n\n    @property\n    def is_none_type(self) -> bool:\n        \"\"\"Whether the annotation is NoneType or not.\"\"\"\n        return self.annotation is NoneType\n\n    @property\n    def is_collection(self) -> bool:\n        \"\"\"Whether the annotation is a collection type or not.\"\"\"\n        return self.is_subclass_of(Collection)\n\n    @property\n    def is_non_string_collection(self) -> bool:\n        \"\"\"Whether the annotation is a non-string collection type or not.\"\"\"\n        return self.is_collection and not self.is_subclass_of((str, bytes))\n\n    @property\n    def bound_types(self) -> tuple[FieldDefinition, ...] | None:\n        \"\"\"A tuple of bound types - if the annotation is a TypeVar with bound types, otherwise None.\"\"\"\n        if self.is_type_var and (bound := getattr(self.annotation, \"__bound__\", None)):\n            if is_union(bound):\n                return tuple(FieldDefinition.from_annotation(t) for t in get_args(bound))\n            return (FieldDefinition.from_annotation(bound),)\n        return None\n\n    @property\n    def generic_types(self) -> tuple[FieldDefinition, ...] | None:\n        \"\"\"A tuple of generic types passed into the annotation - if its generic.\"\"\"\n        if not (bases := getattr(self.annotation, \"__orig_bases__\", None)):\n            return None\n        args: list[FieldDefinition] = []\n        for base_args in [getattr(base, \"__args__\", ()) for base in bases]:\n            for arg in base_args:\n                field_definition = FieldDefinition.from_annotation(arg)\n                if field_definition.generic_types:\n                    args.extend(field_definition.generic_types)\n                else:\n                    args.append(field_definition)\n        return tuple(args)\n\n    @property\n    def is_dataclass_type(self) -> bool:\n        \"\"\"Whether the annotation is a dataclass type or not.\"\"\"\n\n        return is_dataclass(cast(\"type\", self.origin or self.annotation))\n\n    @property\n    def is_typeddict_type(self) -> bool:\n        \"\"\"Whether the type is TypedDict or not.\"\"\"\n\n        return is_typeddict(self.origin or self.annotation)\n\n    @property\n    def type_(self) -> Any:\n        \"\"\"The type of the annotation with all the wrappers removed, including the generic types.\"\"\"\n\n        return self.origin or self.annotation\n\n    def is_subclass_of(self, cl: type[Any] | tuple[type[Any], ...]) -> bool:\n        \"\"\"Whether the annotation is a subclass of the given type.\n\n        Where ``self.annotation`` is a union type, this method will return ``True`` when all members of the union are\n        a subtype of ``cl``, otherwise, ``False``.\n\n        Args:\n            cl: The type to check, or tuple of types. Passed as 2nd argument to ``issubclass()``.\n\n        Returns:\n            Whether the annotation is a subtype of the given type(s).\n        \"\"\"\n        if self.origin:\n            if self.origin in UnionTypes:\n                return all(t.is_subclass_of(cl) for t in self.inner_types)\n\n            return self.origin not in UnionTypes and is_class_and_subclass(self.origin, cl)\n\n        if self.annotation is AnyStr:\n            return is_class_and_subclass(str, cl) or is_class_and_subclass(bytes, cl)\n\n        return self.annotation is not Any and not self.is_type_var and is_class_and_subclass(self.annotation, cl)\n\n    def has_inner_subclass_of(self, cl: type[Any] | tuple[type[Any], ...]) -> bool:\n        \"\"\"Whether any generic args are a subclass of the given type.\n\n        Args:\n            cl: The type to check, or tuple of types. Passed as 2nd argument to ``issubclass()``.\n\n        Returns:\n            Whether any of the type's generic args are a subclass of the given type.\n        \"\"\"\n        return any(t.is_subclass_of(cl) for t in self.inner_types)\n\n    def get_type_hints(self, *, include_extras: bool = False, resolve_generics: bool = False) -> dict[str, Any]:\n        \"\"\"Get the type hints for the annotation.\n\n        Args:\n            include_extras: Flag to indicate whether to include ``Annotated[T, ...]`` or not.\n            resolve_generics: Flag to indicate whether to resolve the generic types in the type hints or not.\n\n        Returns:\n            The type hints.\n        \"\"\"\n\n        if self.origin is not None or self.is_generic:\n            if resolve_generics:\n                return get_type_hints_with_generics_resolved(self.annotation, include_extras=include_extras)\n            return get_type_hints(self.origin or self.annotation, include_extras=include_extras)\n\n        return get_type_hints(self.annotation, include_extras=include_extras)\n\n    @classmethod\n    def from_annotation(cls, annotation: Any, **kwargs: Any) -> FieldDefinition:\n        \"\"\"Initialize FieldDefinition.\n\n        Args:\n            annotation: The type annotation. This should be extracted from the return of\n                ``get_type_hints(..., include_extras=True)`` so that forward references are resolved and recursive\n                ``Annotated`` types are flattened.\n            **kwargs: Additional keyword arguments to pass to the ``FieldDefinition`` constructor.\n\n        Returns:\n            FieldDefinition\n        \"\"\"\n\n        unwrapped, metadata, wrappers = unwrap_annotation(annotation if annotation is not Empty else Any)\n        origin = get_origin(unwrapped)\n\n        args = () if origin is abc.Callable else get_args(unwrapped)\n\n        if not kwargs.get(\"kwarg_definition\"):\n            if isinstance(kwargs.get(\"default\"), (KwargDefinition, DependencyKwarg)):\n                kwargs[\"kwarg_definition\"] = kwargs.pop(\"default\")\n            elif any(isinstance(v, (KwargDefinition, DependencyKwarg)) for v in metadata):\n                kwargs[\"kwarg_definition\"] = next(  # pragma: no cover\n                    # see https://github.com/nedbat/coveragepy/issues/475\n                    v\n                    for v in metadata\n                    if isinstance(v, (KwargDefinition, DependencyKwarg))\n                )\n                metadata = tuple(v for v in metadata if not isinstance(v, (KwargDefinition, DependencyKwarg)))\n            elif (extra := kwargs.get(\"extra\", {})) and \"kwarg_definition\" in extra:\n                kwargs[\"kwarg_definition\"] = extra.pop(\"kwarg_definition\")\n            else:\n                kwargs[\"kwarg_definition\"], kwargs[\"extra\"] = cls._extract_metadata(\n                    annotation=annotation,\n                    name=kwargs.get(\"name\", \"\"),\n                    default=kwargs.get(\"default\", Empty),\n                    metadata=metadata,\n                    extra=kwargs.get(\"extra\"),\n                )\n\n        kwargs.setdefault(\"annotation\", unwrapped)\n        kwargs.setdefault(\"args\", args)\n        kwargs.setdefault(\"default\", Empty)\n        kwargs.setdefault(\"extra\", {})\n        kwargs.setdefault(\"inner_types\", tuple(FieldDefinition.from_annotation(arg) for arg in args))\n        kwargs.setdefault(\"instantiable_origin\", get_instantiable_origin(origin, unwrapped))\n        kwargs.setdefault(\"kwarg_definition\", None)\n        kwargs.setdefault(\"metadata\", metadata)\n        kwargs.setdefault(\"name\", \"\")\n        kwargs.setdefault(\"origin\", origin)\n        kwargs.setdefault(\"raw\", annotation)\n        kwargs.setdefault(\"safe_generic_origin\", get_safe_generic_origin(origin, unwrapped))\n        kwargs.setdefault(\"type_wrappers\", wrappers)\n\n        instance = FieldDefinition(**kwargs)\n        if not instance.has_default and instance.kwarg_definition:\n            return replace(instance, default=instance.kwarg_definition.default)\n\n        return instance\n\n    @classmethod\n    def from_kwarg(\n        cls,\n        annotation: Any,\n        name: str,\n        default: Any = Empty,\n        inner_types: tuple[FieldDefinition, ...] | None = None,\n        kwarg_definition: KwargDefinition | DependencyKwarg | None = None,\n        extra: dict[str, Any] | None = None,\n    ) -> FieldDefinition:\n        \"\"\"Create a new FieldDefinition instance.\n\n        Args:\n            annotation: The type of the kwarg.\n            name: Field name.\n            default: A default value.\n            inner_types: A tuple of FieldDefinition instances representing the inner types, if any.\n            kwarg_definition: Kwarg Parameter.\n            extra: A mapping of extra values.\n\n        Returns:\n            FieldDefinition instance.\n        \"\"\"\n\n        return cls.from_annotation(\n            annotation,\n            name=name,\n            default=default,\n            **{\n                k: v\n                for k, v in {\n                    \"inner_types\": inner_types,\n                    \"kwarg_definition\": kwarg_definition,\n                    \"extra\": extra,\n                }.items()\n                if v is not None\n            },\n        )\n\n    @classmethod\n    def from_parameter(cls, parameter: Parameter, fn_type_hints: dict[str, Any]) -> FieldDefinition:\n        \"\"\"Initialize ParsedSignatureParameter.\n\n        Args:\n            parameter: inspect.Parameter\n            fn_type_hints: mapping of names to types. Should be result of ``get_type_hints()``, preferably via the\n                :attr:``get_fn_type_hints() <.utils.signature_parsing.get_fn_type_hints>`` helper.\n\n        Returns:\n            ParsedSignatureParameter.\n\n        \"\"\"\n        from litestar.datastructures import ImmutableState\n\n        try:\n            annotation = fn_type_hints[parameter.name]\n        except KeyError as e:\n            raise ImproperlyConfiguredException(\n                f\"'{parameter.name}' does not have a type annotation. If it should receive any value, use 'Any'.\"\n            ) from e\n\n        if parameter.name == \"state\" and not issubclass(annotation, ImmutableState):\n            raise ImproperlyConfiguredException(\n                f\"The type annotation `{annotation}` is an invalid type for the 'state' reserved kwarg. \"\n                \"It must be typed to a subclass of `litestar.datastructures.ImmutableState` or \"\n                \"`litestar.datastructures.State`.\"\n            )\n\n        return FieldDefinition.from_kwarg(\n            annotation=annotation,\n            name=parameter.name,\n            default=Empty if parameter.default is Signature.empty else parameter.default,\n        )\n\n    def match_predicate_recursively(self, predicate: Callable[[FieldDefinition], bool]) -> bool:\n        \"\"\"Recursively test the passed in predicate against the field and any of its inner fields.\n\n        Args:\n            predicate: A callable that receives a field definition instance as an arg and returns a boolean.\n\n        Returns:\n            A boolean.\n        \"\"\"\n        return predicate(self) or any(t.match_predicate_recursively(predicate) for t in self.inner_types)"
    },
    {
        "task_id": "pyvista__pyvista-4853_DAtaoBJeCT",
        "class_name": "DAtaoBJeCT",
        "file": "pyvista__pyvista-4853/pyvista/core/dataobject.py",
        "sketchy_description": "The 'DAtaoBJeCT' class is a part of the 'pyvista.core.dataobject' module. It is an abstract class as indicated by the '@abstract_class' decorator. The class has an '__init__' method that takes variable arguments and keyword arguments but does not return anything. This method is used to initialize the data object.\n\nThe class has a method 'sHaLLOw_CopY' which takes an argument 'to_copy' of type '_vtk.vtkDataObject' and returns a shallow copy of the given mesh to this mesh. \n\nThe 'dEEP_COpy' method takes an argument 'to_copy' of type '_vtk.vtkDataObject' and returns a deep copy of the given data object to this data object.\n\nThe '_fROM_FiLe' method takes a 'filename' argument of type 'Union[str, Path]' and other keyword arguments. This method is used to read data objects from a file.\n\nThe '_pOst_fiLE_LoAd_ProCESSINg' method is used to execute after loading a dataset from a file. This method can be optionally overridden by subclasses.\n\nThe 'save' method takes a 'filename' argument of type 'str', a 'binary' argument of type 'bool' with a default value of 'True', and a 'texture' argument. This method is used to save this vtk object to a file.\n\nThe '_stoRe_METAdATa' method is used to store metadata as field data.\n\nThe '_rEStoRE_mETadaTA' method is used to restore PyVista metadata from field data.\n\nThe 'geT_dATa_rANGE' method is an abstract method that gets the non-NaN min and max of a named array.\n\nThe '_gET_aTtRS' method is used to return the representation methods.\n\nThe 'head' method takes a 'display' argument of type 'bool' with a default value of 'True', and a 'html' argument. This method returns the header stats of this dataset.\n\nThe '_rEPr_HtML_' method returns a pretty representation for Jupyter notebooks.\n\nThe 'CoPY_metA_fROM' method takes variable arguments and keyword arguments. This method is used to copy pyvista meta data onto this object from another object.\n\nThe 'copy' method takes a 'deep' argument of type 'bool' with a default value of 'True'. This method returns a copy of the object.\n\nThe 'aDd_FIELd_daTA' method takes an 'array' argument of type 'np.ndarray', a 'name' argument of type 'str', and a 'deep' argument of type 'bool' with a default value of 'True'. This method is used to add field data.\n\nThe 'field_data' property returns FieldData as DataSetAttributes.\n\nThe 'cLeAR_FIElD_Data' method is used to remove all field data.\n\nThe 'memory_address' property returns the address of the underlying VTK C++ object.\n\nThe 'actual_memory_size' property returns the actual size of the dataset object.\n\nThe 'cOPy_sTRUcTUre' method takes a 'dataset' argument of type '_vtk.vtkDataSet'. This method is used to copy the structure (geometry and topology) of the input dataset object.\n\nThe 'cOPy_ATtrIButES' method takes a 'dataset' argument of type '_vtk.vtkDataSet'. This method is used to copy the data attributes of the input dataset object.\n\nThe '__getattr__' method takes an 'item' argument of type 'str'. This method is used to get attribute from base class if not found.\n\nThe '__eq__' method takes an 'other' argument. This method is used to test equivalency between data objects.\n\nThe '__getstate__' method is used to support pickle by serializing the VTK object data to something which can be pickled natively.\n\nThe '__setstate__' method takes a 'state' argument. This method is used to support unpickle.\n\nThe class has a class variable '_WRITERS' of type 'Dict[str, Union[Type[_vtk.vtkXMLWriter], Type[_vtk.vtkDataWriter]]]'. The class also has instance variables '_association_bitarray_names' and '_association_complex_names'.",
        "detailed_description": "The 'DAtaoBJeCT' class is decorated with '@abstract_class' and is a subclass of '_vtk.vtkDataObject'. The class has a class variable '_WRITERS' which is a dictionary with keys as strings and values as either '_vtk.vtkXMLWriter' or '_vtk.vtkDataWriter' types. The class has an '__init__' method that takes any number of arguments and keyword arguments. This method initializes the superclass and sets the instance variables '_association_bitarray_names' and '_association_complex_names' to instances of 'collections.defaultdict' with 'set' as the default factory.\n\nThe '__getattr__' method takes a string 'item' as an argument and returns any type. This method calls the superclass '__getattribute__' method with 'item' as the argument.\n\nThe 'sHaLLOw_CopY' method takes a '_vtk.vtkDataObject' 'to_copy' as an argument and returns a '_vtk.vtkDataObject'. This method calls the 'ShallowCopy' method with 'to_copy' as the argument.\n\nThe 'dEEP_COpy' method takes a '_vtk.vtkDataObject' 'to_copy' as an argument and returns a '_vtk.vtkDataObject'. This method calls the 'DeepCopy' method with 'to_copy' as the argument.\n\nThe '_fROM_FiLe' method takes a 'filename' of type 'Union[str, Path]' and any number of keyword arguments. This method reads data objects from the file with the given 'filename' and keyword arguments using the 'read' function. If the instance is not of the same type as the read data, it raises a 'ValueError'. The method then calls the 'sHaLLOw_CopY' method with the read data as the argument and calls the '_pOst_fiLE_LoAd_ProCESSINg' method.\n\nThe '_pOst_fiLE_LoAd_ProCESSINg' method is to be optionally overridden by subclasses and does not take any arguments or return anything.\n\nThe 'save' method takes a 'filename' of type 'str', a 'binary' of type 'bool' with a default value of 'True', and a 'texture' with a default value of 'None'. This method saves the vtk object to a file with the given 'filename'. If the '_WRITERS' instance variable is 'None', it raises a 'NotImplementedError'. The method then sets the 'vtk_writer' and 'use_binary' arguments of the 'SET_vTkwrItER_ModE' function to 'writer' and 'binary' respectively. The method sets the 'FileName' and 'InputData' of 'writer' to the string representation of 'file_path' and the instance respectively. If 'file_ext' is '.ply' and 'texture' is not 'None', the method sets the 'ArrayName' of 'writer' to 'texture' if 'texture' is a string, or '_color_array' if 'texture' is an 'np.ndarray'. If the last dimension of the 'array_name' attribute of the instance is 4, the method sets 'EnableAlpha' of 'writer' to 'True'. The method then calls the 'Write' method of 'writer'.\n\nThe '_stoRe_METAdATa' method does not take any arguments or return anything. This method sets the 'fdata' local variable to the 'field_data' attribute of the instance. For each 'assoc_name' in 'bitarray' and 'complex', and for each 'assoc_type' in 'POINT' and 'CELL', the method sets the 'assoc_data' local variable to the '_association_{assoc_name}_names' attribute of the instance, and the 'array_names' local variable to the 'assoc_type' key of 'assoc_data'. If 'array_names' is not 'None', the method sets the 'key' local variable to '_PYVISTA_{assoc_name}_{assoc_type}_'.upper() and sets the 'key' key of 'fdata' to the list of 'array_names'.\n\nThe '_rEStoRE_mETadaTA' method does not take any arguments or return anything. This method sets the 'fdata' local variable to the 'field_data' attribute of the instance. For each 'assoc_name' in 'bitarray' and 'complex', and for each 'assoc_type' in 'POINT' and 'CELL', the method sets the 'key' local variable to '_PYVISTA_{assoc_name}_{assoc_type}_'.upper(). If 'key' is in 'fdata', the method sets the 'assoc_data' local variable to the '_association_{assoc_name}_names' attribute of the instance, sets the 'assoc_type' key of 'assoc_data' to the set of the 'key' key of 'fdata', and deletes the 'key' key of 'fdata'.\n\nThe 'geT_dATa_rANGE' method is an abstract method that does not take any arguments or return anything. This method raises a 'NotImplementedError' with a message indicating that the mesh type does not have a 'get_data_range' method.\n\nThe '_gET_aTtRS' method does not take any arguments or return anything. This method raises a 'NotImplementedError' indicating that it is called only by the inherited class.\n\nThe 'head' method takes a 'display' of type 'bool' with a default value of 'True', and an 'html' with a default value of 'None'. This method returns a string. This method returns the header stats of the dataset. If 'html' is 'True', the method generates an HTML-formatted string and displays it using the 'HTML' and '_display' functions from 'IPython.display' if 'display' is 'True'. If 'html' is not 'True', the method generates a console-friendly string.\n\nThe '_rEPr_HtML_' method does not take any arguments or return anything. This method raises a 'NotImplementedError' indicating that it is called only by the inherited class.\n\nThe 'CoPY_metA_fROM' method takes any number of arguments and keyword arguments. This method does not return anything and is intended to be overridden by subclasses.\n\nThe 'copy' method takes a 'deep' of type 'bool' with a default value of 'True'. This method returns a 'pyvista.DataSet'. This method returns a copy of the object. If 'deep' is 'True', the method calls the 'dEEP_COpy' method with the instance as the argument. If 'deep' is not 'True', the method calls the 'sHaLLOw_CopY' method with the instance as the argument. The method then calls the 'CoPY_metA_fROM' method with the instance and 'deep' as the arguments.\n\nThe '__eq__' method takes an 'other' as an argument and returns a 'bool'. This method tests equivalency between data objects. If 'other' is not of the same type as the instance, the method returns 'False'. If 'other' is the instance, the method returns 'True'. For each 'attr' in 'equal_attrs', if 'attr' is an attribute of the instance, the method checks if the 'attr' attribute of the instance and 'other' are not equal using 'np.array_equal'. If they are not equal, the method returns 'False'. For each 'attr' in 'attrs', if 'attr' is an attribute of the instance, the method checks if the 'attr' attribute of the instance and 'other' are not equal. If they are not equal, the method returns 'False'. If none of the checks fail, the method returns 'True'.\n\nThe 'aDd_FIELd_daTA' method takes an 'array' of type 'np.ndarray', a 'name' of type 'str', and a 'deep' of type 'bool' with a default value of 'True'. This method does not return anything. This method adds field data. If the instance does not have a 'field_data' attribute, the method raises a 'NotImplementedError'. The method then calls the 'set_array' method of the 'field_data' attribute of the instance with 'array', 'name', and 'dEEP_COpy=deep' as the arguments.\n\nThe 'field_data' property returns a 'dAtasetaTtRIbUTes'. This property returns the 'FieldData' as 'DataSetAttributes'. If the instance does not have a 'field_data' attribute, the property raises a 'NotImplementedError'.\n\nThe 'cLeAR_FIElD_Data' method does not take any arguments or return anything. This method removes all field data. If the instance does not have a 'field_data' attribute, the method raises a 'NotImplementedError'. The method then calls the 'clear' method of the 'field_data' attribute of the instance.\n\nThe 'memory_address' property returns a 'str'. This property returns the memory address of the underlying VTK C++ object. The property gets the 'AddressAsString' of the 'GetInformation' method of the instance.\n\nThe 'actual_memory_size' property returns an 'int'. This property returns the actual size of the dataset object in kibibytes (1024 bytes). The property calls the 'GetActualMemorySize' method of the instance.\n\nThe 'cOPy_sTRUcTUre' method takes a 'dataset' of type '_vtk.vtkDataSet' as an argument and does not return anything. This method copies the structure (geometry and topology) of the input dataset object. The method calls the 'CopyStructure' method with 'dataset' as the argument.\n\nThe 'cOPy_ATtrIButES' method takes a 'dataset' of type '_vtk.vtkDataSet' as an argument and does not return anything. This method copies the data attributes of the input dataset object. The method calls the 'CopyAttributes' method with 'dataset' as the argument.\n\nThe '__getstate__' method does not take any arguments. This method supports pickle by serializing the VTK object data to something which can be pickled natively. The method returns the state of the instance. The method sets the 'state' local variable to a copy of the dictionary of the instance. If 'pyvista.PICKLE_FORMAT' is 'xml', the method sets the 'writer' local variable to an instance of 'vtkXMLDataSetWriter', 'vtkXMLStructuredGridWriter', 'vtkXMLRectilinearGridWriter', 'vtkXMLUnstructuredGridWriter', 'vtkXMLPolyDataWriter', or 'vtkXMLTableWriter' depending on the type of the instance. If 'pyvista.PICKLE_FORMAT' is not 'xml', the method sets the 'writer' local variable to an instance of 'vtkDataSetWriter'. The method sets the 'InputDataObject' of 'writer' to the instance, sets 'WriteToOutputString' of 'writer' to 'True', and calls the 'Write' method of 'writer'. The method then sets the 'vtk_serialized' key of 'state' to the 'OutputString' of 'writer' if 'pyvista.PICKLE_FORMAT' is 'xml', or the 'OutputStdString' of 'writer' if 'pyvista.PICKLE_FORMAT' is not 'xml'. The method also sets the 'PICKLE_FORMAT' key of 'state' to 'pyvista.PICKLE_FORMAT'.\n\nThe '__setstate__' method takes a 'state' as an argument and does not return anything. This method supports unpickle. The method sets the 'vtk_serialized' local variable to the 'vtk_serialized' key of 'state', and the 'pickle_format' local variable to the 'PICKLE_FORMAT' key of 'state' with a default value of 'legacy'. The method then updates the dictionary of the instance with 'state'. If 'pickle_format' is 'xml', the method sets the 'reader' local variable to an instance of 'vtkXMLImageDataReader', 'vtkXMLStructuredGridReader', 'vtkXMLRectilinearGridReader', 'vtkXMLUnstructuredGridReader', 'vtkXMLPolyDataReader', or 'vtkXMLTableReader' depending on the type of the instance. If 'pickle_format' is not 'xml', the method sets the 'reader' local variable to an instance of 'vtkDataSetReader'. The method sets 'ReadFromInputStringOn' of 'reader' to 'True', sets 'InputString' of 'reader' to 'vtk_serialized' if 'vtk_serialized' is a string, or sets 'BinaryInputString' of 'reader' to 'vtk_serialized' and its length if 'vtk_serialized' is bytes. The method then calls the 'Update' method of 'reader', sets the 'mesh' local variable to the output of 'reader' wrapped using the 'wRAP' function, calls the 'cOPy_sTRUcTUre' method with 'mesh' as the argument, and calls the 'cOPy_ATtrIButES' method with 'mesh' as the argument.\n\nThe 'aDd_FIELd_daTA' method takes an 'array' of type 'np.ndarray', a 'name' of type 'str', and a 'deep' of type 'bool' with a default value of 'True'. This method does not return anything. This method adds field data. If the instance does not have a 'field_data' attribute, the method raises a 'NotImplementedError'. The method then calls the 'set_array' method of the 'field_data' attribute of the instance with 'array', 'name', and 'dEEP_COpy=deep' as the arguments.\n\nThe 'field_data' property returns a 'dAtasetaTtRIbUTes'. This property returns the 'FieldData' as 'DataSetAttributes'. If the instance does not have a 'field_data' attribute, the property raises a 'NotImplementedError'.\n\nThe 'cLeAR_FIElD_Data' method does not take any arguments or return anything. This method removes all field data. If the instance does not have a 'field_data' attribute, the method raises a 'NotImplementedError'. The method then calls the 'clear' method of the 'field_data' attribute of the instance.\n\nThe 'memory_address' property returns a 'str'. This property returns the memory address of the underlying VTK C++ object. The property gets the 'AddressAsString' of the 'GetInformation' method of the instance.\n\nThe 'actual_memory_size' property returns an 'int'. This property returns the actual size of the dataset object in kibibytes (1024 bytes). The property calls the 'GetActualMemorySize' method of the instance.\n\nThe 'cOPy_sTRUcTUre' method takes a 'dataset' of type '_vtk.vtkDataSet' as an argument and does not return anything. This method copies the structure (geometry and topology) of the input dataset object. The method calls the 'CopyStructure' method with 'dataset' as the argument.\n\nThe 'cOPy_ATtrIButES' method takes a 'dataset' of type '_vtk.vtkDataSet' as an argument and does not return anything. This method copies the data attributes of the input dataset object. The method calls the 'CopyAttributes' method with 'dataset' as the argument.\n\nThe '__getstate__' method does not take any arguments. This method supports pickle by serializing the VTK object data to something which can be pickled natively. The method returns the state of the instance. The method sets the 'state' local variable to a copy of the dictionary of the instance. If 'pyvista.PICKLE_FORMAT' is 'xml', the method sets the 'writer' local variable to an instance of 'vtkXMLDataSetWriter', 'vtkXMLStructuredGridWriter', 'vtkXMLRectilinearGridReader', 'vtkXMLUnstructuredGridWriter', 'vtkXMLPolyDataWriter', or 'vtkXMLTableWriter' depending on the type of the instance. If 'pyvista.PICKLE_FORMAT' is not 'xml', the method sets the 'writer' local variable to an instance of 'vtkDataSetWriter'. The method sets the 'InputDataObject' of 'writer' to the instance, sets 'WriteToOutputString' of 'writer' to 'True', and calls the 'Write' method of 'writer'. The method then sets the 'vtk_serialized' key of 'state' to the 'OutputString' of 'writer' if 'pyvista.PICKLE_FORMAT' is 'xml', or the 'OutputStdString' of 'writer' if 'pyvista.PICKLE_FORMAT' is not 'xml'. The method also sets the 'PICKLE_FORMAT' key of 'state",
        "repo_metadata": {
            "commit_id": "0caa7254d5f42c363ab164a80ec4ec36d79f2df2",
            "issue_id": "pyvista__pyvista-4853",
            "setup_details": {
                "repo": "pyvista/pyvista",
                "instance_id": "pyvista__pyvista-4853",
                "base_commit": "4a44e4c63c6b8d6a3f1db0aa193f4ccb631ed698",
                "version": "0.43",
                "environment_setup_commit": "17ed0eb49a942b297e61a83a1c8ba828c5922b99"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/core/test_composite.py::test_multi_block_save_lines",
                "tests/core/test_polydata.py::test_append",
                "tests/core/test_reader.py::test_xmlmultiblockreader",
                "tests/core/test_composite.py::test_multi_io_erros",
                "tests/core/test_reader.py::test_xmlstructuredgridreader",
                "tests/core/test_cells.py::test_cell_no_field_data",
                "tests/core/test_datasetattributes.py::test_eq",
                "tests/core/test_reader.py::test_xmlimagedatareader",
                "tests/core/test_dataobject.py::test_uniform_eq",
                "tests/core/test_reader.py::test_xmlrectilineargridreader",
                "tests/core/test_dataset.py::test_bad_instantiation",
                "tests/core/test_reader.py::test_xmlunstructuredgridreader",
                "tests/core/test_composite.py::test_multi_block_io[True-.vtm-True]",
                "tests/core/test_composite.py::test_multi_block_io[True-.vtm-False]",
                "tests/core/test_composite.py::test_multi_block_io[True-.vtmb-True]",
                "tests/core/test_composite.py::test_multi_block_io[True-.vtmb-False]",
                "tests/core/test_composite.py::test_multi_block_io[False-.vtm-True]",
                "tests/core/test_composite.py::test_multi_block_io[False-.vtm-False]",
                "tests/core/test_composite.py::test_multi_block_io[False-.vtmb-True]",
                "tests/core/test_composite.py::test_multi_block_io[False-.vtmb-False]"
            ]
        },
        "ground_truth_class_body": "@abstract_class\nclass DAtaoBJeCT:\n    \"\"\"Methods common to all wrapped data objects.\n\n    Parameters\n    ----------\n    *args :\n        Any extra args are passed as option to all wrapped data objects.\n\n    **kwargs :\n        Any extra keyword args are passed as option to all wrapped data objects.\n\n    \"\"\"\n\n    _WRITERS: Dict[str, Union[Type[_vtk.vtkXMLWriter], Type[_vtk.vtkDataWriter]]] = {}\n\n    def __init__(self, *args, **kwargs) -> None:\n        \"\"\"Initialize the data object.\"\"\"\n        super().__init__()\n        # Remember which arrays come from numpy.bool arrays, because there is no direct\n        # conversion from bool to vtkBitArray, such arrays are stored as vtkCharArray.\n        self._association_bitarray_names: DefaultDict = collections.defaultdict(set)\n\n        # view these arrays as complex128 as VTK doesn't support complex types\n        self._association_complex_names: DefaultDict = collections.defaultdict(set)\n\n    def __getattr__(self, item: str) -> Any:\n        \"\"\"Get attribute from base class if not found.\"\"\"\n        return super().__getattribute__(item)\n\n    def sHaLLOw_CopY(self, to_copy: _vtk.vtkDataObject) -> _vtk.vtkDataObject:\n        \"\"\"Shallow copy the given mesh to this mesh.\n\n        Parameters\n        ----------\n        to_copy : pyvista.DataObject or vtk.vtkDataObject\n            Data object to perform a shallow copy from.\n\n        \"\"\"\n        self.ShallowCopy(to_copy)\n\n    def dEEP_COpy(self, to_copy: _vtk.vtkDataObject) -> _vtk.vtkDataObject:\n        \"\"\"Overwrite this data object with another data object as a deep copy.\n\n        Parameters\n        ----------\n        to_copy : pyvista.DataObject or vtk.vtkDataObject\n            Data object to perform a deep copy from.\n\n        \"\"\"\n        self.DeepCopy(to_copy)\n\n    def _fROM_FiLe(self, filename: Union[str, Path], **kwargs):\n        \"\"\"Read data objects from file.\"\"\"\n        data = read(filename, **kwargs)\n        if not isinstance(self, type(data)):\n            raise ValueError(\n                f'Reading file returned data of `{type(data).__name__}`, '\n                f'but `{type(self).__name__}` was expected.'\n            )\n        self.sHaLLOw_CopY(data)\n        self._pOst_fiLE_LoAd_ProCESSINg()\n\n    def _pOst_fiLE_LoAd_ProCESSINg(self):\n        \"\"\"Execute after loading a dataset from file, to be optionally overridden by subclasses.\"\"\"\n        pass\n\n    def save(self, filename: str, binary=True, texture=None):\n        \"\"\"Save this vtk object to file.\n\n        Parameters\n        ----------\n        filename : str, pathlib.Path\n            Filename of output file. Writer type is inferred from\n            the extension of the filename.\n\n        binary : bool, default: True\n            If ``True``, write as binary.  Otherwise, write as ASCII.\n\n        texture : str, np.ndarray, optional\n            Write a single texture array to file when using a PLY\n            file.  Texture array must be a 3 or 4 component array with\n            the datatype ``np.uint8``.  Array may be a cell array or a\n            point array, and may also be a string if the array already\n            exists in the PolyData.\n\n            If a string is provided, the texture array will be saved\n            to disk as that name.  If an array is provided, the\n            texture array will be saved as ``'RGBA'``\n\n            .. note::\n               This feature is only available when saving PLY files.\n\n        Notes\n        -----\n        Binary files write much faster than ASCII and have a smaller\n        file size.\n\n        \"\"\"\n        if self._WRITERS is None:\n            raise NotImplementedError(\n                f'{self.__class__.__name__} writers are not specified,'\n                ' this should be a dict of (file extension: vtkWriter type)'\n            )\n\n        file_path = Path(filename)\n        file_path = file_path.expanduser()\n        file_path = file_path.resolve()\n        file_ext = file_path.suffix\n        if file_ext not in self._WRITERS:\n            raise ValueError(\n                'Invalid file extension for this data type.'\n                f' Must be one of: {self._WRITERS.keys()}'\n            )\n\n        # store complex and bitarray types as field data\n        self._stoRe_METAdATa()\n\n        writer = self._WRITERS[file_ext]()\n        SET_vTkwrItER_ModE(vtk_writer=writer, use_binary=binary)\n        writer.SetFileName(str(file_path))\n        writer.SetInputData(self)\n        if file_ext == '.ply' and texture is not None:\n            if isinstance(texture, str):\n                writer.SetArrayName(texture)\n                array_name = texture\n            elif isinstance(texture, np.ndarray):\n                array_name = '_color_array'\n                self[array_name] = texture\n                writer.SetArrayName(array_name)\n\n            # enable alpha channel if applicable\n            if self[array_name].shape[-1] == 4:  # type: ignore\n                writer.SetEnableAlpha(True)\n        writer.Write()\n\n    def _stoRe_METAdATa(self):\n        \"\"\"Store metadata as field data.\"\"\"\n        fdata = self.field_data\n        for assoc_name in ('bitarray', 'complex'):\n            for assoc_type in ('POINT', 'CELL'):\n                assoc_data = getattr(self, f'_association_{assoc_name}_names')\n                array_names = assoc_data.get(assoc_type)\n                if array_names:\n                    key = f'_PYVISTA_{assoc_name}_{assoc_type}_'.upper()\n                    fdata[key] = list(array_names)\n\n    def _rEStoRE_mETadaTA(self):\n        \"\"\"Restore PyVista metadata from field data.\n\n        Metadata is stored using ``_store_metadata`` and contains entries in\n        the format of f'_PYVISTA_{assoc_name}_{assoc_type}_'. These entries are\n        removed when calling this method.\n\n        \"\"\"\n        fdata = self.field_data\n        for assoc_name in ('bitarray', 'complex'):\n            for assoc_type in ('POINT', 'CELL'):\n                key = f'_PYVISTA_{assoc_name}_{assoc_type}_'.upper()\n                if key in fdata:\n                    assoc_data = getattr(self, f'_association_{assoc_name}_names')\n                    assoc_data[assoc_type] = set(fdata[key])\n                    del fdata[key]\n\n    @abstractmethod\n    def geT_dATa_rANGE(self):  # pragma: no cover\n        \"\"\"Get the non-NaN min and max of a named array.\"\"\"\n        raise NotImplementedError(\n            f'{type(self)} mesh type does not have a `get_data_range` method.'\n        )\n\n    def _gET_aTtRS(self):  # pragma: no cover\n        \"\"\"Return the representation methods (internal helper).\"\"\"\n        raise NotImplementedError('Called only by the inherited class')\n\n    def head(self, display=True, html=None):\n        \"\"\"Return the header stats of this dataset.\n\n        If in IPython, this will be formatted to HTML. Otherwise\n        returns a console friendly string.\n\n        Parameters\n        ----------\n        display : bool, default: True\n            Display this header in iPython.\n\n        html : bool, optional\n            Generate the output as HTML.\n\n        Returns\n        -------\n        str\n            Header statistics.\n\n        \"\"\"\n        # Generate the output\n        if html:\n            fmt = \"\"\n            # HTML version\n            fmt += \"\\n\"\n            fmt += \"<table style='width: 100%;'>\\n\"\n            fmt += f\"<tr><th>{type(self).__name__}</th><th>Information</th></tr>\\n\"\n            row = \"<tr><td>{}</td><td>{}</td></tr>\\n\"\n            # now make a call on the object to get its attributes as a list of len 2 tuples\n            for attr in self._gET_aTtRS():\n                try:\n                    fmt += row.format(attr[0], attr[2].format(*attr[1]))\n                except:\n                    fmt += row.format(attr[0], attr[2].format(attr[1]))\n            if hasattr(self, 'n_arrays'):\n                fmt += row.format('N Arrays', self.n_arrays)\n            fmt += \"</table>\\n\"\n            fmt += \"\\n\"\n            if display:\n                from IPython.display import HTML, display as _display\n\n                _display(HTML(fmt))\n                return\n            return fmt\n        # Otherwise return a string that is Python console friendly\n        fmt = f\"{type(self).__name__} ({hex(id(self))})\\n\"\n        # now make a call on the object to get its attributes as a list of len 2 tuples\n        # get longest row header\n        max_len = max(len(attr[0]) for attr in self._gET_aTtRS()) + 4\n\n        # now make a call on the object to get its attributes as a list of len\n        # 2 tuples\n        row = \"  {:%ds}{}\\n\" % max_len\n        for attr in self._gET_aTtRS():\n            try:\n                fmt += row.format(attr[0] + ':', attr[2].format(*attr[1]))\n            except:\n                fmt += row.format(attr[0] + ':', attr[2].format(attr[1]))\n        if hasattr(self, 'n_arrays'):\n            fmt += row.format('N Arrays:', self.n_arrays)\n        return fmt.strip()\n\n    def _rEPr_HtML_(self):  # pragma: no cover\n        \"\"\"Return a pretty representation for Jupyter notebooks.\n\n        This includes header details and information about all arrays.\n\n        \"\"\"\n        raise NotImplementedError('Called only by the inherited class')\n\n    def CoPY_metA_fROM(self, *args, **kwargs):  # pragma: no cover\n        \"\"\"Copy pyvista meta data onto this object from another object.\n\n        Intended to be overridden by subclasses.\n\n        Parameters\n        ----------\n        *args : tuple\n            Positional arguments.\n\n        **kwargs : dict, optional\n            Keyword arguments.\n\n        \"\"\"\n        pass  # called only by the inherited class\n\n    def copy(self, deep=True):\n        \"\"\"Return a copy of the object.\n\n        Parameters\n        ----------\n        deep : bool, default: True\n            When ``True`` makes a full copy of the object.  When\n            ``False``, performs a shallow copy where the points, cell,\n            and data arrays are references to the original object.\n\n        Returns\n        -------\n        pyvista.DataSet\n            Deep or shallow copy of the input.  Type is identical to\n            the input.\n\n        Examples\n        --------\n        Create and make a deep copy of a PolyData object.\n\n        >>> import pyvista\n        >>> mesh_a = pyvista.Sphere()\n        >>> mesh_b = mesh_a.copy()\n        >>> mesh_a == mesh_b\n        True\n\n        \"\"\"\n        thistype = type(self)\n        newobject = thistype()\n\n        if deep:\n            newobject.dEEP_COpy(self)\n        else:\n            newobject.sHaLLOw_CopY(self)\n        newobject.CoPY_metA_fROM(self, deep)\n        return newobject\n\n    def __eq__(self, other):\n        \"\"\"Test equivalency between data objects.\"\"\"\n        if not isinstance(self, type(other)):\n            return False\n\n        if self is other:\n            return True\n\n        # these attrs use numpy.array_equal\n        equal_attrs = [\n            'verts',  # DataObject\n            'points',  # DataObject\n            'lines',  # DataObject\n            'faces',  # DataObject\n            'cells',  # UnstructuredGrid\n            'celltypes',\n        ]  # UnstructuredGrid\n        for attr in equal_attrs:\n            if hasattr(self, attr):\n                if not np.array_equal(getattr(self, attr), getattr(other, attr)):\n                    return False\n\n        # these attrs can be directly compared\n        attrs = ['field_data', 'point_data', 'cell_data']\n        for attr in attrs:\n            if hasattr(self, attr):\n                if getattr(self, attr) != getattr(other, attr):\n                    return False\n\n        return True\n\n    def aDd_FIELd_daTA(self, array: np.ndarray, name: str, deep=True):\n        \"\"\"Add field data.\n\n        Use field data when size of the data you wish to associate\n        with the dataset does not match the number of points or cells\n        of the dataset.\n\n        Parameters\n        ----------\n        array : sequence\n            Array of data to add to the dataset as a field array.\n\n        name : str\n            Name to assign the field array.\n\n        deep : bool, default: True\n            Perform a deep copy of the data when adding it to the\n            dataset.\n\n        Examples\n        --------\n        Add field data to a PolyData dataset.\n\n        >>> import pyvista\n        >>> import numpy as np\n        >>> mesh = pyvista.Sphere()\n        >>> mesh.aDd_FIELd_daTA(np.arange(10), 'my-field-data')\n        >>> mesh['my-field-data']\n        pyvista_ndarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n        Add field data to a ImageData dataset.\n\n        >>> mesh = pyvista.imagEDATA(dimensions=(2, 2, 1))\n        >>> mesh.aDd_FIELd_daTA(\n        ...     ['I could', 'write', 'notes', 'here'], 'my-field-data'\n        ... )\n        >>> mesh['my-field-data']\n        pyvista_ndarray(['I could', 'write', 'notes', 'here'], dtype='<U7')\n\n        Add field data to a MultiBlock dataset.\n\n        >>> blocks = pyvista.MULTiBLOcK()\n        >>> blocks.append(pyvista.Sphere())\n        >>> blocks[\"cube\"] = pyvista.Cube(center=(0, 0, -1))\n        >>> blocks.aDd_FIELd_daTA([1, 2, 3], 'my-field-data')\n        >>> blocks.field_data['my-field-data']\n        pyvista_ndarray([1, 2, 3])\n\n        \"\"\"\n        if not hasattr(self, 'field_data'):\n            raise NotImplementedError(f'`{type(self)}` does not support field data')\n\n        self.field_data.set_array(array, name, dEEP_COpy=deep)\n\n    @property\n    def field_data(self) -> dAtasetaTtRIbUTes:  # numpydoc ignore=RT01\n        \"\"\"Return FieldData as DataSetAttributes.\n\n        Use field data when size of the data you wish to associate\n        with the dataset does not match the number of points or cells\n        of the dataset.\n\n        Returns\n        -------\n        DataSetAttributes\n            FieldData as DataSetAttributes.\n\n        Examples\n        --------\n        Add field data to a PolyData dataset and then return it.\n\n        >>> import pyvista\n        >>> import numpy as np\n        >>> mesh = pyvista.Sphere()\n        >>> mesh.field_data['my-field-data'] = np.arange(10)\n        >>> mesh.field_data['my-field-data']\n        pyvista_ndarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n        \"\"\"\n        return dAtasetaTtRIbUTes(\n            self.GetFieldData(), dataset=self, association=FIElDasSoCiaTIOn.NONE\n        )\n\n    def cLeAR_FIElD_Data(self):\n        \"\"\"Remove all field data.\n\n        Examples\n        --------\n        Add field data to a PolyData dataset and then remove it.\n\n        >>> import pyvista\n        >>> mesh = pyvista.Sphere()\n        >>> mesh.field_data['my-field-data'] = range(10)\n        >>> len(mesh.field_data)\n        1\n        >>> mesh.cLeAR_FIElD_Data()\n        >>> len(mesh.field_data)\n        0\n\n        \"\"\"\n        if not hasattr(self, 'field_data'):\n            raise NotImplementedError(f'`{type(self)}` does not support field data')\n\n        self.field_data.clear()\n\n    @property\n    def memory_address(self) -> str:  # numpydoc ignore=RT01\n        \"\"\"Get address of the underlying VTK C++ object.\n\n        Returns\n        -------\n        str\n            Memory address formatted as ``'Addr=%p'``.\n\n        Examples\n        --------\n        >>> import pyvista\n        >>> mesh = pyvista.Sphere()\n        >>> mesh.memory_address\n        'Addr=...'\n\n        \"\"\"\n        return self.GetInformation().GetAddressAsString(\"\")\n\n    @property\n    def actual_memory_size(self) -> int:  # numpydoc ignore=RT01\n        \"\"\"Return the actual size of the dataset object.\n\n        Returns\n        -------\n        int\n            The actual size of the dataset object in kibibytes (1024\n            bytes).\n\n        Examples\n        --------\n        >>> from pyvista import examples\n        >>> mesh = examples.load_airplane()\n        >>> mesh.actual_memory_size  # doctest:+SKIP\n        93\n\n        \"\"\"\n        return self.GetActualMemorySize()\n\n    def cOPy_sTRUcTUre(self, dataset: _vtk.vtkDataSet):\n        \"\"\"Copy the structure (geometry and topology) of the input dataset object.\n\n        Parameters\n        ----------\n        dataset : vtk.vtkDataSet\n            Dataset to copy the geometry and topology from.\n\n        Examples\n        --------\n        >>> import pyvista as pv\n        >>> source = pv.imagEDATA(dimensions=(10, 10, 5))\n        >>> target = pv.imagEDATA()\n        >>> target.cOPy_sTRUcTUre(source)\n        >>> target.plot(show_edges=True)\n\n        \"\"\"\n        self.CopyStructure(dataset)\n\n    def cOPy_ATtrIButES(self, dataset: _vtk.vtkDataSet):\n        \"\"\"Copy the data attributes of the input dataset object.\n\n        Parameters\n        ----------\n        dataset : pyvista.DataSet\n            Dataset to copy the data attributes from.\n\n        Examples\n        --------\n        >>> import pyvista as pv\n        >>> source = pv.imagEDATA(dimensions=(10, 10, 5))\n        >>> source = source.COmPuTE_cELL_sizEs()\n        >>> target = pv.imagEDATA(dimensions=(10, 10, 5))\n        >>> target.cOPy_ATtrIButES(source)\n        >>> target.plot(scalars='Volume', show_edges=True)\n\n        \"\"\"\n        self.CopyAttributes(dataset)\n\n    def __getstate__(self):\n        \"\"\"Support pickle by serializing the VTK object data to something which can be pickled natively.\n\n        The format of the serialized VTK object data depends on `pyvista.PICKLE_FORMAT` (case-insensitive).\n        - If `pyvista.PICKLE_FORMAT == 'xml'`, the data is serialized as an XML-formatted string.\n        - If `pyvista.PICKLE_FORMAT == 'legacy'`, the data is serialized to bytes in VTK's binary format.\n        \"\"\"\n        state = self.__dict__.copy()\n\n        if pyvista.PICKLE_FORMAT.lower() == 'xml':\n            # the generic VTK XML writer `vtkXMLDataSetWriter` currently has a bug where it does not pass all\n            # settings down to the sub-writers. Until this is fixed, use the dataset-specific writers\n            # https://gitlab.kitware.com/vtk/vtk/-/issues/18661\n            writers = {\n                _vtk.vtkImageData: _vtk.vtkXMLImageDataWriter,\n                _vtk.vtkStructuredGrid: _vtk.vtkXMLStructuredGridWriter,\n                _vtk.vtkRectilinearGrid: _vtk.vtkXMLRectilinearGridWriter,\n                _vtk.vtkUnstructuredGrid: _vtk.vtkXMLUnstructuredGridWriter,\n                _vtk.vtkPolyData: _vtk.vtkXMLPolyDataWriter,\n                _vtk.vtkTable: _vtk.vtkXMLTableWriter,\n            }\n\n            for parent_type, writer_type in writers.items():\n                if isinstance(self, parent_type):\n                    writer = writer_type()\n                    break\n            else:\n                raise TypeError(f'Cannot pickle dataset of type {self.GetDataObjectType()}')\n\n            writer.SetInputDataObject(self)\n            writer.SetWriteToOutputString(True)\n            writer.SetDataModeToBinary()\n            writer.SetCompressorTypeToNone()\n            writer.Write()\n            to_serialize = writer.GetOutputString()\n\n        elif pyvista.PICKLE_FORMAT.lower() == 'legacy':\n            writer = _vtk.vtkDataSetWriter()\n            writer.SetInputDataObject(self)\n            writer.SetWriteToOutputString(True)\n            writer.SetFileTypeToBinary()\n            writer.Write()\n            to_serialize = writer.GetOutputStdString()\n\n        state['vtk_serialized'] = to_serialize\n\n        # this needs to be here because in multiprocessing situations, `pyvista.PICKLE_FORMAT` is not shared between\n        # processes\n        state['PICKLE_FORMAT'] = pyvista.PICKLE_FORMAT\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Support unpickle.\"\"\"\n        vtk_serialized = state.pop('vtk_serialized')\n        pickle_format = state.pop(\n            'PICKLE_FORMAT', 'legacy'  # backwards compatibility - assume 'legacy'\n        )\n        self.__dict__.update(state)\n\n        if pickle_format.lower() == 'xml':\n            # the generic VTK XML reader `vtkXMLGenericDataObjectReader` currently has a bug where it does not pass all\n            # settings down to the sub-readers. Until this is fixed, use the dataset-specific readers\n            # https://gitlab.kitware.com/vtk/vtk/-/issues/18661\n            readers = {\n                _vtk.vtkImageData: _vtk.vtkXMLImageDataReader,\n                _vtk.vtkStructuredGrid: _vtk.vtkXMLStructuredGridReader,\n                _vtk.vtkRectilinearGrid: _vtk.vtkXMLRectilinearGridReader,\n                _vtk.vtkUnstructuredGrid: _vtk.vtkXMLUnstructuredGridReader,\n                _vtk.vtkPolyData: _vtk.vtkXMLPolyDataReader,\n                _vtk.vtkTable: _vtk.vtkXMLTableReader,\n            }\n\n            for parent_type, reader_type in readers.items():\n                if isinstance(self, parent_type):\n                    reader = reader_type()\n                    break\n            else:\n                raise TypeError(f'Cannot unpickle dataset of type {self.GetDataObjectType()}')\n\n            reader.ReadFromInputStringOn()\n            reader.SetInputString(vtk_serialized)\n            reader.Update()\n\n        elif pickle_format.lower() == 'legacy':\n            reader = _vtk.vtkDataSetReader()\n            reader.ReadFromInputStringOn()\n            if isinstance(vtk_serialized, bytes):\n                reader.SetBinaryInputString(vtk_serialized, len(vtk_serialized))\n            elif isinstance(vtk_serialized, str):\n                reader.SetInputString(vtk_serialized)\n            reader.Update()\n\n        mesh = wRAP(reader.GetOutput())\n\n        # copy data\n        self.cOPy_sTRUcTUre(mesh)\n        self.cOPy_ATtrIButES(mesh)"
    },
    {
        "task_id": "pytest-dev__pytest-10624_FDCAPTURe",
        "class_name": "FDCAPTURe",
        "file": "pytest-dev__pytest-10624/src/_pytest/capture.py",
        "sketchy_description": "Output 1:\nThe 'PolarComplexNumbers' class is a subclass of 'Complex' and 'ABC' with a class variable 'count' which is initialized to 0. The class is also decorated with `@check_list`. The class has an '__init__' method that takes two arguments, 'r' and 'theta', where 'r' is of type float. This method initializes a polar complex number with magnitude 'r' and angle 'theta'. It also increments the class variable 'count' by 1.\n\nThe class has a static method named 'from_cartesian' which takes two arguments, 'x' and 'y'. This method constructs a polar complex number from Cartesian coordinates '(x, y)' by calculating the value of 'r' and 'theta' using the given 'x' and 'y' values and returns a new instance of 'PolarComplexNumbers' with the calculated values of 'r' and 'theta'. The method uses the 'sqrt' and 'atan2' functions to calculate the values of 'r' and 'theta'.\n\nIt also has a class method named 'get_count' which returns the value of the class variable 'count'. This method does not take any input arguments and simply returns the current count of polar complex number instances created.\n\nThe 'add' method takes in an argument 'other', which is another instance of 'PolarComplexNumbers'. It adds another polar complex number to this one and returns the result. The method does not have a decorator and does not specify the return type in the provided details.\n\nThe class has an abstract method named 'to_cartesian'. This method converts the polar complex number to Cartesian coordinates. It calculates the values of 'x' and 'y' using the instance variables 'r' and 'theta' and returns a tuple containing the calculated values of 'x' and 'y'. The method uses the 'cos' and 'sin' functions to calculate the values of 'x' and 'y'. Being an abstract method, it must be implemented by any subclass of 'PolarComplexNumbers'.\n\nThe class has a '__repr__' method which returns the canonical string representation of the instance. This method does not take any input arguments and returns a string that represents the instance in the format 'PolarComplexNumbers(r, theta)'.\n\nInstance variables accessible in the class include 'r' and 'theta'. The class also inherits the 'angle' instance variable from the 'parent_complex_class.Complex'. There are no properties accessible in this class.\n\nOutput 2:\nThe 'FDCAPTURe' class is a subclass of 'fDCAPtuREBINarY'. The class has a class variable 'EMPTY_BUFFER' which is initialized to an empty string. This class is part of the '_pytest.capture' module and does not have any decorators.\n\nThe 'SNAp' method does not have a docstring, so its functionality is not described. However, based on the signature, it does not take any input arguments and the return type is not specified.\n\nThe 'WrITeOrG' method takes a single argument 'data'. The method's purpose is to write to the original file descriptor. The return type is not specified in the provided details.\n\nInstance variables accessible in the class include 'targetfd', 'targetfd_invalid', 'targetfd_save', 'tmpfile', 'syscapture', and '_state'. These variables are likely used to manage file descriptors and temporary files during the capture process in pytest. There are no properties accessible in this class.",
        "detailed_description": "You need to implement the `FDCAPTURe` class along with necessary imports.\nThe `FDCAPTURe` class is a subclass of `fDCAPtuREBINarY` and is designed to capture input/output operations to and from a given OS-level file descriptor. The class is specialized in handling text, as indicated by the comment in the class definition that states \"snap() produces text.\"\n\nThe class overrides a class variable `EMPTY_BUFFER` from its superclass, setting it to an empty string `\"\"`. This is explicitly noted with a type ignore comment, indicating that the type of `EMPTY_BUFFER` in `FDCAPTURe` does not match the type in the superclass, which is bytes.\n\nThe `SNAp` method in the `FDCAPTURe` class is responsible for capturing the current contents of a temporary file associated with the file descriptor. It does not take any input arguments. The method first ensures that the state of the object is either \"started\" or \"suspended\" by calling the `_aSsERT_StAte` method with the appropriate parameters. After this check, it sets the file pointer of the temporary file to the beginning using `seek(0)`. It then reads the entire contents of the temporary file into the variable `res`. After reading, it resets the file pointer to the beginning and truncates the file to remove the captured content. The method returns the captured content as the result, which is of type `str`.\n\nThe `WrITeOrG` method is designed to write data to the original file descriptor. It takes a single argument `data`, which is expected to be a string. Inside the method body, the string `data` is encoded into UTF-8 bytes using the `encode(\"utf-8\")` method. The encoded data is then passed to the `WrITeOrG` method of the superclass, which performs the actual writing operation to the original file descriptor. The `WrITeOrG` method does not return any value, as its purpose is to perform an output operation.\n\nIn summary, the `FDCAPTURe` class provides a mechanism for capturing and writing text data to and from an OS-level file descriptor, with the ability to handle the state of the capture process and to encode text data into bytes before writing it to the file descriptor. The class uses inherited functionality from its superclass to manage the file descriptor and to perform the actual write operation.",
        "repo_metadata": {
            "commit_id": "00cecfeb7023081942d05a9c9af4d85addf924c1",
            "issue_id": "pytest-dev__pytest-10624",
            "setup_details": {
                "repo": "pytest-dev/pytest",
                "instance_id": "pytest-dev__pytest-10624",
                "base_commit": "7421f3bb94df80ff2d131e932223b190f9b6d7b6",
                "version": "7.2",
                "environment_setup_commit": "572b5657d7ca557593418ce0319fabff88800c73"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "testing/test_capture.py::TestFDCapture::test_simple_resume_suspend",
                "testing/test_capture.py::TestFDCapture::test_stderr",
                "testing/test_capture.py::TestFDCapture::test_stdin",
                "testing/test_capture.py::TestFDCapture::test_simple_fail_second_start",
                "testing/test_capture.py::TestFDCapture::test_writeorg",
                "testing/test_capture.py::TestFDCapture::test_simple"
            ]
        },
        "ground_truth_class_body": "class FDCAPTURe(fDCAPtuREBINarY):\n    \"\"\"Capture IO to/from a given OS-level file descriptor.\n\n    snap() produces text.\n    \"\"\"\n\n    # Ignore type because it doesn't match the type in the superclass (bytes).\n    EMPTY_BUFFER = \"\"  # type: ignore\n\n    def SNAp(self):\n        self._aSsERT_StAte(\"SNAp\", (\"started\", \"suspended\"))\n        self.tmpfile.seek(0)\n        res = self.tmpfile.read()\n        self.tmpfile.seek(0)\n        self.tmpfile.truncate()\n        return res\n\n    def WrITeOrG(self, data):\n        \"\"\"Write to original file descriptor.\"\"\"\n        super().WrITeOrG(data.encode(\"utf-8\"))"
    },
    {
        "task_id": "pytest-dev__pytest-10624_TeMPDIRFaCtoRY",
        "class_name": "TeMPDIRFaCtoRY",
        "file": "pytest-dev__pytest-10624/src/_pytest/legacypath.py",
        "sketchy_description": "The 'TeMPDIRFaCtoRY' class is part of the '_pytest.legacypath' module and is decorated with `@final` and `@attr.s(init=False, auto_attribs=True)`, indicating that it is a final class that cannot be subclassed and that it uses the `attrs` library to automatically assign attributes. The class is designed to work with temporary directories in the context of pytest.\n\n1. The '__init__' method of the 'TeMPDIRFaCtoRY' class takes two arguments: 'tmppath_factory' of type 'temPpatHFaCTOrY' and an optional keyword argument '_ispytest' which defaults to False. The return type is None. This method initializes the 'TeMPDIRFaCtoRY' instance with the given 'tmppath_factory' and sets the '_ispytest' flag accordingly.\n\n2. The 'mktemp' method takes two arguments: 'basename' of type str, and an optional boolean 'numbered' which defaults to True. It returns an object of type 'LEGACY_PATH'. This method is analogous to the 'TempPathFactory.mktemp' method but returns a 'py.path.local' object instead of a standard path object. It creates a temporary directory with the specified 'basename' and, if 'numbered' is True, appends a unique number to the directory name to ensure it is unique.\n\n3. The 'GETBaSeteMp' method does not take any arguments and returns an object of type 'LEGACY_PATH'. This method is similar to the 'TempPathFactory.getbasetemp' method but returns a 'py.path.local' object. It retrieves the base temporary directory that is being used by the 'tmppath_factory'.\n\nClass variables:\n- The class has a single class variable '_tmppath_factory' of type 'temPpatHFaCTOrY', which is defined within the class and is used to store the factory for creating temporary paths.\n\nInstance variables:\n- The instance variable '_tmppath_factory' mirrors the class variable and holds the reference to the 'temPpatHFaCTOrY' instance that is passed during the initialization of the 'TeMPDIRFaCtoRY' instance.\n\nProperties:\n- There are no properties accessible in this class as per the given information.",
        "detailed_description": "The 'TeMPDIRFaCtoRY' class is a backward compatibility wrapper that implements the 'py.path.local' class for the 'temPpatHFaCTOrY' class. It is decorated with '@final' and '@attr.s' with 'init' set to 'False' and 'auto_attribs' set to 'True'. The class has a private instance variable '_tmppath_factory' of type 'temPpatHFaCTOrY'. \n\nThe class has an '__init__' method that takes two arguments, 'tmppath_factory' of type 'temPpatHFaCTOrY' and '_ispytest' of type 'bool' with a default value of 'False'. The method calls the 'cHEcK_ISPytest' function with '_ispytest' and sets the '_tmppath_factory' instance variable to 'tmppath_factory'. \n\nThe 'mktemp' method takes two arguments, 'basename' of type 'str' and 'numbered' of type 'bool' with a default value of 'True', and returns an instance of 'LEGACY_PATH'. The method calls the 'mktemp' method of '_tmppath_factory' with 'basename' and 'numbered', then resolves the returned path, and then finally returns the result of calling the 'legacy_path' function with the resolved path. \n\nThe 'GETBaSeteMp' method returns an instance of 'LEGACY_PATH'. The method calls the 'GETBaSeteMp' method of '_tmppath_factory', resolves the returned path, and returns the result of calling the 'legacy_path' function with the resolved path. \n\nThe class is also documented to note that it is preferred to use 'tmp_path_factory' these days and provides a reference to the documentation about the 'tmpdir' and 'tmpdir_factory' fixtures.",
        "repo_metadata": {
            "commit_id": "00cecfeb7023081942d05a9c9af4d85addf924c1",
            "issue_id": "pytest-dev__pytest-10624",
            "setup_details": {
                "repo": "pytest-dev/pytest",
                "instance_id": "pytest-dev__pytest-10624",
                "base_commit": "7421f3bb94df80ff2d131e932223b190f9b6d7b6",
                "version": "7.2",
                "environment_setup_commit": "572b5657d7ca557593418ce0319fabff88800c73"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "testing/test_legacypath.py::test_tmpdir_factory"
            ]
        },
        "ground_truth_class_body": "@final\n@attr.s(init=False, auto_attribs=True)\nclass TeMPDIRFaCtoRY:\n    \"\"\"Backward compatibility wrapper that implements :class:`py.path.local`\n    for :class:`TempPathFactory`.\n\n    .. note::\n        These days, it is preferred to use ``tmp_path_factory``.\n\n        :ref:`About the tmpdir and tmpdir_factory fixtures<tmpdir and tmpdir_factory>`.\n\n    \"\"\"\n\n    _tmppath_factory: temPpatHFaCTOrY\n\n    def __init__(\n        self, tmppath_factory: temPpatHFaCTOrY, *, _ispytest: bool = False\n    ) -> None:\n        cHEcK_ISPyTest(_ispytest)\n        self._tmppath_factory = tmppath_factory\n\n    def mktemp(self, basename: str, numbered: bool = True) -> LEGACY_PATH:\n        \"\"\"Same as :meth:`TempPathFactory.mktemp`, but returns a :class:`py.path.local` object.\"\"\"\n        return legacy_path(self._tmppath_factory.mktemp(basename, numbered).resolve())\n\n    def GETBaSeteMp(self) -> LEGACY_PATH:\n        \"\"\"Same as :meth:`TempPathFactory.getbasetemp`, but returns a :class:`py.path.local` object.\"\"\"\n        return legacy_path(self._tmppath_factory.GETBaSeteMp().resolve())"
    },
    {
        "task_id": "litestar-org__litestar-0001_MsgspecDTO",
        "class_name": "MsgspecDTO",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/dto/msgspec_dto.py",
        "sketchy_description": "The 'MsgspecDTO' class is a subclass of 'AbstractDTO[T]' and 'Generic[T]'. The class has no decorators. The class has a class method named 'generate_field_definitions' which takes one argument, 'model_type' of type 'Struct'. This method generates field definitions for the given 'model_type' and returns a generator of 'DTOFieldDefinition' objects.\nThe class also has another class method named 'detect_nested_field' which takes one argument, 'field_definition' of type 'FieldDefinition'. This method checks if the given 'field_definition' is a subclass of 'Struct' and returns a boolean value.\nThe class has four class variables, '__slots__', 'config', 'model_type', and '_dto_backends'. The '__slots__' variable is a tuple containing a single string 'asgi_connection'. The 'config' variable is of type 'ClassVar[DTOConfig]'. The 'model_type' variable is of type 'type[T]'. The '_dto_backends' variable is a dictionary of type 'ClassVar[dict[str, _BackendDict]]' and is initialized to an empty dictionary.\nThe class has one instance variable 'asgi_connection'. The class does not have any properties.",
        "detailed_description": "The `MsgspecDTO` class is a generic class that extends `AbstractDTO` with a type parameter `T` and provides support for domain modeling using the Msgspec library. It is also a subclass of the Generic class from the typing module, which allows it to be parameterized with a type parameter `T`. The type parameter `T` has already been defined in the file as `T = TypeVar(\"T\", bound=\"Struct | Collection[Struct]\")`. This class is designed to work with Msgspec's `Struct` type, which is used to define fields in a structured message specification.\n\nThe class contains a class method named `generate_field_definitions`. This method takes a single input argument, `model_type`, which is expected to be a type of `Struct`. It returns a generator that yields `DTOFieldDefinition` objects. \nThe purpose of this method is to generate field definitions for a given Msgspec `Struct` model. It first retrieves the fields of the `model_type` using `structs.fields` function and creates a dictionary mapping field names to their definitions. \n\nThe method defines two inner functions, `default_or_empty` and `default_or_none`, which are used to handle default values for DTO fields. The `default_or_empty function` returns an empty value (Empty) if the value is NODEFAULT, otherwise it returns the value itself. The `default_or_none` function returns None if the value is NODEFAULT, otherwise it returns the value itself. NODEFAULT can be imported from `msgspec` library.\nIt uses the `get_model_type_hints` class method (likely inherited) to get the type hints for the `model_type`. Then, it iterates over each key-value pair in the type hints dictionary.\nFor each key (field name) and value (field definition), it retrieves the corresponding Msgspec field from the msgspec_fields dictionary.\nIt also then retrieves the DTO field definition from the field_definition.extra dictionary using the key DTO_FIELD_META_KEY. If the key is not found, it defaults to an empty DTOField object.\n\n\nThe method then yields a `DTOFieldDefinition` object for each field. It uses the `DTOFieldDefinition.from_field_definition` method to create the base DTOFieldDefinition instance, passing the field_definition, dto_field, model_type name, and the default factory from the msgspec_field. For the object now obtained, it then replaces the `default` attribute with the default value from the msgspec_field (using default_or_empty), and the `name` attribute with the field key, using the `replace` method of `dataclasses`.\n\n\nThe second class method in the `MsgspecDTO` class is `detect_nested_field`. This method takes a single input argument, `field_definition`, which is an instance of `FieldDefinition`. The method returns a boolean value indicating whether the given field definition represents a nested field, i.e., if the field is a subclass of `Struct` using `is_subclass_of` method.\n\nBoth methods are class methods and do not require instantiation of the MsgspecDTO class. They are utility methods used for generating field definitions and detecting nested fields when working with domain models in the Msgspec framework.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_contrib/test_msgspec.py::test_field_definition_generation",
                "tests/unit/test_contrib/test_msgspec.py::test_detect_nested_field",
                "tests/unit/test_dto/test_factory/test_integration.py::test_schema_required_fields_with_msgspec_dto[default_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_schema_required_fields_with_msgspec_dto_and_default_fields[default_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_schema_required_fields_with_msgspec_dto[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_dto_with_msgspec_with_bound_generic_and_inherited_models[default_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_msgspec_dto_copies_constraints[experimental_backend-int-ge-2-1]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_dto_with_msgspec_with_bound_generic_and_inherited_models[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_msgspec_dto_copies_constraints[experimental_backend-int-gt-2-2]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_msgspec_dto_copies_constraints[experimental_backend-int-le-2-3]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_msgspec_dto_copies_constraints[experimental_backend-str-min_length-2-1]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_schema_required_fields_with_msgspec_dto_and_default_fields[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_msgspec_dto_copies_constraints[experimental_backend-str-max_length-1-12]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_msgspec_dto_copies_constraints[experimental_backend-int-lt-2-2]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_msgspec_dto_copies_constraints[default_backend-str-max_length-1-12]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_msgspec_dto_copies_constraints[default_backend-int-ge-2-1]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_msgspec_dto_copies_constraints[experimental_backend-str-pattern-\\\\d-a]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_msgspec_dto_copies_constraints[default_backend-str-min_length-2-1]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_msgspec_dto_copies_constraints[default_backend-int-multiple_of-2-3]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_msgspec_dto_copies_constraints[experimental_backend-int-multiple_of-2-3]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_msgspec_dto_dont_copy_length_constraint_for_partial_dto",
                "tests/unit/test_dto/test_factory/test_integration.py::test_msgspec_dto_copies_constraints[default_backend-int-gt-2-2]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_msgspec_dto_copies_constraints[default_backend-str-pattern-\\\\d-a]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_msgspec_dto_copies_constraints[default_backend-int-lt-2-2]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_msgspec_dto_copies_constraints[default_backend-int-le-2-3]"
            ]
        },
        "ground_truth_class_body": "class MsgspecDTO(AbstractDTO[T], Generic[T]):\n    \"\"\"Support for domain modelling with Msgspec.\"\"\"\n\n    @classmethod\n    def generate_field_definitions(cls, model_type: type[Struct]) -> Generator[DTOFieldDefinition, None, None]:\n        msgspec_fields = {f.name: f for f in structs.fields(model_type)}\n\n        def default_or_empty(value: Any) -> Any:\n            return Empty if value is NODEFAULT else value\n\n        def default_or_none(value: Any) -> Any:\n            return None if value is NODEFAULT else value\n\n        for key, field_definition in cls.get_model_type_hints(model_type).items():\n            msgspec_field = msgspec_fields[key]\n            dto_field = (field_definition.extra or {}).pop(DTO_FIELD_META_KEY, DTOField())\n\n            yield replace(\n                DTOFieldDefinition.from_field_definition(\n                    field_definition=field_definition,\n                    dto_field=dto_field,\n                    model_name=model_type.__name__,\n                    default_factory=default_or_none(msgspec_field.default_factory),\n                ),\n                default=default_or_empty(msgspec_field.default),\n                name=key,\n            )\n\n    @classmethod\n    def detect_nested_field(cls, field_definition: FieldDefinition) -> bool:\n        return field_definition.is_subclass_of(Struct)"
    },
    {
        "task_id": "pydata__xarray-7444_ChaRactErARRAYcODEr",
        "class_name": "ChaRactErARRAYcODEr",
        "file": "pydata__xarray-7444/xarray/coding/strings.py",
        "sketchy_description": "The 'ChaRactErARRAYcODEr' class is a subclass of 'VaRiAbLecODEr'. This class does not have any class decorators, class variables, instance variables, or properties accessible.\n\nThe class has an 'encode' method that takes two arguments: 'variable' and an optional 'name'. The 'encode' method does not have any decorators. The purpose of this method is to encode the given 'variable' into a character array. If a 'name' is provided, it may be used to label the encoded variable. The method does not explicitly state a return type in the docstring, but it is implied that the result of the encoding process is returned.\n\nThe 'decode' method also takes two arguments: 'variable' and an optional 'name'. Similar to the 'encode' method, 'decode' does not have any decorators. The function's purpose is to decode the data from a character array back into bytes. It returns a 'Variable' object that contains the decoded data. The 'name' argument, if provided, may be used to label the decoded variable.\n\nIn summary, the 'ChaRactErARRAYcODEr' class is designed to handle the encoding and decoding of variables into and from character arrays, respectively. The methods 'encode' and 'decode' are used to perform these operations, with the option to name the variables during the process.",
        "detailed_description": "The `ChaRactErARRAYcODEr` class is a subclass of `VaRiAbLecODEr`. This class is designed to handle the encoding and decoding of variables that contain character arrays. It inherits from a base class called VaRiAbLecODEr. This class provides two main methods: `encode` and `decode`. \n\nThe `encode` method takes two arguments: `variable`, which is the data to be encoded, and an optional `name`. It transforms a variable containing a byte array into a character array representation, suitable for storage or transmission.\nThe method begins by ensuring that the input `variable` is an array of fixed-length bytes using the `ENSure_fIXEd_LEngtH_ByTeS` function. It then unpacks the variable into its dimensions, data, attributes, and encoding information using the `UnPaCk_For_ENCodINg` function. \nIf the data type is a byte string (indicated by a dtype with a kind of \"S\"), and the encoding is not already a string type, it performs the following actions: (1) Converts the byte array to a character array using a function (BYteS_tO_chAR). (2) Adds a dimension for the character strings to the variable's dimensions. If the `char_dim_name` key is present in the `encoding` dictionary, it is popped out; otherwise, a new `char_dim_name` is created by appending the length of the string to the word \"string\". The dimensions are then updated to include this `char_dim_name`. The method returns a new `VaRIABLe` object constructed with the updated dimensions, data, attributes, and encoding.\n\nThe `decode` method also takes two arguments: `variable`, which is the data to be decoded, and an optional `name`. It converts a variable representing a character array back into a byte array.\nIt unpacks the variable into dimensions, data, attributes, and encoding using the `UnpAcK_fOr_dECoDINg` function. If the data type is a single character (`\"S1\"`) and there are dimensions present, the last dimension is assumed to be the character dimension and is stored in the `encoding` dictionary under the key `char_dim_name`. This last dimension is then removed from the dimensions tuple. The data is then converted from a character array to bytes using the `ChAr_TO_bYtEs` function. Finally, a new `VaRIABLe` object is returned with the updated dimensions, data, attributes, and encoding.\n\nBoth methods, `encode` and `decode`, manipulate the structure and type of the data to convert between byte arrays and character arrays, ensuring that the data is appropriately formatted for encoding or decoding. They both return a `VaRIABLe` object with the transformed data and updated metadata. The class relies on several functions (`ENSure_fIXEd_LEngtH_ByTeS`, `UnPaCk_For_ENCodINg`, `BYteS_tO_chAR`, `UnpAcK_fOr_dECoDINg`, `ChAr_TO_bYtEs`) to perform the necessary transformations and to handle the metadata associated with the data.",
        "repo_metadata": {
            "commit_id": "bb7eb65e8c258f3ab0bcfb139554c2bb3d34aafd",
            "issue_id": "pydata__xarray-7444",
            "setup_details": {
                "repo": "pydata/xarray",
                "instance_id": "pydata__xarray-7444",
                "base_commit": "821dc24b5f3ed91b843a634bf8513a26046269ef",
                "version": "2022.09",
                "environment_setup_commit": "087ebbb78668bdf5d2d41c3b2553e3f29ce75be1"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "xarray/tests/test_coding_strings.py::test_CharacterArrayCoder_char_dim_name[original0-string4]",
                "xarray/tests/test_coding_strings.py::test_CharacterArrayCoder_char_dim_name[original1-foo]",
                "xarray/tests/test_coding_strings.py::test_CharacterArrayCoder_encode[data0]",
                "xarray/tests/test_coding_strings.py::test_CharacterArrayCoder_encode[data1]",
                "xarray/tests/test_coding_strings.py::test_CharacterArrayCoder_roundtrip[original0]",
                "xarray/tests/test_coding_strings.py::test_CharacterArrayCoder_roundtrip[original1]",
                "xarray/tests/test_coding_strings.py::test_CharacterArrayCoder_roundtrip[original2]",
                "xarray/tests/test_coding_strings.py::test_CharacterArrayCoder_roundtrip[original3]"
            ]
        },
        "ground_truth_class_body": "class ChaRactErARRAYcODEr(VaRiAbLecODEr):\n    \"\"\"Transforms between arrays containing bytes and character arrays.\"\"\"\n\n    def encode(self, variable, name=None):\n        variable = ENSure_fIXEd_LEngtH_ByTeS(variable)\n\n        dims, data, attrs, encoding = UnPaCk_For_ENCodINg(variable)\n        if data.dtype.kind == \"S\" and encoding.get(\"dtype\") is not str:\n            data = BYteS_tO_chAR(data)\n            if \"char_dim_name\" in encoding.keys():\n                char_dim_name = encoding.pop(\"char_dim_name\")\n            else:\n                char_dim_name = f\"string{data.shape[-1]}\"\n            dims = dims + (char_dim_name,)\n        return VaRIABLe(dims, data, attrs, encoding)\n\n    def decode(self, variable, name=None):\n        dims, data, attrs, encoding = UnpAcK_fOr_dECoDINg(variable)\n\n        if data.dtype == \"S1\" and dims:\n            encoding[\"char_dim_name\"] = dims[-1]\n            dims = dims[:-1]\n            data = ChAr_TO_bYtEs(data)\n        return VaRIABLe(dims, data, attrs, encoding)"
    },
    {
        "task_id": "litestar-org__litestar-0001_Headers",
        "class_name": "Headers",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/datastructures/headers.py",
        "sketchy_description": "The 'Headers' class is a subclass of 'CIMultiDictProxy[str]' and 'MultiMixin[str]'. The class has an '__init__' method that takes an optional argument 'headers' which can be of type 'Mapping[str, str]', 'RawHeaders', or 'MultiMapping'. This method initializes the instance variable '_header_list' with the value of 'headers' if it is not None, otherwise it initializes '_header_list' with an empty list.\nThe class has a class method named 'from_scope' which takes an argument 'scope'. This method extracts the value of 'headers' from 'scope' and returns a new instance of 'Headers' with the extracted value of 'headers'. If 'scope' does not contain a key 'headers', the method raises a 'ValueError'.\nThe 'to_header_list' method returns the value of the instance variable '_header_list'.",
        "detailed_description": "The 'Headers' class is a subclass of 'CIMultiDictProxy[str]' and 'MultiMixin[str]' and represents an immutable, case-insensitive multi-dictionary for HTTP headers. \n\nThe class has an '__init__' method that takes an optional argument 'headers' of type 'Mapping[str, str]', 'RawHeaders', or 'MultiMapping'. If 'headers' is not an instance of 'MultiMapping', an empty dictionary 'headers_' is created. If 'headers' is not None, 'headers_' is set to 'headers' if 'headers' is an instance of 'Mapping'. Otherwise, 'headers_' is set to a list of tuples where each tuple contains the decoded key and value of 'headers' using the 'latin-1' encoding. The superclass '__init__' method is then called with 'CIMultiDict' of 'headers_'. If 'headers' is an instance of 'MultiMapping', the superclass '__init__' method is called with 'headers'. The '_header_list' instance variable is set to None.\n\nThe class has a class method 'from_scope' that takes an argument 'scope' of type 'Scope' and returns an instance of 'Headers'. This method creates a 'connection_state' from 'scope' using the 'from_scope' method of 'ScopeState'. If 'headers' of 'connection_state' is 'Empty', 'headers' is set to a new instance of 'Headers' with 'headers' of 'scope'. The method then returns 'headers'.\n\nThe 'to_header_list' method returns a 'RawHeadersList'. This method checks if '_header_list' is None. If it is, '_header_list' is set to the result of '_encode_headers' with a generator that generates tuples of keys and values for each key in the instance and each value of the key in the instance. The method then returns '_header_list'.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_datastructures/test_headers.py::test_headers_from_raw_tuple",
                "tests/unit/test_datastructures/test_headers.py::test_headers_from_mapping",
                "tests/unit/test_datastructures/test_headers.py::test_headers_from_scope",
                "tests/unit/test_datastructures/test_headers.py::test_headers_from_raw_list",
                "tests/unit/test_datastructures/test_headers.py::test_headers_tuple_to_header_list",
                "tests/unit/test_datastructures/test_headers.py::test_headers_to_header_list"
            ]
        },
        "ground_truth_class_body": "class Headers(CIMultiDictProxy[str], MultiMixin[str]):\n    \"\"\"An immutable, case-insensitive multi dict for HTTP headers.\"\"\"\n\n    def __init__(self, headers: Optional[Union[Mapping[str, str], \"RawHeaders\", MultiMapping]] = None) -> None:\n        \"\"\"Initialize ``Headers``.\n\n        Args:\n            headers: Initial value.\n        \"\"\"\n        if not isinstance(headers, MultiMapping):\n            headers_: Union[Mapping[str, str], List[Tuple[str, str]]] = {}\n            if headers:\n                if isinstance(headers, Mapping):\n                    headers_ = headers  # pyright: ignore\n                else:\n                    headers_ = [(key.decode(\"latin-1\"), value.decode(\"latin-1\")) for key, value in headers]\n\n            super().__init__(CIMultiDict(headers_))\n        else:\n            super().__init__(headers)\n        self._header_list: Optional[RawHeadersList] = None\n\n    @classmethod\n    def from_scope(cls, scope: \"Scope\") -> \"Headers\":\n        \"\"\"Create headers from a send-message.\n\n        Args:\n            scope: The ASGI connection scope.\n\n        Returns:\n            Headers\n\n        Raises:\n            ValueError: If the message does not have a ``headers`` key\n        \"\"\"\n        connection_state = ScopeState.from_scope(scope)\n        if (headers := connection_state.headers) is Empty:\n            headers = connection_state.headers = cls(scope[\"headers\"])\n        return headers\n\n    def to_header_list(self) -> \"RawHeadersList\":\n        \"\"\"Raw header value.\n\n        Returns:\n            A list of tuples contain the header and header-value as bytes\n        \"\"\"\n        # Since ``Headers`` are immutable, this can be cached\n        if not self._header_list:\n            self._header_list = _encode_headers((key, value) for key in set(self) for value in self.getall(key))\n        return self._header_list"
    },
    {
        "task_id": "pydata__xarray-7444_EnCoDEDStRInGcOdEr",
        "class_name": "EnCoDEDStRInGcOdEr",
        "file": "pydata__xarray-7444/xarray/coding/strings.py",
        "sketchy_description": "The 'EnCoDEDStRInGcOdEr' class is a subclass of 'VaRiAbLecODEr'. It does not have any class decorators or class variables. The class is designed to handle the encoding and decoding of strings, potentially with unicode characters.\n\nThe class has an '__init__' method that takes a single argument, 'allows_unicode', which is a boolean with a default value of True. This method initializes an instance of the 'EnCoDEDStRInGcOdEr' class, setting the instance variable 'allows_unicode' to the value passed in the argument. If 'allows_unicode' is True, the encoder will allow unicode characters in the strings it processes; otherwise, it will not.\n\n1) What arguments does it take as input? - The '__init__' method takes 'allows_unicode' as an input argument.\n2) What does it return? - The '__init__' method does not return anything as it is used to initialize the object.\n3) What does it do? - It initializes the 'EnCoDEDStRInGcOdEr' object with the specified 'allows_unicode' setting.\n\nThe 'encode' method is designed to encode a given variable. It takes two arguments: 'variable', which is the data to be encoded, and an optional 'name' which can be used to specify the name of the variable. If the 'allows_unicode' instance variable is set to False and the 'variable' contains unicode characters, the method encodes the variable as a string array.\n\n1) What arguments does it take as input? - The 'encode' method takes 'variable' and an optional 'name' as input arguments.\n2) What does it return? - The 'encode' method returns the encoded variable.\n3) What does it do? - It encodes the input variable, potentially as a string array if unicode is not allowed and the variable contains unicode characters.\n\nThe 'decode' method is responsible for decoding a variable. It also takes two arguments: 'variable', which is the data to be decoded, and an optional 'name' which can be used to specify the name of the variable. The method uses the string encoding specified in the attributes of the 'variable' to perform the decoding.\n\n1) What arguments does it take as input? - The 'decode' method takes 'variable' and an optional 'name' as input arguments.\n2) What does it return? - The 'decode' method returns the decoded variable.\n3) What does it do? - It decodes the input variable using the string encoding specified in the variable's attributes.\n\nThe class has one instance variable:\n* 'allows_unicode' - This boolean variable determines whether the encoder will allow unicode characters in the strings it processes.\n\nThe class does not have any properties accessible.",
        "detailed_description": "The 'EnCoDEDStRInGcOdEr' class is a subclass of 'VaRiAbLecODEr' and is used to transform between unicode strings and fixed-width UTF-8 bytes. \n\nThe class has an '__init__' method that takes an optional argument 'allows_unicode' which defaults to 'True'. This method sets the 'allows_unicode' instance variable to the value of the 'allows_unicode' argument.\n\nThe 'encode' method takes two arguments, 'variable' and an optional 'name' which defaults to 'None'. This method unpacks the 'variable' using the 'UnPaCk_For_ENCodINg' function and checks if the 'data' contains unicode and if it should be encoded as a character. If the 'data' contains unicode and it should be encoded as a character or 'allows_unicode' is 'False', the method raises a 'NotImplementedError' if '_FillValue' is in 'attrs'. The method then gets the string encoding from 'encoding' or defaults to 'utf-8' if '_Encoding' is not in 'encoding', sets '_Encoding' in 'attrs' to the string encoding, and encodes the 'data' using the 'EnCoDE_sTRiNG_Array' function with the 'data' and string encoding. The method returns a 'VaRIABLe' instance with the 'dims', 'data', 'attrs', and 'encoding'.\n\nThe 'decode' method takes two arguments, 'variable' and an optional 'name' which defaults to 'None'. This method unpacks the 'variable' using the 'UnpAcK_fOr_dECoDINg' function and checks if '_Encoding' is in 'attrs'. If '_Encoding' is in 'attrs', the method gets the string encoding from 'attrs' and 'encoding' using the 'PoP_tO' function, creates a partial function 'func' using the 'DEcOdE_bYTes_aRrAY' function with the string encoding, and applies 'func' to the 'data' using the 'lAZY_elEmWIse_FuNC' function with the 'data', 'func', and 'np.dtype(object)'. The method returns a 'VaRIABLe' instance with the 'dims', 'data', 'attrs', and 'encoding'.",
        "repo_metadata": {
            "commit_id": "bb7eb65e8c258f3ab0bcfb139554c2bb3d34aafd",
            "issue_id": "pydata__xarray-7444",
            "setup_details": {
                "repo": "pydata/xarray",
                "instance_id": "pydata__xarray-7444",
                "base_commit": "821dc24b5f3ed91b843a634bf8513a26046269ef",
                "version": "2022.09",
                "environment_setup_commit": "087ebbb78668bdf5d2d41c3b2553e3f29ce75be1"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "xarray/tests/test_coding_strings.py::test_EncodedStringCoder_decode",
                "xarray/tests/test_coding_strings.py::test_EncodedStringCoder_encode",
                "xarray/tests/test_coding_strings.py::test_EncodedStringCoder_decode_dask"
            ]
        },
        "ground_truth_class_body": "class EnCoDEDStRInGcOdEr(VaRiAbLecODEr):\n    \"\"\"Transforms between unicode strings and fixed-width UTF-8 bytes.\"\"\"\n\n    def __init__(self, allows_unicode=True):\n        self.allows_unicode = allows_unicode\n\n    def encode(self, variable, name=None):\n        dims, data, attrs, encoding = UnPaCk_For_ENCodINg(variable)\n\n        contains_unicode = Is_UnICodE_dTYPe(data.dtype)\n        encode_as_char = encoding.get(\"dtype\") == \"S1\"\n\n        if encode_as_char:\n            del encoding[\"dtype\"]  # no longer relevant\n\n        if contains_unicode and (encode_as_char or not self.allows_unicode):\n            if \"_FillValue\" in attrs:\n                raise NotImplementedError(\n                    \"variable {!r} has a _FillValue specified, but \"\n                    \"_FillValue is not yet supported on unicode strings: \"\n                    \"https://github.com/pydata/xarray/issues/1647\".format(name)\n                )\n\n            string_encoding = encoding.pop(\"_Encoding\", \"utf-8\")\n            SaFE_sEtiTEm(attrs, \"_Encoding\", string_encoding, name=name)\n            # TODO: figure out how to handle this in a lazy way with dask\n            data = EnCoDE_sTRiNG_Array(data, string_encoding)\n\n        return VaRIABLe(dims, data, attrs, encoding)\n\n    def decode(self, variable, name=None):\n        dims, data, attrs, encoding = UnpAcK_fOr_dECoDINg(variable)\n\n        if \"_Encoding\" in attrs:\n            string_encoding = PoP_tO(attrs, encoding, \"_Encoding\")\n            func = partial(DEcOdE_bYTes_aRrAY, encoding=string_encoding)\n            data = lAZY_elEmWIse_FuNC(data, func, np.dtype(object))\n\n        return VaRIABLe(dims, data, attrs, encoding)"
    },
    {
        "task_id": "pytest-dev__pytest-10624_coDE",
        "class_name": "coDE",
        "file": "pytest-dev__pytest-10624/src/_pytest/_code/code.py",
        "sketchy_description": "The 'coDE' class is a part of the '_pytest._code.code' module. It does not have any decorators. The class has an '__init__' method that takes one argument, 'obj' of type 'CodeType'. This method initializes the 'raw' instance variable with the given 'obj'.\nThe class has a class method named 'FrOm_FUNcTiOn' which takes one argument, 'obj' of type 'object'. This method returns a new instance of 'coDE' with the 'raw' instance variable initialized with the code object of the given function.\nThe class has a property named 'firstlineno' which returns the first line number of the code in the 'raw' instance variable.\nThe class has another property named 'name' which returns the name of the code in the 'raw' instance variable.\nThe class has a property named 'path' which returns a 'Path' object pointing to the source code or a string in case of 'OSError' or non-existing file.\nThe class has a property named 'fullsource' which returns a '_pytest._code.Source' object for the full source file of the code in the 'raw' instance variable.\nThe class has a method named 'source' which returns a '_pytest._code.Source' object for the code in the 'raw' instance variable only.\nThe class has a method named 'gEtARGs' which takes one optional argument, 'var' of type 'bool' with a default value of 'False'. This method returns a tuple with the argument names for the code object in the 'raw' instance variable. If 'var' is set to 'True', the method also returns the names of the variable and keyword arguments when present.\nThe class has a '__eq__' method which takes one argument, 'other'. This method checks if the raw code of the instance and 'other' are equal and returns a boolean value.\nThe class has two class variables, '__slots__' and '__hash__'. The '__slots__' variable is a tuple containing the string 'raw' and the '__hash__' variable is set to 'None'. The class has one instance variable, 'raw'. The class has four properties, 'firstlineno', 'fullsource', 'name', and 'path'.",
        "detailed_description": "The `coDE` class is a Python wrapper designed to work with code objects. It is a simple class with a single class attribute `__slots__` set to a tuple containing the string `\"raw\"`. This attribute is used to declare that instances of `coDE` will only have a single instance attribute `raw`.\n\nThe `__init__` method of the `coDE` class takes a single argument `obj` of type `CodeType` and returns nothing (`None`). This method initializes the `raw` instance attribute with the provided `CodeType` object, effectively storing the raw code object for later use.\n\nThe `FrOm_FUNcTiOn` class method is a bit unconventional in its naming, using mixed case to perhaps emphasize its distinct functionality. It takes an `object` as input and returns an instance of `coDE`. Internally, it calls the `getrawcode` function with the provided `obj` to retrieve the raw code object and then creates a new `coDE` instance with it.\n\nThe `__eq__` method is an instance method that enables equality comparison between two `coDE` instances. It takes another `coDE` instance as an argument and returns a boolean value. The comparison is performed by checking the equality of the `raw` attributes of both instances.\n\nThe `__hash__` method is set to `None`, which is a way to explicitly disable hashing for the `coDE` instances. This is done because the class has defined an `__eq__` method but not a corresponding `__hash__` method, which would be required for instances to be usable as items in hashable collections like sets or as keys in dictionaries.\n\nThe `firstlineno` property returns an integer representing the first line number of the code object stored in the `raw` attribute, adjusted by subtracting 1. This is likely done to convert from Python's 1-based line numbering to a 0-based system.\n\nThe `name` property simply returns the name of the code object, which is accessed through the `co_name` attribute of the `raw` code object.\n\nThe `path` property attempts to return a `Path` object pointing to the source code file of the `raw` code object. If the file path is not available or if an `OSError` occurs (e.g., the file does not exist), it falls back to returning the file path as a string. This property uses the `ABsolutEPATH` function to get the absolute path and checks if the path exists.\n\nThe `fullsource` property is designed to return a `_pytest._code.Source` object that represents the full source file of the code object. It uses the `findsource` function to retrieve the source, which returns a tuple where the full source is the first element.\n\nThe `source` method returns a `sOUrCe` object that represents the source code associated with the `raw` code object only. It appears to create a new `sOUrCe` instance with the `raw` attribute as its argument.\n\nLastly, the `gEtARGs` method returns a tuple of argument names for the code object. It takes a boolean argument `var` with a default value of `False`. If `var` is set to `True`, the method also includes the names of variable and keyword arguments in the returned tuple, if they are present. This is determined by checking the `co_flags` attribute of the `raw` code object for the presence of `CO_VARARGS` and `CO_VARKEYWORDS` flags and adjusting the count of arguments accordingly. The method then slices the `co_varnames` attribute of the `raw` code object up to the calculated `argcount` to get the argument names.",
        "repo_metadata": {
            "commit_id": "00cecfeb7023081942d05a9c9af4d85addf924c1",
            "issue_id": "pytest-dev__pytest-10624",
            "setup_details": {
                "repo": "pytest-dev/pytest",
                "instance_id": "pytest-dev__pytest-10624",
                "base_commit": "7421f3bb94df80ff2d131e932223b190f9b6d7b6",
                "version": "7.2",
                "environment_setup_commit": "572b5657d7ca557593418ce0319fabff88800c73"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_source_failing_fullsource",
                "testing/code/test_code.py::test_code_from_function_with_class",
                "testing/code/test_excinfo.py::TestTraceback_f_g_h::test_traceback_getcrashentry_empty",
                "testing/code/test_code.py::test_code_source",
                "testing/code/test_code.py::test_code_fullsource",
                "testing/code/test_code.py::test_code_from_func",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_traceback_short_no_source",
                "testing/code/test_code.py::test_code_gives_back_name_for_not_existing_file",
                "testing/code/test_excinfo.py::TestTraceback_f_g_h::test_traceback_cut",
                "testing/code/test_code.py::test_ne",
                "testing/code/test_excinfo.py::TestTraceback_f_g_h::test_traceback_getcrashentry",
                "testing/code/test_source.py::test_code_of_object_instance_with_call",
                "testing/code/test_code.py::test_code_getargs"
            ]
        },
        "ground_truth_class_body": "class coDE:\n    \"\"\"Wrapper around Python code objects.\"\"\"\n\n    __slots__ = (\"raw\",)\n\n    def __init__(self, obj: CodeType) -> None:\n        self.raw = obj\n\n    @classmethod\n    def FrOm_FUNcTiOn(cls, obj: object) -> \"coDE\":\n        return cls(getrawcode(obj))\n\n    def __eq__(self, other):\n        return self.raw == other.raw\n\n    # Ignore type because of https://github.com/python/mypy/issues/4266.\n    __hash__ = None  # type: ignore\n\n    @property\n    def firstlineno(self) -> int:\n        return self.raw.co_firstlineno - 1\n\n    @property\n    def name(self) -> str:\n        return self.raw.co_name\n\n    @property\n    def path(self) -> Union[Path, str]:\n        \"\"\"Return a path object pointing to source code, or an ``str`` in\n        case of ``OSError`` / non-existing file.\"\"\"\n        if not self.raw.co_filename:\n            return \"\"\n        try:\n            p = ABsolutEPATH(self.raw.co_filename)\n            # maybe don't try this checking\n            if not p.exists():\n                raise OSError(\"path check failed.\")\n            return p\n        except OSError:\n            # XXX maybe try harder like the weird logic\n            # in the standard lib [linecache.updatecache] does?\n            return self.raw.co_filename\n\n    @property\n    def fullsource(self) -> Optional[\"sOUrCe\"]:\n        \"\"\"Return a _pytest._code.Source object for the full source file of the code.\"\"\"\n        full, _ = findsource(self.raw)\n        return full\n\n    def source(self) -> \"sOUrCe\":\n        \"\"\"Return a _pytest._code.Source object for the code object's source only.\"\"\"\n        # return source only for that part of code\n        return sOUrCe(self.raw)\n\n    def gEtARGs(self, var: bool = False) -> Tuple[str, ...]:\n        \"\"\"Return a tuple with the argument names for the code object.\n\n        If 'var' is set True also return the names of the variable and\n        keyword arguments when present.\n        \"\"\"\n        # Handy shortcut for getting args.\n        raw = self.raw\n        argcount = raw.co_argcount\n        if var:\n            argcount += raw.co_flags & CO_VARARGS\n            argcount += raw.co_flags & CO_VARKEYWORDS\n        return raw.co_varnames[:argcount]"
    },
    {
        "task_id": "pydata__xarray-7444_CfDATetIMECOdeR",
        "class_name": "CfDATetIMECOdeR",
        "file": "pydata__xarray-7444/xarray/coding/times.py",
        "sketchy_description": "The 'CfDATetIMECOdeR' class is a subclass of 'VaRiAbLecODEr' and does not have any class decorators. The class is designed to handle encoding and decoding of datetime objects for use with CF (Climate and Forecast) conventions in xarray.\n\nThe class has an '__init__' method that takes a single optional argument 'use_cftime' which can be of type bool or None, with a default value of None. This method initializes an instance of 'CfDATetIMECOdeR' by setting the instance variable 'use_cftime' to the provided value.\n(1) Input arguments: 'use_cftime' (bool | None, default=None)\n(2) Return type: None\n(3) Functionality: Initializes the 'CfDATetIMECOdeR' instance with the specified 'use_cftime' value.\n\nThe class has a method named 'encode' which takes two arguments, 'variable' of type 'VaRIABLe' and an optional 'name' of type 'T_Name' with a default value of None. This method is responsible for encoding the given variable if it is of datetime type and returns the encoded variable.\n(1) Input arguments: 'variable' (VaRIABLe), 'name' (T_Name, default=None)\n(2) Return type: VaRIABLe\n(3) Functionality: Encodes the given variable if it is of datetime type and returns the encoded variable.\n\nAnother method in the class is 'decode', which also takes two arguments, 'variable' of type 'VaRIABLe' and an optional 'name' of type 'T_Name' with a default value of None. This method decodes a variable that has units of time specified as a string containing 'since', converting it to a datetime object if applicable, and returns the variable.\n(1) Input arguments: 'variable' (VaRIABLe), 'name' (T_Name, default=None)\n(2) Return type: VaRIABLe\n(3) Functionality: Decodes a variable with time units into a datetime object if applicable and returns the variable.\n\nThe class does not have any class variables or properties accessible. The only instance variable accessible is 'use_cftime', which indicates whether to use cftime objects for decoding times outside the pandas.Timestamp valid range.",
        "detailed_description": "The 'CfDATetIMECOdeR' class is a subclass of 'VaRiAbLecODEr'. It contains an '__init__' method, an 'encode' method, and a 'decode' method.\n\nThe '__init__' method takes an optional boolean argument 'use_cftime' which defaults to 'None'. It initializes the instance variable 'use_cftime' with the value of the 'use_cftime' argument.\n\nThe 'encode' method takes two arguments, 'variable' of type 'VaRIABLe' and an optional 'name' of type 'T_Name' which defaults to 'None'. It returns a 'VaRIABLe' object. The method checks if the data type of 'variable.data' is a subtype of 'np.datetime64' or if 'variable' contains 'cfTIME_daTETIMEs'. If the condition is true, it unpacks 'variable' into 'dims', 'data', 'attrs', and 'encoding' using the 'UnPaCk_For_ENCodINg' function. It then encodes 'data' into 'data', 'units', and 'calendar' using the 'eNCoDE_Cf_dATetImE' function and removes 'units' and 'calendar' from 'encoding'. It sets 'units' and 'calendar' in 'attrs' using the 'SaFE_sEtiTEm' function and returns a new 'VaRIABLe' object with 'dims', 'data', 'attrs', and 'encoding'. If the condition is false, it simply returns 'variable'.\n\nThe 'decode' method also takes two arguments, 'variable' of type 'VaRIABLe' and an optional 'name' of type 'T_Name' which defaults to 'None'. It returns a 'VaRIABLe' object. The method checks if 'units' is a string and contains 'since'. If the condition is true, it unpacks 'variable' into 'dims', 'data', 'attrs', and 'encoding' using the 'UnpAcK_fOr_dECoDINg' function. It then pops 'units' and 'calendar' from 'attrs' and 'encoding' using the 'PoP_tO' function and determines the data type using the '_DeCode_CF_DAteTimE_dTyPE' function. It transforms 'data' using the 'dEcODe_CF_DAteTiMe' function and the 'lAZY_elEmWIse_FuNC' function and returns a new 'VaRIABLe' object with 'dims', 'data', 'attrs', and 'encoding'. If the condition is false, it simply returns 'variable'.",
        "repo_metadata": {
            "commit_id": "bb7eb65e8c258f3ab0bcfb139554c2bb3d34aafd",
            "issue_id": "pydata__xarray-7444",
            "setup_details": {
                "repo": "pydata/xarray",
                "instance_id": "pydata__xarray-7444",
                "base_commit": "821dc24b5f3ed91b843a634bf8513a26046269ef",
                "version": "2022.09",
                "environment_setup_commit": "087ebbb78668bdf5d2d41c3b2553e3f29ce75be1"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "xarray/tests/test_coding_times.py::test_decoded_cf_datetime_array_2d",
                "xarray/tests/test_coding_times.py::test_scalar_unit",
                "xarray/tests/test_dataarray.py::TestIrisConversion::test_to_and_from_iris_dask",
                "xarray/tests/test_dataarray.py::TestIrisConversion::test_to_and_from_iris"
            ]
        },
        "ground_truth_class_body": "class CfDATetIMECOdeR(VaRiAbLecODEr):\n    def __init__(self, use_cftime: bool | None = None) -> None:\n        self.use_cftime = use_cftime\n\n    def encode(self, variable: VaRIABLe, name: T_Name = None) -> VaRIABLe:\n        if np.issubdtype(\n            variable.data.dtype, np.datetime64\n        ) or contAIns_cfTIME_daTETIMEs(variable):\n            dims, data, attrs, encoding = UnPaCk_For_ENCodINg(variable)\n\n            (data, units, calendar) = eNCoDE_Cf_dATetImE(\n                data, encoding.pop(\"units\", None), encoding.pop(\"calendar\", None)\n            )\n            SaFE_sEtiTEm(attrs, \"units\", units, name=name)\n            SaFE_sEtiTEm(attrs, \"calendar\", calendar, name=name)\n\n            return VaRIABLe(dims, data, attrs, encoding, fastpath=True)\n        else:\n            return variable\n\n    def decode(self, variable: VaRIABLe, name: T_Name = None) -> VaRIABLe:\n        units = variable.attrs.get(\"units\", None)\n        if isinstance(units, str) and \"since\" in units:\n            dims, data, attrs, encoding = UnpAcK_fOr_dECoDINg(variable)\n\n            units = PoP_tO(attrs, encoding, \"units\")\n            calendar = PoP_tO(attrs, encoding, \"calendar\")\n            dtype = _DeCode_CF_DAteTimE_dTyPE(data, units, calendar, self.use_cftime)\n            transform = partial(\n                dEcODe_CF_DAteTiMe,\n                units=units,\n                calendar=calendar,\n                use_cftime=self.use_cftime,\n            )\n            data = lAZY_elEmWIse_FuNC(data, transform, dtype)\n\n            return VaRIABLe(dims, data, attrs, encoding, fastpath=True)\n        else:\n            return variable"
    },
    {
        "task_id": "pydata__xarray-7444_cFscALeOfFSeTcodER",
        "class_name": "cFscALeOfFSeTcodER",
        "file": "pydata__xarray-7444/xarray/coding/variables.py",
        "sketchy_description": "The 'cFscALeOfFSeTcodER' class is a subclass of 'VaRiAbLecODEr'. It does not have any class or instance variables, nor does it have any properties. It does not have any decorators. The class has two methods, 'encode' and 'decode'. \n\nThe 'encode' method takes two arguments, 'variable' and 'name'. 'variable' is of type 'VaRIABLe' and 'name' is of type 'T_Name' with a default value of 'None'. The method returns a 'VaRIABLe' type. The 'encode' method encodes the given 'variable' using a scale factor and offset if provided. \n\nThe 'decode' method also takes two arguments, 'variable' and 'name', similar to the 'encode' method. The 'decode' method decodes the given 'variable' and returns the decoded variable. If the 'variable' does not contain a 'scale_factor' or 'add_offset', the original 'variable' is returned.",
        "detailed_description": "The `cFscALeOfFSeTcodER` class is a subclass of `VaRiAbLecODEr` and is designed to scale and offset variables according to CF (Climate and Forecast) conventions. The class implements the formula `decode_values = encoded_values * scale_factor + add_offset` to adjust the values of variables.\n\nThe class contains two methods: `encode` and `decode`.\n\nThe `encode` method takes two arguments: `variable` of type `VaRIABLe` and an optional `name` of type `T_Name` with a default value of `None`. It returns an object of type `VaRIABLe`. The method begins by unpacking the input `variable` into its dimensions, data, attributes, and encoding using the `UnPaCk_For_ENCodINg` function. If the encoding contains a `scale_factor` or `add_offset`, the method determines the appropriate floating-point data type for the operation using the `_cHOOSE_fLoat_dTyPE` function, considering whether `add_offset` is present. The data is then cast to this data type. If `add_offset` is in the encoding, it is subtracted from the data using the `PoP_tO` function, which also removes it from the attributes. Similarly, if `scale_factor` is in the encoding, the data is divided by it. The method concludes by returning a new `VaRIABLe` object constructed with the modified data and original dimensions, attributes, and encoding, with `fastpath` set to `True`.\n\nThe `decode` method also takes two arguments: `variable` of type `VaRIABLe` and an optional `name` of type `T_Name` with a default value of `None`. It returns an object of type `VaRIABLe`. This method checks if the attributes of the input `variable` contain `scale_factor` or `add_offset`. If they do, it unpacks the variable into its components using `UnpAcK_fOr_dECoDINg`. It then retrieves and removes the `scale_factor` and `add_offset` from the attributes and encoding, respectively, using the `PoP_tO` function. The appropriate floating-point data type is selected with `_cHOOSE_fLoat_dTyPE`. If `scale_factor` or `add_offset` are arrays, they are converted to scalar values. A transformation function `_scALE_oFFseT_deCoDInG` is partially applied with the retrieved scale factor, add offset, and data type. The `lAZY_elEmWIse_FuNC` function is then used to apply this transformation lazily to the data, and the method returns a new `VaRIABLe` object with the transformed data. If neither `scale_factor` nor `add_offset` is present in the attributes, the method simply returns the input `variable` unchanged.\n\nIn summary, the `cFscALeOfFSeTcodER` class provides functionality to encode and decode data by applying or reversing a scale and offset transformation, following CF conventions. It uses helper functions to handle data type selection, attribute manipulation, and lazy function application to efficiently process the variable data.",
        "repo_metadata": {
            "commit_id": "bb7eb65e8c258f3ab0bcfb139554c2bb3d34aafd",
            "issue_id": "pydata__xarray-7444",
            "setup_details": {
                "repo": "pydata/xarray",
                "instance_id": "pydata__xarray-7444",
                "base_commit": "821dc24b5f3ed91b843a634bf8513a26046269ef",
                "version": "2022.09",
                "environment_setup_commit": "087ebbb78668bdf5d2d41c3b2553e3f29ce75be1"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "xarray/tests/test_coding.py::test_scaling_converts_to_float32[u1]",
                "xarray/tests/test_coding.py::test_scaling_converts_to_float32[u2]",
                "xarray/tests/test_coding.py::test_scaling_converts_to_float32[i1]",
                "xarray/tests/test_coding.py::test_scaling_converts_to_float32[i2]",
                "xarray/tests/test_coding.py::test_scaling_converts_to_float32[f2]",
                "xarray/tests/test_coding.py::test_scaling_converts_to_float32[f4]",
                "properties/test_encode_decode.py::test_CFScaleOffset_coder_roundtrip",
                "xarray/tests/test_coding.py::test_scaling_offset_as_list[0.1-10]",
                "xarray/tests/test_coding.py::test_scaling_offset_as_list[0.1-scale_factor1]",
                "xarray/tests/test_coding.py::test_scaling_offset_as_list[add_offset1-10]",
                "xarray/tests/test_coding.py::test_scaling_offset_as_list[add_offset1-scale_factor1]"
            ]
        },
        "ground_truth_class_body": "class cFscALeOfFSeTcodER(VaRiAbLecODEr):\n    \"\"\"Scale and offset variables according to CF conventions.\n\n    Follows the formula:\n        decode_values = encoded_values * scale_factor + add_offset\n    \"\"\"\n\n    def encode(self, variable: VaRIABLe, name: T_Name = None) -> VaRIABLe:\n        dims, data, attrs, encoding = UnPaCk_For_ENCodINg(variable)\n\n        if \"scale_factor\" in encoding or \"add_offset\" in encoding:\n            dtype = _cHOOSE_fLoat_dTyPE(data.dtype, \"add_offset\" in encoding)\n            data = data.astype(dtype=dtype, copy=True)\n        if \"add_offset\" in encoding:\n            data -= PoP_tO(encoding, attrs, \"add_offset\", name=name)\n        if \"scale_factor\" in encoding:\n            data /= PoP_tO(encoding, attrs, \"scale_factor\", name=name)\n\n        return VaRIABLe(dims, data, attrs, encoding, fastpath=True)\n\n    def decode(self, variable: VaRIABLe, name: T_Name = None) -> VaRIABLe:\n        _attrs = variable.attrs\n        if \"scale_factor\" in _attrs or \"add_offset\" in _attrs:\n            dims, data, attrs, encoding = UnpAcK_fOr_dECoDINg(variable)\n\n            scale_factor = PoP_tO(attrs, encoding, \"scale_factor\", name=name)\n            add_offset = PoP_tO(attrs, encoding, \"add_offset\", name=name)\n            dtype = _cHOOSE_fLoat_dTyPE(data.dtype, \"add_offset\" in encoding)\n            if np.ndim(scale_factor) > 0:\n                scale_factor = np.asarray(scale_factor).item()\n            if np.ndim(add_offset) > 0:\n                add_offset = np.asarray(add_offset).item()\n            transform = partial(\n                _scALE_oFFseT_deCoDInG,\n                scale_factor=scale_factor,\n                add_offset=add_offset,\n                dtype=dtype,\n            )\n            data = lAZY_elEmWIse_FuNC(data, transform, dtype)\n\n            return VaRIABLe(dims, data, attrs, encoding, fastpath=True)\n        else:\n            return variable"
    },
    {
        "task_id": "litestar-org__litestar-0001_MemoryStore",
        "class_name": "MemoryStore",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/stores/memory.py",
        "sketchy_description": "The 'MemoryStore' class is a subclass of 'Store' and does not have any class decorators. It is designed to provide an in-memory storage mechanism for key-value pairs, with optional expiration times for the keys. The class uses `__slots__` to explicitly declare data members `_store` and `_lock` to potentially save memory and prevent the creation of a `__dict__` for each instance.\n\nThe class has an '__init__' method that takes no arguments and returns `None`. This method initializes the `MemoryStore` class by setting up the internal storage and locking mechanisms used to manage the in-memory store.\n\nThe 'set' method takes three arguments: `key` of type `str`, `value` of type `str` or `bytes`, and an optional `expires_in` of type `int`, `timedelta`, or `None`. It returns `None`. This method associates the given `key` with the provided `value` in the memory store. If `expires_in` is provided, the key-value pair will expire after the specified time.\n\nThe 'get' method takes two arguments: `key` of type `str` and an optional `renew_for` of type `int`, `timedelta`, or `None`. It returns the value associated with `key` as `bytes` or `None` if the key does not exist or has expired. If `renew_for` is provided and the key had an initial expiry time, this method will renew the expiry time for the key by the specified amount.\n\nThe 'delete' method takes one argument: `key` of type `str`. It returns `None`. This method deletes the value associated with the given `key` from the memory store. If the key does not exist, the method does nothing.\n\nThe 'delete_all' method takes no arguments and returns `None`. This method deletes all key-value pairs stored in the memory store.\n\nThe 'delete_expired' method also takes no arguments and returns `None`. This method is used to delete all expired items from the memory store. Since expired items are normally cleared on access, this method should be called at regular intervals to free up memory.\n\nThe 'exists' method takes one argument: `key` of type `str`. It returns a `bool` indicating whether the given `key` exists in the memory store.\n\nThe 'expires_in' method takes one argument: `key` of type `str`. It returns an `int` representing the number of seconds until the `key` expires, or `None` if the key does not exist or has no expiry time set.\n\nInstance variables `_store` and `_lock` are used internally to manage the storage and synchronization of the key-value pairs in the memory store. No properties are accessible in this class.",
        "detailed_description": "The 'MemoryStore' class is a subclass of 'Store' and represents an in-memory, atomic, asynchronous key/value store. The class has two instance variables, '_store' and '_lock', which are defined in the '__slots__' attribute for memory optimization. The '_store' variable is a dictionary where keys are strings and values are 'StorageObject' instances. The '_lock' variable is an instance of 'Lock' which is used to ensure thread safety.\n\nThe class has an '__init__' method that initializes the '_store' instance variable to an empty dictionary and the '_lock' instance variable to a new 'Lock' instance. This method does not take any arguments and does not return anything.\n\nThe 'set' method is an asynchronous method that takes three arguments: 'key' of type str, 'value' of type str or bytes, and 'expires_in' of type int, timedelta, or None with a default value of None. This method sets a value in the '_store' dictionary with the given 'key' and 'value'. If the 'value' is a string, it is encoded to bytes using the 'utf-8' encoding. The 'value' is wrapped in a 'StorageObject' instance with the given 'expires_in' value. The method does not return anything.\n\nThe 'get' method is an asynchronous method that takes two arguments: 'key' of type str and 'renew_for' of type int, timedelta, or None with a default value of None. This method returns the value associated with the given 'key' in the '_store' dictionary if it exists and is not expired, else it returns None. If the 'renew_for' argument is given and the value had an initial expiry time set, the expiry time is renewed for 'renew_for' seconds. If the value has not been set with an expiry time this operation is a no-op.\n\nThe 'delete' method is an asynchronous method that takes one argument: 'key' of type str. This method deletes the value associated with the given 'key' in the '_store' dictionary. If no such key exists, this operation is a no-op. The method does not return anything.\n\nThe 'delete_all' method is an asynchronous method that does not take any arguments. This method deletes all stored values in the '_store' dictionary. The method does not return anything.\n\nThe 'delete_expired' method is an asynchronous method that does not take any arguments. This method deletes expired items in the '_store' dictionary. Since expired items are normally only cleared on access (i.e., when calling the 'get' method), this method should be called in regular intervals to free memory. The method does not return anything.\n\nThe 'exists' method is an asynchronous method that takes one argument: 'key' of type str. This method checks if a given 'key' exists in the '_store' dictionary and returns a boolean value.\n\nThe 'expires_in' method is an asynchronous method that takes one argument: 'key' of type str. This method returns the time in seconds the given 'key' expires in. If no such 'key' exists or no expiry time was set, the method returns None.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/examples/test_stores.py::test_delete_expired_after_response",
                "tests/examples/test_stores.py::test_registry",
                "tests/unit/test_stores.py::test_registry_get",
                "tests/unit/test_stores.py::test_registry_register",
                "tests/unit/test_stores.py::test_registry_register_exist_raises",
                "tests/unit/test_stores.py::test_registry_register_exist_override",
                "tests/unit/test_middleware/test_session/test_server_side_backend.py::test_non_default_store",
                "tests/unit/test_middleware/test_session/test_server_side_backend.py::test_get_set",
                "tests/unit/test_middleware/test_session/test_server_side_backend.py::test_set_store_name",
                "tests/unit/test_middleware/test_session/test_server_side_backend.py::test_get_renew_on_access",
                "tests/unit/test_middleware/test_session/test_server_side_backend.py::test_delete_idempotence",
                "tests/unit/test_middleware/test_session/test_server_side_backend.py::test_max_age_expires",
                "tests/unit/test_middleware/test_session/test_server_side_backend.py::test_delete",
                "tests/unit/test_middleware/test_session/test_server_side_backend.py::test_get_set_multiple_returns_correct_identity",
                "tests/unit/test_security/test_jwt/test_auth.py::test_jwt_cookie_auth",
                "tests/unit/test_security/test_jwt/test_auth.py::test_jwt_auth",
                "tests/unit/test_security/test_jwt/test_auth.py::test_oauth2_password_bearer_auth_openapi"
            ]
        },
        "ground_truth_class_body": "class MemoryStore(Store):\n    \"\"\"In memory, atomic, asynchronous key/value store.\"\"\"\n\n    __slots__ = (\"_store\", \"_lock\")\n\n    def __init__(self) -> None:\n        \"\"\"Initialize :class:`MemoryStore`\"\"\"\n        self._store: dict[str, StorageObject] = {}\n        self._lock = Lock()\n\n    async def set(self, key: str, value: str | bytes, expires_in: int | timedelta | None = None) -> None:\n        \"\"\"Set a value.\n\n        Args:\n            key: Key to associate the value with\n            value: Value to store\n            expires_in: Time in seconds before the key is considered expired\n\n        Returns:\n            ``None``\n        \"\"\"\n        if isinstance(value, str):\n            value = value.encode(\"utf-8\")\n        async with self._lock:\n            self._store[key] = StorageObject.new(data=value, expires_in=expires_in)\n\n    async def get(self, key: str, renew_for: int | timedelta | None = None) -> bytes | None:\n        \"\"\"Get a value.\n\n        Args:\n            key: Key associated with the value\n            renew_for: If given and the value had an initial expiry time set, renew the\n                expiry time for ``renew_for`` seconds. If the value has not been set\n                with an expiry time this is a no-op\n\n        Returns:\n            The value associated with ``key`` if it exists and is not expired, else\n            ``None``\n        \"\"\"\n        async with self._lock:\n            storage_obj = self._store.get(key)\n\n            if not storage_obj:\n                return None\n\n            if storage_obj.expired:\n                self._store.pop(key)\n                return None\n\n            if renew_for and storage_obj.expires_at:\n                # don't use .set() here, so we can hold onto the lock for the whole operation\n                storage_obj = StorageObject.new(data=storage_obj.data, expires_in=renew_for)\n                self._store[key] = storage_obj\n\n            return storage_obj.data\n\n    async def delete(self, key: str) -> None:\n        \"\"\"Delete a value.\n\n        If no such key exists, this is a no-op.\n\n        Args:\n            key: Key of the value to delete\n        \"\"\"\n        async with self._lock:\n            self._store.pop(key, None)\n\n    async def delete_all(self) -> None:\n        \"\"\"Delete all stored values.\"\"\"\n        async with self._lock:\n            self._store.clear()\n\n    async def delete_expired(self) -> None:\n        \"\"\"Delete expired items.\n\n        Since expired items are normally only cleared on access (i.e. when calling\n        :meth:`.get`), this method should be called in regular intervals\n        to free memory.\n        \"\"\"\n        async with self._lock:\n            new_store = {}\n            for i, (key, storage_obj) in enumerate(self._store.items()):\n                if not storage_obj.expired:\n                    new_store[key] = storage_obj\n                if i % 1000 == 0:\n                    await anyio.sleep(0)\n            self._store = new_store\n\n    async def exists(self, key: str) -> bool:\n        \"\"\"Check if a given ``key`` exists.\"\"\"\n        return key in self._store\n\n    async def expires_in(self, key: str) -> int | None:\n        \"\"\"Get the time in seconds ``key`` expires in. If no such ``key`` exists or no\n        expiry time was set, return ``None``.\n        \"\"\"\n        if storage_obj := self._store.get(key):\n            return storage_obj.expires_in\n        return None"
    },
    {
        "task_id": "litestar-org__litestar-0001_PydanticDTO",
        "class_name": "PydanticDTO",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/contrib/pydantic/pydantic_dto_factory.py",
        "sketchy_description": "The 'PydanticDTO' class is a subclass of 'AbstractDTO[T]' and 'Generic[T]'. The class has four methods and four class variables. \n\nThe first method, 'decode_builtins', takes a dictionary 'value' as an argument and returns any type. This method decodes built-in types and raises a ValidationException if a validation error occurs. \n\nThe second method, 'decode_bytes', takes a 'value' of type bytes as an argument and returns any type. This method decodes bytes to a PydanticDTO.\n\nThe third method, 'generate_field_definitions', is a class method that takes 'model_type' as an argument. The 'model_type' should be a subclass of either 'pydantic_v1.BaseModel' or 'pydantic_v2.BaseModel'. This method generates field definitions for a given model type and returns a generator yielding DTOFieldDefinition instances.\n\nThe fourth method, 'detect_nested_field', is a class method that takes 'field_definition' as an argument. This method checks if the field definition is a subclass of the BaseModel and returns True if it is, False otherwise.\n\nThe class variables include '__slots__', 'config', 'model_type', and '_dto_backends'. The '__slots__' variable is a tuple containing 'asgi_connection'. The 'config' variable is of type ClassVar[DTOConfig]. The 'model_type' variable is of type 'type[T]'. The '_dto_backends' variable is a dictionary of type ClassVar[dict[str, _BackendDict]] and is initialized to an empty dictionary.\n\nThe instance variable 'asgi_connection' is accessible. There are no accessible properties.",
        "detailed_description": "The `PydanticDTO` class is a subclass of `AbstractDTO[T]` and `Generic[T]` and provides support for domain modelling with Pydantic. Here, `T = TypeVar(\"T\", bound=\"ModelType | Collection[ModelType]\")`.\n\nThe class has a method `decode_builtins` that overrides the superclass method. It takes a dictionary `value` as an argument where the keys are strings and the values can be of any type. The method tries to call the superclass `decode_builtins` method with the given `value`. If a `ValidationErrorV2` or `ValidationErrorV1` exception is raised, the method raises a `ValidationException` with the errors from the exception as the `extra` argument. The return type of this method is `Any`.\n\nThe `decode_bytes` method also overrides the superclass method. It takes a `bytes` argument `value`. The method tries to call the superclass `decode_bytes` method with the given `value`. If a `ValidationErrorV2` or `ValidationErrorV1` exception is raised, the method raises a `ValidationException` with the errors from the exception as the `extra` argument. The return type of this method is `Any`.\n\nThe `decode_bytes` method overrides the superclass method. It accepts a `bytes` argument named `value`. The method attempts to invoke the superclass`s `decode_bytes` method with the provided `value`. If a `ValidationErrorV2` or `ValidationErrorV1` exception occurs during this process, the method raises a new `ValidationException`, incorporating the errors from the original exception as the `extra` argument. This is done using the `from ` syntax to chain the exceptions, indicating that the `ValidationException` is a direct result of the caught validation error. The return type of the `decode_bytes` method is `Any`.\n\nThe class has a class method `generate_field_definitions` that takes a `model_type` argument of type `pydantic_v1.BaseModel` or `pydantic_v2.BaseModel`. It returns a generator that yields instances of the DTOFieldDefinition class. \n\n\n This method is responsible for generating field definitions for a given Pydantic model class. It starts by calling the class method `get_model_type_hints` to potentially retrieve type hints from the model class. Then it retrieves the model's fields information using either model_type.model_fields (for Pydantic v1) or iterates over the __fields__ attribute (for Pydantic v2) to handle potential compatibility issues between Pydantic version 1 and 2.\nThe method then iterates over each field name and its corresponding field information. For each field, it extracts the field definition using the retrieved model field definitions and any custom field definition information stored using the DTO_FIELD_META_KEY. If the `field_info` has a default value (checked using `is_pydantic_undefined`), the method sets `default` to the default value of the `field_info`. If the `field_definition` is optional, the method sets `default` to `None`. Otherwise, the method sets `default` to `Empty`. \n\nThe method then yields a `DTOFieldDefinition` instance created from the `field_definition`, `dto_field`, `model_type` name, and `field_info` default factory if it is not undefined (checked using `is_pydantic_undefined`). The `default` and `name` are replaced in the `DTOFieldDefinition` instance using replace() methos. `DTOFieldDefinition` instance is created using `from_field_definition`\n#######\n\nThe class also has a class method `detect_nested_field` that takes a `field_definition` argument of type `FieldDefinition`. The method checks if `pydantic_v2` is not `Empty` and returns whether the `field_definition` is a subclass of `pydantic_v1.BaseModel` and `pydantic_v2.BaseModel`. If `pydantic_v2` is `Empty`, the method returns whether the `field_definition` is a subclass of `pydantic_v1.BaseModel`. The return type of this method is `bool`. Also, subclass checks are performed using `is_subclass_of` method.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_contrib/test_pydantic/test_dto.py::test_schema_required_fields_with_pydantic_dto[default_backend-v2]",
                "tests/unit/test_contrib/test_pydantic/test_dto.py::test_schema_required_fields_with_pydantic_dto[experimental_backend-v2]",
                "tests/unit/test_contrib/test_pydantic/test_beanie_integration.py::test_generate_field_definitions_from_beanie_models[v1]",
                "tests/unit/test_contrib/test_pydantic/test_dto.py::test_schema_required_fields_with_pydantic_dto[experimental_backend-v1]",
                "tests/unit/test_contrib/test_pydantic/test_beanie_integration.py::test_generate_field_definitions_from_beanie_models[v2]",
                "tests/unit/test_contrib/test_pydantic/test_dto.py::test_detect_nested_field_pydantic_v1",
                "tests/unit/test_contrib/test_pydantic/test_integration.py::test_pydantic_v2_validation_error_raises_400[None]",
                "tests/unit/test_contrib/test_pydantic/test_dto.py::test_field_definition_implicit_optional_default[v2]",
                "tests/unit/test_contrib/test_pydantic/test_dto.py::test_schema_required_fields_with_pydantic_dto[default_backend-v1]",
                "tests/unit/test_contrib/test_pydantic/test_dto.py::test_field_definition_implicit_optional_default[v1]",
                "tests/unit/test_contrib/test_pydantic/test_integration.py::test_pydantic_v1_validation_error_raises_400[None]",
                "tests/unit/test_contrib/test_pydantic/test_integration.py::test_pydantic_v2_validation_error_raises_400[meta1]",
                "tests/unit/test_contrib/test_pydantic/test_integration.py::test_pydantic_v1_validation_error_raises_400[meta1]",
                "tests/unit/test_contrib/test_pydantic/test_pydantic_dto_factory.py::test_field_definition_generation_v1",
                "tests/unit/test_contrib/test_pydantic/test_pydantic_dto_factory.py::test_detect_nested_field[v2]",
                "tests/unit/test_contrib/test_pydantic/test_pydantic_dto_factory.py::test_field_definition_generation_v2",
                "tests/unit/test_contrib/test_pydantic/test_pydantic_dto_factory.py::test_detect_nested_field[v1]"
            ]
        },
        "ground_truth_class_body": "class PydanticDTO(AbstractDTO[T], Generic[T]):\n    \"\"\"Support for domain modelling with Pydantic.\"\"\"\n\n    @override\n    def decode_builtins(self, value: dict[str, Any]) -> Any:\n        try:\n            return super().decode_builtins(value)\n        except (ValidationErrorV2, ValidationErrorV1) as ex:\n            raise ValidationException(extra=ex.errors()) from ex\n\n    @override\n    def decode_bytes(self, value: bytes) -> Any:\n        try:\n            return super().decode_bytes(value)\n        except (ValidationErrorV2, ValidationErrorV1) as ex:\n            raise ValidationException(extra=ex.errors()) from ex\n\n    @classmethod\n    def generate_field_definitions(\n        cls, model_type: type[pydantic_v1.BaseModel | pydantic_v2.BaseModel]\n    ) -> Generator[DTOFieldDefinition, None, None]:\n        model_field_definitions = cls.get_model_type_hints(model_type)\n\n        model_fields: dict[str, pydantic_v1.fields.FieldInfo | pydantic_v2.fields.FieldInfo]\n        try:\n            model_fields = dict(model_type.model_fields)  # type: ignore[union-attr]\n        except AttributeError:\n            model_fields = {\n                k: model_field.field_info\n                for k, model_field in model_type.__fields__.items()  # type: ignore[union-attr]\n            }\n\n        for field_name, field_info in model_fields.items():\n            field_definition = model_field_definitions[field_name]\n            dto_field = (field_definition.extra or {}).pop(DTO_FIELD_META_KEY, DTOField())\n\n            if not is_pydantic_undefined(field_info.default):\n                default = field_info.default\n            elif field_definition.is_optional:\n                default = None\n            else:\n                default = Empty\n\n            yield replace(\n                DTOFieldDefinition.from_field_definition(\n                    field_definition=field_definition,\n                    dto_field=dto_field,\n                    model_name=model_type.__name__,\n                    default_factory=field_info.default_factory\n                    if field_info.default_factory and not is_pydantic_undefined(field_info.default_factory)\n                    else None,\n                ),\n                default=default,\n                name=field_name,\n            )\n\n    @classmethod\n    def detect_nested_field(cls, field_definition: FieldDefinition) -> bool:\n        if pydantic_v2 is not Empty:  # type: ignore[comparison-overlap]\n            return field_definition.is_subclass_of((pydantic_v1.BaseModel, pydantic_v2.BaseModel))\n        return field_definition.is_subclass_of(pydantic_v1.BaseModel)"
    },
    {
        "task_id": "litestar-org__litestar-0001_FileSystemAdapter",
        "class_name": "FileSystemAdapter",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/file_system.py",
        "sketchy_description": "The 'FileSystemAdapter' class is a part of the 'litestar.file_system' module. It does not have any class decorators. This class is designed to adapt a given file system to conform to a specific protocol, presumably 'FileSystemProtocol'.\n\n1. The '__init__' method takes one argument, 'file_system', which must adhere to the 'FileSystemProtocol'. This method does not return anything as it is a constructor. Its purpose is to initialize an instance of the 'FileSystemAdapter' class with the provided 'file_system' object.\n\n2. The 'info' method takes one argument, 'path', which is of type 'PathType'. It returns an object of type 'FileInfo'. This method acts as a proxy to the underlying file system's 'info' method, ensuring that the call is made asynchronously and with strong typing. It retrieves information about the file located at the given 'path'.\n\n3. The 'open' method takes three arguments: 'file' of type 'PathType', 'mode' of type 'OpenBinaryMode' with a default value of \"rb\", and 'buffering' of type int with a default value of -1. It returns an 'AsyncFile[bytes]' object. This method is designed to return a file-like object from the file system that can be used within a context manager (i.e., a 'with' block). The 'mode' and 'buffering' parameters dictate how the file is opened and how the I/O operations are buffered, respectively.\n\n4. The 'parse_stat_result' method is a static method that takes two arguments: 'path' of type 'PathType' and 'result' of type 'stat_result'. It returns an object of type 'FileInfo'. This method converts a 'stat_result' instance, which contains raw statistical information about a file, into a more structured 'FileInfo' object. This conversion allows for easier access and interpretation of the file's metadata.\n\nThe class does not have any class variables or properties accessible. However, it has one instance variable:\n\n- 'file_system': This variable holds the reference to the file system object that the adapter is wrapping. It is expected to conform to the 'FileSystemProtocol' interface, which defines the methods and properties that the file system must provide.\n\nOverall, the 'FileSystemAdapter' class serves as a bridge between a file system that follows the 'FileSystemProtocol' and code that requires file system operations to be performed in a specific manner, such as asynchronously or with strong typing.",
        "detailed_description": "The `FileSystemAdapter` class is designed as a wrapper around a `FileSystemProtocol`, with the purpose of normalizing its interface. This class ensures that the underlying file system's methods are accessed in a consistent and asynchronous manner, with strong typing for return values and arguments.\n\nThe class is initialized through the `__init__` method, which takes a single argument `file_system` of type `FileSystemProtocol`. This method simply stores the provided `file_system` object in an instance variable for later use.\n\nThe `info` method is an asynchronous method that takes a single argument `path` of type `PathType`. It acts as a proxy to the underlying file system's `info` method. Internally, it checks if the `info` method of the provided file system is an asynchronous callable; if so, it directly awaits the result. If the `info` method is not asynchronous, it uses the `sync_to_thread` function to run the method in a separate thread, ensuring non-blocking behavior. The method returns a `FileInfo` dictionary containing details about the file at the given path. It handles `FileNotFoundError` and `PermissionError` by re-raising them as appropriate exceptions, and any other `OSError` as an `InternalServerException`.\n\nThe `open` method is another asynchronous method that returns a file-like object from the filesystem. It accepts three arguments: `file` of type `PathType`, `mode` of type `OpenBinaryMode` with a default value of `\"rb\"`, and `buffering` of type `int` with a default value of `-1`. This method also checks if the `open` method of the underlying file system is asynchronous and either awaits the result or uses `sync_to_thread` to execute it in a separate thread. The return value is an `AsyncFile[bytes]` object that can be used in an asynchronous context manager (`with` block). It handles `PermissionError` and `OSError` by raising `NotAuthorizedException` and `InternalServerException`, respectively.\n\nLastly, the `parse_stat_result` static method is an asynchronous method that takes two arguments: `path` of type `PathType` and `result` of type `stat_result`. This method converts the `stat_result` instance into a `FileInfo` dictionary. It uses the `Path` class to determine if the given path is a symbolic link and to read the link destination if it is. It also attempts to get the actual size of the linked file, falling back to the size in the `stat_result` if it fails. The method returns the `FileInfo` dictionary with various file attributes such as creation time, group ID, inode number, link status, file mode, modification time, file name, number of hard links, file size, file type, and user ID. If the file is a symbolic link, it also includes the link destination and updates the file size accordingly.\n\nIn summary, the `FileSystemAdapter` class provides a consistent and asynchronous interface to interact with file systems that conform to the `FileSystemProtocol`. It abstracts away the details of checking for asynchronous capabilities and error handling, providing a clean and easy-to-use API for file system operations.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_file_system.py::test_file_adapter_open[file_system0]",
                "tests/unit/test_file_system.py::test_file_adapter_open_handles_file_not_found_exception[file_system0]",
                "tests/unit/test_file_system.py::test_file_adapter_info[file_system0]",
                "tests/unit/test_file_system.py::test_file_adapter_info_handles_permission_exception[file_system0]",
                "tests/unit/test_file_system.py::test_file_adapter_info_handles_permission_exception[file_system1]",
                "tests/unit/test_file_system.py::test_file_adapter_info_handles_file_not_found_exception[file_system1]",
                "tests/unit/test_file_system.py::test_file_adapter_open_handles_file_not_found_exception[file_system1]",
                "tests/unit/test_file_system.py::test_file_adapter_info[file_system1]",
                "tests/unit/test_file_system.py::test_file_adapter_open[file_system1]",
                "tests/unit/test_file_system.py::test_file_adapter_open_handles_permission_exception[file_system1]",
                "tests/unit/test_file_system.py::test_file_adapter_open_handles_permission_exception[file_system0]",
                "tests/unit/test_file_system.py::test_file_adapter_info_handles_file_not_found_exception[file_system0]",
                "tests/unit/test_response/test_file_response.py::test_file_iterator[4]",
                "tests/unit/test_response/test_file_response.py::test_file_iterator[16]",
                "tests/unit/test_response/test_file_response.py::test_file_iterator[256]",
                "tests/unit/test_response/test_file_response.py::test_file_iterator[1024]",
                "tests/unit/test_response/test_file_response.py::test_file_iterator[512]",
                "tests/unit/test_response/test_file_response.py::test_file_iterator[2048]",
                "tests/unit/test_response/test_file_response.py::test_file_iterator[8]"
            ]
        },
        "ground_truth_class_body": "class FileSystemAdapter:\n    \"\"\"Wrapper around a ``FileSystemProtocol``, normalising its interface.\"\"\"\n\n    def __init__(self, file_system: FileSystemProtocol) -> None:\n        \"\"\"Initialize an adapter from a given ``file_system``\n\n        Args:\n            file_system: A filesystem class adhering to the :class:`FileSystemProtocol <litestar.types.FileSystemProtocol>`\n        \"\"\"\n        self.file_system = file_system\n\n    async def info(self, path: PathType) -> FileInfo:\n        \"\"\"Proxies the call to the underlying FS Spec's ``info`` method, ensuring it's done in an async fashion and with\n        strong typing.\n\n        Args:\n            path: A file path to load the info for.\n\n        Returns:\n            A dictionary of file info.\n        \"\"\"\n        try:\n            awaitable = (\n                self.file_system.info(str(path))\n                if is_async_callable(self.file_system.info)\n                else sync_to_thread(self.file_system.info, str(path))\n            )\n            return cast(\"FileInfo\", await awaitable)\n        except FileNotFoundError as e:\n            raise e\n        except PermissionError as e:\n            raise NotAuthorizedException(f\"failed to read {path} due to missing permissions\") from e\n        except OSError as e:  # pragma: no cover\n            raise InternalServerException from e\n\n    async def open(\n        self,\n        file: PathType,\n        mode: OpenBinaryMode = \"rb\",\n        buffering: int = -1,\n    ) -> AsyncFile[bytes]:\n        \"\"\"Return a file-like object from the filesystem.\n\n        Notes:\n            - The return value must function correctly in a context ``with`` block.\n\n        Args:\n            file: Path to the target file.\n            mode: Mode, similar to the built ``open``.\n            buffering: Buffer size.\n        \"\"\"\n        try:\n            if is_async_callable(self.file_system.open):  # pyright: ignore\n                return cast(\n                    \"AsyncFile[bytes]\",\n                    await self.file_system.open(\n                        file=file,\n                        mode=mode,\n                        buffering=buffering,\n                    ),\n                )\n            return AsyncFile(await sync_to_thread(self.file_system.open, file, mode, buffering))  # type: ignore[arg-type]\n        except PermissionError as e:\n            raise NotAuthorizedException(f\"failed to open {file} due to missing permissions\") from e\n        except OSError as e:\n            raise InternalServerException from e\n\n    @staticmethod\n    async def parse_stat_result(path: PathType, result: stat_result) -> FileInfo:\n        \"\"\"Convert a ``stat_result`` instance into a ``FileInfo``.\n\n        Args:\n            path: The file path for which the :func:`stat_result <os.stat_result>` is provided.\n            result: The :func:`stat_result <os.stat_result>` instance.\n\n        Returns:\n            A dictionary of file info.\n        \"\"\"\n        file_info: FileInfo = {\n            \"created\": result.st_ctime,\n            \"gid\": result.st_gid,\n            \"ino\": result.st_ino,\n            \"islink\": await Path(path).is_symlink(),\n            \"mode\": result.st_mode,\n            \"mtime\": result.st_mtime,\n            \"name\": str(path),\n            \"nlink\": result.st_nlink,\n            \"size\": result.st_size,\n            \"type\": \"directory\" if S_ISDIR(result.st_mode) else \"file\",\n            \"uid\": result.st_uid,\n        }\n\n        if file_info[\"islink\"]:\n            file_info[\"destination\"] = str(await Path(path).readlink()).encode(\"utf-8\")\n            try:\n                file_info[\"size\"] = (await Path(path).stat()).st_size\n            except OSError:  # pragma: no cover\n                file_info[\"size\"] = result.st_size\n\n        return file_info"
    },
    {
        "task_id": "litestar-org__litestar-0001_Token",
        "class_name": "Token",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/security/jwt/token.py",
        "sketchy_description": "The 'Token' class is part of the 'litestar.security.jwt.token' module and is decorated with `@dataclass`. This class is responsible for encoding and decoding JSON Web Tokens (JWTs).\n\nThe class has a class method named 'decode' which takes three arguments: 'encoded_token' of type str, 'secret' which can be either a str or a dict of str to str, and 'algorithm' of type str. This method decodes the provided JWT string using the given secret and algorithm, and returns an instance of 'Token'. If the token is invalid, it raises a 'NotAuthorizedException'.\n\nThe 'encode' method is an instance method that takes two arguments: 'secret' of type str and 'algorithm' of type str. It encodes the instance of 'Token' into a JWT string using the provided secret and algorithm, and returns the encoded JWT string. If encoding fails, it raises an 'ImproperlyConfiguredException'.\n\nThe '__post_init__' method is automatically called after the object is initialized. It validates the 'sub' field to ensure it is a non-empty string, checks that the 'exp' field is a future datetime, and verifies that the 'iat' field is either the current time or a past time. This method does not take any arguments and does not return anything.\n\nClass variables include:\n- 'exp': a datetime object representing the expiration time of the token.\n- 'sub': a string representing the subject of the token.\n- 'iat': a datetime object representing the issued at time of the token, with a default value of the current time in UTC.\n- 'iss': an optional string representing the issuer of the token.\n- 'aud': an optional string representing the audience of the token.\n- 'jti': an optional string representing the JWT ID.\n- 'extras': a dictionary for any additional information that needs to be included in the token.\n\nInstance variables mirror the class variables and hold the actual data for each token instance.\n\nThere are no properties accessible in this class.",
        "detailed_description": "The 'Token' class is a dataclass that represents a JWT Token DTO. The class has several instance variables: 'exp', 'sub', 'iat', 'iss', 'aud', 'jti', and 'extras'. 'exp' is of type datetime and represents the expiration datetime of the token. 'sub' is a string that usually represents a unique identifier of the user or equivalent entity. 'iat' is a datetime that represents when the token was issued and is set to the current datetime in UTC by default. 'iss' is an optional string that represents the unique identifier for the issuer. 'aud' is an optional string that represents the intended audience. 'jti' is an optional string that represents a unique identifier of the JWT between different issuers. 'extras' is a dictionary of string keys and any type values that represents extra fields that were found on the JWT token and is set to an empty dictionary by default.\n\nThe class has a '__post_init__' method that does not take any arguments and does not return anything. This method checks if the 'sub' instance variable is a string with a length greater than 0 and raises an 'ImproperlyConfiguredException' if it is not. The method also checks if the 'exp' instance variable is a datetime in the future and if the 'iat' instance variable is a current or past time and raises an 'ImproperlyConfiguredException' if they are not.\n\nThe class has a class method named 'decode' that takes three arguments: 'encoded_token', 'secret', and 'algorithm'. 'encoded_token' is a base64 string containing an encoded JWT. 'secret' is the secret with which the JWT is encoded and can be a string or a dictionary of string keys and string values. 'algorithm' is the algorithm used to encode the JWT. This method returns a decoded 'Token' instance. The method tries to decode the 'encoded_token' using the 'jwt.decode' function with the given 'secret' and 'algorithm' and raises a 'NotAuthorizedException' if the decoding fails. The method then gets the 'exp' and 'iat' values from the decoded payload and removes them from the payload. The method also gets the extra fields from the payload and adds them to the 'extras' dictionary. The method then returns a new 'Token' instance with the decoded values.\n\nThe class has an 'encode' method that takes two arguments: 'secret' and 'algorithm'. 'secret' is the secret with which the JWT is encoded. 'algorithm' is the algorithm used to encode the JWT. This method returns an encoded token string. The method tries to encode the instance into a string using the 'jwt.encode' function with the given 'secret' and 'algorithm' and raises an 'ImproperlyConfiguredException' if the encoding fails. The method uses the 'asdict' function to get a dictionary of the instance's variables and their values and removes any variables with a value of 'None' before encoding.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_security/test_jwt/test_auth.py::test_oauth2_password_bearer_auth_openapi",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-None-None-None-HS384]",
                "tests/unit/test_security/test_jwt/test_auth.py::test_jwt_cookie_auth",
                "tests/unit/test_security/test_jwt/test_auth.py::test_path_exclusion",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-None-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS512]",
                "tests/unit/test_security/test_jwt/test_auth.py::test_type_encoders",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-None-None-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS256]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-None-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS256]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-None-None-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS384]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-None-None-None-HS512]",
                "tests/unit/test_security/test_jwt/test_auth.py::test_jwt_auth",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS256]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-None-HS384]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-None-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-None-HS512]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-None-HS512]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS384]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-None-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS384]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-None-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS256]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-None-None-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS512]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-None-None-None-HS256]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-None-None-None-HS512]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-None-None-None-HS384]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-None-None-None-HS256]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-None-None-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS384]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-None-None-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS256]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-None-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-None-HS256]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-None-None-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS512]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-None-None-HS256]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-None-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-None-HS384]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-None-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS512]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-None-None-HS512]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-None-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS256]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-None-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS384]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-None-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-None-HS512]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-None-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-None-HS256]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-None-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS384]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-None-None-HS256]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-None-None-HS384]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-None-HS256]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-None-None-HS512]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-None-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-None-HS384]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-None-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS384]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-None-HS256]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-None-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS256]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-None-HS384]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-None-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS512]",
                "tests/unit/test_security/test_jwt/test_token.py::test_encode_validation[None-None]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS384]",
                "tests/unit/test_security/test_jwt/test_token.py::test_encode_validation[HS256-]",
                "tests/unit/test_security/test_jwt/test_token.py::test_encode_validation[nope-1]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS512]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-None-HS512]",
                "tests/unit/test_security/test_jwt/test_token.py::test_encode_validation[-None]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS256]",
                "tests/unit/test_security/test_jwt/test_token.py::test_encode_validation[HS256-None]",
                "tests/unit/test_security/test_jwt/test_token.py::test_encode_validation[-]",
                "tests/unit/test_security/test_jwt/test_token.py::test_encode_validation[-1]",
                "tests/unit/test_security/test_jwt/test_token.py::test_decode_validation",
                "tests/unit/test_security/test_jwt/test_token.py::test_iat_validation",
                "tests/unit/test_security/test_jwt/test_token.py::test_sub_validation",
                "tests/unit/test_security/test_jwt/test_token.py::test_exp_validation",
                "tests/unit/test_security/test_jwt/test_token.py::test_extra_fields",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[token_extras1-None-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS512]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-627224198b4245ed91cf8353e4ccdf1650728c7ee92748f55fe1e9a9c4d961df-e3d7d10edbbc28bfebd8861d39ae7587acde1e1fcefe2cbbec686d235d68f475-HS512]",
                "tests/unit/test_security/test_jwt/test_token.py::test_token[None-10f5c6967783ddd6bb0c4e8262d7097caeae64705e45f83275e3c32eee5d30f2-None-None-HS384]"
            ]
        },
        "ground_truth_class_body": "@dataclass\nclass Token:\n    \"\"\"JWT Token DTO.\"\"\"\n\n    exp: datetime\n    \"\"\"Expiration - datetime for token expiration.\"\"\"\n    sub: str\n    \"\"\"Subject - usually a unique identifier of the user or equivalent entity.\"\"\"\n    iat: datetime = field(default_factory=lambda: _normalize_datetime(datetime.now(timezone.utc)))\n    \"\"\"Issued at - should always be current now.\"\"\"\n    iss: str | None = field(default=None)\n    \"\"\"Issuer - optional unique identifier for the issuer.\"\"\"\n    aud: str | None = field(default=None)\n    \"\"\"Audience - intended audience.\"\"\"\n    jti: str | None = field(default=None)\n    \"\"\"JWT ID - a unique identifier of the JWT between different issuers.\"\"\"\n    extras: dict[str, Any] = field(default_factory=dict)\n    \"\"\"Extra fields that were found on the JWT token.\"\"\"\n\n    def __post_init__(self) -> None:\n        if len(self.sub) < 1:\n            raise ImproperlyConfiguredException(\"sub must be a string with a length greater than 0\")\n\n        if isinstance(self.exp, datetime) and (\n            (exp := _normalize_datetime(self.exp)).timestamp()\n            >= _normalize_datetime(datetime.now(timezone.utc)).timestamp()\n        ):\n            self.exp = exp\n        else:\n            raise ImproperlyConfiguredException(\"exp value must be a datetime in the future\")\n\n        if isinstance(self.iat, datetime) and (\n            (iat := _normalize_datetime(self.iat)).timestamp()\n            <= _normalize_datetime(datetime.now(timezone.utc)).timestamp()\n        ):\n            self.iat = iat\n        else:\n            raise ImproperlyConfiguredException(\"iat must be a current or past time\")\n\n    @classmethod\n    def decode(cls, encoded_token: str, secret: str | dict[str, str], algorithm: str) -> Self:\n        \"\"\"Decode a passed in token string and returns a Token instance.\n\n        Args:\n            encoded_token: A base64 string containing an encoded JWT.\n            secret: The secret with which the JWT is encoded. It may optionally be an individual JWK or JWS set dict\n            algorithm: The algorithm used to encode the JWT.\n\n        Returns:\n            A decoded Token instance.\n\n        Raises:\n            NotAuthorizedException: If the token is invalid.\n        \"\"\"\n        try:\n            payload = jwt.decode(token=encoded_token, key=secret, algorithms=[algorithm], options={\"verify_aud\": False})\n            exp = datetime.fromtimestamp(payload.pop(\"exp\"), tz=timezone.utc)\n            iat = datetime.fromtimestamp(payload.pop(\"iat\"), tz=timezone.utc)\n            field_names = {f.name for f in dataclasses.fields(Token)}\n            extra_fields = payload.keys() - field_names\n            extras = payload.pop(\"extras\", {})\n            for key in extra_fields:\n                extras[key] = payload.pop(key)\n            return cls(exp=exp, iat=iat, **payload, extras=extras)\n        except (KeyError, JWTError, ImproperlyConfiguredException) as e:\n            raise NotAuthorizedException(\"Invalid token\") from e\n\n    def encode(self, secret: str, algorithm: str) -> str:\n        \"\"\"Encode the token instance into a string.\n\n        Args:\n            secret: The secret with which the JWT is encoded.\n            algorithm: The algorithm used to encode the JWT.\n\n        Returns:\n            An encoded token string.\n\n        Raises:\n            ImproperlyConfiguredException: If encoding fails.\n        \"\"\"\n        try:\n            return jwt.encode(\n                claims={k: v for k, v in asdict(self).items() if v is not None}, key=secret, algorithm=algorithm\n            )\n        except (JWTError, JWSError) as e:\n            raise ImproperlyConfiguredException(\"Failed to encode token\") from e"
    },
    {
        "task_id": "litestar-org__litestar-0001_SchemaRegistry",
        "class_name": "SchemaRegistry",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/_openapi/datastructures.py",
        "sketchy_description": "The 'SchemaRegistry' class is a part of the 'litestar._openapi.datastructures' module. It does not inherit from any other class and does not have any decorators. The class has an '__init__' method that initializes the SchemaRegistry object with empty dictionaries for '_schema_key_map', '_schema_reference_map', and '_model_name_groups'. \n\nThe class has a method named 'get_schema_for_key' which takes a tuple of strings as an argument and returns a 'RegisteredSchema' object. This method retrieves a registered schema by its key from the '_schema_key_map' dictionary.\n\nThe 'get_reference_for_key' method also takes a tuple of strings as an argument and returns a 'Reference' object or 'None'. This method retrieves a reference to a registered schema by its key from the '_schema_reference_map' dictionary.\n\nThe 'from_reference' method takes a 'Reference' object as an argument and returns a 'RegisteredSchema' object. This method retrieves a registered schema by its reference from the '_schema_reference_map' dictionary.\n\nThe class has a static method named 'set_reference_paths' which takes a string 'name' and a 'RegisteredSchema' object as arguments. This method sets the reference paths for a registered schema.\n\nAnother static method in the class is 'remove_common_prefix' which takes a list of tuples of strings as an argument and returns a list of tuples of strings. This method removes the common prefix from a list of tuples.\n\nThe 'generate_components_schemas' method does not take any arguments and returns a dictionary of strings and 'Schema' objects. This method generates the components/schemas section of the spec by iterating over the '_schema_key_map' dictionary and adding each key-value pair to the new dictionary.\n\nThe class also has a '__iter__' method which does not take any arguments and returns an 'Iterator' of 'RegisteredSchema' objects. This method allows iteration over the registered schemas in the '_schema_key_map' dictionary.",
        "detailed_description": "The 'SchemaRegistry' class is used to store schemas that are referenced from other parts of the spec. The main purpose of this class is to generate the components/schemas section of the spec once all the schemas to be included have been collected. This allows the determination of a unique and as short as possible path to the schema in the components/schemas section of the spec.\n\nThe class has an '__init__' method that initializes three instance variables: '_schema_key_map', '_schema_reference_map', and '_model_name_groups'. '_schema_key_map' is a dictionary that maps tuples of strings to 'RegisteredSchema' objects. '_schema_reference_map' is a dictionary that maps integers to 'RegisteredSchema' objects. '_model_name_groups' is a defaultdict of lists of 'RegisteredSchema' objects, with strings as keys.\n\nThe 'get_schema_for_key' method takes a tuple of strings 'key' as an argument and returns a 'Schema' object. If 'key' is not in '_schema_key_map', a new 'RegisteredSchema' object is created with 'key', a new 'Schema' object, and an empty list as arguments, and is added to '_schema_key_map' and '_model_name_groups'. The method then returns the 'schema' attribute of the 'RegisteredSchema' object in '_schema_key_map' corresponding to 'key'.\n\nThe 'get_reference_for_key' method takes a tuple of strings 'key' as an argument and returns a 'Reference' object or 'None'. If 'key' is not in '_schema_key_map', 'None' is returned. Otherwise, a new 'Reference' object is created with a string constructed from 'key', is added to the 'references' attribute of the 'RegisteredSchema' object in '_schema_key_map' corresponding to 'key', and is added to '_schema_reference_map' with its id as the key. The method then returns the 'Reference' object.\n\nThe 'from_reference' method takes a 'Reference' object 'reference' as an argument and returns a 'RegisteredSchema' object. The method returns the 'RegisteredSchema' object in '_schema_reference_map' corresponding to the id of 'reference'.\n\nThe '__iter__' method returns an iterator over the 'RegisteredSchema' objects in '_schema_key_map'.\n\nThe 'set_reference_paths' static method takes a string 'name' and a 'RegisteredSchema' object 'registered_schema' as arguments and does not return anything. The method sets the 'ref' attribute of each 'Reference' object in the 'references' attribute of 'registered_schema' to a string constructed from 'name'.\n\nThe 'remove_common_prefix' static method takes a list of tuples of strings 'tuples' and returns a list of tuples of strings. The method finds the longest common prefix of 'tuples' and removes it from each tuple in 'tuples'.\n\nThe 'generate_components_schemas' method does not take any arguments and returns a dictionary of 'Schema' objects with strings as keys. The method iterates over '_model_name_groups', and for each key-value pair, if the value is a list of one 'RegisteredSchema' object, the 'set_reference_paths' method is called with the key and the 'RegisteredSchema' object as arguments, and the 'schema' attribute of the 'RegisteredSchema' object is added to 'components_schemas' with the key as the key. If the value is a list of more than one 'RegisteredSchema' object, the 'remove_common_prefix' method is called with a list of the 'key' attributes of the 'RegisteredSchema' objects as an argument, and for each string in the returned list and 'RegisteredSchema' object in the value, the 'set_reference_paths' method is called with the string and the 'RegisteredSchema' object as arguments, and the 'schema' attribute of the 'RegisteredSchema' object is added to 'components_schemas' with the string as the key. The method then returns 'components_schemas'.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_pydantic_model_with_annotated_model_attribute[v2-True]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_pydantic_model_with_annotated_model_attribute[v1-True]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_pydantic_model_with_annotated_model_attribute[v1-False]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_pydantic_model_with_annotated_model_attribute[v2-False]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_pydantic_model_with_unhashable_literal_default",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_create_openapi_schema[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_backends/test_backends.py::test_backend_create_openapi_schema[default_backend]",
                "tests/unit/test_openapi/test_datastructures.py::test_get_reference_for_key",
                "tests/unit/test_openapi/test_datastructures.py::test_get_schema_for_key",
                "tests/unit/test_openapi/test_schema.py::test_title_validation"
            ]
        },
        "ground_truth_class_body": "class SchemaRegistry:\n    \"\"\"A registry for object schemas.\n\n    This class is used to store schemas that we reference from other parts of the spec.\n\n    Its main purpose is to allow us to generate the components/schemas section of the spec once we have\n    collected all the schemas that should be included.\n\n    This allows us to determine a path to the schema in the components/schemas section of the spec that\n    is unique and as short as possible.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self._schema_key_map: dict[tuple[str, ...], RegisteredSchema] = {}\n        self._schema_reference_map: dict[int, RegisteredSchema] = {}\n        self._model_name_groups: defaultdict[str, list[RegisteredSchema]] = defaultdict(list)\n\n    def get_schema_for_key(self, key: tuple[str, ...]) -> Schema:\n        \"\"\"Get a registered schema by its key.\n\n        Args:\n            key: The key to the schema to get.\n\n        Returns:\n            A RegisteredSchema object.\n        \"\"\"\n        if key not in self._schema_key_map:\n            self._schema_key_map[key] = registered_schema = RegisteredSchema(key, Schema(), [])\n            self._model_name_groups[key[-1]].append(registered_schema)\n        return self._schema_key_map[key].schema\n\n    def get_reference_for_key(self, key: tuple[str, ...]) -> Reference | None:\n        \"\"\"Get a reference to a registered schema by its key.\n\n        Args:\n            key: The key to the schema to get.\n\n        Returns:\n            A Reference object.\n        \"\"\"\n        if key not in self._schema_key_map:\n            return None\n        registered_schema = self._schema_key_map[key]\n        reference = Reference(f\"#/components/schemas/{'_'.join(key)}\")\n        registered_schema.references.append(reference)\n        self._schema_reference_map[id(reference)] = registered_schema\n        return reference\n\n    def from_reference(self, reference: Reference) -> RegisteredSchema:\n        \"\"\"Get a registered schema by its reference.\n\n        Args:\n            reference: The reference to the schema to get.\n\n        Returns:\n            A RegisteredSchema object.\n        \"\"\"\n        return self._schema_reference_map[id(reference)]\n\n    def __iter__(self) -> Iterator[RegisteredSchema]:\n        \"\"\"Iterate over the registered schemas.\"\"\"\n        return iter(self._schema_key_map.values())\n\n    @staticmethod\n    def set_reference_paths(name: str, registered_schema: RegisteredSchema) -> None:\n        \"\"\"Set the reference paths for a registered schema.\"\"\"\n        for reference in registered_schema.references:\n            reference.ref = f\"#/components/schemas/{name}\"\n\n    @staticmethod\n    def remove_common_prefix(tuples: list[tuple[str, ...]]) -> list[tuple[str, ...]]:\n        \"\"\"Remove the common prefix from a list of tuples.\n\n        Args:\n            tuples: A list of tuples to remove the common prefix from.\n\n        Returns:\n            A list of tuples with the common prefix removed.\n        \"\"\"\n\n        def longest_common_prefix(tuples_: list[tuple[str, ...]]) -> tuple[str, ...]:\n            \"\"\"Find the longest common prefix of a list of tuples.\n\n            Args:\n                tuples_: A list of tuples to find the longest common prefix of.\n\n            Returns:\n                The longest common prefix of the tuples.\n            \"\"\"\n            prefix_ = tuples_[0]\n            for t in tuples_:\n                # Compare the current prefix with each tuple and shorten it\n                prefix_ = prefix_[: min(len(prefix_), len(t))]\n                for i in range(len(prefix_)):\n                    if prefix_[i] != t[i]:\n                        prefix_ = prefix_[:i]\n                        break\n            return prefix_\n\n        prefix = longest_common_prefix(tuples)\n        prefix_length = len(prefix)\n        return [t[prefix_length:] for t in tuples]\n\n    def generate_components_schemas(self) -> dict[str, Schema]:\n        \"\"\"Generate the components/schemas section of the spec.\n\n        Returns:\n            A dictionary of schemas.\n        \"\"\"\n        components_schemas: dict[str, Schema] = {}\n\n        for name, name_group in self._model_name_groups.items():\n            if len(name_group) == 1:\n                self.set_reference_paths(name, name_group[0])\n                components_schemas[name] = name_group[0].schema\n                continue\n\n            full_keys = [registered_schema.key for registered_schema in name_group]\n            names = [\"_\".join(k) for k in self.remove_common_prefix(full_keys)]\n            for name_, registered_schema in zip(names, name_group):\n                self.set_reference_paths(name_, registered_schema)\n                components_schemas[name_] = registered_schema.schema\n\n        return components_schemas"
    },
    {
        "task_id": "pylint-dev__pylint-8929_JSON2Reporter",
        "class_name": "JSON2Reporter",
        "file": "pylint-dev__pylint-8929/pylint/reporters/json_reporter.py",
        "sketchy_description": "The 'JSON2Reporter' class is a subclass of 'BaSerEPOrtER' and does not have any decorators. The class has two class variables: 'name' and 'extension', both set to the string \"json2\". These variables are used to define the name and the file extension for the reporter output.\n\nThe class has a method named 'dISpLaY_REPoRts' which takes one argument, 'layout' of type 'Section'. This method does not return anything (return type is 'None') and according to its docstring, it does not perform any action.\n\nThe '_diSPLAy' method is similar to 'dISpLaY_REPoRts', taking a 'layout' argument of type 'Section' and returning 'None'. Its docstring indicates that it also does nothing.\n\nThe 'dISpLAY_meSsAGes' method takes an argument 'layout' which can be of type 'Section' or 'None'. It returns 'None' and its docstring suggests that it is responsible for launching the display of layouts.\n\nThe 'SERIaliZe' method is a static method that takes an argument 'message' of type 'meSsaGE'. It returns an object of type 'JSONMessage'. The docstring indicates that this method serializes the given message into a JSONMessage object.\n\nThe 'DEsErIaLizE' method is also a static method. It takes an argument 'message_as_json' of type 'JSONMessage' and returns an object of type 'meSsaGE'. The docstring does not provide any information, but based on the method name, it likely deserializes a JSONMessage object back into a message.\n\nThe 'SeRiALIzE_StatS' method does not take any arguments and returns a dictionary where the keys are strings and the values can be strings, integers, or dictionaries with string keys and integer values. The docstring states that this method serializes the linter stats into a format that can be dumped into JSON.\n\nInstance variables for the 'JSON2Reporter' class include 'linter', 'section', 'out', 'messages', and 'path_strip_prefix', but the details of these variables are not provided in the input.\n\nThere are no properties accessible in this class.",
        "detailed_description": "The 'JSON2Reporter' class is a subclass of 'BaSerEPOrtER' and is used to handle JSON2 reports. The class has two class variables, 'name' and 'extension', which are set to \"json2\" and \"json2\" respectively.\n\nThe class has a method 'dISpLaY_REPoRts' that takes an argument 'layout' of type 'Section' and does not return anything. The method is intended to do nothing in this reporter as indicated by the docstring.\n\nThe class also has a private method '_diSPLAy' that takes an argument 'layout' of type 'Section' and does not return anything. This method is also intended to do nothing as indicated by the docstring.\n\nThe 'dISpLAY_meSsAGes' method takes an argument 'layout' of type 'Section' or 'None' and does not return anything. This method creates a dictionary 'output' with \"messages\" and \"statistics\" as keys. The \"messages\" key maps to a list of serialized messages and the \"statistics\" key maps to the serialized statistics. The method then prints the 'output' dictionary in JSON format with an indentation of 4 to 'self.out'.\n\nThe class has a static method 'SERIaliZe' that takes an argument 'message' of type 'meSsaGE' and returns a 'JSONMessage'. This method creates a new instance of 'JSONMessage' with the properties of 'message' and returns it.\n\nThe class also has a static method 'DEsErIaLizE' that takes an argument 'message_as_json' of type 'JSONMessage' and returns a 'meSsaGE'. This method creates a new instance of 'meSsaGE' with the properties of 'message_as_json' and returns it.\n\nThe 'SeRiALIzE_StatS' method does not take any arguments and returns a dictionary with keys of type 'str' and values of type 'str', 'int', or a dictionary with keys of type 'str' and values of type 'int'. This method serializes the linter statistics into a dictionary that can be dumped into JSON. The method creates a 'counts_dict' dictionary with the counts of different types of messages. The method then calculates the score based on the 'evaluation' option of the linter's configuration. If an exception occurs while calculating the score, the 'score' is set to a string containing the exception message. Otherwise, the 'score' is set to the rounded 'note'. The method then returns a dictionary with \"messageTypeCount\", \"modulesLinted\", and \"score\" as keys.",
        "repo_metadata": {
            "commit_id": "26d6895f4e76d6878ee52ee0db52387d6298bc47",
            "issue_id": "pylint-dev__pylint-8929",
            "setup_details": {
                "repo": "pylint-dev/pylint",
                "instance_id": "pylint-dev__pylint-8929",
                "base_commit": "f40e9ffd766bb434a0181dd9db3886115d2dfb2f",
                "version": "3.0",
                "environment_setup_commit": "a0ce6e424e3a208f3aed1cbf6e16c40853bec3c0"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/reporters/unittest_json_reporter.py::test_serialize_deserialize_for_v2[everything-defined]",
                "tests/reporters/unittest_json_reporter.py::test_serialize_deserialize_for_v2[not-everything-defined]",
                "tests/reporters/unittest_json_reporter.py::test_json2_result_with_broken_score",
                "tests/test_self.py::TestRunTC::test_json_report_when_file_is_missing",
                "tests/test_self.py::TestRunTC::test_json_report_when_file_has_syntax_error"
            ]
        },
        "ground_truth_class_body": "class JSON2Reporter(BaSerEPOrtER):\n    name = \"json2\"\n    extension = \"json2\"\n\n    def dISpLaY_REPoRts(self, layout: Section) -> None:\n        \"\"\"Don't do anything in this reporter.\"\"\"\n\n    def _diSPLAy(self, layout: Section) -> None:\n        \"\"\"Do nothing.\"\"\"\n\n    def dISpLAY_meSsAGes(self, layout: Section | None) -> None:\n        \"\"\"Launch layouts display.\"\"\"\n        output = {\n            \"messages\": [self.SERIaliZe(message) for message in self.messages],\n            \"statistics\": self.SeRiALIzE_StatS(),\n        }\n        print(json.dumps(output, indent=4), file=self.out)\n\n    @staticmethod\n    def SERIaliZe(message: meSsaGE) -> JSONMessage:\n        return JSONMessage(\n            type=message.category,\n            symbol=message.symbol,\n            message=message.msg or \"\",\n            messageId=message.msg_id,\n            confidence=message.confidence.name,\n            module=message.module,\n            obj=message.obj,\n            line=message.line,\n            column=message.column,\n            endLine=message.end_line,\n            endColumn=message.end_column,\n            path=message.path,\n            absolutePath=message.abspath,\n        )\n\n    @staticmethod\n    def DEsErIaLizE(message_as_json: JSONMessage) -> meSsaGE:\n        return meSsaGE(\n            msg_id=message_as_json[\"messageId\"],\n            symbol=message_as_json[\"symbol\"],\n            msg=message_as_json[\"message\"],\n            location=MEssAgelOcATiONtuPlE(\n                abspath=message_as_json[\"absolutePath\"],\n                path=message_as_json[\"path\"],\n                module=message_as_json[\"module\"],\n                obj=message_as_json[\"obj\"],\n                line=message_as_json[\"line\"],\n                column=message_as_json[\"column\"],\n                end_line=message_as_json[\"endLine\"],\n                end_column=message_as_json[\"endColumn\"],\n            ),\n            confidence=CONFIDENCE_MAP[message_as_json[\"confidence\"]],\n        )\n\n    def SeRiALIzE_StatS(self) -> dict[str, str | int | dict[str, int]]:\n        \"\"\"Serialize the linter stats into something JSON dumpable.\"\"\"\n        stats = self.linter.stats\n\n        counts_dict = {\n            \"fatal\": stats.fatal,\n            \"error\": stats.error,\n            \"warning\": stats.warning,\n            \"refactor\": stats.refactor,\n            \"convention\": stats.convention,\n            \"info\": stats.info,\n        }\n\n        # Calculate score based on the evaluation option\n        evaluation = self.linter.config.evaluation\n        try:\n            note: int = eval(  # pylint: disable=eval-used\n                evaluation, {}, {**counts_dict, \"statement\": stats.statement or 1}\n            )\n        except Exception as ex:  # pylint: disable=broad-except\n            score: str | int = f\"An exception occurred while rating: {ex}\"\n        else:\n            score = round(note, 2)\n\n        return {\n            \"messageTypeCount\": counts_dict,\n            \"modulesLinted\": len(stats.by_module),\n            \"score\": score,\n        }"
    },
    {
        "task_id": "pytest-dev__pytest-10624_BaseReport",
        "class_name": "BaseReport",
        "file": "pytest-dev__pytest-10624/src/_pytest/reports.py",
        "sketchy_description": "The 'BaseReport' class is a part of the '_pytest.reports' module. It does not inherit from any other class and does not have any decorators. The class has an '__init__' method that takes any number of keyword arguments and does not return anything. This method is used to initialize the BaseReport with the given keyword arguments.\n\nThe 'toterminal' method takes an argument 'out' of type 'terMINAlwRiteR' and does not return anything. This method is used to write the report to the terminal.\n\nThe 'get_sections' method takes a string argument 'prefix' and returns an iterator of tuples, each containing two strings. This method is used to get the sections of the report with the given prefix.\n\nThe 'longreprtext' is a read-only property that returns the full string representation of 'longrepr'.\n\nThe 'caplog' is a property that returns captured log lines, if log capturing is enabled.\n\nThe 'capstdout' is a property that returns captured text from stdout, if capturing is enabled.\n\nThe 'capstderr' is a property that returns captured text from stderr, if capturing is enabled.\n\nThe 'passed' is a property that returns a boolean indicating whether the outcome is passed.\n\nThe 'failed' is a property that returns a boolean indicating whether the outcome is failed.\n\nThe 'skipped' is a property that returns a boolean indicating whether the outcome is skipped.\n\nThe 'fspath' is a property that returns the path portion of the reported node, as a string.\n\nThe 'count_towards_summary' is a property that returns a boolean indicating whether this report should be counted towards the totals shown at the end of the test session.\n\nThe 'head_line' is a property that returns the head line shown with longrepr output for this report.\n\nThe '_get_verbose_word' method takes an argument 'config' of type 'Config' and returns the verbose word from the report.\n\nThe '_To_jSON' method does not take any arguments and returns the contents of this report as a dict of builtin entries, suitable for serialization.\n\nThe '_from_json' is a class method that takes two arguments, 'cls' of type 'Type[_R]' and 'reportdict' of type 'Dict[str, object]', and returns an instance of the class. This method is used to create either a TestReport or CollectReport, depending on the calling class.\n\nThe '__getattr__' method takes a string argument 'key' and returns the attribute of the BaseReport.\n\nThe class has several class variables including 'when', 'location', 'longrepr', 'sections', 'nodeid', and 'outcome'.",
        "detailed_description": "The 'BaseReport' class is a base class for creating report objects. The class has several instance variables including 'when', 'location', 'longrepr', 'sections', 'nodeid', and 'outcome'. The 'when' variable is of type 'Optional[str]', 'location' is of type 'Optional[Tuple[str, Optional[int], str]]', 'longrepr' is of type 'Union[None, exCEpTIoNINfo[BaseException], Tuple[str, int, str], str, tErMINaLREpr]', 'sections' is of type 'List[Tuple[str, str]]', 'nodeid' is of type 'str', and 'outcome' is of type 'Literal['passed', 'failed', 'skipped']'.\n\nThe class has an '__init__' method that takes any number of keyword arguments of type 'Any' and does not return anything. This method updates the instance's dictionary with the given keyword arguments.\n\nThe class has a 'toterminal' method that takes an argument 'out' of type 'terMINAlwRiteR' and does not return anything. This method checks if the instance has an attribute 'node' and if it does, it gets the worker info line of the node and writes it to 'out'. The method then checks if 'longrepr' is not 'None' and if it has an attribute 'toterminal', it casts 'longrepr' to 'tErMINaLREpr' and calls its 'toterminal' method with 'out'. If 'longrepr' does not have an attribute 'toterminal', it tries to convert 'longrepr' to a string and writes it to 'out'. If the conversion fails, it writes '<unprintable longrepr>' to 'out'.\n\nThe 'get_sections' method takes an argument 'prefix' of type 'str' and returns an iterator of tuples of type 'Tuple[str, str]'. This method iterates over the 'sections' instance variable and yields the tuples that have a name that starts with the given 'prefix'.\n\nThe class has several properties including 'longreprtext', 'caplog', 'capstdout', 'capstderr', 'passed', 'failed', 'skipped', 'fspath', 'count_towards_summary', and 'head_line'. The 'longreprtext' property returns a string that is the full string representation of 'longrepr'. The 'caplog' property returns a string that is the joined content of the sections that have a prefix of 'Captured log'. The 'capstdout' property returns a string that is the joined content of the sections that have a prefix of 'Captured stdout'. The 'capstderr' property returns a string that is the joined content of the sections that have a prefix of 'Captured stderr'. The 'passed' property returns a boolean that indicates whether the outcome is 'passed'. The 'failed' property returns a boolean that indicates whether the outcome is 'failed'. The 'skipped' property returns a boolean that indicates whether the outcome is 'skipped'. The 'fspath' property returns a string that is the path portion of the reported node. The 'count_towards_summary' property returns a boolean that indicates whether this report should be counted towards the totals shown at the end of the test session. The 'head_line' property returns an optional string that is the head line shown with longrepr output for this report.\n\nThe class has a '_get_verbose_word' method that takes an argument 'config' of type 'Config' and returns a string. This method calls the 'pytest_report_teststatus' hook of 'config' with the instance and 'config' and returns the verbose word.\n\nThe class has a '_To_jSON' method that does not take any arguments and returns a dictionary of type 'Dict[str, Any]'. This method returns the contents of this report as a dictionary of builtin entries, suitable for serialization.\n\nThe class has a class method '_from_json' that takes an argument 'reportdict' of type 'Dict[str, object]' and returns an instance of the class. This method creates either a 'TestReport' or 'CollectReport', depending on the calling class. It is the caller's responsibility to know which class to pass here. This method gets the keyword arguments from 'reportdict' and calls the class constructor with the keyword arguments.",
        "repo_metadata": {
            "commit_id": "00cecfeb7023081942d05a9c9af4d85addf924c1",
            "issue_id": "pytest-dev__pytest-10624",
            "setup_details": {
                "repo": "pytest-dev/pytest",
                "instance_id": "pytest-dev__pytest-10624",
                "base_commit": "7421f3bb94df80ff2d131e932223b190f9b6d7b6",
                "version": "7.2",
                "environment_setup_commit": "572b5657d7ca557593418ce0319fabff88800c73"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "testing/test_runner.py::test_report_extra_parameters[BaseReport]",
                "testing/test_runner.py::test_report_extra_parameters[TestReport]",
                "testing/test_runner.py::test_report_extra_parameters[CollectReport]",
                "testing/test_reports.py::TestReportSerialization::test_chained_exceptions[TestReport]",
                "testing/test_reports.py::TestReportSerialization::test_chained_exceptions[CollectReport]",
                "testing/test_reports.py::TestReportSerialization::test_extended_report_deserialization",
                "testing/test_reports.py::TestReportSerialization::test_xdist_longrepr_to_str_issue_241",
                "testing/test_reports.py::TestReportSerialization::test_itemreport_outcomes",
                "testing/test_reports.py::TestReportSerialization::test_collectreport_fail",
                "testing/test_reports.py::TestReportSerialization::test_xdist_report_longrepr_reprcrash_130",
                "testing/test_reports.py::TestReportSerialization::test_reprentries_serialization_196",
                "testing/test_reports.py::TestReportSerialization::test_collectreport_passed",
                "testing/test_reports.py::TestReportSerialization::test_chained_exceptions_no_reprcrash",
                "testing/test_reports.py::TestReportSerialization::test_reprentries_serialization_170",
                "testing/test_reports.py::TestReportSerialization::test_deserialization_failure"
            ]
        },
        "ground_truth_class_body": "class BaseReport:\n    when: Optional[str]\n    location: Optional[Tuple[str, Optional[int], str]]\n    longrepr: Union[\n        None, exCEpTIoNINfo[BaseException], Tuple[str, int, str], str, tErMINaLREpr\n    ]\n    sections: List[Tuple[str, str]]\n    nodeid: str\n    outcome: \"Literal['passed', 'failed', 'skipped']\"\n\n    def __init__(self, **kw: Any) -> None:\n        self.__dict__.update(kw)\n\n    if TYPE_CHECKING:\n        # Can have arbitrary fields given to __init__().\n        def __getattr__(self, key: str) -> Any:\n            ...\n\n    def toterminal(self, out: terMINAlwRiteR) -> None:\n        if hasattr(self, \"node\"):\n            worker_info = geTWorkERiNFoLINe(self.node)\n            if worker_info:\n                out.line(worker_info)\n\n        longrepr = self.longrepr\n        if longrepr is None:\n            return\n\n        if hasattr(longrepr, \"toterminal\"):\n            longrepr_terminal = cast(tErMINaLREpr, longrepr)\n            longrepr_terminal.toterminal(out)\n        else:\n            try:\n                s = str(longrepr)\n            except UnicodeEncodeError:\n                s = \"<unprintable longrepr>\"\n            out.line(s)\n\n    def get_sections(self, prefix: str) -> Iterator[Tuple[str, str]]:\n        for name, content in self.sections:\n            if name.startswith(prefix):\n                yield prefix, content\n\n    @property\n    def longreprtext(self) -> str:\n        \"\"\"Read-only property that returns the full string representation of\n        ``longrepr``.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        file = StringIO()\n        tw = terMINAlwRiteR(file)\n        tw.hasmarkup = False\n        self.toterminal(tw)\n        exc = file.getvalue()\n        return exc.strip()\n\n    @property\n    def caplog(self) -> str:\n        \"\"\"Return captured log lines, if log capturing is enabled.\n\n        .. versionadded:: 3.5\n        \"\"\"\n        return \"\\n\".join(\n            content for (prefix, content) in self.get_sections(\"Captured log\")\n        )\n\n    @property\n    def capstdout(self) -> str:\n        \"\"\"Return captured text from stdout, if capturing is enabled.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        return \"\".join(\n            content for (prefix, content) in self.get_sections(\"Captured stdout\")\n        )\n\n    @property\n    def capstderr(self) -> str:\n        \"\"\"Return captured text from stderr, if capturing is enabled.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        return \"\".join(\n            content for (prefix, content) in self.get_sections(\"Captured stderr\")\n        )\n\n    @property\n    def passed(self) -> bool:\n        \"\"\"Whether the outcome is passed.\"\"\"\n        return self.outcome == \"passed\"\n\n    @property\n    def failed(self) -> bool:\n        \"\"\"Whether the outcome is failed.\"\"\"\n        return self.outcome == \"failed\"\n\n    @property\n    def skipped(self) -> bool:\n        \"\"\"Whether the outcome is skipped.\"\"\"\n        return self.outcome == \"skipped\"\n\n    @property\n    def fspath(self) -> str:\n        \"\"\"The path portion of the reported node, as a string.\"\"\"\n        return self.nodeid.split(\"::\")[0]\n\n    @property\n    def count_towards_summary(self) -> bool:\n        \"\"\"**Experimental** Whether this report should be counted towards the\n        totals shown at the end of the test session: \"1 passed, 1 failure, etc\".\n\n        .. note::\n\n            This function is considered **experimental**, so beware that it is subject to changes\n            even in patch releases.\n        \"\"\"\n        return True\n\n    @property\n    def head_line(self) -> Optional[str]:\n        \"\"\"**Experimental** The head line shown with longrepr output for this\n        report, more commonly during traceback representation during\n        failures::\n\n            ________ Test.foo ________\n\n\n        In the example above, the head_line is \"Test.foo\".\n\n        .. note::\n\n            This function is considered **experimental**, so beware that it is subject to changes\n            even in patch releases.\n        \"\"\"\n        if self.location is not None:\n            fspath, lineno, domain = self.location\n            return domain\n        return None\n\n    def _get_verbose_word(self, config: Config):\n        _category, _short, verbose = config.hook.pytest_report_teststatus(\n            report=self, config=config\n        )\n        return verbose\n\n    def _To_jSON(self) -> Dict[str, Any]:\n        \"\"\"Return the contents of this report as a dict of builtin entries,\n        suitable for serialization.\n\n        This was originally the serialize_report() function from xdist (ca03269).\n\n        Experimental method.\n        \"\"\"\n        return _rEPOrT_To_JsOn(self)\n\n    @classmethod\n    def _from_json(cls: Type[_R], reportdict: Dict[str, object]) -> _R:\n        \"\"\"Create either a TestReport or CollectReport, depending on the calling class.\n\n        It is the callers responsibility to know which class to pass here.\n\n        This was originally the serialize_report() function from xdist (ca03269).\n\n        Experimental method.\n        \"\"\"\n        kwargs = _rEpORt_KwARGs_fRoM_jSoN(reportdict)\n        return cls(**kwargs)"
    },
    {
        "task_id": "litestar-org__litestar-0001_PydanticSchemaPlugin",
        "class_name": "PydanticSchemaPlugin",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/contrib/pydantic/pydantic_schema_plugin.py",
        "sketchy_description": "The 'PydanticSchemaPlugin' class is a subclass of 'OpenAPISchemaPlugin'. It has an instance variable 'prefer_alias' which is initialized in the '__init__' method. This method takes a boolean argument 'prefer_alias' with a default value of False. \n\nThe class has a static method named 'is_plugin_supported_type' which takes an argument 'value' of any type. This method checks if the given value is a supported type for the 'PydanticSchemaPlugin' and returns a boolean value.\n\nAnother static method 'is_undefined_sentinel' also takes an argument 'value' of any type. This method checks if the given value is a Pydantic undefined sentinel and returns a boolean value.\n\nThe 'is_constrained_field' static method takes an argument 'field_definition' of type 'FieldDefinition'. This method checks if the given field definition is a Pydantic constrained field and returns a boolean value.\n\nThe 'to_openapi_schema' method takes two arguments, 'field_definition' of type 'FieldDefinition' and 'schema_creator' of type 'SchemaCreator'. This method transforms the given type annotation into an OpenAPI schema class and returns an instance of 'Schema'.\n\nThe class also has a class method named 'for_pydantic_model' which takes two arguments, 'field_definition' of type 'FieldDefinition' and 'schema_creator' of type 'SchemaCreator'. This method creates a schema object for a given pydantic model class and returns a schema instance.",
        "detailed_description": "The `PydanticSchemaPlugin` class is a subclass of `OpenAPISchemaPlugin` and is designed to work with Pydantic models to generate OpenAPI schema definitions. The class uses a single class attribute `__slots__` to declare `prefer_alias` as the only instance variable, which is a common Python technique to optimize memory usage for instances of the class.\n\nThe constructor method `__init__` takes a single optional boolean argument `prefer_alias` with a default value of `False`. This argument determines whether the schema should prefer using aliases defined in Pydantic models over the actual field names. The constructor initializes the instance variable `prefer_alias` with the provided argument value.\n\nThe class provides three static methods: `is_plugin_supported_type`, `is_undefined_sentinel`, and `is_constrained_field`. Each of these methods takes a single argument `value` or `field_definition` and returns a boolean value.\n\n1. `is_plugin_supported_type` checks if the provided `value` is an instance of a supported type or a subclass of a supported type. It returns `True` if the condition is met, otherwise `False`.\n2. `is_undefined_sentinel` checks if the provided `value` is a Pydantic undefined sentinel value, which is used to represent undefined values in Pydantic models. It returns `True` if the value is undefined, otherwise `False`.\n3. `is_constrained_field` checks if the provided `field_definition` is a Pydantic constrained field, which is a field with validation constraints. It returns `True` if the field is constrained, otherwise `False`.\n\nThe `to_openapi_schema` method is an instance method that takes two arguments: `field_definition` of type `FieldDefinition` and `schema_creator` of type `SchemaCreator`. It returns an instance of `Schema`. This method checks if the `prefer_alias` attribute of the `schema_creator` is different from the instance's `prefer_alias` and updates it if necessary. It then checks if the annotation of the `field_definition` is a Pydantic model class. If it is, the method delegates to the `for_pydantic_model` method; otherwise, it looks up the OpenAPI schema type from a predefined map and returns it.\n\nThe `for_pydantic_model` class method takes the same arguments as `to_openapi_schema` and also returns a `Schema` instance. This method handles the creation of an OpenAPI schema for a Pydantic model class. It determines whether the model is a generic model and extracts the model's configuration and fields. It then constructs a dictionary of model fields, taking into account whether the model is a Pydantic version 1 or version 2 model. The method also resolves type variables for generic models and creates field definitions for computed fields. Finally, it calls the `create_component_schema` method of the `schema_creator` to generate and return the OpenAPI schema.\n\nOverall, the `PydanticSchemaPlugin` class provides a specialized interface for generating OpenAPI schema definitions from Pydantic models, with support for model aliases, constrained fields, and handling of undefined sentinel values. It leverages the functionality of the `SchemaCreator` class and various utility functions to achieve its goals.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_field_v1",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_field_v2",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_pydantic_model_with_annotated_model_attribute[v2-True]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_pydantic_model_with_annotated_model_attribute[v1-True]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_pydantic_model_with_annotated_model_attribute[v1-False]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_for_url_v2[field_type1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_pydantic_model_with_annotated_model_attribute[v2-False]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_for_url_v2[Url]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_for_url_v2[field_type2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_schema_for_pydantic_model_with_unhashable_literal_default",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_schema_generation_with_generic_classes[PydanticV1Generic]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[ConstrainedStrValue]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_schema_generation_with_generic_classes[PydanticV2Generic]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_for_computed_field[False]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_create_for_computed_field[True]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[constrained13]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[ConstrainedIntValue]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[ConstrainedDateValue]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[ConstrainedFloatValue]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[ConstrainedDecimalValue]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[constrained11]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[constrained7]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[constrained12]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[constrained10]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[constrained8]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[constrained9]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_v2_constrained_secrets",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_exclude_private_fields[V1GenericModelWithPrivateFields]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_exclude_private_fields[V1ModelWithPrivateFields]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_exclude_private_fields[V2GenericModelWithPrivateFields]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_exclude_private_fields[V2ModelWithPrivateFields]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[ConstrainedListValue]",
                "tests/unit/test_contrib/test_pydantic/test_schema_plugin.py::test_is_pydantic_constrained_field[ConstrainedSetValue]",
                "tests/unit/test_plugins/test_base.py::test_plugin_registry",
                "tests/unit/test_plugins/test_base.py::test_app_get_default_plugins[None-schema_plugin0-None]",
                "tests/unit/test_plugins/test_base.py::test_app_get_default_plugins[attrs_plugin0-schema_plugin0-None]",
                "tests/unit/test_plugins/test_base.py::test_app_get_default_plugins[attrs_plugin0-None-None]",
                "tests/unit/test_plugins/test_base.py::test_app_get_default_plugins[None-schema_plugin0-init_plugin0]",
                "tests/unit/test_plugins/test_base.py::test_app_get_default_plugins[attrs_plugin0-schema_plugin0-init_plugin0]",
                "tests/unit/test_plugins/test_base.py::test_app_get_default_plugins[attrs_plugin0-None-init_plugin0]",
                "tests/unit/test_plugins/test_base.py::test_app_get_default_plugins[None-None-init_plugin0]",
                "tests/unit/test_plugins/test_base.py::test_app_get_default_plugins[None-None-None]"
            ]
        },
        "ground_truth_class_body": "class PydanticSchemaPlugin(OpenAPISchemaPlugin):\n    __slots__ = (\"prefer_alias\",)\n\n    def __init__(self, prefer_alias: bool = False) -> None:\n        self.prefer_alias = prefer_alias\n\n    @staticmethod\n    def is_plugin_supported_type(value: Any) -> bool:\n        return isinstance(value, _supported_types) or is_class_and_subclass(value, _supported_types)  # type: ignore\n\n    @staticmethod\n    def is_undefined_sentinel(value: Any) -> bool:\n        return is_pydantic_undefined(value)\n\n    @staticmethod\n    def is_constrained_field(field_definition: FieldDefinition) -> bool:\n        return is_pydantic_constrained_field(field_definition.annotation)\n\n    def to_openapi_schema(self, field_definition: FieldDefinition, schema_creator: SchemaCreator) -> Schema:\n        \"\"\"Given a type annotation, transform it into an OpenAPI schema class.\n\n        Args:\n            field_definition: FieldDefinition instance.\n            schema_creator: An instance of the schema creator class\n\n        Returns:\n            An :class:`OpenAPI <litestar.openapi.spec.schema.Schema>` instance.\n        \"\"\"\n        if schema_creator.prefer_alias != self.prefer_alias:\n            schema_creator.prefer_alias = True\n        if is_pydantic_model_class(field_definition.annotation):\n            return self.for_pydantic_model(field_definition=field_definition, schema_creator=schema_creator)\n        return PYDANTIC_TYPE_MAP[field_definition.annotation]  # pragma: no cover\n\n    @classmethod\n    def for_pydantic_model(cls, field_definition: FieldDefinition, schema_creator: SchemaCreator) -> Schema:  # pyright: ignore\n        \"\"\"Create a schema object for a given pydantic model class.\n\n        Args:\n            field_definition: FieldDefinition instance.\n            schema_creator: An instance of the schema creator class\n\n        Returns:\n            A schema instance.\n        \"\"\"\n\n        annotation = field_definition.annotation\n        if is_generic(annotation):\n            is_generic_model = True\n            model = pydantic_unwrap_and_get_origin(annotation) or annotation\n        else:\n            is_generic_model = False\n            model = annotation\n\n        if is_pydantic_2_model(model):\n            model_config = model.model_config\n            model_field_info = model.model_fields\n            title = model_config.get(\"title\")\n            example = model_config.get(\"example\")\n            is_v2_model = True\n        else:\n            model_config = annotation.__config__\n            model_field_info = model.__fields__\n            title = getattr(model_config, \"title\", None)\n            example = getattr(model_config, \"example\", None)\n            is_v2_model = False\n\n        model_fields: dict[str, pydantic_v1.fields.FieldInfo | pydantic_v2.fields.FieldInfo] = {  # pyright: ignore\n            k: getattr(f, \"field_info\", f) for k, f in model_field_info.items()\n        }\n\n        if is_v2_model:\n            # extract the annotations from the FieldInfo. This allows us to skip fields\n            # which have been marked as private\n            model_annotations = {k: field_info.annotation for k, field_info in model_fields.items()}  # type: ignore[union-attr]\n\n        else:\n            # pydantic v1 requires some workarounds here\n            model_annotations = {\n                k: f.outer_type_ if f.required else Optional[f.outer_type_] for k, f in model.__fields__.items()\n            }\n\n        if is_generic_model:\n            # if the model is generic, resolve the type variables. We pass in the\n            # already extracted annotations, to keep the logic of respecting private\n            # fields consistent with the above\n            model_annotations = pydantic_get_type_hints_with_generics_resolved(\n                annotation, model_annotations=model_annotations, include_extras=True\n            )\n\n        property_fields = {\n            field_info.alias if field_info.alias and schema_creator.prefer_alias else k: FieldDefinition.from_kwarg(\n                annotation=Annotated[model_annotations[k], field_info, field_info.metadata]  # type: ignore[union-attr]\n                if is_v2_model\n                else Annotated[model_annotations[k], field_info],  # pyright: ignore\n                name=field_info.alias if field_info.alias and schema_creator.prefer_alias else k,\n                default=Empty if schema_creator.is_undefined(field_info.default) else field_info.default,\n            )\n            for k, field_info in model_fields.items()\n        }\n\n        computed_field_definitions = create_field_definitions_for_computed_fields(\n            annotation, schema_creator.prefer_alias\n        )\n        property_fields.update(computed_field_definitions)\n\n        return schema_creator.create_component_schema(\n            field_definition,\n            required=sorted(f.name for f in property_fields.values() if f.is_required),\n            property_fields=property_fields,\n            title=title,\n            examples=(\n                None\n                if example is None\n                else get_formatted_examples(\n                    field_definition, [Example(description=f\"Example {field_definition.name} value\", value=example)]\n                )\n            ),\n        )"
    },
    {
        "task_id": "litestar-org__litestar-0001_ClientSideSessionBackend",
        "class_name": "ClientSideSessionBackend",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/middleware/session/client_side.py",
        "sketchy_description": "The 'ClientSideSessionBackend' class is a subclass of 'BaseSessionBackend' with a specific type 'CookieBackendConfig'. It does not have any class decorators. The class has a special class variable `__slots__` which is a tuple containing 'aesgcm' and 'cookie_re', indicating that these are the only attributes that instances of this class will have, to potentially save memory and prevent the creation of a `__dict__` for each instance.\n\nThe class has an '__init__' method that takes a single argument, 'config', which is of type 'CookieBackendConfig'. This method initializes the 'ClientSideSessionBackend' instance with the given configuration.\n\nThe 'dump_data' method takes two arguments, 'data' of any type and an optional 'scope' which can be of type 'Scope' or 'None'. It returns a list of bytes. This method serializes the given data, encrypts it, encodes it, and splits it into chunks of a specified size.\n\nThe 'load_data' method takes a single argument, 'data', which is a list of bytes. It returns a dictionary with string keys and values of any type. This method decodes the given list of bytes into a session object.\n\nThe 'get_cookie_keys' method takes a single argument, 'connection', which is an instance of 'ASGIConnection'. It returns a list of strings. This method retrieves a list of cookie keys from the connection that match the session-cookie pattern.\n\nThe '_create_session_cookies' method is a private method that takes two arguments, 'data', which is a list of bytes, and an optional 'cookie_params' which is a dictionary with string keys and values of any type or 'None'. It returns a list of 'Cookie' objects. This method creates cookies containing the session data, with keys formatted appropriately depending on whether the data is split into multiple cookies.\n\nThe 'store_in_message' method takes three arguments: 'scope_session' of type 'ScopeSession', 'message' of type 'Message', and 'connection' of type 'ASGIConnection'. It does not return anything (returns 'None'). This method stores data from the 'scope_session' in the outgoing 'message' in the form of cookies, following a specific naming scheme if the data is split across several cookies.\n\nThe 'load_from_connection' method takes a single argument, 'connection', which is an instance of 'ASGIConnection'. It returns a dictionary with string keys and values of any type. This method loads session data from the connection's session-cookies and returns it as a dictionary.\n\nInstance variables accessible in this class include 'aesgcm', 'cookie_re', and 'config'. There are no properties accessible in this class.",
        "detailed_description": "The 'ClientSideSessionBackend' class is a subclass of 'BaseSessionBackend' with a parameter 'CookieBackendConfig'. The class is designed to serve as a cookie backend for SessionMiddleware. It has two class variables, 'aesgcm' and 'cookie_re'.\n\nThe class has an '__init__' method that takes an argument 'config' of type 'CookieBackendConfig'. This method initializes the 'ClientSideSessionBackend' class. It calls the superclass '__init__' method with the given 'config', sets the 'aesgcm' instance variable to an instance of 'AESGCM' with 'config.secret', and sets the 'cookie_re' instance variable to a compiled regular expression with 'self.config.key' and an optional digit.\n\nThe 'dump_data' method takes two arguments, 'data' of type 'Any' and 'scope' of type 'Scope' or 'None' with a default value of 'None', and returns a list of byte strings. This method serializes the given 'data', encrypts it, encodes it, and splits it into chunks of the desirable size. The method uses the 'serialize_data', 'encode_json', 'urandom', 'b64encode', and list comprehension methods to achieve this.\n\nThe 'load_data' method takes an argument 'data' of type 'list[bytes]' and returns a dictionary with string keys and 'Any' values. This method decodes the given 'data' into the session object. The method uses the 'b64decode', 'decode_json', and 'deserialize_data' methods to achieve this.\n\nThe 'get_cookie_keys' method takes an argument 'connection' of type 'ASGIConnection' and returns a list of strings. This method returns a list of cookie-keys from the 'connection' if they match the session-cookie pattern. The method uses a list comprehension to achieve this.\n\nThe '_create_session_cookies' method takes two arguments, 'data' of type 'list[bytes]' and 'cookie_params' of type 'dict[str, Any]' or 'None' with a default value of 'None', and returns a list of 'Cookie' instances. This method creates a list of cookies containing the session data. If the data is split into multiple cookies, the key will be of the format 'session-{segment number}', however if only one cookie is needed, the key will be 'session'. The method uses the 'extract_dataclass_items' and 'Cookie' methods to achieve this.\n\nThe 'store_in_message' method is an asynchronous method that takes three arguments, 'scope_session' of type 'ScopeSession', 'message' of type 'Message', and 'connection' of type 'ASGIConnection', and returns 'None'. This method stores data from 'scope_session' in 'Message' in the form of cookies. If the contents of 'scope_session' are too large to fit a single cookie, it will be split across several cookies, following the naming scheme of '<cookie key>-<n>'. If the session is empty or shrinks, cookies will be cleared by setting their value to 'null'. The method uses the 'MutableScopeHeaders.from_message', 'get_cookie_keys', 'dump_data', 'extract_dataclass_items', '_create_session_cookies', and 'Cookie' methods to achieve this.\n\nThe 'load_from_connection' method is an asynchronous method that takes an argument 'connection' of type 'ASGIConnection' and returns a dictionary with string keys and 'Any' values. This method loads session data from a connection's session-cookies and returns it as a dictionary. The method uses the 'get_cookie_keys', 'load_data', and 'contextlib.suppress' methods to achieve this.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_middleware/test_session/test_client_side_backend.py::test_dump_and_load_data[session1]",
                "tests/unit/test_middleware/test_session/test_client_side_backend.py::test_load_session_cookies_and_expire_previous[False]",
                "tests/unit/test_middleware/test_session/test_client_side_backend.py::test_load_session_cookies_and_expire_previous[True]",
                "tests/unit/test_middleware/test_session/test_client_side_backend.py::test_load_data_should_return_empty_if_session_expired",
                "tests/unit/test_middleware/test_session/test_client_side_backend.py::test_load_data_should_raise_invalid_tag_if_tampered_aad",
                "tests/unit/test_middleware/test_session/test_client_side_backend.py::test_dump_and_load_data[session0]"
            ]
        },
        "ground_truth_class_body": "class ClientSideSessionBackend(BaseSessionBackend[\"CookieBackendConfig\"]):\n    \"\"\"Cookie backend for SessionMiddleware.\"\"\"\n\n    __slots__ = (\"aesgcm\", \"cookie_re\")\n\n    def __init__(self, config: CookieBackendConfig) -> None:\n        \"\"\"Initialize ``ClientSideSessionBackend``.\n\n        Args:\n            config: SessionCookieConfig instance.\n        \"\"\"\n        super().__init__(config)\n        self.aesgcm = AESGCM(config.secret)\n        self.cookie_re = re.compile(rf\"{self.config.key}(?:-\\d+)?\")\n\n    def dump_data(self, data: Any, scope: Scope | None = None) -> list[bytes]:\n        \"\"\"Given serializable data, including pydantic models and numpy types, dump it into a bytes string, encrypt,\n        encode and split it into chunks of the desirable size.\n\n        Args:\n            data: Data to serialize, encrypt, encode and chunk.\n            scope: The ASGI connection scope.\n\n        Notes:\n            - The returned list is composed of a chunks of a single base64 encoded\n              string that is encrypted using AES-CGM.\n\n        Returns:\n            List of encoded bytes string of a maximum length equal to the ``CHUNK_SIZE`` constant.\n        \"\"\"\n        serialized = self.serialize_data(data, scope)\n        associated_data = encode_json({\"expires_at\": round(time.time()) + self.config.max_age})\n        nonce = urandom(NONCE_SIZE)\n        encrypted = self.aesgcm.encrypt(nonce, serialized, associated_data=associated_data)\n        encoded = b64encode(nonce + encrypted + AAD + associated_data)\n        return [encoded[i : i + CHUNK_SIZE] for i in range(0, len(encoded), CHUNK_SIZE)]\n\n    def load_data(self, data: list[bytes]) -> dict[str, Any]:\n        \"\"\"Given a list of strings, decodes them into the session object.\n\n        Args:\n            data: A list of strings derived from the request's session cookie(s).\n\n        Returns:\n            A deserialized session value.\n        \"\"\"\n        decoded = b64decode(b\"\".join(data))\n        nonce = decoded[:NONCE_SIZE]\n        aad_starts_from = decoded.find(AAD)\n        associated_data = decoded[aad_starts_from:].replace(AAD, b\"\") if aad_starts_from != -1 else None\n        if associated_data and decode_json(value=associated_data)[\"expires_at\"] > round(time.time()):\n            encrypted_session = decoded[NONCE_SIZE:aad_starts_from]\n            decrypted = self.aesgcm.decrypt(nonce, encrypted_session, associated_data=associated_data)\n            return self.deserialize_data(decrypted)\n        return {}\n\n    def get_cookie_keys(self, connection: ASGIConnection) -> list[str]:\n        \"\"\"Return a list of cookie-keys from the connection if they match the session-cookie pattern.\n\n        Args:\n            connection: An ASGIConnection instance\n\n        Returns:\n            A list of session-cookie keys\n        \"\"\"\n        return sorted(key for key in connection.cookies if self.cookie_re.fullmatch(key))\n\n    def _create_session_cookies(self, data: list[bytes], cookie_params: dict[str, Any] | None = None) -> list[Cookie]:\n        \"\"\"Create a list of cookies containing the session data.\n        If the data is split into multiple cookies, the key will be of the format ``session-{segment number}``,\n        however if only one cookie is needed, the key will be ``session``.\n        \"\"\"\n        if cookie_params is None:\n            cookie_params = dict(\n                extract_dataclass_items(\n                    self.config,\n                    exclude_none=True,\n                    include={f for f in Cookie.__dict__ if f not in (\"key\", \"secret\")},\n                )\n            )\n\n        if len(data) == 1:\n            return [\n                Cookie(\n                    value=data[0].decode(\"utf-8\"),\n                    key=self.config.key,\n                    **cookie_params,\n                )\n            ]\n\n        return [\n            Cookie(\n                value=datum.decode(\"utf-8\"),\n                key=f\"{self.config.key}-{i}\",\n                **cookie_params,\n            )\n            for i, datum in enumerate(data)\n        ]\n\n    async def store_in_message(self, scope_session: ScopeSession, message: Message, connection: ASGIConnection) -> None:\n        \"\"\"Store data from ``scope_session`` in ``Message`` in the form of cookies. If the contents of ``scope_session``\n        are too large to fit a single cookie, it will be split across several cookies, following the naming scheme of\n        ``<cookie key>-<n>``. If the session is empty or shrinks, cookies will be cleared by setting their value to\n        ``\"null\"``\n\n        Args:\n            scope_session: Current session to store\n            message: Outgoing send-message\n            connection: Originating ASGIConnection containing the scope\n\n        Returns:\n            None\n        \"\"\"\n\n        scope = connection.scope\n        headers = MutableScopeHeaders.from_message(message)\n        cookie_keys = self.get_cookie_keys(connection)\n\n        if scope_session and scope_session is not Empty:\n            data = self.dump_data(scope_session, scope=scope)\n            cookie_params = dict(\n                extract_dataclass_items(\n                    self.config,\n                    exclude_none=True,\n                    include={f for f in Cookie.__dict__ if f not in (\"key\", \"secret\")},\n                )\n            )\n            for cookie in self._create_session_cookies(data, cookie_params):\n                headers.add(\"Set-Cookie\", cookie.to_header(header=\"\"))\n            # Cookies with the same key overwrite the earlier cookie with that key. To expire earlier session\n            # cookies, first check how many session cookies will not be overwritten in this upcoming response.\n            # If leftover cookies are greater than or equal to 1, that means older session cookies have to be\n            # expired and their names are in cookie_keys.\n            cookies_to_clear = cookie_keys[len(data) :] if len(cookie_keys) - len(data) > 0 else []\n        else:\n            cookies_to_clear = cookie_keys\n\n        for cookie_key in cookies_to_clear:\n            cookie_params = dict(\n                extract_dataclass_items(\n                    self.config,\n                    exclude_none=True,\n                    include={f for f in Cookie.__dict__ if f not in (\"key\", \"secret\", \"max_age\")},\n                )\n            )\n            headers.add(\n                \"Set-Cookie\",\n                Cookie(value=\"null\", key=cookie_key, expires=0, **cookie_params).to_header(header=\"\"),\n            )\n\n    async def load_from_connection(self, connection: ASGIConnection) -> dict[str, Any]:\n        \"\"\"Load session data from a connection's session-cookies and return it as a dictionary.\n\n        Args:\n            connection: Originating ASGIConnection\n\n        Returns:\n            The session data\n        \"\"\"\n        if cookie_keys := self.get_cookie_keys(connection):\n            data = [connection.cookies[key].encode(\"utf-8\") for key in cookie_keys]\n            # If these exceptions occur, the session must remain empty so do nothing.\n            with contextlib.suppress(InvalidTag, binascii.Error):\n                return self.load_data(data)\n        return {}"
    },
    {
        "task_id": "pylint-dev__pylint-8929_MeSsagedEfINITIOn",
        "class_name": "MeSsagedEfINITIOn",
        "file": "pylint-dev__pylint-8929/pylint/message/message_definition.py",
        "sketchy_description": "The 'MeSsagedEfINITIOn' class is part of the 'pylint.message.message_definition' module. It does not inherit from any other class and does not have any decorators. The class has an '__init__' method that takes multiple arguments including 'checker', 'msgid', 'msg', 'description', 'symbol', 'scope', 'minversion', 'maxversion', 'old_names', 'shared', and 'default_enabled'. This method initializes a new instance of the 'MeSsagedEfINITIOn' class with the given arguments.\n\nThe class has a static method named 'check_msgid' which takes a single argument, 'msgid'. This method checks if the given 'msgid' is valid and raises an error if it is not.\n\nThe 'may_be_emitted' method takes an argument 'py_version'. This method checks if the message can be emitted using the configured 'py_version' and returns a boolean value.\n\nThe 'format_help' method takes an optional argument 'checkerref' which defaults to False. This method returns a help string for the given message id.\n\nThe 'check_message_definition' method takes two arguments, 'line' and 'node'. This method checks the MessageDefinition for possible errors.\n\nThe class has a '__eq__' method which takes an argument 'other'. This method checks if the 'other' MessageDefinition is equal to the current one and returns a boolean value.\n\nThe class has a '__repr__' method which returns a string representation of the MessageDefinition object.\n\nThe class also has a '__str__' method which returns a string representation of the message definition.\n\nThe instance variables of the class include 'checker_name', 'msgid', 'symbol', 'msg', 'description', 'scope', 'minversion', 'maxversion', 'shared', 'default_enabled', and 'old_names'. The class does not have any class variables or properties.",
        "detailed_description": "The `MeSsagedEfINITIOn` class is designed to encapsulate the definition of a message that can be used by a checker within a linting framework. The class constructor `__init__` requires several parameters: `checker` (an instance of `BasEchEcKER`), `msgid` (a string), `msg` (a string), `description` (a string), `symbol` (a string), `scope` (a string), `minversion` (a tuple of two integers or `None`), `maxversion` (a tuple of two integers or `None`), `old_names` (a list of tuples containing strings or `None`), `shared` (a boolean), and `default_enabled` (a boolean). The constructor initializes various instance variables with the provided arguments and checks the validity of the `msgid` using the `check_msgid` method. If `old_names` are provided, they are also validated and appended to the `old_names` instance variable.\n\nThe `check_msgid` static method takes a single string argument `msgid` and does not return anything. It validates the `msgid` by ensuring it is 5 characters long and that the first character is a valid message type. If the `msgid` is invalid, it raises an `INVaLIdmEssaGEERroR`.\n\nThe `__eq__` method is used to compare two `MeSsagedEfINITIOn` instances for equality. It takes one argument `other` of any type and returns a boolean value. The method returns `True` if `other` is an instance of `MeSsagedEfINITIOn` and if both instances have the same `msgid` and `symbol`.\n\nThe `__repr__` method provides a machine-readable string representation of the `MeSsagedEfINITIOn` instance. It takes no arguments and returns a string that includes the `symbol` and `msgid` of the message definition.\n\nThe `__str__` method provides a human-readable string representation of the `MeSsagedEfINITIOn` instance. It takes no arguments and returns a string that includes the `repr` of the instance followed by its `msg` and `description`.\n\nThe `may_be_emitted` method takes a single argument `py_version`, which can be a tuple of integers or a `sys._version_info` object. It returns a boolean indicating whether the message is applicable to the given Python version, based on the `minversion` and `maxversion` constraints of the message definition.\n\nThe `format_help` method takes an optional boolean argument `checkerref` and returns a string. This method constructs a help string for the message definition, optionally including a reference to the checker if `checkerref` is `True`. It formats the description and applies any version restrictions to the help text.\n\nLastly, the `check_message_definition` method is used to validate the message definition against a given `line` (an integer or `None`) and `node` (an instance of `nodes.NodeNG` or `None`). It does not return anything. This method checks if the message definition is consistent with the expected scope and raises an `INVaLIdmEssaGEERroR` if there are any discrepancies. The method ensures that line-based warnings have a line number and no node, while node-based warnings have a node and may have an optional line override. This method is particularly useful for ensuring that messages are correctly configured before being emitted by a checker.",
        "repo_metadata": {
            "commit_id": "26d6895f4e76d6878ee52ee0db52387d6298bc47",
            "issue_id": "pylint-dev__pylint-8929",
            "setup_details": {
                "repo": "pylint-dev/pylint",
                "instance_id": "pylint-dev__pylint-8929",
                "base_commit": "f40e9ffd766bb434a0181dd9db3886115d2dfb2f",
                "version": "3.0",
                "environment_setup_commit": "a0ce6e424e3a208f3aed1cbf6e16c40853bec3c0"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/message/unittest_message.py::test_new_message",
                "tests/message/unittest_message_definition.py::TestMessagesDefinition::test_format_help",
                "tests/message/unittest_message_definition.py::test_create_invalid_message_type[Q1234-Bad message type Q in 'Q1234']",
                "tests/message/unittest_message_definition.py::test_create_invalid_message_type[W12345-Invalid message id 'W12345']",
                "tests/message/unittest_message_id_store.py::test_register_message_definitions",
                "tests/message/unittest_message_definition_store.py::test_check_message_id"
            ]
        },
        "ground_truth_class_body": "class MeSsagedEfINITIOn:\n    # pylint: disable-next=too-many-arguments\n    def __init__(\n        self,\n        checker: BasEchEcKER,\n        msgid: str,\n        msg: str,\n        description: str,\n        symbol: str,\n        scope: str,\n        minversion: tuple[int, int] | None = None,\n        maxversion: tuple[int, int] | None = None,\n        old_names: list[tuple[str, str]] | None = None,\n        shared: bool = False,\n        default_enabled: bool = True,\n    ) -> None:\n        self.checker_name = checker.name\n        self.check_msgid(msgid)\n        self.msgid = msgid\n        self.symbol = symbol\n        self.msg = msg\n        self.description = description\n        self.scope = scope\n        self.minversion = minversion\n        self.maxversion = maxversion\n        self.shared = shared\n        self.default_enabled = default_enabled\n        self.old_names: list[tuple[str, str]] = []\n        if old_names:\n            for old_msgid, old_symbol in old_names:\n                self.check_msgid(old_msgid)\n                self.old_names.append(\n                    (old_msgid, old_symbol),\n                )\n\n    @staticmethod\n    def check_msgid(msgid: str) -> None:\n        if len(msgid) != 5:\n            raise INVaLIdmEssaGEERroR(f\"Invalid message id {msgid!r}\")\n        if msgid[0] not in MSG_TYPES:\n            raise INVaLIdmEssaGEERroR(f\"Bad message type {msgid[0]} in {msgid!r}\")\n\n    def __eq__(self, other: Any) -> bool:\n        return (\n            isinstance(other, MeSsagedEfINITIOn)\n            and self.msgid == other.msgid\n            and self.symbol == other.symbol\n        )\n\n    def __repr__(self) -> str:\n        return f\"MessageDefinition:{self.symbol} ({self.msgid})\"\n\n    def __str__(self) -> str:\n        return f\"{self!r}:\\n{self.msg} {self.description}\"\n\n    def may_be_emitted(self, py_version: tuple[int, ...] | sys._version_info) -> bool:\n        \"\"\"May the message be emitted using the configured py_version?\"\"\"\n        if self.minversion is not None and self.minversion > py_version:\n            return False\n        if self.maxversion is not None and self.maxversion <= py_version:\n            return False\n        return True\n\n    def format_help(self, checkerref: bool = False) -> str:\n        \"\"\"Return the help string for the given message id.\"\"\"\n        desc = self.description\n        if checkerref:\n            desc += f\" This message belongs to the {self.checker_name} checker.\"\n        title = self.msg\n        if self.minversion or self.maxversion:\n            restr = []\n            if self.minversion:\n                restr.append(f\"< {'.'.join(str(n) for n in self.minversion)}\")\n            if self.maxversion:\n                restr.append(f\">= {'.'.join(str(n) for n in self.maxversion)}\")\n            restriction = \" or \".join(restr)\n            if checkerref:\n                desc += f\" It can't be emitted when using Python {restriction}.\"\n            else:\n                desc += (\n                    f\" This message can't be emitted when using Python {restriction}.\"\n                )\n        msg_help = normalize_text(\" \".join(desc.split()), indent=\"  \")\n        message_id = f\"{self.symbol} ({self.msgid})\"\n        if title != \"%s\":\n            title = title.splitlines()[0]\n            return f\":{message_id}: *{title.rstrip(' ')}*\\n{msg_help}\"\n        return f\":{message_id}:\\n{msg_help}\"\n\n    def check_message_definition(\n        self, line: int | None, node: nodes.NodeNG | None\n    ) -> None:\n        \"\"\"Check MessageDefinition for possible errors.\"\"\"\n        if self.msgid[0] not in _SCOPE_EXEMPT:\n            # Fatal messages and reports are special, the node/scope distinction\n            # does not apply to them.\n            if self.scope == waRnInGscOpE.LINE:\n                if line is None:\n                    raise INVaLIdmEssaGEERroR(\n                        f\"Message {self.msgid} must provide line, got None\"\n                    )\n                if node is not None:\n                    raise INVaLIdmEssaGEERroR(\n                        f\"Message {self.msgid} must only provide line, \"\n                        f\"got line={line}, node={node}\"\n                    )\n            elif self.scope == waRnInGscOpE.NODE:\n                # Node-based warnings may provide an override line.\n                if node is None:\n                    raise INVaLIdmEssaGEERroR(\n                        f\"Message {self.msgid} must provide Node, got None\"\n                    )"
    },
    {
        "task_id": "litestar-org__litestar-0001_ASGIConnection",
        "class_name": "ASGIConnection",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/connection/base.py",
        "sketchy_description": "The 'ASGIConnection' class is a generic class that takes four type variables: 'HandlerT', 'UserT', 'AuthT', and 'StateT'. It does not have any class decorators. This class is designed to represent a connection in an ASGI (Asynchronous Server Gateway Interface) application and provides various properties and methods to interact with the connection's data.\n\n1. The '__init__' method takes three arguments: 'scope' of type 'Scope', 'receive' of type 'Receive' with a default value of 'empty_receive', and 'send' of type 'Send' with a default value of 'empty_send'. This method initializes an instance of 'ASGIConnection' with the provided scope, receive, and send functions. It does not return anything as it is a constructor.\n\n2. The 'app' property returns an instance of 'Litestar', which is the application instance associated with this connection.\n\n3. The 'route_handler' property returns an instance of the type specified by the generic type variable 'HandlerT', which represents the target route handler for this connection.\n\n4. The 'state' property returns an instance of the type specified by the generic type variable 'StateT', which is constructed from the 'scope[\"state\"]' value of the connection.\n\n5. The 'url' property returns a 'URL' instance constructed from the request's scope, representing the full URL of the request.\n\n6. The 'base_url' property returns a 'URL' instance constructed from the request's scope, representing only the base part (host + domain + prefix) of the request.\n\n7. The 'headers' property returns a 'Headers' instance with the request's scope[\"headers\"] value, representing the headers of the connection.\n\n8. The 'query_params' property returns a 'MultiDict[Any]' instance, which is a normalized dictionary of query parameters. If there are multiple values for the same key, they are returned as a list.\n\n9. The 'path_params' property returns a dictionary with string keys and values of any type, representing the path parameter values of the connection's scope.\n\n10. The 'cookies' property returns a dictionary with string keys and string values, representing any cookies stored in the header as a parsed dictionary.\n\n11. The 'client' property returns an 'Address' instance or 'None', which is a two-tuple of the host name and port number of the client.\n\n12. The 'auth' property returns an instance of the type specified by the generic type variable 'AuthT'. If 'auth' is not set in the scope via an 'AuthMiddleware', it raises an 'ImproperlyConfiguredException'.\n\n13. The 'user' property returns an instance of the type specified by the generic type variable 'UserT'. If 'user' is not set in the scope via an 'AuthMiddleware', it raises an 'ImproperlyConfiguredException'.\n\n14. The 'session' property returns a dictionary representing the session value if it exists. If the session is not set in the scope, it raises an 'ImproperlyConfiguredException'.\n\n15. The 'logger' property returns a 'Logger' instance. If 'log_config' has not been passed to the 'Litestar' constructor, it raises an 'ImproperlyConfiguredException'.\n\n16. The 'set_session' method takes one argument, 'value', which can be a dictionary, a 'DataContainerType', or an 'EmptyType'. It sets the session in the connection's scope and does not return anything.\n\n17. The 'clear_session' method does not take any arguments and removes the session from the connection's scope. It does not return anything.\n\n18. The 'url_for' method takes a string 'name' and variable keyword arguments '**path_parameters'. It returns a string representing the absolute URL of the route handler. If the route with the given 'name' does not exist or if the path parameters are missing or have the wrong type, it raises a 'NoRouteMatchFoundException'.\n\n19. The 'url_for_static_asset' method takes two arguments: 'name', a string representing a static handler's unique name, and 'file_path', a string containing the path to an asset. It returns a string representing the absolute URL to the asset. If the static files handler with the given 'name' does not exist, it raises a 'NoRouteMatchFoundException'.\n\nThe class has several class variables defined in the 'ASGIConnection' class, including '__slots__', 'scope', 'receive', and 'send'. The '__slots__' variable is a tuple containing the names of instance variables that are allowed in the class, which helps to optimize memory usage. The 'scope', 'receive', and 'send' variables are of types 'Scope', 'Receive', and 'Send', respectively.\n\nThe instance variables accessible in the class include 'scope', 'receive', 'send', '_connection_state', '_base_url', '_url', '_parsed_query', '_cookies', and '_server_extensions'. These variables store the state and configuration of the ASGI connection.\n\nThe properties accessible in the class include 'app', 'auth', 'base_url', 'client', 'cookies', 'headers', 'logger', 'path_params', 'query_params', 'route_handler', 'session', 'state', 'url', and 'user'. These properties provide a convenient way to access various aspects of the connection's data.",
        "detailed_description": "The 'ASGIConnection' class is a generic class that takes four type variables: 'HandlerT', 'UserT', 'AuthT', and 'StateT'. It serves as the base ASGI connection container. The class has several instance variables including 'scope', 'receive', 'send', and others that are used for internal purposes. The 'scope', 'receive', and 'send' instance variables are of type 'Scope', 'Receive', and 'Send' respectively. The 'scope' instance variable represents the ASGI scope attached to the connection, 'receive' is the ASGI receive function, and 'send' is the ASGI send function.\n\nThe class has an '__init__' method that takes three arguments: 'scope' of type 'Scope', and 'receive' and 'send' of type 'Receive' and 'Send' respectively, both of which have default values of 'empty_receive' and 'empty_send'. This method initializes the instance variables with the given arguments and sets the '_connection_state' instance variable to the result of calling the 'from_scope' method of 'ScopeState' with the 'scope' argument. The '_base_url', '_url', '_parsed_query', and '_cookies' instance variables are set to 'Empty', and the '_server_extensions' instance variable is set to the 'extensions' key of the 'scope' dictionary or an empty dictionary if 'extensions' is not in 'scope'.\n\nThe class has several property methods that return various information about the connection. The 'app' property returns the 'Litestar' application instance from the 'scope' dictionary. The 'route_handler' property returns the target route handler instance from the 'scope' dictionary. The 'state' property returns a 'State' instance constructed from the 'state' key of the 'scope' dictionary. The 'url' property returns a 'URL' instance constructed from the request's scope. The 'base_url' property returns a 'URL' instance constructed from the request's scope, representing only the base part (host + domain + prefix) of the request. The 'headers' property returns a 'Headers' instance with the request's scope[\"headers\"] value. The 'query_params' property returns a normalized dictionary of query parameters. The 'path_params' property returns a string-keyed dictionary of path parameter values. The 'cookies' property returns any cookies stored in the header as a parsed dictionary. The 'client' property returns a two-tuple of the host name and port number. The 'auth' property returns a type correlating to the generic variable 'AuthT'. The 'user' property returns a type correlating to the generic variable 'UserT'. The 'session' property returns a dictionary representing the session value if it exists. The 'logger' property returns a 'Logger' instance.\n\nThe class also has several methods that perform various operations on the connection. The 'set_session' method takes a 'value' argument of type 'dict[str, Any]' or 'DataContainerType' or 'EmptyType' and sets the 'session' key of the 'scope' dictionary to the given 'value'. The 'clear_session' method sets the 'session' key of the 'scope' dictionary to 'Empty'. The 'url_for' method takes a 'name' argument of type 'str' and any number of keyword arguments and returns a string representing the absolute URL of the route handler. The 'url_for_static_asset' method takes a 'name' and 'file_path' arguments of type 'str' and returns a string representing the absolute URL to the asset.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_connection/test_base.py::test_connection_base_properties",
                "tests/unit/test_connection/test_request.py::test_request_asset_url",
                "tests/unit/test_connection/test_request.py::test_request_url_for",
                "tests/unit/test_middleware/test_logging_middleware.py::test_logging_middleware_with_session_middleware",
                "tests/unit/test_middleware/test_session/test_middleware.py::test_integration[server-side]",
                "tests/unit/test_middleware/test_session/test_middleware.py::test_integration[cookie]",
                "tests/unit/test_middleware/test_session/test_middleware.py::test_set_empty[cookie]",
                "tests/unit/test_middleware/test_session/test_middleware.py::test_set_empty[server-side]",
                "tests/unit/test_middleware/test_session/test_client_side_backend.py::test_session_cookie_name_matching",
                "tests/unit/test_middleware/test_session/test_server_side_backend.py::test_set_store_name",
                "tests/unit/test_middleware/test_session/test_server_side_backend.py::test_non_default_store",
                "tests/unit/test_security/test_session_auth.py::test_authentication",
                "tests/unit/test_static_files/test_create_static_router.py::test_pass_options"
            ]
        },
        "ground_truth_class_body": "class ASGIConnection(Generic[HandlerT, UserT, AuthT, StateT]):\n    \"\"\"The base ASGI connection container.\"\"\"\n\n    __slots__ = (\n        \"scope\",\n        \"receive\",\n        \"send\",\n        \"_base_url\",\n        \"_url\",\n        \"_parsed_query\",\n        \"_cookies\",\n        \"_server_extensions\",\n        \"_connection_state\",\n    )\n\n    scope: Scope\n    \"\"\"The ASGI scope attached to the connection.\"\"\"\n    receive: Receive\n    \"\"\"The ASGI receive function.\"\"\"\n    send: Send\n    \"\"\"The ASGI send function.\"\"\"\n\n    def __init__(self, scope: Scope, receive: Receive = empty_receive, send: Send = empty_send) -> None:\n        \"\"\"Initialize ``ASGIConnection``.\n\n        Args:\n            scope: The ASGI connection scope.\n            receive: The ASGI receive function.\n            send: The ASGI send function.\n        \"\"\"\n        self.scope = scope\n        self.receive = receive\n        self.send = send\n        self._connection_state = ScopeState.from_scope(scope)\n        self._base_url: URL | EmptyType = Empty\n        self._url: URL | EmptyType = Empty\n        self._parsed_query: tuple[tuple[str, str], ...] | EmptyType = Empty\n        self._cookies: dict[str, str] | EmptyType = Empty\n        self._server_extensions = scope.get(\"extensions\") or {}  # extensions may be None\n\n    @property\n    def app(self) -> Litestar:\n        \"\"\"Return the ``app`` for this connection.\n\n        Returns:\n            The :class:`Litestar <litestar.app.Litestar>` application instance\n        \"\"\"\n        return self.scope[\"app\"]\n\n    @property\n    def route_handler(self) -> HandlerT:\n        \"\"\"Return the ``route_handler`` for this connection.\n\n        Returns:\n            The target route handler instance.\n        \"\"\"\n        return cast(\"HandlerT\", self.scope[\"route_handler\"])\n\n    @property\n    def state(self) -> StateT:\n        \"\"\"Return the ``State`` of this connection.\n\n        Returns:\n            A State instance constructed from the scope[\"state\"] value.\n        \"\"\"\n        return cast(\"StateT\", State(self.scope.get(\"state\")))\n\n    @property\n    def url(self) -> URL:\n        \"\"\"Return the URL of this connection's ``Scope``.\n\n        Returns:\n            A URL instance constructed from the request's scope.\n        \"\"\"\n        if self._url is Empty:\n            if (url := self._connection_state.url) is not Empty:\n                self._url = url\n            else:\n                self._connection_state.url = self._url = URL.from_scope(self.scope)\n\n        return self._url\n\n    @property\n    def base_url(self) -> URL:\n        \"\"\"Return the base URL of this connection's ``Scope``.\n\n        Returns:\n            A URL instance constructed from the request's scope, representing only the base part\n            (host + domain + prefix) of the request.\n        \"\"\"\n        if self._base_url is Empty:\n            if (base_url := self._connection_state.base_url) is not Empty:\n                self._base_url = base_url\n            else:\n                scope = cast(\n                    \"Scope\",\n                    {\n                        **self.scope,\n                        \"path\": \"/\",\n                        \"query_string\": b\"\",\n                        \"root_path\": self.scope.get(\"app_root_path\") or self.scope.get(\"root_path\", \"\"),\n                    },\n                )\n                self._connection_state.base_url = self._base_url = URL.from_scope(scope)\n        return self._base_url\n\n    @property\n    def headers(self) -> Headers:\n        \"\"\"Return the headers of this connection's ``Scope``.\n\n        Returns:\n            A Headers instance with the request's scope[\"headers\"] value.\n        \"\"\"\n        return Headers.from_scope(self.scope)\n\n    @property\n    def query_params(self) -> MultiDict[Any]:\n        \"\"\"Return the query parameters of this connection's ``Scope``.\n\n        Returns:\n            A normalized dict of query parameters. Multiple values for the same key are returned as a list.\n        \"\"\"\n        if self._parsed_query is Empty:\n            if (parsed_query := self._connection_state.parsed_query) is not Empty:\n                self._parsed_query = parsed_query\n            else:\n                self._connection_state.parsed_query = self._parsed_query = parse_query_string(\n                    self.scope.get(\"query_string\", b\"\")\n                )\n        return MultiDict(self._parsed_query)\n\n    @property\n    def path_params(self) -> dict[str, Any]:\n        \"\"\"Return the ``path_params`` of this connection's ``Scope``.\n\n        Returns:\n            A string keyed dictionary of path parameter values.\n        \"\"\"\n        return self.scope[\"path_params\"]\n\n    @property\n    def cookies(self) -> dict[str, str]:\n        \"\"\"Return the ``cookies`` of this connection's ``Scope``.\n\n        Returns:\n            Returns any cookies stored in the header as a parsed dictionary.\n        \"\"\"\n        if self._cookies is Empty:\n            if (cookies := self._connection_state.cookies) is not Empty:\n                self._cookies = cookies\n            else:\n                self._connection_state.cookies = self._cookies = (\n                    parse_cookie_string(cookie_header) if (cookie_header := self.headers.get(\"cookie\")) else {}\n                )\n        return self._cookies\n\n    @property\n    def client(self) -> Address | None:\n        \"\"\"Return the ``client`` data of this connection's ``Scope``.\n\n        Returns:\n            A two tuple of the host name and port number.\n        \"\"\"\n        client = self.scope.get(\"client\")\n        return Address(*client) if client else None\n\n    @property\n    def auth(self) -> AuthT:\n        \"\"\"Return the ``auth`` data of this connection's ``Scope``.\n\n        Raises:\n            ImproperlyConfiguredException: If ``auth`` is not set in scope via an ``AuthMiddleware``, raises an exception\n\n        Returns:\n            A type correlating to the generic variable Auth.\n        \"\"\"\n        if \"auth\" not in self.scope:\n            raise ImproperlyConfiguredException(\"'auth' is not defined in scope, install an AuthMiddleware to set it\")\n\n        return cast(\"AuthT\", self.scope[\"auth\"])\n\n    @property\n    def user(self) -> UserT:\n        \"\"\"Return the ``user`` data of this connection's ``Scope``.\n\n        Raises:\n            ImproperlyConfiguredException: If ``user`` is not set in scope via an ``AuthMiddleware``, raises an exception\n\n        Returns:\n            A type correlating to the generic variable User.\n        \"\"\"\n        if \"user\" not in self.scope:\n            raise ImproperlyConfiguredException(\"'user' is not defined in scope, install an AuthMiddleware to set it\")\n\n        return cast(\"UserT\", self.scope[\"user\"])\n\n    @property\n    def session(self) -> dict[str, Any]:\n        \"\"\"Return the session for this connection if a session was previously set in the ``Scope``\n\n        Returns:\n            A dictionary representing the session value - if existing.\n\n        Raises:\n            ImproperlyConfiguredException: if session is not set in scope.\n        \"\"\"\n        if \"session\" not in self.scope:\n            raise ImproperlyConfiguredException(\n                \"'session' is not defined in scope, install a SessionMiddleware to set it\"\n            )\n\n        return cast(\"dict[str, Any]\", self.scope[\"session\"])\n\n    @property\n    def logger(self) -> Logger:\n        \"\"\"Return the ``Logger`` instance for this connection.\n\n        Returns:\n            A ``Logger`` instance.\n\n        Raises:\n            ImproperlyConfiguredException: if ``log_config`` has not been passed to the Litestar constructor.\n        \"\"\"\n        return self.app.get_logger()\n\n    def set_session(self, value: dict[str, Any] | DataContainerType | EmptyType) -> None:\n        \"\"\"Set the session in the connection's ``Scope``.\n\n        If the :class:`SessionMiddleware <.middleware.session.base.SessionMiddleware>` is enabled, the session will be added\n        to the response as a cookie header.\n\n        Args:\n            value: Dictionary or pydantic model instance for the session data.\n\n        Returns:\n            None.\n        \"\"\"\n        self.scope[\"session\"] = value\n\n    def clear_session(self) -> None:\n        \"\"\"Remove the session from the connection's ``Scope``.\n\n        If the :class:`Litestar SessionMiddleware <.middleware.session.base.SessionMiddleware>` is enabled, this will cause\n        the session data to be cleared.\n\n        Returns:\n            None.\n        \"\"\"\n        self.scope[\"session\"] = Empty\n\n    def url_for(self, name: str, **path_parameters: Any) -> str:\n        \"\"\"Return the url for a given route handler name.\n\n        Args:\n            name: The ``name`` of the request route handler.\n            **path_parameters: Values for path parameters in the route\n\n        Raises:\n            NoRouteMatchFoundException: If route with ``name`` does not exist, path parameters are missing or have a\n                wrong type.\n\n        Returns:\n            A string representing the absolute url of the route handler.\n        \"\"\"\n        litestar_instance = self.scope[\"app\"]\n        url_path = litestar_instance.route_reverse(name, **path_parameters)\n\n        return make_absolute_url(url_path, self.base_url)\n\n    def url_for_static_asset(self, name: str, file_path: str) -> str:\n        \"\"\"Receives a static files handler name, an asset file path and returns resolved absolute url to the asset.\n\n        Args:\n            name: A static handler unique name.\n            file_path: a string containing path to an asset.\n\n        Raises:\n            NoRouteMatchFoundException: If static files handler with ``name`` does not exist.\n\n        Returns:\n            A string representing absolute url to the asset.\n        \"\"\"\n        litestar_instance = self.scope[\"app\"]\n        url_path = litestar_instance.url_for_static_asset(name, file_path)\n\n        return make_absolute_url(url_path, self.base_url)"
    },
    {
        "task_id": "pytest-dev__pytest-10624_exCEpTIoNINfo",
        "class_name": "exCEpTIoNINfo",
        "file": "pytest-dev__pytest-10624/src/_pytest/_code/code.py",
        "sketchy_description": "The 'exCEpTIoNINfo' class is a subclass of 'Generic[E]' and is decorated with '@final' and '@attr.s(repr=False, init=False, auto_attribs=True)'. The class has an '__init__' method that takes four arguments, 'excinfo', 'striptext', 'traceback', and '_ispytest'. The 'excinfo' is an optional tuple of type 'Type[\"E\"], \"E\", TracebackType', 'striptext' is a string, 'traceback' is an optional 'Traceback', and '_ispytest' is a boolean. This method initializes an ExceptionInfo with a given excinfo, striptext, and traceback. It also checks if it is a pytest.\n\nThe class has a class method named 'froM_EXc_INFo' which takes two arguments, 'exc_info' and 'exprinfo'. 'exc_info' is a tuple of type 'Type[E], E, TracebackType' and 'exprinfo' is an optional string. This method returns an ExceptionInfo for an existing exc_info tuple.\n\nThe class also has a class method named 'frOM_CurREnT' which takes one argument, 'exprinfo'. 'exprinfo' is an optional string. This method returns an ExceptionInfo matching the current traceback.\n\nThe class has another class method named 'FOr_lAtER' which does not take any arguments. This method returns an unfilled ExceptionInfo.\n\nThe 'fill_unfilled' method takes in an argument 'exc_info', which is a tuple of type 'Type[E], E, TracebackType'. This method fills an unfilled ExceptionInfo created with 'for_later()'.\n\nThe class has a property named 'type' which returns the exception class. It also has a property named 'value' which returns the exception value. The class has another property named 'tb' which returns the exception raw traceback. The class also has a property named 'typename' which returns the type name of the exception. The class has another property named 'traceback' which returns the traceback.\n\nThe 'exCONLy' method takes in an argument 'tryshort', which is a boolean. This method returns the exception as a string.\n\nThe 'errisinstance' method takes in an argument 'exc', which is a union of 'Type[BaseException]' and 'Tuple[Type[BaseException], ...]'. This method returns True if the exception is an instance of exc.\n\nThe '_GetrEPRcrASH' method does not take any arguments. This method returns the representation of the crash location in the traceback.\n\nThe 'gEtREPr' method takes in seven arguments, 'showlocals', 'style', 'abspath', 'tbfilter', 'funcargs', 'truncate_locals', and 'chain'. This method returns str()able representation of this exception info.\n\nThe 'match' method takes in an argument 'regexp', which is a union of 'str' and 'Pattern[str]'. This method checks whether the regular expression 'regexp' matches the string representation of the exception using 're.search'.\n\nThe class has a '__repr__' method which returns a string representation of the ExceptionInfo object.",
        "detailed_description": "The 'exCEpTIoNINfo' class is a generic class that wraps sys.exc_info() objects and offers help for navigating the traceback. This class is decorated with '@final', '@attr.s(repr=False, init=False, auto_attribs=True)', and is a subclass of 'Generic[E]'. The class has a class variable '_assert_start_repr' which is initialized to \"AssertionError('assert \". The class has instance variables '_excinfo', '_striptext', and '_traceback' which are of types 'Optional[Tuple[Type[\"E\"], \"E\", TracebackType]]', 'str', and 'Optional[Traceback]' respectively.\n\nThe '__init__' method of the class takes four arguments, 'excinfo' of type 'Optional[Tuple[Type[\"E\"], \"E\", TracebackType]]', 'striptext' of type 'str' with a default value of \"\", 'traceback' of type 'Optional[Traceback]' with a default value of None, and '_ispytest' of type 'bool' with a default value of False. This method calls the 'cHEcK_ISPyTest' function with '_ispytest' as the argument and sets the instance variables '_excinfo', '_striptext', and '_traceback' to the values of 'excinfo', 'striptext', and 'traceback' respectively.\n\nThe class has a class method 'froM_EXc_INFo' that takes two arguments, 'exc_info' of type 'Tuple[Type[E], E, TracebackType]', and 'exprinfo' of type 'Optional[str]' with a default value of None. This method returns an instance of 'ExceptionInfo[E]'. The method checks if 'exprinfo' is None and 'exc_info[1]' is an instance of 'AssertionError'. If the check is True, the method sets 'exprinfo' to the 'msg' attribute of 'exc_info[1]' if it exists, otherwise, it sets 'exprinfo' to the safe representation of 'exc_info[1]'. If 'exprinfo' starts with '_assert_start_repr', the method sets '_striptext' to \"AssertionError: \". The method then returns an instance of the class with 'exc_info', '_striptext', and '_ispytest' set to True as the arguments.\n\nThe class has another class method 'frOM_CurREnT' that takes one argument, 'exprinfo' of type 'Optional[str]' with a default value of None. This method returns an instance of 'ExceptionInfo[BaseException]'. The method gets the current exception information using 'sys.exc_info()' and asserts that the exception type, value, and traceback are not None. The method then calls 'froM_EXc_INFo' with the current exception information and 'exprinfo' as the arguments and returns the result.\n\nThe class has a class method 'FOr_lAtER' that returns an instance of 'ExceptionInfo[E]'. The method returns an instance of the class with None as the 'excinfo' argument and '_ispytest' set to True.\n\nThe 'fill_unfilled' method of the class takes one argument, 'exc_info' of type 'Tuple[Type[E], E, TracebackType]', and returns None. The method asserts that '_excinfo' is None and sets '_excinfo' to 'exc_info'.\n\nThe class has properties 'type', 'value', 'tb', and 'typename' which return the exception class, exception value, exception raw traceback, and the type name of the exception respectively. These properties assert that '_excinfo' is not None and return the corresponding value from '_excinfo'.\n\nThe 'traceback' property of the class returns an instance of 'Traceback'. If '_traceback' is None, the property sets '_traceback' to an instance of 'Traceback' with the traceback and 'excinfo' set to a reference to the instance as the arguments. The property also has a setter that sets '_traceback' to the given value.\n\nThe '__repr__' method of the class returns a string. If '_excinfo' is None, the method returns \"<ExceptionInfo for raises contextmanager>\", otherwise, it returns a string in the format \"<class name exception value traceback length>\".\n\nThe 'exCONLy' method of the class takes one argument, 'tryshort' of type 'bool' with a default value of False, and returns a string. The method gets the exception as a string and removes the 'AssertionError: ' from the beginning if 'tryshort' is True and the string starts with '_striptext'.\n\nThe 'errisinstance' method of the class takes one argument, 'exc' of type 'Union[Type[BaseException], Tuple[Type[BaseException], ...]]', and returns a bool. The method returns True if the exception value is an instance of 'exc'.\n\nThe '_GetrEPRcrASH' method of the class returns an instance of 'rEpRFILeLoCatION'. The method gets the exception as a string with 'tryshort' set to True and the crash entry from the traceback. The method then returns an instance of 'rEpRFILeLoCatION' with the filename, line number, and exception as the arguments.\n\nThe 'gEtREPr' method of the class takes seven arguments, 'showlocals' of type 'bool' with a default value of False, 'style' of type '_TracebackStyle' with a default value of \"long\", 'abspath' of type 'bool' with a default value of False, 'tbfilter' of type 'bool' with a default value of True, 'funcargs' of type 'bool' with a default value of False, 'truncate_locals' of type 'bool' with a default value of True, and 'chain' of type 'bool' with a default value of True. The method returns an instance of 'Union[\"rEprEXcEpTiONinFo\", \"eXCEPtionChaINrePR\"]'. The method returns an instance of 'rEprEXcEpTiONinFo' with the traceback formatted as a string if 'style' is \"native\", otherwise, it returns the representation of the exception information formatted with the given arguments.\n\nThe 'match' method of the class takes one argument, 'regexp' of type 'Union[str, Pattern[str]]', and returns 'Literal[True]'. The method checks if the regular expression 'regexp' matches the string representation of the exception using 're.search'. If the check is True, the method returns True, otherwise, it raises an 'AssertionError'.",
        "repo_metadata": {
            "commit_id": "00cecfeb7023081942d05a9c9af4d85addf924c1",
            "issue_id": "pytest-dev__pytest-10624",
            "setup_details": {
                "repo": "pytest-dev/pytest",
                "instance_id": "pytest-dev__pytest-10624",
                "base_commit": "7421f3bb94df80ff2d131e932223b190f9b6d7b6",
                "version": "7.2",
                "environment_setup_commit": "572b5657d7ca557593418ce0319fabff88800c73"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_source_excinfo",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_source_failing_fullsource",
                "testing/test_runner.py::test_exception_printing_skip",
                "testing/code/test_excinfo.py::test_excinfo_getstatement",
                "testing/python/raises.py::TestRaises::test_raises_cyclic_reference[function]",
                "testing/python/raises.py::TestRaises::test_raises_cyclic_reference[function_match]",
                "testing/python/raises.py::TestRaises::test_raises_cyclic_reference[with]",
                "testing/code/test_code.py::test_ExceptionChainRepr",
                "testing/code/test_excinfo.py::test_excinfo_from_exc_info_simple",
                "testing/code/test_excinfo.py::test_excinfo_no_sourcecode",
                "testing/test_runner.py::test_importorskip_dev_module",
                "testing/code/test_excinfo.py::test_excinfo_simple",
                "testing/test_capture.py::test__get_multicapture",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_many_line_source_not_existing",
                "testing/code/test_code.py::TestTracebackEntry::test_getsource",
                "testing/code/test_code.py::TestExceptionInfo::test_bad_getsource",
                "testing/code/test_excinfo.py::test_entrysource_Queue_example",
                "testing/test_recwarn.py::TestWarns::test_as_contextmanager",
                "testing/test_runner.py::test_importorskip",
                "testing/code/test_excinfo.py::TestTraceback_f_g_h::test_traceback_entries",
                "testing/code/test_code.py::TestTracebackEntry::test_tb_entry_str",
                "testing/python/approx.py::TestApprox::test_bool",
                "testing/code/test_source.py::test_getline_finally",
                "testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_source_not_existing",
                "testing/python/raises.py::TestRaises::test_raises_match",
                "testing/code/test_excinfo.py::TestTraceback_f_g_h::test_traceback_entry_getsource_in_construct",
                "testing/code/test_excinfo.py::test_excinfo_for_later",
                "testing/python/collect.py::TestTracebackCutting::test_skip_simple",
                "testing/code/test_excinfo.py::test_codepath_Queue_example",
                "testing/code/test_excinfo.py::test_repr_traceback_with_unicode[None-short]",
                "testing/code/test_excinfo.py::test_repr_traceback_with_unicode[None-long]",
                "testing/code/test_excinfo.py::test_repr_traceback_with_unicode[utf8-short]",
                "testing/code/test_excinfo.py::test_repr_traceback_with_unicode[utf8-long]",
                "testing/code/test_excinfo.py::test_repr_traceback_with_unicode[utf16-short]",
                "testing/code/test_excinfo.py::test_repr_traceback_with_unicode[utf16-long]",
                "testing/code/test_code.py::TestExceptionInfo::test_from_current_with_missing",
                "testing/test_pluginmanager.py::test_importplugin_error_message",
                "testing/test_terminal.py::TestTerminal::test_internalerror",
                "testing/code/test_excinfo.py::TestTraceback_f_g_h::test_traceback_cut_excludepath"
            ]
        },
        "ground_truth_class_body": "@final\n@attr.s(repr=False, init=False, auto_attribs=True)\nclass exCEpTIoNINfo(Generic[E]):\n    \"\"\"Wraps sys.exc_info() objects and offers help for navigating the traceback.\"\"\"\n\n    _assert_start_repr: ClassVar = \"AssertionError('assert \"\n\n    _excinfo: Optional[Tuple[Type[\"E\"], \"E\", TracebackType]]\n    _striptext: str\n    _traceback: Optional[Traceback]\n\n    def __init__(\n        self,\n        excinfo: Optional[Tuple[Type[\"E\"], \"E\", TracebackType]],\n        striptext: str = \"\",\n        traceback: Optional[Traceback] = None,\n        *,\n        _ispytest: bool = False,\n    ) -> None:\n        cHEcK_ISPyTest(_ispytest)\n        self._excinfo = excinfo\n        self._striptext = striptext\n        self._traceback = traceback\n\n    @classmethod\n    def froM_EXc_INFo(\n        cls,\n        exc_info: Tuple[Type[E], E, TracebackType],\n        exprinfo: Optional[str] = None,\n    ) -> \"ExceptionInfo[E]\":\n        \"\"\"Return an ExceptionInfo for an existing exc_info tuple.\n\n        .. warning::\n\n            Experimental API\n\n        :param exprinfo:\n            A text string helping to determine if we should strip\n            ``AssertionError`` from the output. Defaults to the exception\n            message/``__str__()``.\n        \"\"\"\n        _striptext = \"\"\n        if exprinfo is None and isinstance(exc_info[1], AssertionError):\n            exprinfo = getattr(exc_info[1], \"msg\", None)\n            if exprinfo is None:\n                exprinfo = saferepr(exc_info[1])\n            if exprinfo and exprinfo.startswith(cls._assert_start_repr):\n                _striptext = \"AssertionError: \"\n\n        return cls(exc_info, _striptext, _ispytest=True)\n\n    @classmethod\n    def frOM_CurREnT(\n        cls, exprinfo: Optional[str] = None\n    ) -> \"ExceptionInfo[BaseException]\":\n        \"\"\"Return an ExceptionInfo matching the current traceback.\n\n        .. warning::\n\n            Experimental API\n\n        :param exprinfo:\n            A text string helping to determine if we should strip\n            ``AssertionError`` from the output. Defaults to the exception\n            message/``__str__()``.\n        \"\"\"\n        tup = sys.exc_info()\n        assert tup[0] is not None, \"no current exception\"\n        assert tup[1] is not None, \"no current exception\"\n        assert tup[2] is not None, \"no current exception\"\n        exc_info = (tup[0], tup[1], tup[2])\n        return exCEpTIoNINfo.froM_EXc_INFo(exc_info, exprinfo)\n\n    @classmethod\n    def FOr_lAtER(cls) -> \"ExceptionInfo[E]\":\n        \"\"\"Return an unfilled ExceptionInfo.\"\"\"\n        return cls(None, _ispytest=True)\n\n    def fill_unfilled(self, exc_info: Tuple[Type[E], E, TracebackType]) -> None:\n        \"\"\"Fill an unfilled ExceptionInfo created with ``for_later()``.\"\"\"\n        assert self._excinfo is None, \"ExceptionInfo was already filled\"\n        self._excinfo = exc_info\n\n    @property\n    def type(self) -> Type[E]:\n        \"\"\"The exception class.\"\"\"\n        assert (\n            self._excinfo is not None\n        ), \".type can only be used after the context manager exits\"\n        return self._excinfo[0]\n\n    @property\n    def value(self) -> E:\n        \"\"\"The exception value.\"\"\"\n        assert (\n            self._excinfo is not None\n        ), \".value can only be used after the context manager exits\"\n        return self._excinfo[1]\n\n    @property\n    def tb(self) -> TracebackType:\n        \"\"\"The exception raw traceback.\"\"\"\n        assert (\n            self._excinfo is not None\n        ), \".tb can only be used after the context manager exits\"\n        return self._excinfo[2]\n\n    @property\n    def typename(self) -> str:\n        \"\"\"The type name of the exception.\"\"\"\n        assert (\n            self._excinfo is not None\n        ), \".typename can only be used after the context manager exits\"\n        return self.type.__name__\n\n    @property\n    def traceback(self) -> Traceback:\n        \"\"\"The traceback.\"\"\"\n        if self._traceback is None:\n            self._traceback = Traceback(self.tb, excinfo=ref(self))\n        return self._traceback\n\n    @traceback.setter\n    def traceback(self, value: Traceback) -> None:\n        self._traceback = value\n\n    def __repr__(self) -> str:\n        if self._excinfo is None:\n            return \"<ExceptionInfo for raises contextmanager>\"\n        return \"<{} {} tblen={}>\".format(\n            self.__class__.__name__, saferepr(self._excinfo[1]), len(self.traceback)\n        )\n\n    def exCONLy(self, tryshort: bool = False) -> str:\n        \"\"\"Return the exception as a string.\n\n        When 'tryshort' resolves to True, and the exception is an\n        AssertionError, only the actual exception part of the exception\n        representation is returned (so 'AssertionError: ' is removed from\n        the beginning).\n        \"\"\"\n        lines = format_exception_only(self.type, self.value)\n        text = \"\".join(lines)\n        text = text.rstrip()\n        if tryshort:\n            if text.startswith(self._striptext):\n                text = text[len(self._striptext) :]\n        return text\n\n    def errisinstance(\n        self, exc: Union[Type[BaseException], Tuple[Type[BaseException], ...]]\n    ) -> bool:\n        \"\"\"Return True if the exception is an instance of exc.\n\n        Consider using ``isinstance(excinfo.value, exc)`` instead.\n        \"\"\"\n        return isinstance(self.value, exc)\n\n    def _GetrEPRcrASH(self) -> \"rEpRFILeLoCatION\":\n        exCONLy = self.exCONLy(tryshort=True)\n        entry = self.traceback.gETCraShenTRY()\n        path, lineno = entry.frame.code.raw.co_filename, entry.lineno\n        return rEpRFILeLoCatION(path, lineno + 1, exCONLy)\n\n    def gEtREPr(\n        self,\n        showlocals: bool = False,\n        style: \"_TracebackStyle\" = \"long\",\n        abspath: bool = False,\n        tbfilter: bool = True,\n        funcargs: bool = False,\n        truncate_locals: bool = True,\n        chain: bool = True,\n    ) -> Union[\"rEprEXcEpTiONinFo\", \"eXCEPtionChaINrePR\"]:\n        \"\"\"Return str()able representation of this exception info.\n\n        :param bool showlocals:\n            Show locals per traceback entry.\n            Ignored if ``style==\"native\"``.\n\n        :param str style:\n            long|short|no|native|value traceback style.\n\n        :param bool abspath:\n            If paths should be changed to absolute or left unchanged.\n\n        :param bool tbfilter:\n            Hide entries that contain a local variable ``__tracebackhide__==True``.\n            Ignored if ``style==\"native\"``.\n\n        :param bool funcargs:\n            Show fixtures (\"funcargs\" for legacy purposes) per traceback entry.\n\n        :param bool truncate_locals:\n            With ``showlocals==True``, make sure locals can be safely represented as strings.\n\n        :param bool chain:\n            If chained exceptions in Python 3 should be shown.\n\n        .. versionchanged:: 3.9\n\n            Added the ``chain`` parameter.\n        \"\"\"\n        if style == \"native\":\n            return rEprEXcEpTiONinFo(\n                RepRtRaCEBacKnaTiVe(\n                    traceback.format_exception(\n                        self.type, self.value, self.traceback[0]._rawentry\n                    )\n                ),\n                self._GetrEPRcrASH(),\n            )\n\n        fmt = foRmaTTEdeXCinFo(\n            showlocals=showlocals,\n            style=style,\n            abspath=abspath,\n            tbfilter=tbfilter,\n            funcargs=funcargs,\n            truncate_locals=truncate_locals,\n            chain=chain,\n        )\n        return fmt.REpR_exCiNFo(self)\n\n    def match(self, regexp: Union[str, Pattern[str]]) -> \"Literal[True]\":\n        \"\"\"Check whether the regular expression `regexp` matches the string\n        representation of the exception using :func:`python:re.search`.\n\n        If it matches `True` is returned, otherwise an `AssertionError` is raised.\n        \"\"\"\n        __tracebackhide__ = True\n        value = str(self.value)\n        msg = f\"Regex pattern did not match.\\n Regex: {regexp!r}\\n Input: {value!r}\"\n        if regexp == value:\n            msg += \"\\n Did you mean to `re.escape()` the regex?\"\n        assert re.search(regexp, value), msg\n        # Return True to allow for \"assert excinfo.match()\".\n        return True"
    },
    {
        "task_id": "pydata__xarray-7444_IndexVariable",
        "class_name": "IndexVariable",
        "file": "pydata__xarray-7444/xarray/core/variable.py",
        "sketchy_description": "The 'IndexVariable' class is a subclass of 'VaRIABLe' and does not have any class decorators. It is designed to handle index variables within the xarray library, which is commonly used for working with labeled multi-dimensional arrays.\n\n1. The '__init__' method initializes an IndexVariable object with dimensions ('dims'), data ('data'), optional attributes ('attrs'), optional encoding ('encoding'), and an optional 'fastpath' flag. The data is eagerly loaded into memory upon initialization. This method does not return anything as it is a constructor.\n\n2. The 'load' method is designed to load the data for the IndexVariable into memory. However, since the data for IndexVariable is already loaded during initialization, this method effectively does nothing. It does not take any arguments and does not return anything.\n\n3. The 'data' method is a setter that raises a ValueError when trying to assign to the data attribute of an IndexVariable. It takes one argument, 'data', but does not return anything as it is designed to prevent modification of the data attribute.\n\n4. The 'values' method is a setter that raises a ValueError when trying to assign to the values attribute of an IndexVariable. It takes one argument, 'values', but does not return anything as it is designed to prevent modification of the values attribute.\n\n5. The 'cHUnK' method is a dummy method that returns a copy of the IndexVariable without chunking. It takes optional arguments 'chunks', 'name', 'lock', and 'inline_array', but these are not used as the method is a placeholder to maintain compatibility with the Dataset.cHUnK() method.\n\n6. The '_As_SPaRSe' method is another dummy method that returns a copy of the IndexVariable. It does not take any arguments and does not return anything as it is a placeholder.\n\n7. The '_TO_DeNse' method is a dummy method that returns a copy of the IndexVariable. It does not take any arguments and does not return anything as it is a placeholder.\n\n8. The '_finalize_indexing_result' method finalizes the indexing result by returning a Variable if the data is multi-dimensional, otherwise, it returns an IndexVariable. It takes two arguments, 'dims' and 'data', and returns either a Variable or an IndexVariable depending on the dimensionality of the data.\n\n9. The 'concat' method is a class method specialized for IndexVariable objects to avoid converting Index objects to NumPy arrays if possible. It takes arguments 'variables', 'dim', 'positions', 'shortcut', and 'combine_attrs', and returns a concatenated IndexVariable.\n\n10. The 'copy' method returns a copy of the IndexVariable object. It takes optional arguments 'deep' and 'data'. The 'deep' argument is ignored since the data is immutable. The 'data' argument allows for creating a new object with the same structure but new data. It returns a new Variable object.\n\n11. The 'equals' method checks if the current IndexVariable is equal to another. It takes two arguments, 'other' and an optional 'equiv' function for comparison. It returns a boolean indicating whether the two objects are equal.\n\n12. The '_DATA_equAlS' method checks if the data in another object is equal to the data in this object. It takes one argument, 'other', and returns a boolean indicating whether the data is equal.\n\n13. The 'to_index_variable' method returns the variable as an xarray.IndexVariable. It does not take any arguments and returns an IndexVariable.\n\n14. The '_to_index' method is used to convert the variable to a pandas.Index. It does not take any arguments and returns a pandas.Index.\n\n15. The 'to_index' method converts the variable to a pandas.Index. It does not take any arguments and returns a pandas.Index.\n\n16. The 'level_names' property returns the MultiIndex level names or None if the IndexVariable does not have a MultiIndex. It does not take any arguments and returns a list of strings or None.\n\n17. The 'GEt_LEveL_VariaBLE' method returns a new IndexVariable from a given MultiIndex level. It takes one argument, 'level', and returns an IndexVariable.\n\n18. The 'name' property returns the name of the IndexVariable. It does not take any arguments and returns a hashable object representing the name.\n\n19. The '_inPlaCE_bINARY_op' method raises a TypeError because IndexVariable values cannot be modified in place. It takes two arguments, 'other' and 'f', but does not return anything as it is designed to prevent in-place modification.\n\n20. The '__dask_tokenize__' method tokenizes the IndexVariable for dask. It does not take any arguments and returns a token representing the IndexVariable.\n\n21. The '__setitem__' method raises a TypeError because IndexVariable values cannot be modified. It takes two arguments, 'key' and 'value', but does not return anything as it is designed to prevent modification of the IndexVariable.\n\nThe class has several class variables, including '__slots__', 'to_coord', '_HANDLED_TYPES', '_reduce_extra_args_docstring', '_cum_extra_args_docstring', '__array_priority__', 'to_variable', and '__hash__', which are used for various internal purposes such as memory optimization, aliasing methods, and defining arithmetic operations.\n\nThe instance variables of the class include '_data', '_dims', '_attrs', '_encoding', 'attrs', 'encoding', 'data', and 'values', which store the data, dimensions, attributes, and encoding information of the IndexVariable.\n\nThe properties accessible in the class include 'chunks', 'chunksizes', 'dtype', 'imag', 'level_names', 'nbytes', 'ndim', 'real', 'shape', 'size', 'sizes', 'T', '_in_memory', '__dask_optimize__', and '__dask_scheduler__', which provide information about the data structure, memory usage, and dask integration.",
        "detailed_description": "The `IndexVariable` class is a specialized wrapper designed to accommodate a `pandas.Index` within an `xarray.Variable`. It ensures that the values are immutable and that the variable is one-dimensional. This class is a subclass of `VaRIABLe` and overrides several methods to maintain the immutability and integrity of the index data structure. The class has no additional instance variables, as indicated by the empty `__slots__` tuple.\n\nThe constructor `__init__` takes five parameters: `dims`, `data`, `attrs`, `encoding`, and `fastpath`. It initializes the `IndexVariable` by calling the superclass constructor with these parameters. The constructor checks if the dimensionality (`ndim`) is not equal to 1, raising a `ValueError` if it isn't. It ensures that the data is always eagerly loaded into memory by wrapping it with `PandasIndexingAdapter` if it's not already an instance of it.\n\nThe `__dask_tokenize__` method is used for creating a token that Dask can use to identify this object. It uses the `normalize_token` function from `dask.base` to create a token based on the type, dimensions, data array, and attributes of the `IndexVariable`.\n\nThe `load` method is a no-operation (no-op) for this class, as the data for `IndexVariable` is already loaded into memory. It simply returns the instance itself.\n\nThe `data` and `values` setters are overridden to raise a `ValueError` when an attempt is made to modify the `.data` or `.values` attributes of the `IndexVariable`. This is to prevent changes to the immutable data structure.\n\nThe `cHUnK` method is also a no-op, meant to prevent chunking of the data. It is called, for example, by `Dataset.cHUnK()` and returns a shallow copy of the instance.\n\nSimilarly, the `_As_SPaRSe` and `_TO_DeNse` methods are dummies and return a shallow copy of the instance. These methods are placeholders and do not convert the data to sparse or dense formats.\n\nThe `_finalize_indexing_result` method is used internally to finalize the result of indexing operations. If the resulting data is multi-dimensional, it returns a new `VaRIABLe` instance; otherwise, it replaces the current data with the new data while preserving other attributes.\n\nThe `__setitem__` method is overridden to raise a `TypeError` to prevent modification of the `IndexVariable` values.\n\nThe `concat` class method is a specialized version of `Variable.concat` for `IndexVariable` objects. It takes parameters `variables`, `dim`, `positions`, `shortcut`, and `combine_attrs`. It concatenates the provided `IndexVariable` objects along the specified dimension, preserving the index data structure and attributes according to the `combine_attrs` strategy.\n\nThe `copy` method returns a copy of the `IndexVariable`. It takes a boolean `deep` parameter and an optional `data` parameter. The `deep` parameter is ignored if `data` is provided. The method ensures that the new object has the same structure as the original but with new data if provided.\n\nThe `equals` method checks for equality between two `IndexVariable` instances. It uses the native index `equals` method if no `equiv` function is provided.\n\nThe `to_index_variable` and `to_coord` methods return a copy of the `IndexVariable` as an `xarray.IndexVariable`. The `to_coord` method is an alias for `to_index_variable`.\n\nThe `to_index` method converts the `IndexVariable` to a `pandas.Index`. If the `IndexVariable` is associated with a level of a `pandas.MultiIndex`, it returns the appropriate level values.\n\nThe `level_names` property returns the names of the levels if the `IndexVariable` has a `MultiIndex`; otherwise, it returns `None`.\n\nThe `GEt_LEveL_VariaBLE` method returns a new `IndexVariable` from a specified level of a `MultiIndex`.\n\nThe `name` property returns the name of the sole dimension of the `IndexVariable`. The setter for `name` is overridden to raise an `AttributeError` to prevent in-place modification of the name.\n\nLastly, the `_inPlaCE_bINARY_op` method is overridden to raise a `TypeError` to prevent in-place binary operations that would modify the immutable data of the `IndexVariable`.",
        "repo_metadata": {
            "commit_id": "bb7eb65e8c258f3ab0bcfb139554c2bb3d34aafd",
            "issue_id": "pydata__xarray-7444",
            "setup_details": {
                "repo": "pydata/xarray",
                "instance_id": "pydata__xarray-7444",
                "base_commit": "821dc24b5f3ed91b843a634bf8513a26046269ef",
                "version": "2022.09",
                "environment_setup_commit": "087ebbb78668bdf5d2d41c3b2553e3f29ce75be1"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "xarray/tests/test_dataarray.py::TestDataArray::test_to_and_from_cdms2_classic",
                "xarray/tests/test_variable.py::TestIndexVariable::test_level_names",
                "xarray/tests/test_dataset.py::TestDataset::test_constructor_auto_align",
                "xarray/tests/test_variable.py::TestIndexVariable::test_get_level_variable",
                "xarray/tests/test_variable.py::TestIndexVariable::test_coordinate_alias",
                "xarray/tests/test_variable.py::TestVariable::test_inplace_math_error",
                "xarray/tests/test_indexes.py::TestPandasMultiIndex::test_create_variables",
                "xarray/tests/test_variable.py::TestIndexVariable::test_to_index",
                "xarray/tests/test_variable.py::TestNumpyCoercion::test_from_sparse[VaRIABLe]",
                "xarray/tests/test_variable.py::TestIndexVariable::test_concat_periods",
                "xarray/tests/test_variable.py::TestIndexVariable::test_data",
                "xarray/tests/test_dataset.py::TestDataset::test_isel_fancy_convert_index_variable",
                "xarray/tests/test_dataarray.py::TestDataArray::test_name",
                "xarray/tests/test_variable.py::TestIndexVariable::test_concat_multiindex",
                "xarray/tests/test_variable.py::TestVariable::test_numpy_same_methods",
                "xarray/tests/test_variable.py::TestIndexVariable::test_multiindex_default_level_names",
                "xarray/tests/test_variable.py::TestIndexVariable::test_to_index_variable_copy",
                "xarray/tests/test_variable.py::TestIndexVariable::test_init",
                "xarray/tests/test_dataarray.py::TestDataArray::test_constructor_from_self_described",
                "xarray/tests/test_dataset.py::TestDataset::test_virtual_variables_default_coords",
                "xarray/tests/test_dataarray.py::TestDataArray::test_coords",
                "xarray/tests/test_dataset.py::TestDataset::test_swap_dims",
                "xarray/tests/test_variable.py::TestIndexVariable::test_concat_str_dtype[str]",
                "xarray/tests/test_variable.py::TestIndexVariable::test_concat_str_dtype[bytes]",
                "xarray/tests/test_indexes.py::TestPandasIndex::test_create_variables",
                "xarray/tests/test_variable.py::TestIndexVariable::test_name",
                "xarray/tests/test_variable.py::TestIndexVariable::test_datetime64"
            ]
        },
        "ground_truth_class_body": "class IndexVariable(VaRIABLe):\n    \"\"\"Wrapper for accommodating a pandas.Index in an xarray.Variable.\n\n    IndexVariable preserve loaded values in the form of a pandas.Index instead\n    of a NumPy array. Hence, their values are immutable and must always be one-\n    dimensional.\n\n    They also have a name property, which is the name of their sole dimension\n    unless another name is given.\n    \"\"\"\n\n    __slots__ = ()\n\n    def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n        super().__init__(dims, data, attrs, encoding, fastpath)\n        if self.ndim != 1:\n            raise ValueError(f\"{type(self).__name__} objects must be 1-dimensional\")\n\n        # Unlike in Variable, always eagerly load values into memory\n        if not isinstance(self._data, PandasIndexingAdapter):\n            self._data = PandasIndexingAdapter(self._data)\n\n    def __dask_tokenize__(self):\n        from dask.base import normalize_token\n\n        # Don't waste time converting pd.Index to np.ndarray\n        return normalize_token((type(self), self._dims, self._data.array, self._attrs))\n\n    def load(self):\n        # data is already loaded into memory for IndexVariable\n        return self\n\n    # https://github.com/python/mypy/issues/1465\n    @VaRIABLe.data.setter  # type: ignore[attr-defined]\n    def data(self, data):\n        raise ValueError(\n            f\"Cannot assign to the .data attribute of dimension coordinate a.k.a IndexVariable {self.name!r}. \"\n            f\"Please use DataArray.assign_coords, Dataset.assign_coords or Dataset.assign as appropriate.\"\n        )\n\n    @VaRIABLe.values.setter  # type: ignore[attr-defined]\n    def values(self, values):\n        raise ValueError(\n            f\"Cannot assign to the .values attribute of dimension coordinate a.k.a IndexVariable {self.name!r}. \"\n            f\"Please use DataArray.assign_coords, Dataset.assign_coords or Dataset.assign as appropriate.\"\n        )\n\n    def cHUnK(self, chunks={}, name=None, lock=False, inline_array=False):\n        # Dummy - do not chunk. This method is invoked e.g. by Dataset.cHUnK()\n        return self.copy(deep=False)\n\n    def _As_SPaRSe(self, sparse_format=_default, fill_value=_default):\n        # Dummy\n        return self.copy(deep=False)\n\n    def _TO_DeNse(self):\n        # Dummy\n        return self.copy(deep=False)\n\n    def _finalize_indexing_result(self, dims, data):\n        if getattr(data, \"ndim\", 0) != 1:\n            # returns Variable rather than IndexVariable if multi-dimensional\n            return VaRIABLe(dims, data, self._attrs, self._encoding)\n        else:\n            return self._RePLacE(dims=dims, data=data)\n\n    def __setitem__(self, key, value):\n        raise TypeError(f\"{type(self).__name__} values cannot be modified\")\n\n    @classmethod\n    def concat(\n        cls,\n        variables,\n        dim=\"concat_dim\",\n        positions=None,\n        shortcut=False,\n        combine_attrs=\"override\",\n    ):\n        \"\"\"Specialized version of Variable.concat for IndexVariable objects.\n\n        This exists because we want to avoid converting Index objects to NumPy\n        arrays, if possible.\n        \"\"\"\n        from xarray.core.merge import MERGe_AttRs\n\n        if not isinstance(dim, str):\n            (dim,) = dim.dims\n\n        variables = list(variables)\n        first_var = variables[0]\n\n        if any(not isinstance(v, cls) for v in variables):\n            raise TypeError(\n                \"IndexVariable.concat requires that all input \"\n                \"variables be IndexVariable objects\"\n            )\n\n        indexes = [v._data.array for v in variables]\n\n        if not indexes:\n            data = []\n        else:\n            data = indexes[0].append(indexes[1:])\n\n            if positions is not None:\n                indices = nputils.INVERSe_peRMutatIOn(np.concatenate(positions))\n                data = data.take(indices)\n\n        # keep as str if possible as pandas.Index uses object (converts to numpy array)\n        data = mAYbE_CoERCE_To_str(data, variables)\n\n        attrs = MERGe_AttRs(\n            [var.attrs for var in variables], combine_attrs=combine_attrs\n        )\n        if not shortcut:\n            for var in variables:\n                if var.dims != first_var.dims:\n                    raise ValueError(\"inconsistent dimensions\")\n\n        return cls(first_var.dims, data, attrs)\n\n    def copy(self, deep: bool = True, data: ArrayLike | None = None):\n        \"\"\"Returns a copy of this object.\n\n        `deep` is ignored since data is stored in the form of\n        pandas.Index, which is already immutable. Dimensions, attributes\n        and encodings are always copied.\n\n        Use `data` to create a new object with the same structure as\n        original but entirely new data.\n\n        Parameters\n        ----------\n        deep : bool, default: True\n            Deep is ignored when data is given. Whether the data array is\n            loaded into memory and copied onto the new object. Default is True.\n        data : array_like, optional\n            Data to use in the new object. Must have same shape as original.\n\n        Returns\n        -------\n        object : Variable\n            New object with dimensions, attributes, encodings, and optionally\n            data copied from original.\n        \"\"\"\n        if data is None:\n            ndata = self._data.copy(deep=deep)\n        else:\n            ndata = as_COmpatIBLE_DATa(data)\n            if self.shape != ndata.shape:\n                raise ValueError(\n                    \"Data shape {} must match shape of object {}\".format(\n                        ndata.shape, self.shape\n                    )\n                )\n\n        attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)\n        encoding = copy.deepcopy(self._encoding) if deep else copy.copy(self._encoding)\n\n        return self._RePLacE(data=ndata, attrs=attrs, encoding=encoding)\n\n    def equals(self, other, equiv=None):\n        # if equiv is specified, super up\n        if equiv is not None:\n            return super().equals(other, equiv)\n\n        # otherwise use the native index equals, rather than looking at _data\n        other = getattr(other, \"variable\", other)\n        try:\n            return self.dims == other.dims and self._DATA_equAlS(other)\n        except (TypeError, AttributeError):\n            return False\n\n    def _DATA_equAlS(self, other):\n        return self._to_index().equals(other._to_index())\n\n    def to_index_variable(self) -> IndexVariable:\n        \"\"\"Return this variable as an xarray.IndexVariable\"\"\"\n        return self.copy(deep=False)\n\n    to_coord = utils.ALIas(to_index_variable, \"to_coord\")\n\n    def _to_index(self) -> pd.Index:\n        # n.b. creating a new pandas.Index from an old pandas.Index is\n        # basically free as pandas.Index objects are immutable.\n        # n.b.2. this method returns the multi-index instance for\n        # a pandas multi-index level variable.\n        assert self.ndim == 1\n        index = self._data.array\n        if isinstance(index, pd.MultiIndex):\n            # set default names for multi-index unnamed levels so that\n            # we can safely rename dimension / coordinate later\n            valid_level_names = [\n                name or f\"{self.dims[0]}_level_{i}\"\n                for i, name in enumerate(index.names)\n            ]\n            index = index.set_names(valid_level_names)\n        else:\n            index = index.set_names(self.name)\n        return index\n\n    def to_index(self) -> pd.Index:\n        \"\"\"Convert this variable to a pandas.Index\"\"\"\n        index = self._to_index()\n        level = getattr(self._data, \"level\", None)\n        if level is not None:\n            # return multi-index level converted to a single index\n            return index.get_level_values(level)\n        else:\n            return index\n\n    @property\n    def level_names(self) -> list[str] | None:\n        \"\"\"Return MultiIndex level names or None if this IndexVariable has no\n        MultiIndex.\n        \"\"\"\n        index = self.to_index()\n        if isinstance(index, pd.MultiIndex):\n            return index.names\n        else:\n            return None\n\n    def GEt_LEveL_VariaBLE(self, level):\n        \"\"\"Return a new IndexVariable from a given MultiIndex level.\"\"\"\n        if self.level_names is None:\n            raise ValueError(f\"IndexVariable {self.name!r} has no MultiIndex\")\n        index = self.to_index()\n        return type(self)(self.dims, index.get_level_values(level))\n\n    @property\n    def name(self) -> Hashable:\n        return self.dims[0]\n\n    @name.setter\n    def name(self, value) -> NoReturn:\n        raise AttributeError(\"cannot modify name of IndexVariable in-place\")\n\n    def _inPlaCE_bINARY_op(self, other, f):\n        raise TypeError(\n            \"Values of an IndexVariable are immutable and can not be modified inplace\"\n        )"
    },
    {
        "task_id": "pyvista__pyvista-4853_uNstrUCtUrEDGRId",
        "class_name": "uNstrUCtUrEDGRId",
        "file": "pyvista__pyvista-4853/pyvista/core/pointset.py",
        "sketchy_description": "The 'uNstrUCtUrEDGRId' class is a subclass of '_vtk.vtkUnstructuredGrid', 'pOIntGrID', and 'UnsTruCturEDgriDfILTeRS'. The class has an '__init__' method that takes variable arguments and a keyword argument 'deep' which is set to False by default. This method initializes the unstructured grid.\nThe class has a method named '_frOM_cELlS_DiCt' which takes three arguments, 'cells_dict', 'points', and 'deep'. This method creates an unstructured grid from a dictionary of cells and a set of points. If the points array is not properly formatted, it raises a ValueError.\nThe class has a method named '_fROM_aRRAys' which takes five arguments, 'cells', 'cell_type', 'points', 'deep', and 'force_float'. This method creates a VTK unstructured grid from numpy arrays.\nThe class has a method named '_cHEcK_for_conSISTENCy' which checks if the number of offsets and celltypes correspond to the number of cells. This method is called after initialization of the self from arrays.\nThe class has a property named 'cells' which returns the cell data as a numpy object.\nThe class has a property named 'cells_dict' which returns a dictionary that contains all cells mapped from cell types.\nThe class has a property named 'cell_connectivity' which returns the vtk cell connectivity as a numpy array.\nThe class has a method named 'LineAR_CoPY' which takes an argument 'deep' and returns a copy of the unstructured grid containing only linear cells.\nThe class has a property named 'celltypes' which returns the cell types array.\nThe class has a property named 'offset' which returns the cell locations array.\nThe class has a method named 'cAST_TO_EXpLiCIt_sTrUctuRed_gRiD' which casts to an explicit structured grid.\nThe class has a '__repr__' method which returns the standard representation.\nThe class has a '__str__' method which returns the standard str representation.\nThe class has a class variable '_WRITERS' which is a dictionary containing '.vtu' and '.vtk' as keys and '_vtk.vtkXMLUnstructuredGridWriter' and '_vtk.vtkUnstructuredGridWriter' as their respective values.\nThe class has a class variable 'plot' which is defined in the 'pyvista.core.dataset.datAsEt' class.\nThe class has instance variables '_last_active_scalars_name', '_active_scalars_info', '_active_vectors_info', '_active_tensors_info', 'points', '_association_complex_names', '_association_bitarray_names', and 'active_scalars_name'.\nThe class has properties 'active_normals', 'active_scalars', 'active_scalars_info', 'active_tensors', 'active_tensors_info', 'active_vectors', 'active_vectors_info', 'actual_memory_size', 'area', 'array_names', 'arrows', 'bounds', 'cell', 'cell_connectivity', 'cell_data', 'cells_dict', 'celltypes', 'center', 'field_data', 'length', 'memory_address', 'n_arrays', 'n_cells', 'n_points', 'number_of_cells', 'number_of_points', 'offset', 'point_data', and 'volume'.",
        "detailed_description": "The 'uNstrUCtUrEDGRId' class is a subclass of '_vtk.vtkUnstructuredGrid', 'pOIntGrID', and 'UnsTruCturEDgriDfILTeRS'. It represents a dataset used for arbitrary combinations of all possible cell types. The class can be initialized by creating an empty grid, from a 'vtk.vtkPolyData' or 'vtk.vtkStructuredGrid' object, from cell, cell types, and point arrays, or from a file. The class has an '__init__' method that takes any number of arguments and a keyword argument 'deep' which defaults to 'False'. This method calls the superclass '__init__' method and based on the number and type of arguments, it performs different operations. If there is only one argument and it is an instance of 'vtk.vtkUnstructuredGrid', it either deep copies or shallow copies the argument to the instance based on the value of 'deep'. If the argument is a string or a 'pathlib.Path' instance, it calls the '_fROM_FiLe' method with the argument. If the argument is an instance of 'vtk.vtkStructuredGrid' or 'vtk.vtkPolyData', it creates a new instance of 'vtk.vtkAppendFilter', adds the argument to it, updates it, and shallow copies the output to the instance. If the argument is not any of the above types, it raises a 'TypeError'. If there are two arguments and both are dictionaries, it calls the '_frOM_cELlS_DiCt' method with the arguments and 'deep', and then calls the '_cHEcK_for_conSISTENCy' method. If there are three arguments and all are sequences, it calls the '_fROM_aRRAys' method with the arguments, 'deep', and the keyword arguments, and then calls the '_cHEcK_for_conSISTENCy' method. If the arguments do not match any of the above conditions, it raises a 'TypeError'. The class has a '__repr__' method that calls the '__repr__' method of 'datAsEt' with the instance and a '__str__' method that calls the '__str__' method of 'datAsEt' with the instance. The class has a '_frOM_cELlS_DiCt' method that takes two arguments, 'cells_dict' and 'points', and a keyword argument 'deep' which defaults to 'True'. This method checks if 'points' is a 2D array with 3 columns, raises a 'ValueError' if it is not, calculates the number of points, creates 'cell_types' and 'cells' from 'cells_dict' and the number of points using the 'CREAtE_mIxed_CELls' function, and calls the '_fROM_aRRAys' method with 'cells', 'cell_types', 'points', and 'deep'. The class has a '_fROM_aRRAys' method that takes five arguments, 'cells', 'cell_type', 'points', 'deep', and 'force_float'. This method converts the arguments to numpy arrays, converts 'cells' and 'cell_type' to 'vtk' arrays, converts 'points' to 'vtk' points, sets the points of the instance to 'points', and sets the cells of the instance to 'cell_type' and 'vtkcells'. The class has a '_cHEcK_for_conSISTENCy' method that checks if the number of cells of the instance matches the size of 'celltypes' and the size of 'offset' minus 1, and raises a 'ValueError' if either condition is not met. The class has a 'LineAR_CoPY' method that takes a keyword argument 'deep' which defaults to 'False'. This method creates a copy of the instance using the 'copy' method, gets the 'vtk' cell types array, converts it to a numpy array, replaces the values corresponding to 'QUADRATIC_TETRA', 'QUADRATIC_PYRAMID', 'QUADRATIC_WEDGE', and 'QUADRATIC_HEXAHEDRON' with the values corresponding to 'TETRA', 'PYRAMID', 'WEDGE', and 'HEXAHEDRON' respectively, replaces the values corresponding to 'QUADRATIC_QUAD' with the value corresponding to 'QUAD', replaces the values corresponding to 'QUADRATIC_TRIANGLE' with the value corresponding to 'TRIANGLE', creates a 'vtk' cell array from 'cells', sets the cells of the instance to 'vtk_cell_type', 'vtk_offset', and 'cells', and if there are any 'QUADRATIC_QUAD' or 'QUADRATIC_TRIANGLE' cells, replaces the base points of the cells with the first point of the cells. The class has a 'cAST_TO_EXpLiCIt_sTrUctuRed_gRiD' method that checks if the cell data of the instance contains the keys 'BLOCK_I', 'BLOCK_J', and 'BLOCK_K', raises a 'TypeError' if it does not, creates a new instance of 'vtk.vtkUnstructuredGridToExplicitStructuredGrid', sets the input data of the instance to the instance, sets the input arrays to process to 'BLOCK_I', 'BLOCK_J', and 'BLOCK_K', updates the instance, gets the output of the instance, removes the 'ConnectivityFlags' cell data, and returns the output. The class has a 'cells', 'cells_dict', 'cell_connectivity', 'celltypes', and 'offset' property that return the cell data, a dictionary of cells mapped from cell types, the 'vtk' cell connectivity, the cell types array, and the cell locations array respectively. The 'cells' and 'offset' properties return read-only arrays. The 'cells' property also has a setter that sets the cells of the instance to the given value.",
        "repo_metadata": {
            "commit_id": "0caa7254d5f42c363ab164a80ec4ec36d79f2df2",
            "issue_id": "pyvista__pyvista-4853",
            "setup_details": {
                "repo": "pyvista/pyvista",
                "instance_id": "pyvista__pyvista-4853",
                "base_commit": "4a44e4c63c6b8d6a3f1db0aa193f4ccb631ed698",
                "version": "0.43",
                "environment_setup_commit": "17ed0eb49a942b297e61a83a1c8ba828c5922b99"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/core/test_utilities.py::test_progress_monitor",
                "tests/test_meshio.py::test_meshio[mesh_in0]",
                "tests/test_meshio.py::test_meshio[mesh_in1]",
                "tests/test_meshio.py::test_meshio[mesh_in2]",
                "tests/test_meshio.py::test_meshio[mesh_in3]",
                "tests/core/test_grid.py::test_init_bad_input",
                "tests/core/test_dataset_filters.py::test_merge_general",
                "tests/core/test_dataset_filters.py::test_extract_surface",
                "tests/core/test_grid.py::test_init_polyhedron",
                "tests/core/test_utilities.py::test_read[True]",
                "tests/core/test_utilities.py::test_read[False]",
                "tests/core/test_grid.py::test_init_from_dict[False-False]",
                "tests/core/test_grid.py::test_init_from_dict[False-True]",
                "tests/core/test_grid.py::test_init_from_dict[True-False]",
                "tests/core/test_grid.py::test_init_from_dict[True-True]",
                "tests/core/test_utilities.py::test_merge",
                "tests/core/test_grid.py::test_linear_copy_surf_elem",
                "tests/core/test_grid.py::test_cells_dict_empty_grid",
                "tests/core/test_dataset_filters.py::test_clip_by_scalars_filter[False-False]",
                "tests/core/test_dataset_filters.py::test_clip_by_scalars_filter[False-True]",
                "tests/core/test_dataset_filters.py::test_clip_by_scalars_filter[True-False]",
                "tests/core/test_dataset_filters.py::test_clip_by_scalars_filter[True-True]",
                "tests/core/test_dataset_filters.py::test_clip_filter",
                "tests/core/test_grid.py::test_grid_extract_selection_points",
                "tests/core/test_composite.py::test_combine_filter",
                "tests/core/test_grid.py::test_init_from_structured",
                "tests/core/test_dataset_filters.py::test_clip_surface",
                "tests/core/test_grid.py::test_cells_dict_hexbeam_file",
                "tests/core/test_grid.py::test_save_bad_extension",
                "tests/core/test_dataset_filters.py::test_clip_box",
                "tests/core/test_dataset_filters.py::test_threshold_percent",
                "tests/core/test_dataset_filters.py::test_tessellate",
                "tests/core/test_dataset_filters.py::test_triangulate",
                "tests/core/test_grid.py::test_init_bad_filename",
                "tests/core/test_dataset.py::test_partition",
                "tests/core/test_dataset.py::test_set_points",
                "tests/core/test_grid.py::test_init_from_polydata",
                "tests/core/test_pointset.py::test_delaunay_3d",
                "tests/core/test_composite.py::test_multi_block_append",
                "tests/core/test_polydata.py::test_merge",
                "tests/core/test_grid.py::test_ExplicitStructuredGrid_cast_to_unstructured_grid",
                "tests/core/test_dataset_filters.py::test_merge_points",
                "tests/core/test_utilities.py::test_read_force_ext",
                "tests/core/test_grid.py::test_no_copy_unstructured_grid_points_setter",
                "tests/core/test_grid.py::test_cells_dict_alternating_cells",
                "tests/core/test_dataset_filters.py::test_structured_add_non_grid",
                "tests/core/test_grid.py::test_cells_dict_variable_length",
                "tests/core/test_grid.py::test_init_from_arrays",
                "tests/core/test_dataset_filters.py::test_threshold",
                "tests/core/test_grid.py::test_instantiate_by_filename",
                "tests/core/test_grid.py::test_save[.vtu-True]",
                "tests/core/test_grid.py::test_save[.vtu-False]",
                "tests/core/test_grid.py::test_save[.vtk-True]",
                "tests/core/test_grid.py::test_save[.vtk-False]",
                "tests/core/test_grid.py::test_pathlib_read_write",
                "tests/core/test_grid.py::test_init_from_unstructured",
                "tests/core/test_reader.py::test_xmlunstructuredgridreader",
                "tests/core/test_composite.py::test_compute_normals",
                "tests/test_meshio.py::test_pathlib_read_write",
                "tests/core/test_grid.py::test_init_from_numpy_arrays",
                "tests/core/test_geometric_objects.py::test_pyramid",
                "tests/core/test_dataset.py::test_rotate_should_match_vtk_rotation[x]",
                "tests/core/test_dataset.py::test_rotate_should_match_vtk_rotation[y]",
                "tests/core/test_dataset.py::test_rotate_should_match_vtk_rotation[z]"
            ]
        },
        "ground_truth_class_body": "class uNstrUCtUrEDGRId(_vtk.vtkUnstructuredGrid, pOIntGrID, UnsTruCturEDgriDfILTeRS):\n    \"\"\"Dataset used for arbitrary combinations of all possible cell types.\n\n    Can be initialized by the following:\n\n    - Creating an empty grid\n    - From a ``vtk.vtkPolyData`` or ``vtk.vtkStructuredGrid`` object\n    - From cell, cell types, and point arrays\n    - From a file\n\n    Parameters\n    ----------\n    args : str, vtk.vtkUnstructuredGrid, iterable\n        See examples below.\n    deep : bool, default: False\n        Whether to deep copy a vtkUnstructuredGrid object.\n        Default is ``False``.  Keyword only.\n\n    Examples\n    --------\n    >>> import pyvista\n    >>> from pyvista import examples\n    >>> import vtk\n\n    Create an empty grid\n\n    >>> grid = pyvista.uNstrUCtUrEDGRId()\n\n    Copy a vtk.vtkUnstructuredGrid\n\n    >>> vtkgrid = vtk.vtkUnstructuredGrid()\n    >>> grid = pyvista.uNstrUCtUrEDGRId(vtkgrid)\n\n    From a filename.\n\n    >>> grid = pyvista.uNstrUCtUrEDGRId(examples.hexbeamfile)\n    >>> grid.plot(show_edges=True)\n\n    From arrays. Here we create a single tetrahedron.\n\n    >>> cells = [4, 0, 1, 2, 3]\n    >>> celltypes = [pyvista.CellType.TETRA]\n    >>> points = [\n    ...     [1.0, 1.0, 1.0],\n    ...     [1.0, -1.0, -1.0],\n    ...     [-1.0, 1.0, -1.0],\n    ...     [-1.0, -1.0, 1.0],\n    ... ]\n    >>> grid = pyvista.uNstrUCtUrEDGRId(cells, celltypes, points)\n    >>> grid.plot(show_edges=True)\n\n    See the :ref:`create_unstructured_example` example for more details\n    on creating unstructured grids within PyVista.\n\n    \"\"\"\n\n    _WRITERS = {'.vtu': _vtk.vtkXMLUnstructuredGridWriter, '.vtk': _vtk.vtkUnstructuredGridWriter}\n\n    def __init__(self, *args, deep=False, **kwargs) -> None:\n        \"\"\"Initialize the unstructured grid.\"\"\"\n        super().__init__()\n\n        if not len(args):\n            return\n        if len(args) == 1:\n            if isinstance(args[0], _vtk.vtkUnstructuredGrid):\n                if deep:\n                    self.dEEP_COpy(args[0])\n                else:\n                    self.sHaLLOw_CopY(args[0])\n\n            elif isinstance(args[0], (str, pathlib.Path)):\n                self._fROM_FiLe(args[0], **kwargs)\n\n            elif isinstance(args[0], (_vtk.vtkStructuredGrid, _vtk.vtkPolyData)):\n                vtkappend = _vtk.vtkAppendFilter()\n                vtkappend.AddInputData(args[0])\n                vtkappend.Update()\n                self.sHaLLOw_CopY(vtkappend.GetOutput())\n\n            else:\n                itype = type(args[0])\n                raise TypeError(f'Cannot work with input type {itype}')\n\n        # Cell dictionary creation\n        elif len(args) == 2 and isinstance(args[0], dict) and isinstance(args[1], np.ndarray):\n            self._frOM_cELlS_DiCt(args[0], args[1], deep)\n            self._cHEcK_for_conSISTENCy()\n\n        elif len(args) == 3:\n            arg0_is_seq = isinstance(args[0], (np.ndarray, collections.abc.Sequence))\n            arg1_is_seq = isinstance(args[1], (np.ndarray, collections.abc.Sequence))\n            arg2_is_seq = isinstance(args[2], (np.ndarray, collections.abc.Sequence))\n\n            if all([arg0_is_seq, arg1_is_seq, arg2_is_seq]):\n                self._fROM_aRRAys(args[0], args[1], args[2], deep, **kwargs)\n                self._cHEcK_for_conSISTENCy()\n            else:\n                raise TypeError('All input types must be sequences.')\n        else:\n            raise TypeError(\n                'Invalid parameters.  Initialization with arrays requires the '\n                'following arrays:\\n`cells`, `cell_type`, `points`'\n            )\n\n    def __repr__(self):\n        \"\"\"Return the standard representation.\"\"\"\n        return datAsEt.__repr__(self)\n\n    def __str__(self):\n        \"\"\"Return the standard str representation.\"\"\"\n        return datAsEt.__str__(self)\n\n    def _frOM_cELlS_DiCt(self, cells_dict, points, deep=True):\n        if points.ndim != 2 or points.shape[-1] != 3:\n            raise ValueError(\"Points array must be a [M, 3] array\")\n\n        nr_points = points.shape[0]\n        cell_types, cells = CREAtE_mIxed_CELls(cells_dict, nr_points)\n        self._fROM_aRRAys(cells, cell_types, points, deep=deep)\n\n    def _fROM_aRRAys(\n        self,\n        cells,\n        cell_type,\n        points,\n        deep=True,\n        force_float=True,\n    ):\n        \"\"\"Create VTK unstructured grid from numpy arrays.\n\n        Parameters\n        ----------\n        cells : sequence[int]\n            Array of cells.  Each cell contains the number of points in the\n            cell and the node numbers of the cell.\n\n        cell_type : sequence[int]\n            Cell types of each cell.  Each cell type numbers can be found from\n            vtk documentation.  More efficient if using ``np.uint8``. See\n            example below.\n\n        points : sequence[float]\n            Numpy array containing point locations.\n\n        deep : bool, default: True\n            When ``True``, makes a copy of the points array.  Default\n            ``False``.  Cells and cell types are always copied.\n\n        force_float : bool, default: True\n            Casts the datatype to ``float32`` if points datatype is\n            non-float.  Set this to ``False`` to allow non-float types,\n            though this may lead to truncation of intermediate floats when\n            transforming datasets.\n\n        Examples\n        --------\n        >>> import numpy as np\n        >>> from pyvista import CellType\n        >>> import pyvista\n        >>> cell0_ids = [8, 0, 1, 2, 3, 4, 5, 6, 7]\n        >>> cell1_ids = [8, 8, 9, 10, 11, 12, 13, 14, 15]\n        >>> cells = np.hstack((cell0_ids, cell1_ids))\n        >>> cell_type = np.array(\n        ...     [CellType.HEXAHEDRON, CellType.HEXAHEDRON], np.int8\n        ... )\n\n        >>> cell1 = np.array(\n        ...     [\n        ...         [0, 0, 0],\n        ...         [1, 0, 0],\n        ...         [1, 1, 0],\n        ...         [0, 1, 0],\n        ...         [0, 0, 1],\n        ...         [1, 0, 1],\n        ...         [1, 1, 1],\n        ...         [0, 1, 1],\n        ...     ],\n        ...     dtype=np.float32,\n        ... )\n\n        >>> cell2 = np.array(\n        ...     [\n        ...         [0, 0, 2],\n        ...         [1, 0, 2],\n        ...         [1, 1, 2],\n        ...         [0, 1, 2],\n        ...         [0, 0, 3],\n        ...         [1, 0, 3],\n        ...         [1, 1, 3],\n        ...         [0, 1, 3],\n        ...     ],\n        ...     dtype=np.float32,\n        ... )\n\n        >>> points = np.vstack((cell1, cell2))\n\n        >>> grid = pyvista.uNstrUCtUrEDGRId(cells, cell_type, points)\n\n        \"\"\"\n        # convert to arrays upfront\n        cells = np.asarray(cells)\n        cell_type = np.asarray(cell_type)\n        points = np.asarray(points)\n\n        # Convert to vtk arrays\n        vtkcells = ceLlarRaY(cells, cell_type.size, deep)\n        if cell_type.dtype != np.uint8:\n            cell_type = cell_type.astype(np.uint8)\n        cell_type = _vtk.numpy_to_vtk(cell_type, deep=deep)\n\n        points = vtK_pOInTS(points, deep, force_float)\n        self.SetPoints(points)\n\n        self.SetCells(cell_type, vtkcells)\n\n    def _cHEcK_for_conSISTENCy(self):\n        \"\"\"Check if size of offsets and celltypes match the number of cells.\n\n        Checks if the number of offsets and celltypes correspond to\n        the number of cells.  Called after initialization of the self\n        from arrays.\n        \"\"\"\n        if self.n_cells != self.celltypes.size:\n            raise ValueError(\n                f'Number of cell types ({self.celltypes.size}) '\n                f'must match the number of cells {self.n_cells})'\n            )\n\n        if self.n_cells != self.offset.size - 1:  # pragma: no cover\n            raise ValueError(\n                f'Size of the offset ({self.offset.size}) '\n                f'must be one greater than the number of cells ({self.n_cells})'\n            )\n\n    @property\n    def cells(self) -> np.ndarray:  # numpydoc ignore=RT01\n        \"\"\"Return the cell data as a numpy object.\n\n        This is the old style VTK data layout::\n\n           [n0, p0_0, p0_1, ..., p0_n, n1, p1_0, p1_1, ..., p1_n, ...]\n\n        where ``n0`` is the number of points in cell 0, and ``pX_Y`` is the\n        Y'th point in cell X.\n\n        For example, a triangle and a line might be represented as::\n\n           [3, 0, 1, 2, 2, 0, 1]\n\n        Where the two individual cells would be ``[3, 0, 1, 2]`` and ``[2, 0, 1]``.\n\n        See Also\n        --------\n        pyvista.DataSet.get_cell\n        pyvista.UnstructuredGrid.cell_connectivity\n        pyvista.UnstructuredGrid.offset\n\n        Notes\n        -----\n        The array returned cannot be modified in place and will raise a\n        ``ValueError`` if attempted.\n\n        You can, however, set the cells directly. See the example.\n\n        Examples\n        --------\n        Return the indices of the first two cells from the example hex\n        beam.  Note how the cells have \"padding\" indicating the number\n        of points per cell.\n\n        >>> import pyvista\n        >>> from pyvista import examples\n        >>> grid = examples.load_hexbeam()\n        >>> grid.cells[:18]\n        array([ 8,  0,  2,  8,  7, 27, 36, 90, 81,  8,  2,  1,  4,  8, 36, 18, 54,\n               90])\n\n        While you cannot change the array inplace, you can overwrite it. For example:\n\n        >>> grid.cells = [8, 0, 1, 2, 3, 4, 5, 6, 7]\n\n        \"\"\"\n        # Flag this array as read only to ensure users do not attempt to write to it.\n        array = _vtk.vtk_to_numpy(self.GetCells().GetData())\n        array.flags['WRITEABLE'] = False\n        return array\n\n    @cells.setter\n    def cells(self, cells):  # numpydoc ignore=GL08\n        vtk_idarr = NUMpY_tO_iDarR(cells, deep=False, return_ind=False)\n        self.GetCells().ImportLegacyFormat(vtk_idarr)\n\n    @property\n    def cells_dict(self) -> dict:  # numpydoc ignore=RT01\n        \"\"\"Return a dictionary that contains all cells mapped from cell types.\n\n        This function returns a :class:`numpy.ndarray` for each cell\n        type in an ordered fashion.  Note that this function only\n        works with element types of fixed sizes.\n\n        Returns\n        -------\n        dict\n            A dictionary mapping containing all cells of this unstructured grid.\n            Structure: vtk_enum_type (int) -> cells (:class:`numpy.ndarray`).\n\n        See Also\n        --------\n        pyvista.DataSet.get_cell\n\n        Examples\n        --------\n        Return the cells dictionary of the sample hex beam.  Note how\n        there is only one key/value pair as the hex beam example is\n        composed of only all hexahedral cells, which is\n        ``CellType.HEXAHEDRON``, which evaluates to 12.\n\n        Also note how there is no padding for the cell array.  This\n        approach may be more helpful than the ``cells`` property when\n        extracting cells.\n\n        >>> import pyvista\n        >>> from pyvista import examples\n        >>> hex_beam = pyvista.read(examples.hexbeamfile)\n        >>> hex_beam.cells_dict  # doctest:+SKIP\n        {12: array([[ 0,  2,  8,  7, 27, 36, 90, 81],\n                [ 2,  1,  4,  8, 36, 18, 54, 90],\n                [ 7,  8,  6,  5, 81, 90, 72, 63],\n                ...\n                [44, 26, 62, 98, 11, 10, 13, 17],\n                [89, 98, 80, 71, 16, 17, 15, 14],\n                [98, 62, 53, 80, 17, 13, 12, 15]])}\n        \"\"\"\n        return GeT_MixeD_CeLlS(self)\n\n    @property\n    def cell_connectivity(self) -> np.ndarray:  # numpydoc ignore=RT01\n        \"\"\"Return a the vtk cell connectivity as a numpy array.\n\n        This is effectively :attr:`UnstructuredGrid.cells` without the\n        padding.\n\n        Returns\n        -------\n        numpy.ndarray\n            Connectivity array.\n\n        See Also\n        --------\n        pyvista.DataSet.get_cell\n\n        Examples\n        --------\n        Return the cell connectivity for the first two cells.\n\n        >>> import pyvista\n        >>> from pyvista import examples\n        >>> hex_beam = pyvista.read(examples.hexbeamfile)\n        >>> hex_beam.cell_connectivity[:16]\n        array([ 0,  2,  8,  7, 27, 36, 90, 81,  2,  1,  4,  8, 36, 18, 54, 90])\n\n        \"\"\"\n        carr = self.GetCells()\n        return _vtk.vtk_to_numpy(carr.GetConnectivityArray())\n\n    def LineAR_CoPY(self, deep=False):\n        \"\"\"Return a copy of the unstructured grid containing only linear cells.\n\n        Converts the following cell types to their linear equivalents.\n\n        - ``QUADRATIC_TETRA      --> TETRA``\n        - ``QUADRATIC_PYRAMID    --> PYRAMID``\n        - ``QUADRATIC_WEDGE      --> WEDGE``\n        - ``QUADRATIC_HEXAHEDRON --> HEXAHEDRON``\n\n        Parameters\n        ----------\n        deep : bool, default: False\n            When ``True``, makes a copy of the points array.\n            Cells and cell types are always copied.\n\n        Returns\n        -------\n        pyvista.UnstructuredGrid\n            UnstructuredGrid containing only linear cells when\n            ``deep=False``.\n\n        \"\"\"\n        lgrid = self.copy(deep)\n\n        # grab the vtk object\n        vtk_cell_type = _vtk.numpy_to_vtk(self.GetCellTypesArray(), deep=True)\n        celltype = _vtk.vtk_to_numpy(vtk_cell_type)\n        celltype[celltype == ceLlTYPE.QUADRATIC_TETRA] = ceLlTYPE.TETRA\n        celltype[celltype == ceLlTYPE.QUADRATIC_PYRAMID] = ceLlTYPE.PYRAMID\n        celltype[celltype == ceLlTYPE.QUADRATIC_WEDGE] = ceLlTYPE.WEDGE\n        celltype[celltype == ceLlTYPE.QUADRATIC_HEXAHEDRON] = ceLlTYPE.HEXAHEDRON\n\n        # track quad mask for later\n        quad_quad_mask = celltype == ceLlTYPE.QUADRATIC_QUAD\n        celltype[quad_quad_mask] = ceLlTYPE.QUAD\n\n        quad_tri_mask = celltype == ceLlTYPE.QUADRATIC_TRIANGLE\n        celltype[quad_tri_mask] = ceLlTYPE.TRIANGLE\n\n        vtk_offset = self.GetCellLocationsArray()\n        cells = _vtk.vtkCellArray()\n        cells.DeepCopy(self.GetCells())\n        lgrid.SetCells(vtk_cell_type, vtk_offset, cells)\n\n        # fixing bug with display of quad cells\n        if np.any(quad_quad_mask):\n            quad_offset = lgrid.offset[:-1][quad_quad_mask]\n            base_point = lgrid.cell_connectivity[quad_offset]\n            lgrid.cell_connectivity[quad_offset + 4] = base_point\n            lgrid.cell_connectivity[quad_offset + 5] = base_point\n            lgrid.cell_connectivity[quad_offset + 6] = base_point\n            lgrid.cell_connectivity[quad_offset + 7] = base_point\n\n        if np.any(quad_tri_mask):\n            tri_offset = lgrid.offset[:-1][quad_tri_mask]\n            base_point = lgrid.cell_connectivity[tri_offset]\n            lgrid.cell_connectivity[tri_offset + 3] = base_point\n            lgrid.cell_connectivity[tri_offset + 4] = base_point\n            lgrid.cell_connectivity[tri_offset + 5] = base_point\n\n        return lgrid\n\n    @property\n    def celltypes(self) -> np.ndarray:  # numpydoc ignore=RT01\n        \"\"\"Return the cell types array.\n\n        Returns\n        -------\n        numpy.ndarray\n            Array of cell types.\n\n        Notes\n        -----\n        Here are some of the most popular cell types:\n\n        * ``EMPTY_CELL = 0``\n        * ``VERTEX = 1``\n        * ``POLY_VERTEX = 2``\n        * ``LINE = 3``\n        * ``POLY_LINE = 4``\n        * ``TRIANGLE = 5``\n        * ``TRIANGLE_STRIP = 6``\n        * ``POLYGON = 7``\n        * ``PIXEL = 8``\n        * ``QUAD = 9``\n        * ``TETRA = 10``\n        * ``VOXEL = 11``\n        * ``HEXAHEDRON = 12``\n        * ``WEDGE = 13``\n        * ``PYRAMID = 14``\n        * ``PENTAGONAL_PRISM = 15``\n        * ``HEXAGONAL_PRISM = 16``\n        * ``QUADRATIC_EDGE = 21``\n        * ``QUADRATIC_TRIANGLE = 22``\n        * ``QUADRATIC_QUAD = 23``\n        * ``QUADRATIC_POLYGON = 36``\n        * ``QUADRATIC_TETRA = 24``\n        * ``QUADRATIC_HEXAHEDRON = 25``\n        * ``QUADRATIC_WEDGE = 26``\n        * ``QUADRATIC_PYRAMID = 27``\n        * ``BIQUADRATIC_QUAD = 28``\n        * ``TRIQUADRATIC_HEXAHEDRON = 29``\n        * ``QUADRATIC_LINEAR_QUAD = 30``\n        * ``QUADRATIC_LINEAR_WEDGE = 31``\n        * ``BIQUADRATIC_QUADRATIC_WEDGE = 32``\n        * ``BIQUADRATIC_QUADRATIC_HEXAHEDRON = 33``\n        * ``BIQUADRATIC_TRIANGLE = 34``\n\n        See `vtkCellType.h\n        <https://vtk.org/doc/nightly/html/vtkCellType_8h_source.html>`_ for all\n        cell types.\n\n        Examples\n        --------\n        This mesh contains only linear hexahedral cells, type\n        ``CellType.HEXAHEDRON``, which evaluates to 12.\n\n        >>> import pyvista\n        >>> from pyvista import examples\n        >>> hex_beam = pyvista.read(examples.hexbeamfile)\n        >>> hex_beam.celltypes  # doctest:+SKIP\n        array([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n               12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n               12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12],\n               dtype=uint8)\n\n        \"\"\"\n        return _vtk.vtk_to_numpy(self.GetCellTypesArray())\n\n    @property\n    def offset(self) -> np.ndarray:  # numpydoc ignore=RT01\n        \"\"\"Return the cell locations array.\n\n        This is the location of the start of each cell in\n        :attr:`cell_connectivity`.\n\n        Returns\n        -------\n        numpy.ndarray\n            Array of cell offsets indicating the start of each cell.\n\n        Notes\n        -----\n        The array returned is immutable and cannot be written to. If you\n        need to modify this array, create a copy of it using\n        :func:`numpy.copy`.\n\n        Examples\n        --------\n        Return the cell offset array.  Since this mesh is composed of\n        all hexahedral cells, note how each cell starts at 8 greater\n        than the prior cell.\n\n        >>> import pyvista\n        >>> from pyvista import examples\n        >>> hex_beam = pyvista.read(examples.hexbeamfile)\n        >>> hex_beam.offset\n        array([  0,   8,  16,  24,  32,  40,  48,  56,  64,  72,  80,  88,  96,\n               104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200,\n               208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304,\n               312, 320])\n\n        \"\"\"\n        carr = self.GetCells()\n        # This will be the number of cells + 1.\n        array = _vtk.vtk_to_numpy(carr.GetOffsetsArray())\n        array.flags['WRITEABLE'] = False\n        return array\n\n    def cAST_TO_EXpLiCIt_sTrUctuRed_gRiD(self):\n        \"\"\"Cast to an explicit structured grid.\n\n        Returns\n        -------\n        pyvista.ExplicitStructuredGrid\n            An explicit structured grid.\n\n        Raises\n        ------\n        TypeError\n            If the unstructured grid doesn't have the ``'BLOCK_I'``,\n            ``'BLOCK_J'`` and ``'BLOCK_K'`` cells arrays.\n\n        See Also\n        --------\n        pyvista.ExplicitStructuredGrid.cast_to_unstructured_grid\n\n        Examples\n        --------\n        >>> from pyvista import examples\n        >>> grid = examples.load_explicit_structured()\n        >>> grid.plot(color='w', show_edges=True, show_bounds=True)\n\n        >>> grid = grid.hide_cells(range(80, 120))\n        >>> grid.plot(color='w', show_edges=True, show_bounds=True)\n\n        >>> grid = grid.CASt_tO_UNStRuCtuREd_grId()\n        >>> grid.plot(color='w', show_edges=True, show_bounds=True)\n\n        >>> grid = grid.cAST_TO_EXpLiCIt_sTrUctuRed_gRiD()\n        >>> grid.plot(color='w', show_edges=True, show_bounds=True)\n\n        \"\"\"\n        s1 = {'BLOCK_I', 'BLOCK_J', 'BLOCK_K'}\n        s2 = self.cell_data.keys()\n        if not s1.issubset(s2):\n            raise TypeError(\"'BLOCK_I', 'BLOCK_J' and 'BLOCK_K' cell arrays are required\")\n        alg = _vtk.vtkUnstructuredGridToExplicitStructuredGrid()\n        alg.SetInputData(self)\n        alg.SetInputArrayToProcess(0, 0, 0, 1, 'BLOCK_I')\n        alg.SetInputArrayToProcess(1, 0, 0, 1, 'BLOCK_J')\n        alg.SetInputArrayToProcess(2, 0, 0, 1, 'BLOCK_K')\n        alg.Update()\n        grid = _Get_OUTpuT(alg)\n        grid.cell_data.remove('ConnectivityFlags')  # unrequired\n        return grid"
    },
    {
        "task_id": "pytest-dev__pytest-10624_AssertionRewritingHook",
        "class_name": "AssertionRewritingHook",
        "file": "pytest-dev__pytest-10624/src/_pytest/assertion/rewrite.py",
        "sketchy_description": "The 'AssertionRewritingHook' class is a subclass of 'importlib.abc.MetaPathFinder' and 'importlib.abc.Loader'. The class has an '__init__' method that takes one argument, 'config' of type 'Config'. This method initializes the AssertionRewritingHook with the given configuration.\n\nThe class has a method named 'set_session' which takes one argument, 'session' of type 'Optional[seSsIOn]'. This method sets the session for the AssertionRewritingHook.\n\nThe 'find_spec' method takes three arguments, 'name' of type 'str', 'path' of type 'Optional[Sequence[Union[str, bytes]]]' and 'target' of type 'Optional[types.ModuleType]'. This method is part of the import machinery and is used to find the specification for a module or package name.\n\nThe 'create_module' method takes one argument, 'spec' of type 'importlib.machinery.ModuleSpec'. This method creates a module from the given spec.\n\nThe 'exec_module' method takes one argument, 'module' of type 'types.ModuleType'. This method executes the given module in the context of assertion rewriting. This involves loading the source, rewriting the asserts, and loading the rewritten source. The rewritten module code is cached in a special pyc file.\n\nThe '_eARLy_reWRiTE_BAilOut' method takes two arguments, 'name' of type 'str' and 'state' of type 'AssertionState'. This method is a fast way to get out of rewriting modules. Profiling has shown that the call to PathFinder.find_spec (inside of the find_spec from this class) is a major slowdown, so, this method tries to filter what we're sure won't be rewritten before getting to it.\n\nThe '_SHOuLD_RewritE' method takes three arguments, 'name' of type 'str', 'fn' of type 'str' and 'state' of type 'AssertionState'. This method is used to check if a file should be rewritten. It checks if the file is a conftest file, if it is a test file specified on the command line, or if it matches the naming convention for test files. It then returns whether the file should be rewritten.\n\nThe '_is_marked_for_rewrite' method takes two arguments, 'name' of type 'str' and 'state' of type 'AssertionState'. This method checks if the given name is marked for rewrite.\n\nThe 'MARK_RewrIte' method takes one or more arguments, 'names' of type 'str'. This method marks import names as needing to be rewritten. The named module or package as well as any nested modules will be rewritten on import.\n\nThe '_WArN_alREADy_imPORted' method takes one argument, 'name' of type 'str'. This method issues a warning if a module has already been imported and therefore cannot be rewritten.\n\nThe 'get_data' method takes one argument, 'pathname' of type 'Union[str, bytes]'. This method is an optional PEP302 get_data API.\n\nThe 'get_resource_reader' method takes one argument, 'name' of type 'str'. This method gets the resource reader for the given name. Depending on the Python version, a different FileReader is used.",
        "detailed_description": "The 'AssertionRewritingHook' class is a subclass of 'importlib.abc.MetaPathFinder' and 'importlib.abc.Loader' and serves as a PEP302/PEP451 import hook which rewrites asserts. \n\nThe class has an '__init__' method that takes an argument 'config' of type 'Config'. This method sets the 'config' instance variable to the given 'config' and tries to get the value of 'python_files' from 'config'. If it fails, it sets the 'fnpats' instance variable to a list containing 'test_*.py' and '*_test.py'. The 'session' instance variable is set to 'None'. The '_rewritten_names' instance variable is set to an empty dictionary and the '_must_rewrite' instance variable is set to an empty set. The '_writing_pyc' instance variable is set to 'False'. The '_basenames_to_check_rewrite' instance variable is set to a set containing 'conftest'. The '_marked_for_rewrite_cache' instance variable is set to an empty dictionary and the '_session_paths_checked' instance variable is set to 'False'.\n\nThe 'set_session' method takes an argument 'session' of type 'Optional[seSsIOn]'. This method sets the 'session' instance variable to the given 'session' and sets the '_session_paths_checked' instance variable to 'False'.\n\nThe '_find_spec' class variable is set to 'importlib.machinery.PathFinder.find_spec'.\n\nThe 'find_spec' method takes three arguments, 'name' of type 'str', 'path' of type 'Optional[Sequence[Union[str, bytes]]]' with a default value of 'None', and 'target' of type 'Optional[types.ModuleType]' with a default value of 'None'. This method returns 'Optional[importlib.machinery.ModuleSpec]'. If '_writing_pyc' is 'True', the method returns 'None'. The method gets the value of 'assertstate_key' from 'config.stash' and assigns it to 'state'. If '_eARLy_reWRiTE_BAilOut' returns 'True' with 'name' and 'state' as arguments, the method returns 'None'. The method calls '_find_spec' with 'name' and 'path' as arguments and assigns the result to 'spec'. If 'spec' is 'None' or 'spec.origin' is 'None' or 'spec.loader' is not an instance of 'importlib.machinery.SourceFileLoader' or 'spec.origin' does not exist, the method returns 'None'. If '_SHOuLD_RewritE' returns 'False' with 'name', 'fn', and 'state' as arguments, the method returns 'None'. The method returns the result of calling 'importlib.util.spec_from_file_location' with 'name', 'fn', 'self', and 'spec.submodule_search_locations' as arguments.\n\nThe 'create_module' method takes an argument 'spec' of type 'importlib.machinery.ModuleSpec'. This method returns 'Optional[types.ModuleType]'. The method returns 'None'.\n\nThe 'exec_module' method takes an argument 'module' of type 'types.ModuleType'. This method does not return anything. The method asserts that 'module.__spec__' and 'module.__spec__.origin' are not 'None'. The method sets the value of 'module.__name__' in '_rewritten_names' to 'fn'. The method gets the value of 'assertstate_key' from 'config.stash' and assigns it to 'state'. The method sets 'write' to the negation of 'sys.dont_write_bytecode'. The method calls 'GET_caCHe_dIr' with 'fn' as an argument and assigns the result to 'cache_dir'. If 'write' is 'True', the method calls 'tRy_MAKediRS' with 'cache_dir' as an argument and assigns the result to 'ok'. If 'ok' is 'False', the method sets 'write' to 'False'. The method sets 'cache_name' to the result of replacing the last three characters of 'fn.name' with 'PYC_TAIL' and sets 'pyc' to the result of joining 'cache_dir' and 'cache_name'. The method calls '_reaD_PYC' with 'fn', 'pyc', and 'state.trace' as arguments and assigns the result to 'co'. If 'co' is 'None', the method calls '_RewriTE_TESt' with 'fn' and 'self.config' as arguments and assigns the result to 'source_stat' and 'co'. If 'write' is 'True', the method sets '_writing_pyc' to 'True', tries to call '_write_pyc' with 'state', 'co', 'source_stat', and 'pyc' as arguments, and finally sets '_writing_pyc' to 'False'. The method calls 'exec' with 'co' and 'module.__dict__' as arguments.\n\nThe '_eARLy_reWRiTE_BAilOut' method takes two arguments, 'name' of type 'str' and 'state' of type 'AssertionState'. This method returns 'bool'. If 'session' is not 'None' and '_session_paths_checked' is 'False', the method sets '_session_paths_checked' to 'True' and adds the basename of 'initial_path' without the extension to '_basenames_to_check_rewrite' for each 'initial_path' in 'session._initialpaths'. The method splits 'name' by '.' and assigns the result to 'parts'. If the last element of 'parts' is in '_basenames_to_check_rewrite', the method returns 'False'. The method sets 'path' to the result of joining 'parts' with '.' and adding '.py' to the end. For each 'pat' in 'fnpats', if 'pat' contains a directory or 'fnmatch_ex' returns 'True' with 'pat' and 'path' as arguments, the method returns 'False'. If '_is_marked_for_rewrite' returns 'True' with 'name' and 'state' as arguments, the method returns 'False'. The method returns 'True'.\n\nThe '_SHOuLD_RewritE' method takes three arguments, 'name' and 'fn' of type 'str' and 'state' of type 'AssertionState'. This method returns 'bool'. If the basename of 'fn' is 'conftest.py', the method returns 'True'. If 'session' is not 'None' and 'session.isinitpath' returns 'True' with the absolute path of 'fn' as an argument, the method returns 'True'. If 'fn_path' matches any pattern in 'fnpats', the method returns 'True'. The method returns the result of calling '_is_marked_for_rewrite' with 'name' and 'state' as arguments.\n\nThe '_is_marked_for_rewrite' method takes two arguments, 'name' of type 'str' and 'state' of type 'AssertionState'. This method returns 'bool'. If 'name' is in '_marked_for_rewrite_cache', the method returns the value of 'name' in '_marked_for_rewrite_cache'. For each 'marked' in '_must_rewrite', if 'name' is equal to 'marked' or 'name' starts with 'marked' followed by '.', the method sets the value of 'name' in '_marked_for_rewrite_cache' to 'True' and returns 'True'. The method sets the value of 'name' in '_marked_for_rewrite_cache' to 'False' and returns 'False'.\n\nThe 'MARK_RewrIte' method takes any number of arguments 'names' of type 'str'. This method does not return anything. The method updates '_must_rewrite' with 'names' and clears '_marked_for_rewrite_cache'. If any name in 'names' is in 'sys.modules' and not in '_rewritten_names', the method gets the module with that name from 'sys.modules', checks if 'ASSErtiOnREWRiTeR.is_rewrite_disabled' returns 'False' with 'mod.__doc__' or an empty string as an argument and 'mod.__loader__' is not an instance of 'type(self)', and calls '_WArN_alREADy_imPORted' with that name as an argument.\n\nThe '_WArN_alREADy_imPORted' method takes an argument 'name' of type 'str'. This method does not return anything. The method calls 'config.issue_config_time_warning' with an instance of 'PYTeSTASSERTrewrIteWaRnInG' with a string in the format 'Module already imported so cannot be rewritten: %s' with 'name' as an argument and 'stacklevel' set to 5 as arguments.\n\nThe 'get_data' method takes an argument 'pathname' of type 'Union[str, bytes]'. This method returns 'bytes'. The method opens 'pathname' in binary read mode, reads the data, and returns it.\n\nIf the Python version is 3.10 or higher, the class has a 'get_resource_reader' method that takes an argument 'name' of type 'str'. This method returns 'TraversableResources'. The method returns an instance of 'FileReader' with a namespace containing 'path' set to the value of 'name' in '_rewritten_names'.",
        "repo_metadata": {
            "commit_id": "00cecfeb7023081942d05a9c9af4d85addf924c1",
            "issue_id": "pytest-dev__pytest-10624",
            "setup_details": {
                "repo": "pytest-dev/pytest",
                "instance_id": "pytest-dev__pytest-10624",
                "base_commit": "7421f3bb94df80ff2d131e932223b190f9b6d7b6",
                "version": "7.2",
                "environment_setup_commit": "572b5657d7ca557593418ce0319fabff88800c73"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "testing/test_assertrewrite.py::TestRewriteOnImport::test_remember_rewritten_modules",
                "testing/test_assertrewrite.py::test_rewrite_infinite_recursion",
                "testing/test_assertrewrite.py::TestEarlyRewriteBailout::test_basic",
                "testing/test_assertrewrite.py::TestEarlyRewriteBailout::test_pattern_contains_subdirectories"
            ]
        },
        "ground_truth_class_body": "class AssertionRewritingHook(importlib.abc.MetaPathFinder, importlib.abc.Loader):\n    \"\"\"PEP302/PEP451 import hook which rewrites asserts.\"\"\"\n\n    def __init__(self, config: Config) -> None:\n        self.config = config\n        try:\n            self.fnpats = config.getini(\"python_files\")\n        except ValueError:\n            self.fnpats = [\"test_*.py\", \"*_test.py\"]\n        self.session: Optional[seSsIOn] = None\n        self._rewritten_names: Dict[str, Path] = {}\n        self._must_rewrite: Set[str] = set()\n        # flag to guard against trying to rewrite a pyc file while we are already writing another pyc file,\n        # which might result in infinite recursion (#3506)\n        self._writing_pyc = False\n        self._basenames_to_check_rewrite = {\"conftest\"}\n        self._marked_for_rewrite_cache: Dict[str, bool] = {}\n        self._session_paths_checked = False\n\n    def set_session(self, session: Optional[seSsIOn]) -> None:\n        self.session = session\n        self._session_paths_checked = False\n\n    # Indirection so we can mock calls to find_spec originated from the hook during testing\n    _find_spec = importlib.machinery.PathFinder.find_spec\n\n    def find_spec(\n        self,\n        name: str,\n        path: Optional[Sequence[Union[str, bytes]]] = None,\n        target: Optional[types.ModuleType] = None,\n    ) -> Optional[importlib.machinery.ModuleSpec]:\n        if self._writing_pyc:\n            return None\n        state = self.config.stash[assertstate_key]\n        if self._eARLy_reWRiTE_BAilOut(name, state):\n            return None\n        state.trace(\"find_module called for: %s\" % name)\n\n        # Type ignored because mypy is confused about the `self` binding here.\n        spec = self._find_spec(name, path)  # type: ignore\n        if (\n            # the import machinery could not find a file to import\n            spec is None\n            # this is a namespace package (without `__init__.py`)\n            # there's nothing to rewrite there\n            or spec.origin is None\n            # we can only rewrite source files\n            or not isinstance(spec.loader, importlib.machinery.SourceFileLoader)\n            # if the file doesn't exist, we can't rewrite it\n            or not os.path.exists(spec.origin)\n        ):\n            return None\n        else:\n            fn = spec.origin\n\n        if not self._SHOuLD_RewritE(name, fn, state):\n            return None\n\n        return importlib.util.spec_from_file_location(\n            name,\n            fn,\n            loader=self,\n            submodule_search_locations=spec.submodule_search_locations,\n        )\n\n    def create_module(\n        self, spec: importlib.machinery.ModuleSpec\n    ) -> Optional[types.ModuleType]:\n        return None  # default behaviour is fine\n\n    def exec_module(self, module: types.ModuleType) -> None:\n        assert module.__spec__ is not None\n        assert module.__spec__.origin is not None\n        fn = Path(module.__spec__.origin)\n        state = self.config.stash[assertstate_key]\n\n        self._rewritten_names[module.__name__] = fn\n\n        # The requested module looks like a test file, so rewrite it. This is\n        # the most magical part of the process: load the source, rewrite the\n        # asserts, and load the rewritten source. We also cache the rewritten\n        # module code in a special pyc. We must be aware of the possibility of\n        # concurrent pytest processes rewriting and loading pycs. To avoid\n        # tricky race conditions, we maintain the following invariant: The\n        # cached pyc is always a complete, valid pyc. Operations on it must be\n        # atomic. POSIX's atomic rename comes in handy.\n        write = not sys.dont_write_bytecode\n        cache_dir = GET_caCHe_dIr(fn)\n        if write:\n            ok = tRy_MAKediRS(cache_dir)\n            if not ok:\n                write = False\n                state.trace(f\"read only directory: {cache_dir}\")\n\n        cache_name = fn.name[:-3] + PYC_TAIL\n        pyc = cache_dir / cache_name\n        # Notice that even if we're in a read-only directory, I'm going\n        # to check for a cached pyc. This may not be optimal...\n        co = _reaD_PYC(fn, pyc, state.trace)\n        if co is None:\n            state.trace(f\"rewriting {fn!r}\")\n            source_stat, co = _RewriTE_TESt(fn, self.config)\n            if write:\n                self._writing_pyc = True\n                try:\n                    _write_pyc(state, co, source_stat, pyc)\n                finally:\n                    self._writing_pyc = False\n        else:\n            state.trace(f\"found cached rewritten pyc for {fn}\")\n        exec(co, module.__dict__)\n\n    def _eARLy_reWRiTE_BAilOut(self, name: str, state: \"AssertionState\") -> bool:\n        \"\"\"A fast way to get out of rewriting modules.\n\n        Profiling has shown that the call to PathFinder.find_spec (inside of\n        the find_spec from this class) is a major slowdown, so, this method\n        tries to filter what we're sure won't be rewritten before getting to\n        it.\n        \"\"\"\n        if self.session is not None and not self._session_paths_checked:\n            self._session_paths_checked = True\n            for initial_path in self.session._initialpaths:\n                # Make something as c:/projects/my_project/path.py ->\n                #     ['c:', 'projects', 'my_project', 'path.py']\n                parts = str(initial_path).split(os.sep)\n                # add 'path' to basenames to be checked.\n                self._basenames_to_check_rewrite.add(os.path.splitext(parts[-1])[0])\n\n        # Note: conftest already by default in _basenames_to_check_rewrite.\n        parts = name.split(\".\")\n        if parts[-1] in self._basenames_to_check_rewrite:\n            return False\n\n        # For matching the name it must be as if it was a filename.\n        path = PurePath(*parts).with_suffix(\".py\")\n\n        for pat in self.fnpats:\n            # if the pattern contains subdirectories (\"tests/**.py\" for example) we can't bail out based\n            # on the name alone because we need to match against the full path\n            if os.path.dirname(pat):\n                return False\n            if fnmatch_ex(pat, path):\n                return False\n\n        if self._is_marked_for_rewrite(name, state):\n            return False\n\n        state.trace(f\"early skip of rewriting module: {name}\")\n        return True\n\n    def _SHOuLD_RewritE(self, name: str, fn: str, state: \"AssertionState\") -> bool:\n        # always rewrite conftest files\n        if os.path.basename(fn) == \"conftest.py\":\n            state.trace(f\"rewriting conftest file: {fn!r}\")\n            return True\n\n        if self.session is not None:\n            if self.session.isinitpath(ABsolutEPATH(fn)):\n                state.trace(f\"matched test file (was specified on cmdline): {fn!r}\")\n                return True\n\n        # modules not passed explicitly on the command line are only\n        # rewritten if they match the naming convention for test files\n        fn_path = PurePath(fn)\n        for pat in self.fnpats:\n            if fnmatch_ex(pat, fn_path):\n                state.trace(f\"matched test file {fn!r}\")\n                return True\n\n        return self._is_marked_for_rewrite(name, state)\n\n    def _is_marked_for_rewrite(self, name: str, state: \"AssertionState\") -> bool:\n        try:\n            return self._marked_for_rewrite_cache[name]\n        except KeyError:\n            for marked in self._must_rewrite:\n                if name == marked or name.startswith(marked + \".\"):\n                    state.trace(f\"matched marked file {name!r} (from {marked!r})\")\n                    self._marked_for_rewrite_cache[name] = True\n                    return True\n\n            self._marked_for_rewrite_cache[name] = False\n            return False\n\n    def MARK_RewrIte(self, *names: str) -> None:\n        \"\"\"Mark import names as needing to be rewritten.\n\n        The named module or package as well as any nested modules will\n        be rewritten on import.\n        \"\"\"\n        already_imported = (\n            set(names).intersection(sys.modules).difference(self._rewritten_names)\n        )\n        for name in already_imported:\n            mod = sys.modules[name]\n            if not ASSErtiOnREWRiTEr.is_rewrite_disabled(\n                mod.__doc__ or \"\"\n            ) and not isinstance(mod.__loader__, type(self)):\n                self._WArN_alREADy_imPORted(name)\n        self._must_rewrite.update(names)\n        self._marked_for_rewrite_cache.clear()\n\n    def _WArN_alREADy_imPORted(self, name: str) -> None:\n        from _pytest.warning_types import PYTeSTASSERTrewrIteWaRnInG\n\n        self.config.issue_config_time_warning(\n            PYTeSTASSERTrewrIteWaRnInG(\n                \"Module already imported so cannot be rewritten: %s\" % name\n            ),\n            stacklevel=5,\n        )\n\n    def get_data(self, pathname: Union[str, bytes]) -> bytes:\n        \"\"\"Optional PEP302 get_data API.\"\"\"\n        with open(pathname, \"rb\") as f:\n            return f.read()\n\n    if sys.version_info >= (3, 10):\n\n        if sys.version_info >= (3, 12):\n            from importlib.resources.abc import TraversableResources\n        else:\n            from importlib.abc import TraversableResources\n\n        def get_resource_reader(self, name: str) -> TraversableResources:  # type: ignore\n            if sys.version_info < (3, 11):\n                from importlib.readers import FileReader\n            else:\n                from importlib.resources.readers import FileReader\n\n            return FileReader(  # type:ignore[no-any-return]\n                types.SimpleNamespace(path=self._rewritten_names[name])\n            )"
    },
    {
        "task_id": "litestar-org__litestar-0001_TestClient",
        "class_name": "TestClient",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/testing/client/sync_client.py",
        "sketchy_description": "The 'TestClient' class is a subclass of 'Client', 'BaseTestClient', and 'Generic[T]'. It is used for testing applications. The class has an '__init__' method that takes several arguments including 'app', 'base_url', 'raise_server_exceptions', 'root_path', 'backend', 'backend_options', 'session_config', 'timeout', and 'cookies'. This method initializes the TestClient instance with the provided arguments.\n\nThe class has a 'request' method that sends a request with the given 'method', 'url', 'content', 'data', 'files', 'json', 'params', 'headers', 'cookies', 'auth', 'follow_redirects', 'timeout', and 'extensions'. It returns an HTTPX Response.\n\nThe class also has several methods for sending specific types of HTTP requests, including 'get', 'options', 'head', 'post', 'put', 'patch', and 'delete'. Each of these methods takes similar arguments to the 'request' method and also returns an HTTPX Response.\n\nThe 'websocket_connect' method sends a GET request to establish a websocket connection with the given 'url', 'subprotocols', 'params', 'headers', 'cookies', 'auth', 'follow_redirects', 'timeout', and 'extensions'. It returns a 'WebSocketTestSession' instance.\n\nThe 'set_session_data' method sets session data with the given 'data'. It does not return anything.\n\nThe 'get_session_data' method returns a dictionary containing session data.\n\nThe class also has '__enter__' and '__exit__' methods for entering and exiting the context of the TestClient object.\n\nThe class has several instance variables including 'blocking_portal', 'lifespan_handler', 'exit_stack', '_session_backend', 'app', 'base_url', 'backend', 'backend_options', 'cookies', '_base_url', '_auth', '_params', 'headers', '_cookies', '_timeout', 'follow_redirects', 'max_redirects', '_event_hooks', '_trust_env', '_default_encoding', '_headers', '_transport', '_mounts', and '_state'.\n\nThe class has two properties: 'is_closed' and 'session_backend'. The 'is_closed' property checks if the TestClient is closed, and the 'session_backend' property gets the session backend.\n\nThe class also has three class variables: 'lifespan_handler', 'exit_stack', and '__test__'. The 'lifespan_handler' and 'exit_stack' are used for managing the lifespan of the TestClient, and '__test__' is set to False.",
        "detailed_description": "The 'TestClient' class is a subclass of 'Client', 'BaseTestClient', and 'Generic[T]'. It has two instance variables, 'lifespan_handler' of type 'LifeSpanHandler[Any]' and 'exit_stack' of type 'ExitStack'. \n\nThe '__init__' method of the class takes several arguments including 'app' of type 'T', 'base_url' of type 'str' with a default value of \"http://testserver.local\", 'raise_server_exceptions' of type 'bool' with a default value of 'True', 'root_path' of type 'str' with a default value of \"\", 'backend' of type 'AnyIOBackend' with a default value of \"asyncio\", 'backend_options' of type 'Mapping[str, Any]' or 'None' with a default value of 'None', 'session_config' of type 'BaseBackendConfig' or 'None' with a default value of 'None', 'timeout' of type 'float' or 'None' with a default value of 'None', and 'cookies' of type 'CookieTypes' or 'None' with a default value of 'None'. This method initializes the superclass 'BaseTestClient' and 'Client' with the given arguments and sets the 'transport' argument of 'Client' to an instance of 'TestClientTransport'.\n\nThe '__enter__' method returns an instance of 'TestClient[T]'. This method creates an 'ExitStack' context manager and sets the 'blocking_portal' instance variable to the context returned by the 'portal' method. It also sets the 'lifespan_handler' instance variable to an instance of 'LifeSpanHandler' with the instance as the argument. The method then adds two callbacks to the 'ExitStack' context manager to delete the 'blocking_portal' attribute and call the 'wait_shutdown' method of the 'lifespan_handler' instance variable. The 'exit_stack' instance variable is set to the result of calling the 'pop_all' method of the 'ExitStack' context manager.\n\nThe '__exit__' method takes any number of arguments of type 'Any' and does not return anything. This method calls the 'close' method of the 'exit_stack' instance variable.\n\nThe 'request' method takes several arguments including 'method' of type 'str', 'url' of type 'URLTypes', and optional keyword arguments 'content' of type 'RequestContent' or 'None' with a default value of 'None', 'data' of type 'RequestData' or 'None' with a default value of 'None', 'files' of type 'RequestFiles' or 'None' with a default value of 'None', 'json' of type 'Any' or 'None' with a default value of 'None', 'params' of type 'QueryParamTypes' or 'None' with a default value of 'None', 'headers' of type 'HeaderTypes' or 'None' with a default value of 'None', 'cookies' of type 'CookieTypes' or 'None' with a default value of 'None', 'auth' of type 'AuthTypes' or 'UseClientDefault' or 'None' with a default value of 'USE_CLIENT_DEFAULT', 'follow_redirects' of type 'bool' or 'UseClientDefault' with a default value of 'USE_CLIENT_DEFAULT', 'timeout' of type 'TimeoutTypes' or 'UseClientDefault' with a default value of 'USE_CLIENT_DEFAULT', and 'extensions' of type 'Mapping[str, Any]' or 'None' with a default value of 'None'. This method returns an instance of 'Response'. This method calls the 'request' method of the superclass 'Client' with the given arguments and additional processing.\n\nThe class also has several methods named 'get', 'options', 'head', 'post', 'put', 'patch', and 'delete' that take similar arguments as the 'request' method and return an instance of 'Response'. These methods call the corresponding method of the superclass 'Client' with the given arguments and additional processing.\n\nThe 'websocket_connect' method takes several arguments including 'url' of type 'str', and optional keyword arguments 'subprotocols' of type 'Sequence[str]' or 'None' with a default value of 'None', 'params' of type 'QueryParamTypes' or 'None' with a default value of 'None', 'headers' of type 'HeaderTypes' or 'None' with a default value of 'None', 'cookies' of type 'CookieTypes' or 'None' with a default value of 'None', 'auth' of type 'AuthTypes' or 'UseClientDefault' with a default value of 'USE_CLIENT_DEFAULT', 'follow_redirects' of type 'bool' or 'UseClientDefault' with a default value of 'USE_CLIENT_DEFAULT', 'timeout' of type 'TimeoutTypes' or 'UseClientDefault' with a default value of 'USE_CLIENT_DEFAULT', and 'extensions' of type 'Mapping[str, Any]' or 'None' with a default value of 'None'. This method returns an instance of 'WebSocketTestSession'. This method sends a 'GET' request to establish a websocket connection and returns the 'session' attribute of the 'ConnectionUpgradeExceptionError' exception if it is raised.\n\nThe 'set_session_data' method takes an argument 'data' of type 'dict[str, Any]' and does not return anything. This method sets the session data by calling the '_set_session_data' method with the given 'data' in a 'portal' context manager.\n\nThe 'get_session_data' method does not take any arguments and returns a dictionary of type 'dict[str, Any]'. This method gets the session data by calling the '_get_session_data' method in a 'portal' context manager.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/e2e/test_option_requests.py::test_cors_options_request_without_origin_passes",
                "tests/e2e/test_option_requests.py::test_regular_options_request[http_methods2]",
                "tests/e2e/test_option_requests.py::test_cors_options_request_with_correct_origin_passes",
                "tests/e2e/test_option_requests.py::test_cors_options_request_with_correct_origin_passes_with_allow_all_origins",
                "tests/e2e/test_option_requests.py::test_cors_options_request_with_different_domains_matches_regex[allowed_origins0-None-https://moishe.zuchmir]",
                "tests/e2e/test_option_requests.py::test_cors_options_request_with_wrong_origin_fails",
                "tests/e2e/test_exception_handlers.py::test_exception_handling[ValidationException-router]",
                "tests/e2e/test_exception_handlers.py::test_exception_handling[ServiceUnavailableException-handler]",
                "tests/e2e/test_exception_handlers.py::test_exception_handling[NotFoundException-handler]",
                "tests/e2e/test_option_requests.py::test_cors_options_request_with_different_domains_matches_regex[allowed_origins2-None-https://moishe.zuchmir]",
                "tests/e2e/test_advanced_alchemy.py::test_using_pagination",
                "tests/examples/test_contrib/test_sqlalchemy/plugins/test_example_apps.py::test_sqlalchemy_async_plugin_example@sqlalchemy_examples",
                "tests/e2e/test_option_requests.py::test_regular_options_request[http_methods4]",
                "tests/e2e/test_option_requests.py::test_regular_options_request[http_methods3]",
                "tests/e2e/test_option_requests.py::test_regular_options_request[http_methods1]",
                "tests/e2e/test_exception_handlers.py::test_exception_handler_with_custom_request_class",
                "tests/e2e/test_exception_handlers.py::test_exception_handling[InternalServerException-controller]",
                "tests/e2e/test_option_requests.py::test_cors_options_request_with_different_domains_matches_regex[allowed_origins1-None-http://testserver.local]",
                "tests/e2e/test_option_requests.py::test_regular_options_request[http_methods0]",
                "tests/e2e/test_option_requests.py::test_cors_options_request_with_different_domains_matches_regex[allowed_origins4-None-https://moishe.zuchmir.zzz.com]",
                "tests/e2e/test_option_requests.py::test_cors_options_request_with_different_domains_matches_regex[allowed_origins3-None-https://moishe.zuchmir.abc.com]",
                "tests/e2e/test_pydantic.py::test_app_with_v1_and_v2_models",
                "tests/e2e/test_option_requests.py::test_cors_options_request_with_different_domains_matches_regex[allowed_origins6-https://moishe.*.*.com-https://moishe.zuchmir.zzz.com]",
                "tests/examples/test_contrib/test_sqlalchemy/plugins/test_example_apps.py::test_sqlalchemy_sync_plugin_example@sqlalchemy_examples",
                "tests/e2e/test_option_requests.py::test_cors_options_request_with_different_domains_matches_regex[allowed_origins5-https://moishe.*.*.com-https://moishe.zuchmir.zzz.com]",
                "tests/examples/test_contrib/test_sqlalchemy/plugins/test_example_apps.py::test_sqlalchemy_async_init_plugin_example@sqlalchemy_examples",
                "tests/e2e/test_dependency_injection/test_inter_dependencies.py::test_inter_dependencies",
                "tests/e2e/test_dependency_injection/test_injection_of_classes.py::test_inject_dataclass",
                "tests/e2e/test_dependency_injection/test_inter_dependencies.py::test_inter_dependencies_on_same_app_level",
                "tests/e2e/test_dependency_injection/test_request_local_caching.py::test_caching_per_request",
                "tests/e2e/test_dependency_injection/test_injection_of_generic_models.py::test_generic_model_injection",
                "tests/e2e/test_life_cycle_hooks/test_after_request.py::test_after_request_handler_called[None-expected0]",
                "tests/e2e/test_dependency_injection/test_http_handler_dependency_injection.py::test_controller_dependency_injection",
                "tests/e2e/test_dependency_injection/test_websocket_handler_dependency_injection.py::test_function_dependency_injection",
                "tests/e2e/test_life_cycle_hooks/test_after_request.py::test_after_request_handler_resolution[sync_after_request_handler-async_after_request_handler_with_hello_world-None-None-expected5]",
                "tests/e2e/test_dependency_injection/test_injection_of_classes.py::test_injection_of_classes",
                "tests/e2e/test_dependency_injection/test_http_handler_dependency_injection.py::test_function_dependency_injection",
                "tests/e2e/test_dependency_injection/test_websocket_handler_dependency_injection.py::test_dependency_isolation",
                "tests/e2e/test_life_cycle_hooks/test_after_request.py::test_after_request_handler_resolution[None-None-None-None-expected0]",
                "tests/examples/test_contrib/test_sqlalchemy/plugins/test_example_apps.py::test_sqlalchemy_sync_init_plugin_example@sqlalchemy_examples",
                "tests/e2e/test_dependency_injection/test_http_handler_dependency_injection.py::test_dependency_isolation",
                "tests/e2e/test_life_cycle_hooks/test_after_request.py::test_after_request_handler_resolution[None-None-sync_after_request_handler-None-expected3]",
                "tests/e2e/test_dependency_injection/test_injection_of_classes.py::test_inject_msgspec_struct",
                "tests/e2e/test_life_cycle_hooks/test_after_response.py::test_after_response_resolution[controller]",
                "tests/e2e/test_starlette_responses.py::test_starlette_json_response",
                "tests/e2e/test_life_cycle_hooks/test_after_request.py::test_after_request_handler_called[sync_after_request_handler-expected1]",
                "tests/e2e/test_life_cycle_hooks/test_after_request.py::test_after_request_handler_called[async_after_request_handler-expected2]",
                "tests/e2e/test_life_cycle_hooks/test_after_request.py::test_after_request_handler_resolution[sync_after_request_handler-None-None-None-expected1]",
                "tests/e2e/test_life_cycle_hooks/test_after_request.py::test_after_request_handler_resolution[None-sync_after_request_handler-None-None-expected2]",
                "tests/e2e/test_life_cycle_hooks/test_after_request.py::test_after_request_handler_resolution[None-None-None-sync_after_request_handler-expected4]",
                "tests/e2e/test_dependency_injection/test_websocket_handler_dependency_injection.py::test_controller_dependency_injection",
                "tests/examples/test_contrib/test_sqlalchemy/plugins/test_example_apps.py::test_sqlalchemy_async_init_plugin_dependencies@sqlalchemy_examples",
                "tests/e2e/test_life_cycle_hooks/test_after_request.py::test_after_request_handler_resolution[None-sync_after_request_handler-async_after_request_handler_with_hello_world-None-expected6]",
                "tests/e2e/test_life_cycle_hooks/test_after_request.py::test_after_request_handler_resolution[None-None-None-async_after_request_handler_with_hello_world-expected8]",
                "tests/e2e/test_life_cycle_hooks/test_before_request.py::test_before_request_handler_called[async_before_request_handler_with_return_value-expected2]",
                "tests/e2e/test_life_cycle_hooks/test_after_response.py::test_after_response_resolution[handler]",
                "tests/e2e/test_life_cycle_hooks/test_after_response.py::test_after_response_resolution[app]",
                "tests/e2e/test_life_cycle_hooks/test_after_request.py::test_after_request_handler_resolution[None-None-sync_after_request_handler-async_after_request_handler_with_hello_world-expected7]",
                "tests/e2e/test_life_cycle_hooks/test_before_request.py::test_before_request_handler_called[sync_before_request_handler_without_return_value-expected3]",
                "tests/e2e/test_life_cycle_hooks/test_after_response.py::test_after_response_resolution[router]",
                "tests/e2e/test_life_cycle_hooks/test_before_request.py::test_before_request_handler_called[async_before_request_handler_without_return_value-expected4]",
                "tests/e2e/test_life_cycle_hooks/test_before_request.py::test_before_request_handler_called[sync_before_request_handler_with_return_value-expected1]",
                "tests/e2e/test_life_cycle_hooks/test_before_request.py::test_before_request_handler_resolution[None-sync_before_request_handler_with_return_value-None-None-expected2]",
                "tests/e2e/test_life_cycle_hooks/test_before_request.py::test_before_request_handler_resolution[None-None-None-None-expected0]",
                "tests/e2e/test_life_cycle_hooks/test_after_request.py::test_after_request_handles_handlers_that_return_responses",
                "tests/examples/test_contrib/test_sqlalchemy/plugins/test_example_apps.py::test_sqlalchemy_sync_init_plugin_dependencies@sqlalchemy_examples",
                "tests/e2e/test_life_cycle_hooks/test_before_request.py::test_before_request_handler_resolution[None-None-sync_before_request_handler_with_return_value-None-expected3]",
                "tests/e2e/test_life_cycle_hooks/test_before_request.py::test_before_request_handler_called[None-expected0]",
                "tests/e2e/test_life_cycle_hooks/test_before_request.py::test_before_request_handler_resolution[None-None-None-sync_before_request_handler_with_return_value-expected4]",
                "tests/e2e/test_life_cycle_hooks/test_before_request.py::test_before_request_handler_resolution[None-None-sync_before_request_handler_with_return_value-async_before_request_handler_without_return_value-expected7]",
                "tests/e2e/test_routing/test_path_mounting.py::test_supports_mounting",
                "tests/e2e/test_life_cycle_hooks/test_before_request.py::test_before_request_handler_resolution[None-None-None-async_before_request_handler_without_return_value-expected8]",
                "tests/e2e/test_life_cycle_hooks/test_before_request.py::test_before_request_handler_resolution[sync_before_request_handler_with_return_value-None-None-None-expected1]",
                "tests/e2e/test_routing/test_path_resolution.py::test_path_parsing_and_matching[/path/1/2/sub/c892496f-b1fd-4b91-bdb8-b46f92df1716-/path/{first:int}/{second:str}/sub/{third:uuid}-200]",
                "tests/e2e/test_life_cycle_hooks/test_before_request.py::test_before_request_handler_resolution[None-sync_before_request_handler_with_return_value-async_before_request_handler_without_return_value-None-expected6]",
                "tests/e2e/test_routing/test_path_resolution.py::test_root_route_handler[get---delete_handler10]",
                "tests/e2e/test_routing/test_path_resolution.py::test_root_route_handler[get--/something-delete_handler6]",
                "tests/e2e/test_routing/test_path_resolution.py::test_path_parsing_and_matching[--200]",
                "tests/e2e/test_routing/test_path_resolution.py::test_root_route_handler[get--/something-None]",
                "tests/e2e/test_routing/test_path_resolution.py::test_root_route_handler[get-/-/something-delete_handler7]",
                "tests/e2e/test_routing/test_path_resolution.py::test_path_parsing_with_ambiguous_paths",
                "tests/e2e/test_routing/test_path_resolution.py::test_root_route_handler[get-/-/something-None]",
                "tests/e2e/test_routing/test_path_resolution.py::test_root_route_handler[get-/-/-None]",
                "tests/e2e/test_routing/test_path_resolution.py::test_path_parsing_and_matching[/-/-200]",
                "tests/e2e/test_routing/test_path_resolution.py::test_root_route_handler[get---None]",
                "tests/e2e/test_routing/test_path_resolution.py::test_root_route_handler[get-/--None]",
                "tests/e2e/test_routing/test_path_resolution.py::test_root_route_handler[get--/-None]",
                "tests/e2e/test_routing/test_path_resolution.py::test_path_parsing_and_matching[/a/b/c/d/path/1/2/sub/d4aca431-2e02-4818-824b-a2ddc6a64e9c/-/path/{first:int}/{second:str}/sub/{third:uuid}/-404]",
                "tests/e2e/test_routing/test_path_resolution.py::test_root_route_handler[get--/-delete_handler8]",
                "tests/e2e/test_life_cycle_hooks/test_before_request.py::test_before_request_handler_resolution[sync_before_request_handler_with_return_value-async_before_request_handler_without_return_value-None-None-expected5]",
                "tests/e2e/test_routing/test_path_resolution.py::test_root_route_handler[get-/--delete_handler11]",
                "tests/examples/test_contrib/test_sqlalchemy/plugins/test_example_apps.py::test_sqlalchemy_async_serialization_plugin@sqlalchemy_examples",
                "tests/e2e/test_routing/test_path_resolution.py::test_root_route_handler[get-/-/-delete_handler9]",
                "tests/e2e/test_routing/test_path_resolution.py::test_path_parsing_and_matching[/path/1/2/sub/2535a9cb-6554-4d85-bb3b-ad38362f63c7/-/path/{first:int}/{second:str}/sub/{third:uuid}/-200]",
                "tests/examples/test_contrib/test_sqlalchemy/plugins/test_example_apps.py::test_sqlalchemy_sync_serialization_plugin@sqlalchemy_examples",
                "tests/e2e/test_routing/test_validations.py::test_supports_websocket_and_http_handlers",
                "tests/examples/test_exceptions.py::test_per_exception_handlers",
                "tests/e2e/test_routing/test_validations.py::test_controller_supports_websocket_and_http_handlers",
                "tests/examples/test_lifecycle_hooks.py::test_after_request_app",
                "tests/examples/test_contrib/test_sqlalchemy/plugins/test_example_apps.py::test_sqlalchemy_async_serialization_dto@sqlalchemy_examples",
                "tests/examples/test_cache_control_headers.py::test_cache_control_header",
                "tests/examples/test_request_data.py::test_request_data_6",
                "tests/examples/test_exceptions.py::test_override_default_handler",
                "tests/examples/test_lifecycle_hooks.py::test_before_request_app",
                "tests/examples/test_lifecycle_hooks.py::test_after_response_app",
                "tests/examples/test_request_data.py::test_request_data_9",
                "tests/examples/test_request_data.py::test_msgpack_app",
                "tests/examples/test_request_data.py::test_request_data_5",
                "tests/examples/test_request_data.py::test_request_data_1",
                "tests/examples/test_routing.py::test_mounting_asgi_app_example[app1]",
                "tests/examples/test_request_data.py::test_request_data_8",
                "tests/examples/test_lifecycle_hooks.py::test_layered_hooks",
                "tests/examples/test_static_files.py::test_full_example",
                "tests/examples/test_request_data.py::test_request_data_10",
                "tests/examples/test_request_data.py::test_request_data_7",
                "tests/examples/test_openapi.py::test_schema_generation",
                "tests/examples/test_exceptions.py::test_layered_handlers",
                "tests/examples/test_hello_world.py::test_hello_world_example",
                "tests/examples/test_request_data.py::test_request_data_3",
                "tests/examples/test_request_data.py::test_request_data_2",
                "tests/examples/test_signature_namespace.py::test_msgpack_app",
                "tests/examples/test_request_data.py::test_request_data_4",
                "tests/examples/test_startup_and_shutdown.py::test_startup_and_shutdown_example",
                "tests/examples/test_static_files.py::test_html_mode",
                "tests/examples/test_static_files.py::test_send_as_attachment",
                "tests/examples/test_stores.py::test_delete_expired_after_response",
                "tests/examples/test_static_files.py::test_upgrade_from_static",
                "tests/examples/test_todo_app.py::test_update[docs.examples.todo_app.full_app]",
                "tests/examples/test_todo_app.py::test_get_list_dataclass[docs.examples.todo_app.get_list.dict]",
                "tests/examples/test_routing.py::test_mounting_asgi_app_example[app0]",
                "tests/examples/test_stores.py::test_delete_expired_on_startup",
                "tests/examples/test_todo_app.py::test_update[docs.examples.todo_app.update]",
                "tests/examples/test_todo_app.py::test_get_list_dataclass[docs.examples.todo_app.get_list.query_param_default]",
                "tests/examples/test_todo_app.py::test_get_list_dataclass[docs.examples.todo_app.get_list.dataclass]",
                "tests/examples/test_todo_app.py::test_dataclass_create[docs.examples.todo_app.full_app]",
                "tests/examples/test_application_hooks/test_application_after_exception_hook.py::test_application_shutdown_hooks",
                "tests/examples/test_todo_app.py::test_get_list_query_param[docs.examples.todo_app.get_list.query_param_validate_manually]",
                "tests/examples/test_todo_app.py::test_get_list_query_param_invalid[docs.examples.todo_app.get_list.query_param_validate]",
                "tests/examples/test_application_hooks/test_application_before_send.py::test_application_before_send_hooks",
                "tests/examples/test_using_session_auth.py::test_using_session_auth_login_flow",
                "tests/examples/test_todo_app.py::test_get_list_query_param[docs.examples.todo_app.get_list.query_param_validate]",
                "tests/examples/test_application_hooks/test_lifespan_manager.py::test_startup_and_shutdown_example",
                "tests/examples/test_todo_app.py::test_get_list_query_param[docs.examples.todo_app.get_list.query_param]",
                "tests/examples/test_todo_app.py::test_dataclass_create[docs.examples.todo_app.create.dataclass]",
                "tests/examples/test_application_state/test_using_custom_state.py::test_using_custom_state_example",
                "tests/examples/test_using_session_auth.py::test_using_session_auth_signup_flow",
                "tests/examples/test_todo_app.py::test_get_list_query_param_invalid[docs.examples.todo_app.full_app]",
                "tests/examples/test_todo_app.py::test_get_list_query_param_invalid[docs.examples.todo_app.get_list.query_param_validate_manually]",
                "tests/examples/test_dependency_injection/test_dependency_default_value_no_dependency_fn.py::test_optional_dependency_in_openapi_schema",
                "tests/examples/test_contrib/prometheus/test_prometheus_exporter_example_with_extra_config.py::test_prometheus_exporter_with_extra_config_example",
                "tests/examples/test_dependency_injection/test_dependency_default_value_with_dependency_fn.py::test_optional_dependency_not_in_openapi_schema",
                "tests/examples/test_application_state/test_using_immutable_state.py::test_using_custom_state_example",
                "tests/examples/test_dependency_injection/test_dependency_validation_error.py::test_route_returns_internal_server_error",
                "tests/examples/test_application_state/test_using_application_state.py::test_using_application_state",
                "tests/examples/test_dto/test_example_apps.py::test_dto_data_usage_app",
                "tests/examples/test_dto/test_example_apps.py::test_exclude_fields_app",
                "tests/examples/test_dependency_injection/test_dependency_skip_validation.py::test_route_skips_validation",
                "tests/examples/test_application_state/test_passing_initial_state.py::test_passing_initial_state_example",
                "tests/examples/test_dto/test_example_apps.py::test_response_return_data_app",
                "tests/examples/test_dto/test_example_apps.py::test_dto_data_nested_data_create_instance_app",
                "tests/examples/test_dto/test_example_apps.py::test_patch_requests_app",
                "tests/examples/test_contrib/prometheus/test_prometheus_exporter_example.py::test_prometheus_exporter_example",
                "tests/examples/test_dto/test_example_apps.py::test_include_fields_app",
                "tests/examples/test_dto/test_tutorial.py::test_max_nested_depth",
                "tests/examples/test_dto/test_example_apps.py::test_paginated_return_data_app",
                "tests/examples/test_dto/test_tutorial.py::test_simple_dto_exclude",
                "tests/examples/test_dto/test_example_apps.py::test_dto_data_problem_statement_app",
                "tests/examples/test_dto/test_tutorial.py::test_dto_data",
                "tests/examples/test_dto/test_tutorial.py::test_nested_collection_exclude",
                "tests/examples/test_dto/test_tutorial.py::test_field_renaming_strategy",
                "tests/examples/test_todo_app.py::test_dict_create",
                "tests/examples/test_dto/test_tutorial.py::test_nested_exclude",
                "tests/examples/test_dto/test_tutorial.py::test_initial_pattern_app",
                "tests/examples/test_dto/test_tutorial.py::test_patch_handler",
                "tests/examples/test_dto/test_example_apps.py::test_enveloped_return_data_app",
                "tests/examples/test_parameters/test_header_and_cookies_parameters.py::test_header_and_cookie_parameters",
                "tests/examples/test_pagination/test_using_offset_pagination.py::test_using_offset_pagination",
                "tests/examples/test_dto/test_tutorial.py::test_explicit_field_renaming",
                "tests/examples/test_dto/test_tutorial.py::test_multiple_handlers",
                "tests/examples/test_dto/test_tutorial.py::test_read_only_fields",
                "tests/examples/test_middleware/test_abstract_middleware.py::test_base_middleware_example_websocket",
                "tests/examples/test_middleware/test_abstract_middleware.py::test_exclude_by_regex",
                "tests/examples/test_parameters/test_layered_parameters.py::test_layered_parameters[params2-400-None]",
                "tests/examples/test_middleware/test_abstract_middleware.py::test_not_excluded",
                "tests/examples/test_parameters/test_layered_parameters.py::test_layered_parameters[params3-400-None]",
                "tests/examples/test_middleware/test_session_middleware.py::test_session_middleware_example",
                "tests/examples/test_dto/test_tutorial.py::test_simple_receiving_data",
                "tests/examples/test_middleware/test_abstract_middleware.py::test_exclude_by_opt_key",
                "tests/examples/test_middleware/test_call_order.py::test_call_order",
                "tests/examples/test_middleware/test_logging_middleware.py::test_logging_middleware_regular_logger",
                "tests/examples/test_pagination/test_using_classic_pagination.py::test_using_classic_pagination",
                "tests/examples/test_parameters/test_layered_parameters.py::test_layered_parameters[params0-200-expected0]",
                "tests/examples/test_middleware/test_rate_limit_middleware.py::test_rate_limit_middleware_example",
                "tests/examples/test_parameters/test_query_parameters.py::test_query_params_default",
                "tests/examples/test_parameters/test_query_parameters.py::test_query_params_optional",
                "tests/examples/test_parameters/test_query_parameters.py::test_query_params_types",
                "tests/examples/test_pagination/test_using_cursor_pagination.py::test_using_cursor_pagination",
                "tests/examples/test_parameters/test_query_parameters.py::test_query_params_constraint",
                "tests/examples/test_parameters/test_query_parameters.py::test_query_params_remap",
                "tests/examples/test_dto/test_tutorial.py::test_controller",
                "tests/examples/test_responses/test_background_tasks.py::test_background_tasks_2",
                "tests/examples/test_responses/test_background_tasks.py::test_background_tasks_3",
                "tests/examples/test_responses/test_response_cookies.py::test_response_cookies_5",
                "tests/examples/test_plugins/test_example_apps.py::test_dto_data_problem_statement_app",
                "tests/examples/test_responses/test_custom_responses.py::test_custom_responses",
                "tests/examples/test_dto/test_tutorial.py::test_put_handler",
                "tests/examples/test_responses/test_response_cookies.py::test_response_cookies_4",
                "tests/examples/test_responses/test_response_headers.py::test_response_headers",
                "tests/examples/test_plugins/test_di_plugin.py::test_di_plugin_example",
                "tests/examples/test_responses/test_background_tasks.py::test_background_tasks_1",
                "tests/examples/test_responses/test_response_cookies.py::test_response_cookies_2",
                "tests/examples/test_responses/test_response_headers.py::test_response_headers_2",
                "tests/examples/test_parameters/test_path_parameters.py::test_path_parameters_1",
                "tests/examples/test_parameters/test_layered_parameters.py::test_layered_parameters[params1-400-None]",
                "tests/examples/test_responses/test_response_cookies.py::test_response_cookies_3",
                "tests/examples/test_parameters/test_path_parameters.py::test_path_parameters_2",
                "tests/examples/test_parameters/test_query_parameters.py::test_query_params",
                "tests/examples/test_responses/test_response_cookies.py::test_response_cookies",
                "tests/examples/test_responses/test_response_headers.py::test_response_headers_4",
                "tests/examples/test_security/test_jwt/test_using_jwt_cookie_auth.py::test_using_jwt_cookie_auth",
                "tests/examples/test_responses/test_returning_responses.py::test_returning_responses",
                "tests/examples/test_templating/test_template_functions.py::test_template_functions_jinja",
                "tests/examples/test_security/test_jwt/test_using_jwt_auth.py::test_using_jwt_auth",
                "tests/examples/test_templating/test_returning_templates.py::test_returning_templates[string-app2-Minijinja-Hello <strong>Minijinja</strong>-Hello <strong>Minijinja</strong> using strings]",
                "tests/examples/test_templating/test_template_functions.py::test_template_functions_minijinja",
                "tests/examples/test_templating/test_returning_templates.py::test_returning_templates[file-app0-Jinja-Hello <strong>Jinja</strong>-Hello <strong>Jinja</strong> using strings]",
                "tests/examples/test_templating/test_returning_templates.py::test_returning_templates[string-app0-Jinja-Hello <strong>Jinja</strong>-Hello <strong>Jinja</strong> using strings]",
                "tests/examples/test_templating/test_returning_templates.py::test_returning_templates[file-app1-Mako-Hello <strong>Mako</strong>-Hello <strong>Mako</strong> using strings]",
                "tests/examples/test_parameters/test_path_parameters.py::test_path_parameters_3",
                "tests/examples/test_templating/test_returning_templates.py::test_returning_templates[file-app2-Minijinja-Hello <strong>Minijinja</strong>-Hello <strong>Minijinja</strong> using strings]",
                "tests/examples/test_templating/test_returning_templates.py::test_returning_templates[string-app1-Mako-Hello <strong>Mako</strong>-Hello <strong>Mako</strong> using strings]",
                "tests/examples/test_templating/test_template_functions.py::test_template_functions_mako",
                "tests/examples/test_security/test_jwt/test_using_oauth2_password_bearer.py::test_using_oauth2_password_bearer_auth",
                "tests/unit/test_app.py::test_handler_error_return_status_500",
                "tests/examples/test_contrib/test_sqlalchemy/plugins/test_example_apps.py::test_sqlalchemy_async_serialization_plugin_marking_fields@sqlalchemy_examples",
                "tests/unit/test_app.py::test_using_custom_http_exception_handler",
                "tests/unit/test_app.py::test_before_send",
                "tests/unit/test_app.py::test_debug_response_created",
                "tests/unit/test_asgi_router.py::test_multiple_lifespan_managers",
                "tests/unit/test_background_tasks.py::test_background_tasks_task_group_execution",
                "tests/unit/test_background_tasks.py::test_background_tasks_regular_execution",
                "tests/examples/test_contrib/test_sqlalchemy/plugins/test_example_apps.py::test_sqlalchemy_sync_serialization_plugin_marking_fields@sqlalchemy_examples",
                "tests/unit/test_controller.py::test_controller_http_method[get-GET-200-return_value0-return_annotation0]",
                "tests/unit/test_controller.py::test_controller_http_method[post-POST-201-return_value2-DataclassPerson]",
                "tests/unit/test_controller.py::test_controller_with_websocket_handler",
                "tests/unit/test_controller.py::test_controller_http_method[get-GET-200-return_value1-DataclassPerson]",
                "tests/unit/test_controller.py::test_controller_http_method[patch-PATCH-200-return_value4-DataclassPerson]",
                "tests/unit/test_controller.py::test_controller_subclassing",
                "tests/unit/test_controller.py::test_controller_http_method[delete-DELETE-204-None-None]",
                "tests/examples/test_contrib/test_sqlalchemy/plugins/test_tutorial_example_apps.py::test_no_plugins_full_app[docs.examples.contrib.sqlalchemy.plugins.tutorial.full_app_no_plugins]@sqlalchemy_examples",
                "tests/unit/test_events.py::test_event_listener[asyncio-async_listener]",
                "tests/unit/test_events.py::test_event_listener[asyncio-sync_listener]",
                "tests/unit/test_events.py::test_event_listener[trio-sync_listener]",
                "tests/unit/test_events.py::test_event_listener[trio-async_listener]",
                "tests/unit/test_events.py::test_multiple_event_listeners[asyncio]",
                "tests/unit/test_events.py::test_multiple_event_ids[trio]",
                "tests/unit/test_events.py::test_multiple_event_ids[asyncio]",
                "tests/unit/test_events.py::test_event_listener_raises_exception",
                "tests/unit/test_events.py::test_multiple_event_listeners[trio]",
                "tests/unit/test_exceptions.py::test_create_exception_response_utility_starlette_http_exception[application/json]",
                "tests/examples/test_responses/test_response_headers.py::test_response_headers_3",
                "tests/examples/test_contrib/test_sqlalchemy/plugins/test_tutorial_example_apps.py::test_no_plugins_full_app[docs.examples.contrib.sqlalchemy.plugins.tutorial.full_app_with_init_plugin]@sqlalchemy_examples",
                "tests/unit/test_exceptions.py::test_default_exception_handling_of_internal_server_errors[text/html]",
                "tests/unit/test_guards.py::test_guards_with_websocket_handler",
                "tests/unit/test_guards.py::test_guards_with_asgi_handler",
                "tests/unit/test_guards.py::test_guards_with_http_handler",
                "tests/unit/test_pagination.py::test_classic_pagination_data_shape[paginator1]",
                "tests/unit/test_pagination.py::test_limit_offset_pagination_data_shape[paginator0]",
                "tests/unit/test_controller.py::test_controller_http_method[put-PUT-200-return_value3-DataclassPerson]",
                "tests/unit/test_pagination.py::test_classic_pagination_data_shape[paginator0]",
                "tests/examples/test_contrib/test_sqlalchemy/plugins/test_tutorial_example_apps.py::test_no_plugins_full_app[docs.examples.contrib.sqlalchemy.plugins.tutorial.full_app_with_plugin]@sqlalchemy_examples",
                "tests/unit/test_params.py::test_parsing_of_parameter_as_annotated",
                "tests/unit/test_params.py::test_parsing_of_parameter_as_default",
                "tests/unit/test_params.py::test_parsing_of_body_as_annotated",
                "tests/examples/test_contrib/test_sqlalchemy/plugins/test_tutorial_example_apps.py::test_no_plugins_full_app[docs.examples.contrib.sqlalchemy.plugins.tutorial.full_app_with_serialization_plugin]@sqlalchemy_examples",
                "tests/unit/test_params.py::test_optional_query_parameter_consistency_wiht_default_queried_without_param",
                "tests/unit/test_params.py::test_optional_query_parameter_consistency_with_default_queried_with_other_param",
                "tests/examples/test_contrib/test_sqlalchemy/plugins/test_tutorial_example_apps.py::test_no_plugins_full_app[docs.examples.contrib.sqlalchemy.plugins.tutorial.full_app_with_session_di]@sqlalchemy_examples",
                "tests/examples/test_plugins/test_sqlalchemy_init_plugin.py::test_sync_app@sqlalchemy_examples",
                "tests/unit/test_pagination.py::test_limit_offset_pagination_data_shape[paginator1]",
                "tests/examples/test_plugins/test_sqlalchemy_init_plugin.py::test_async_app@sqlalchemy_examples",
                "tests/unit/test_params.py::test_optional_query_parameter_consistency_no_default_queried_with_expected_param",
                "tests/unit/test_params.py::test_optional_query_parameter_consistency_no_default_queried_with_other_param",
                "tests/unit/test_params.py::test_optional_query_parameter_consistency_with_default_queried_with_expected_param",
                "tests/unit/test_params.py::test_optional_query_parameter_consistency_no_default_queried_without_param",
                "tests/unit/test_exceptions.py::test_default_exception_handling_of_internal_server_errors[text/plain]",
                "tests/unit/test_exceptions.py::test_create_exception_response_utility_starlette_http_exception[text/plain]",
                "tests/unit/test_exceptions.py::test_default_exception_handling_of_internal_server_errors[application/json]",
                "tests/unit/test_exceptions.py::test_non_litestar_exception_with_status_code_is_500",
                "tests/unit/test_exceptions.py::test_non_litestar_exception_with_detail_is_not_included",
                "tests/unit/test_channels/test_plugin.py::test_plugin_dependency",
                "tests/unit/test_channels/test_plugin.py::test_handler_sends_history[3-2-2]",
                "tests/unit/test_channels/test_plugin.py::test_handler_sends_history[2-2-2]",
                "tests/unit/test_channels/test_plugin.py::test_handler_sends_history[2-0-0]",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers[memory-None-binary]",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers[memory-None-text]",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers[memory-/ws-text]",
                "tests/unit/test_channels/test_plugin.py::test_ws_route_handlers_receive_arbitrary_message[memory]",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers[memory-/ws-binary]",
                "tests/unit/test_channels/test_plugin.py::test_handler_sends_history[2-1-1]",
                "tests/unit/test_channels/test_plugin.py::test_handler_sends_history[2--1-2]",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers_arbitrary_channels_allowed[memory]",
                "tests/unit/test_connection/test_request.py::test_route_handler_property",
                "tests/unit/test_connection/test_request.py::test_request_url_for",
                "tests/unit/test_connection/test_request.py::test_request_asset_url",
                "tests/unit/test_connection/test_request.py::test_request_query_params",
                "tests/unit/test_connection/test_request.py::test_custom_request_class",
                "tests/unit/test_connection/test_request.py::test_request_stream_then_body",
                "tests/unit/test_connection/test_request.py::test_request_raw_path",
                "tests/unit/test_connection/test_request.py::test_request_url",
                "tests/unit/test_connection/test_request.py::test_request_body_then_stream",
                "tests/unit/test_connection/test_request.py::test_request_without_setting_receive",
                "tests/unit/test_connection/test_request.py::test_request_body",
                "tests/unit/test_connection/test_request.py::test_request_send_push_promise_without_push_extension_raises",
                "tests/unit/test_connection/test_request.py::test_request_send_push_promise",
                "tests/unit/test_connection/test_request.py::test_request_accept_header",
                "tests/unit/test_connection/test_request.py::test_request_send_push_promise_without_setting_send",
                "tests/unit/test_connection/test_websocket.py::test_route_handler_property",
                "tests/unit/test_connection/test_websocket.py::test_accept_set_headers[headers0]",
                "tests/unit/test_connection/test_websocket.py::test_websocket_send_receive_json[text]",
                "tests/unit/test_connection/test_websocket.py::test_accept_set_headers[headers2]",
                "tests/unit/test_connection/test_request.py::test_request_form_urlencoded",
                "tests/unit/test_connection/test_request.py::test_chunked_encoding",
                "tests/unit/test_connection/test_request.py::test_request_cookies",
                "tests/unit/test_connection/test_request.py::test_request_headers",
                "tests/unit/test_connection/test_request.py::test_request_stream",
                "tests/unit/test_connection/test_websocket.py::test_websocket_send_receive_json[binary]",
                "tests/unit/test_connection/test_websocket.py::test_accept_set_headers[headers1]",
                "tests/unit/test_connection/test_request.py::test_request_send_push_promise_without_push_extension",
                "tests/unit/test_connection/test_websocket.py::test_custom_request_class",
                "tests/unit/test_connection/test_websocket.py::test_duplicate_disconnect",
                "tests/unit/test_connection/test_websocket.py::test_receive_json_before_accept",
                "tests/unit/test_connection/test_websocket.py::test_websocket_exception",
                "tests/unit/test_contrib/test_opentelemetry.py::test_open_telemetry_middleware_with_websocket_route",
                "tests/unit/test_contrib/test_opentelemetry.py::test_open_telemetry_middleware_with_http_route",
                "tests/unit/test_connection/test_websocket.py::test_receive_bytes_before_accept",
                "tests/unit/test_contrib/test_attrs/test_inject_attrs_class.py::test_inject_attrs_class",
                "tests/unit/test_connection/test_websocket.py::test_receive_text_before_accept",
                "tests/unit/test_contrib/test_attrs/test_signature.py::test_parse_attrs_data_in_signature",
                "tests/unit/test_contrib/test_htmx/test_htmx_request.py::test_health_check",
                "tests/unit/test_contrib/test_htmx/test_htmx_request.py::test_bool_true",
                "tests/unit/test_contrib/test_htmx/test_htmx_request.py::test_boosted_default",
                "tests/unit/test_contrib/test_htmx/test_htmx_request.py::test_bool_default",
                "tests/unit/test_contrib/test_prometheus.py::test_prometheus_middleware_configurations",
                "tests/unit/test_contrib/test_htmx/test_htmx_request.py::test_boosted_set",
                "tests/unit/test_contrib/test_htmx/test_htmx_request.py::test_bool_false",
                "tests/unit/test_contrib/test_htmx/test_htmx_response.py::test_replace_url_false_response",
                "tests/unit/test_contrib/test_htmx/test_htmx_response.py::test_client_refresh_response",
                "tests/unit/test_contrib/test_htmx/test_htmx_response.py::test_push_url_response",
                "tests/unit/test_contrib/test_htmx/test_htmx_response.py::test_client_redirect_response",
                "tests/unit/test_contrib/test_htmx/test_htmx_response.py::test_hx_stop_polling_response",
                "tests/unit/test_contrib/test_pydantic/test_integration.py::test_pydantic_v2_validation_error_raises_400[None]",
                "tests/unit/test_contrib/test_pydantic/test_dto.py::test_schema_required_fields_with_pydantic_dto[experimental_backend-v2]",
                "tests/unit/test_contrib/test_pydantic/test_integration.py::test_default_error_handling",
                "tests/unit/test_contrib/test_pydantic/test_inject_pydantic.py::test_inject_pydantic_model[BaseModel1]",
                "tests/unit/test_contrib/test_htmx/test_htmx_response.py::test_push_url_false_response",
                "tests/unit/test_contrib/test_pydantic/test_dto.py::test_schema_required_fields_with_pydantic_dto[default_backend-v1]",
                "tests/unit/test_contrib/test_pydantic/test_dto.py::test_schema_required_fields_with_pydantic_dto[experimental_backend-v1]",
                "tests/unit/test_contrib/test_pydantic/test_integration.py::test_signature_model_invalid_input[v1]",
                "tests/unit/test_contrib/test_pydantic/test_inject_pydantic.py::test_inject_pydantic_model[BaseModel0]",
                "tests/unit/test_contrib/test_pydantic/test_integration.py::test_pydantic_v2_validation_error_raises_400[meta1]",
                "tests/unit/test_contrib/test_pydantic/test_dto.py::test_schema_required_fields_with_pydantic_dto[default_backend-v2]",
                "tests/unit/test_contrib/test_pydantic/test_integration.py::test_pydantic_v1_validation_error_raises_400[None]",
                "tests/unit/test_contrib/test_pydantic/test_integration.py::test_pydantic_v1_validation_error_raises_400[meta1]",
                "tests/unit/test_contrib/test_pydantic/test_integration.py::test_default_error_handling_v1",
                "tests/unit/test_contrib/test_pydantic/test_integration.py::test_signature_model_invalid_input[v2]",
                "tests/unit/test_contrib/test_prometheus.py::test_prometheus_exporter_metrics_with_http",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_schema_by_alias[v1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_schema_generation_v2[True]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_schema_generation_v1[True]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_schema_by_alias_plugin_override[v1]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_schema_generation_v1[False]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_schema_generation_v2[False]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_schema_by_alias[v2]",
                "tests/unit/test_contrib/test_pydantic/test_openapi.py::test_schema_by_alias_plugin_override[v2]",
                "tests/unit/test_datastructures/test_upload_file.py::test_cleanup_is_being_performed",
                "tests/unit/test_dto/test_integration.py::test_dto_defined_on_handler[experimental_backend]",
                "tests/unit/test_dto/test_integration.py::test_dto_defined_on_controller[experimental_backend]",
                "tests/unit/test_dto/test_integration.py::test_dto_defined_on_controller[default_backend]",
                "tests/unit/test_dto/test_integration.py::test_dto_defined_on_router[default_backend]",
                "tests/unit/test_dto/test_integration.py::test_dto_defined_on_router[experimental_backend]",
                "tests/unit/test_dto/test_integration.py::test_dto_defined_on_app[experimental_backend]",
                "tests/unit/test_dto/test_integration.py::test_dto_and_return_dto[default_backend]",
                "tests/unit/test_dto/test_integration.py::test_dto_defined_on_app[default_backend]",
                "tests/unit/test_dto/test_integration.py::test_dto_and_return_dto[experimental_backend]",
                "tests/unit/test_dto/test_integration.py::test_set_dto_none_disables_inherited_dto[experimental_backend]",
                "tests/unit/test_dto/test_integration.py::test_set_dto_none_disables_inherited_dto[default_backend]",
                "tests/unit/test_dto/test_integration.py::test_dto_defined_on_handler[default_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_url_encoded_form_data[default_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_fields_alias_generator[experimental_backend-camel-instance3-tested_fields3-data3]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_multipart_encoded_form_data[default_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_url_encoded_form_data[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_renamed_field_nested[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_renamed_field_nested[default_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_renamed_field[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_fields_alias_generator[experimental_backend-upper-instance0-tested_fields0-data0]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_multipart_encoded_form_data[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_fields_alias_generator[experimental_backend-<lambda>-instance2-tested_fields2-data2]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_fields_alias_generator[experimental_backend-lower-instance1-tested_fields1-data1]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_fields_alias_generator[experimental_backend-camel-instance5-tested_fields5-data5]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_renamed_field[default_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_fields_alias_generator[experimental_backend-kebab-instance6-tested_fields6-data6]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_fields_alias_generator[default_backend-lower-instance1-tested_fields1-data1]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_fields_alias_generator[default_backend-pascal-instance4-tested_fields4-data4]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_fields_alias_generator[default_backend-camel-instance5-tested_fields5-data5]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_fields_alias_generator[default_backend-camel-instance3-tested_fields3-data3]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_fields_alias_generator[default_backend-<lambda>-instance2-tested_fields2-data2]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_fields_alias_generator[experimental_backend-pascal-instance4-tested_fields4-data4]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_fields_alias_generator[default_backend-kebab-instance6-tested_fields6-data6]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_dto_data_injection[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_dto_data_injection[default_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_fields_alias_generator[default_backend-upper-instance0-tested_fields0-data0]",
                "tests/unit/test_handlers/test_asgi_handlers/test_handle_asgi.py::test_handle_asgi",
                "tests/unit/test_handlers/test_asgi_handlers/test_handle_asgi.py::test_asgi_signature_namespace",
                "tests/unit/test_handlers/test_asgi_handlers/test_handle_asgi_with_future_annotations.py::test_handle_asgi",
                "tests/unit/test_handlers/test_http_handlers/test_head.py::test_head_decorator",
                "tests/unit/test_handlers/test_http_handlers/test_delete.py::test_handler_return_none_and_204_status_response_empty",
                "tests/unit/test_handlers/test_http_handlers/test_sync.py::test_sync_to_thread[False]",
                "tests/unit/test_handlers/test_http_handlers/test_signature_namespace.py::test_websocket_signature_namespace[GET-get]",
                "tests/unit/test_handlers/test_http_handlers/test_signature_namespace.py::test_websocket_signature_namespace[PUT-put]",
                "tests/unit/test_handlers/test_http_handlers/test_signature_namespace.py::test_websocket_signature_namespace[DELETE-delete]",
                "tests/unit/test_handlers/test_http_handlers/test_sync.py::test_sync_to_thread[True]",
                "tests/unit/test_handlers/test_http_handlers/test_signature_namespace.py::test_websocket_signature_namespace[POST-post]",
                "tests/unit/test_handlers/test_http_handlers/test_signature_namespace.py::test_websocket_signature_namespace[PATCH-patch]",
                "tests/unit/test_handlers/test_websocket_handlers/test_handle_websocket_with_future_annotations.py::test_handle_websocket",
                "tests/unit/test_handlers/test_websocket_handlers/test_handle_websocket.py::test_websocket_signature_namespace",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers[postgres:asyncpg-None-text]@postgres",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_listener_receive_bytes[text]",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_listener_return_bytes[text]",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_listener_receive_bytes[binary]",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_listener_return_bytes[binary]",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_listener_receive_string[text]",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_listener_receive_with_dto[text]",
                "tests/unit/test_handlers/test_websocket_handlers/test_kwarg_handling.py::test_handle_websocket_params_parsing",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_basic_listener[async_listener_callable]",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_basic_listener[sync_listener_callable]",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_listener_receive_string[binary]",
                "tests/unit/test_handlers/test_websocket_handlers/test_handle_websocket.py::test_handle_websocket",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_listener_receive_json[binary]",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_basic_listener[listener_class]",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_listener_receive_json[text]",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_listener_receive_with_dto[binary]",
                "tests/unit/test_kwargs/test_cookie_params.py::test_cookie_params[int-param_dict1-param1-200]",
                "tests/unit/test_kwargs/test_cookie_params.py::test_cookie_params[int-param_dict2-param2-400]",
                "tests/unit/test_kwargs/test_cookie_params.py::test_cookie_params[int-param_dict3-param3-400]",
                "tests/unit/test_kwargs/test_dependency_batches.py::test_dependency_batch_with_exception[exception0-500-Exception Group Traceback]",
                "tests/unit/test_kwargs/test_cookie_params.py::test_cookie_params[t_type0-param_dict0-param0-200]",
                "tests/unit/test_kwargs/test_dependency_batches.py::test_dependency_batch_with_exception[exception1-422-{\"status_code\":422,\"detail\":\"http_exception\"}]",
                "tests/unit/test_kwargs/test_dependency_batches.py::test_dependency_batch_with_exception[exception2-400-{\"status_code\":400,\"detail\":\"validation_exception\"}]",
                "tests/unit/test_kwargs/test_generator_dependencies.py::test_generator_dependency_websocket[async_generator_dependency]",
                "tests/unit/test_kwargs/test_generator_dependencies.py::test_generator_dependency[async_generator_dependency-False]",
                "tests/unit/test_kwargs/test_generator_dependencies.py::test_generator_dependency_exception_during_cleanup_debug_false[async_generator_dependency]",
                "tests/unit/test_kwargs/test_header_params.py::test_header_params[str-param_dict0-param0-False]",
                "tests/unit/test_kwargs/test_generator_dependencies.py::test_generator_dependency_nested[generator_dependency]",
                "tests/unit/test_kwargs/test_generator_dependencies.py::test_generator_dependency_handle_exception_debug_false[async_generator_dependency]",
                "tests/unit/test_kwargs/test_cookie_params.py::test_cookie_params[t_type4-param_dict4-param4-200]",
                "tests/unit/test_kwargs/test_defaults.py::test_params_default",
                "tests/unit/test_kwargs/test_generator_dependencies.py::test_generator_dependency_exception_during_cleanup_debug_false[generator_dependency]",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers[postgres:asyncpg-None-binary]@postgres",
                "tests/unit/test_kwargs/test_generator_dependencies.py::test_generator_dependency[async_generator_dependency-True]",
                "tests/unit/test_kwargs/test_header_params.py::test_header_params[t_type7-param_dict7-param7-False]",
                "tests/unit/test_kwargs/test_header_params.py::test_header_params[t_type3-param_dict3-param3-False]",
                "tests/unit/test_kwargs/test_json_data.py::test_no_body_with_default",
                "tests/unit/test_kwargs/test_generator_dependencies.py::test_generator_dependency_nested[async_generator_dependency]",
                "tests/unit/test_kwargs/test_generator_dependencies.py::test_generator_dependency_nested_error_during_cleanup[async_generator_dependency]",
                "tests/unit/test_kwargs/test_generator_dependencies.py::test_generator_dependency_websocket[generator_dependency]",
                "tests/unit/test_kwargs/test_generator_dependencies.py::test_generator_dependency[generator_dependency-True]",
                "tests/unit/test_kwargs/test_generator_dependencies.py::test_generator_dependency_handle_exception_debug_false[generator_dependency]",
                "tests/unit/test_kwargs/test_layered_params.py::test_layered_parameters_defaults_and_overrides",
                "tests/unit/test_kwargs/test_header_params.py::test_header_params[int-param_dict4-param4-False]",
                "tests/unit/test_kwargs/test_generator_dependencies.py::test_generator_dependency[generator_dependency-False]",
                "tests/unit/test_kwargs/test_header_params.py::test_header_params[str-param_dict1-param1-True]",
                "tests/unit/test_kwargs/test_header_params.py::test_header_params[int-param_dict6-param6-True]",
                "tests/unit/test_kwargs/test_json_data.py::test_request_body_json",
                "tests/unit/test_kwargs/test_json_data.py::test_empty_dict_allowed",
                "tests/unit/test_kwargs/test_layered_params.py::test_layered_parameters_validation[router3-header]",
                "tests/unit/test_kwargs/test_generator_dependencies.py::test_generator_dependency_nested_error_during_cleanup[generator_dependency]",
                "tests/unit/test_kwargs/test_header_params.py::test_header_params[int-param_dict5-param5-True]",
                "tests/unit/test_kwargs/test_layered_params.py::test_layered_parameters_injected_correctly",
                "tests/unit/test_kwargs/test_layered_params.py::test_layered_parameters_validation[app2-query]",
                "tests/unit/test_kwargs/test_layered_params.py::test_layered_parameters_validation[router1-query]",
                "tests/unit/test_kwargs/test_layered_params.py::test_layered_parameters_validation[controller3-query]",
                "tests/unit/test_kwargs/test_layered_params.py::test_layered_parameters_validation[controller1-query]",
                "tests/unit/test_kwargs/test_multipart_data.py::test_request_body_multi_part[FormData]",
                "tests/unit/test_kwargs/test_msgpack_data.py::test_request_body_msgpack",
                "tests/unit/test_kwargs/test_header_params.py::test_header_params[str-param_dict2-param2-True]",
                "tests/unit/test_kwargs/test_multipart_data.py::test_multipart_request_multiple_files_with_headers",
                "tests/unit/test_kwargs/test_msgpack_data.py::test_no_body_with_default",
                "tests/unit/test_kwargs/test_multipart_data.py::test_multipart_request_files_with_content_type",
                "tests/unit/test_kwargs/test_multipart_data.py::test_request_body_multi_part[t_type1]",
                "tests/unit/test_kwargs/test_multipart_data.py::test_request_body_multi_part[UploadFile]",
                "tests/unit/test_kwargs/test_multipart_data.py::test_multipart_request_multiple_files",
                "tests/unit/test_kwargs/test_layered_params.py::test_layered_parameters_validation[app4-cookie]",
                "tests/unit/test_kwargs/test_multipart_data.py::test_multipart_request_files",
                "tests/unit/test_kwargs/test_multipart_data.py::test_request_body_multi_part[t_type2]",
                "tests/unit/test_kwargs/test_path_params.py::test_path_params[params_dict3-True]",
                "tests/unit/test_kwargs/test_path_params.py::test_path_params[params_dict2-True]",
                "tests/unit/test_kwargs/test_multipart_data.py::test_request_body_multi_part_mixed_field_content_types",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers[postgres:asyncpg-/ws-text]@postgres",
                "tests/unit/test_kwargs/test_path_params.py::test_path_param_type_resolution[uuid-UUID-0fcb1054c56e4dd4a127f70a97d1fc21-expected_value3]",
                "tests/unit/test_kwargs/test_path_params.py::test_path_params[params_dict1-True]",
                "tests/unit/test_kwargs/test_path_params.py::test_path_param_type_resolution[timedelta-timedelta-P1D-expected_value10]",
                "tests/unit/test_kwargs/test_path_params.py::test_path_params[params_dict4-False]",
                "tests/unit/test_kwargs/test_query_params.py::test_query_params[params_dict0-False]",
                "tests/unit/test_kwargs/test_path_params.py::test_path_param_type_resolution[path-Path-1/2/3/4/some-file.txt-expected_value13]",
                "tests/unit/test_kwargs/test_query_params.py::test_query_params[params_dict3-True]",
                "tests/unit/test_kwargs/test_query_params.py::test_query_params[params_dict2-True]",
                "tests/unit/test_kwargs/test_query_params.py::test_query_params[params_dict4-True]",
                "tests/unit/test_kwargs/test_path_params.py::test_path_param_type_resolution[path-Path-/1/2/3/4/some-file.txt-expected_value12]",
                "tests/unit/test_kwargs/test_path_params.py::test_path_param_type_resolution[uuid-UUID-542226d1-7199-41a0-9cba-aaa6d85932a3-expected_value4]",
                "tests/unit/test_kwargs/test_path_params.py::test_optional_path_parameter",
                "tests/unit/test_kwargs/test_path_params.py::test_path_param_type_resolution[decimal-Decimal-1.00001-expected_value5]",
                "tests/unit/test_kwargs/test_path_params.py::test_path_params[params_dict0-False]",
                "tests/unit/test_kwargs/test_path_params.py::test_path_param_type_resolution[str-str-abc-abc]",
                "tests/unit/test_kwargs/test_query_params.py::test_query_param_arrays[expected_type0-provided_value0-None-200]",
                "tests/unit/test_kwargs/test_query_params.py::test_query_params[params_dict1-False]",
                "tests/unit/test_kwargs/test_query_params.py::test_query_params[params_dict6-False]",
                "tests/unit/test_kwargs/test_path_params.py::test_path_param_type_resolution[int-int-1-1]",
                "tests/unit/test_kwargs/test_path_params.py::test_differently_named_path_params_on_same_level",
                "tests/unit/test_kwargs/test_path_params.py::test_path_param_type_resolution[datetime-datetime-2023-07-15T15:45:34.073314-expected_value8]",
                "tests/unit/test_kwargs/test_query_params.py::test_query_params[params_dict5-True]",
                "tests/unit/test_kwargs/test_query_params.py::test_query_param_dependency_with_alias",
                "tests/unit/test_kwargs/test_query_params.py::test_query_params[params_dict7-False]",
                "tests/unit/test_kwargs/test_path_params.py::test_path_param_type_resolution[timedelta-timedelta-86400.0-expected_value9]",
                "tests/unit/test_kwargs/test_path_params.py::test_path_param_type_resolution[date-date-2023-07-15-expected_value6]",
                "tests/unit/test_kwargs/test_path_params.py::test_path_param_type_resolution[float-float-1.01-1.01]",
                "tests/unit/test_kwargs/test_query_params.py::test_query_kwarg",
                "tests/unit/test_kwargs/test_query_params.py::test_query_parsing_of_escaped_values[values2]",
                "tests/unit/test_kwargs/test_reserved_kwargs_injection.py::test_application_immutable_state_injection",
                "tests/unit/test_kwargs/test_query_params.py::test_query_param_arrays[expected_type1-provided_value1-None-200]",
                "tests/unit/test_kwargs/test_path_params.py::test_path_param_type_resolution[time-time-01:02:03-expected_value7]",
                "tests/unit/test_kwargs/test_query_params.py::test_query_parsing_of_escaped_values[values3]",
                "tests/unit/test_kwargs/test_query_params.py::test_query_parsing_of_escaped_values[values1]",
                "tests/unit/test_kwargs/test_query_params.py::test_query_parsing_of_escaped_values[values0]",
                "tests/unit/test_kwargs/test_query_params.py::test_query_params[params_dict8-False]",
                "tests/unit/test_kwargs/test_path_params.py::test_path_param_type_resolution[timedelta-timedelta-PT1H1S-expected_value11]",
                "tests/unit/test_kwargs/test_reserved_kwargs_injection.py::test_application_state_injection[CustomState]",
                "tests/unit/test_kwargs/test_reserved_kwargs_injection.py::test_application_state_injection[State]",
                "tests/unit/test_kwargs/test_url_encoded_data.py::test_request_body_url_encoded",
                "tests/unit/test_kwargs/test_url_encoded_data.py::test_optional_request_body_url_encoded",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers[postgres:asyncpg-/ws-binary]@postgres",
                "tests/unit/test_logging/test_logging_config.py::test_connection_logger[handlers0-QueueListenerHandler]",
                "tests/unit/test_logging/test_logging_config.py::test_connection_logger[handlers1-QueueListenerHandler]",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers[postgres:psycopg-None-text]@postgres",
                "tests/unit/test_middleware/test_base_authentication_middleware.py::test_authentication_middleware_http_routes",
                "tests/unit/test_middleware/test_base_authentication_middleware.py::test_authentication_middleware_websocket_routes",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers[postgres:psycopg-None-binary]@postgres",
                "tests/unit/test_middleware/test_base_authentication_middleware.py::test_authentication_middleware_not_installed_raises_for_user_scope_http",
                "tests/unit/test_middleware/test_allowed_hosts_middleware.py::test_middleware_allowed_hosts",
                "tests/unit/test_middleware/test_base_authentication_middleware.py::test_authentication_middleware_not_installed_raises_for_user_scope_websocket",
                "tests/unit/test_middleware/test_base_authentication_middleware.py::test_authentication_middleware_not_installed_raises_for_auth_scope_http",
                "tests/unit/test_middleware/test_base_middleware.py::test_exclude_by_pattern",
                "tests/unit/test_middleware/test_compression_middleware.py::test_compression_skips_small_responses[brotli-br]",
                "tests/unit/test_middleware/test_compression_middleware.py::test_compression_disabled_for_unsupported_client",
                "tests/unit/test_middleware/test_base_middleware.py::test_custom_middleware",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers[postgres:psycopg-/ws-text]@postgres",
                "tests/unit/test_middleware/test_base_middleware.py::test_raises_exception",
                "tests/unit/test_middleware/test_compression_middleware.py::test_brotli_gzip_fallback_disabled",
                "tests/unit/test_middleware/test_compression_middleware.py::test_compression_skips_small_responses[gzip-gzip]",
                "tests/unit/test_middleware/test_base_middleware.py::test_exclude_by_pattern_list",
                "tests/unit/test_middleware/test_compression_middleware.py::test_compression_works_for_streaming_response[brotli-br]",
                "tests/unit/test_middleware/test_compression_middleware.py::test_regular_compressed_response[gzip-gzip]",
                "tests/unit/test_middleware/test_compression_middleware.py::test_compression_works_for_streaming_response[gzip-gzip]",
                "tests/unit/test_middleware/test_base_authentication_middleware.py::test_authentication_middleware_not_installed_raises_for_auth_scope_websocket",
                "tests/unit/test_middleware/test_compression_middleware.py::test_regular_compressed_response[brotli-br]",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers[postgres:psycopg-/ws-binary]@postgres",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-First-Header-True-*-None]",
                "tests/unit/test_middleware/test_compression_middleware.py::test_brotli_with_gzip_fallback_enabled",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-First-Header-True-https://moishe.zuchmir.com-http://www.example.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-First-Header-True-https://moishe.zuchmir.com-None]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-First-Header-True-http://www.example.com-http://www.example.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-First-Header-True-*-http://www.example.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-First-Header-True-*-https://moishe.zuchmir.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-First-Header-True-http://www.example.com-None]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-First-Header-True-https://moishe.zuchmir.com-https://moishe.zuchmir.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-First-Header-False-*-https://moishe.zuchmir.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[SomeOtherHeader-True-*-None]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[SomeOtherHeader-True-http://www.example.com-None]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-First-Header-True-http://www.example.com-https://moishe.zuchmir.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-First-Header-False-*-None]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[SomeOtherHeader-False-http://www.example.com-https://moishe.zuchmir.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[SomeOtherHeader-True-https://moishe.zuchmir.com-http://www.example.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[SomeOtherHeader-False-http://www.example.com-None]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[SomeOtherHeader-True-https://moishe.zuchmir.com-https://moishe.zuchmir.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[SomeOtherHeader-True-http://www.example.com-https://moishe.zuchmir.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-First-Header-False-*-http://www.example.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-First-Header-False-https://moishe.zuchmir.com-None]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[SomeOtherHeader-False-*-https://moishe.zuchmir.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[SomeOtherHeader-True-http://www.example.com-http://www.example.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-First-Header-False-http://www.example.com-https://moishe.zuchmir.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-First-Header-False-http://www.example.com-http://www.example.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-First-Header-False-http://www.example.com-None]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[SomeOtherHeader-False-*-http://www.example.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[SomeOtherHeader-False-http://www.example.com-http://www.example.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-First-Header-False-https://moishe.zuchmir.com-https://moishe.zuchmir.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[SomeOtherHeader-True-*-http://www.example.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-Second-Header-True-*-None]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[SomeOtherHeader-False-https://moishe.zuchmir.com-None]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[SomeOtherHeader-False-https://moishe.zuchmir.com-http://www.example.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-First-Header-False-https://moishe.zuchmir.com-http://www.example.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[SomeOtherHeader-True-*-https://moishe.zuchmir.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[SomeOtherHeader-False-https://moishe.zuchmir.com-https://moishe.zuchmir.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[SomeOtherHeader-False-*-None]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-Second-Header-True-http://www.example.com-http://www.example.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-Second-Header-True-http://www.example.com-https://moishe.zuchmir.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[SomeOtherHeader-True-https://moishe.zuchmir.com-None]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-Second-Header-True-https://moishe.zuchmir.com-http://www.example.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-Second-Header-True-https://moishe.zuchmir.com-https://moishe.zuchmir.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-Second-Header-False-*-http://www.example.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-Second-Header-True-*-http://www.example.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-Second-Header-False-*-https://moishe.zuchmir.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-Second-Header-True-https://moishe.zuchmir.com-None]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-Second-Header-False-*-None]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-Second-Header-True-*-https://moishe.zuchmir.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-Second-Header-False-http://www.example.com-http://www.example.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-Second-Header-True-http://www.example.com-None]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-Second-Header-False-http://www.example.com-None]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-Second-Header-False-https://moishe.zuchmir.com-None]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-Second-Header-False-http://www.example.com-https://moishe.zuchmir.com]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_applied_on_exception_response_if_origin_is_present[http://www.example.com-True]",
                "tests/unit/test_middleware/test_csrf_middleware.py::test_unsafe_method_fails_without_csrf_header[POST]",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-Second-Header-False-https://moishe.zuchmir.com-https://moishe.zuchmir.com]",
                "tests/unit/test_middleware/test_csrf_middleware.py::test_csrf_successful_flow",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_applied_on_exception_response_if_origin_is_present[None-False]",
                "tests/unit/test_middleware/test_csrf_middleware.py::test_invalid_csrf_token",
                "tests/unit/test_middleware/test_cors_middleware.py::test_cors_simple_response[X-Second-Header-False-https://moishe.zuchmir.com-http://www.example.com]",
                "tests/unit/test_middleware/test_csrf_middleware.py::test_unsafe_method_fails_without_csrf_header[DELETE]",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_exception_handler_default_logging[True-logging_config0-True]",
                "tests/unit/test_middleware/test_csrf_middleware.py::test_unsafe_method_fails_without_csrf_header[PUT]",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_exception_handler_middleware_calls_app_level_after_exception_hook",
                "tests/unit/test_middleware/test_csrf_middleware.py::test_unsafe_method_fails_without_csrf_header[PATCH]",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_exception_handler_struct_logging[True-logging_config4-False]",
                "tests/unit/test_channels/test_plugin.py::test_ws_route_handlers_receive_arbitrary_message[postgres:asyncpg]@postgres",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_exception_handler_struct_logging[True-logging_config0-True]",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_exception_handler_struct_logging[True-None-False]",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_exception_handler_default_logging[False-logging_config5-False]",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_exception_handler_struct_logging[True-logging_config2-True]",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_exception_handler_struct_logging[False-logging_config1-False]",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_exception_handler_default_logging[False-logging_config3-True]",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_exception_handler_default_logging[False-logging_config1-False]",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_exception_handler_default_logging[True-logging_config4-False]",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_exception_handler_struct_logging[False-logging_config5-False]",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_exception_handler_default_logging[False-None-False]",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_exception_handler_struct_logging[False-logging_config3-True]",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_traceback_truncate_struct_logging",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_exception_handler_default_logging[True-None-False]",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_exception_handler_default_logging[True-logging_config2-True]",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_traceback_truncate_default_logging",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_exception_handler_struct_logging[False-None-False]",
                "tests/unit/test_middleware/test_rate_limit_middleware.py::test_rate_limiting[minute]",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_pdb_on_exception",
                "tests/unit/test_channels/test_plugin.py::test_ws_route_handlers_receive_arbitrary_message[postgres:psycopg]@postgres",
                "tests/unit/test_middleware/test_rate_limit_middleware.py::test_rate_limiting[day]",
                "tests/unit/test_middleware/test_middleware_handling.py::test_middleware_call_order",
                "tests/unit/test_middleware/test_middleware_handling.py::test_request_body_logging_middleware",
                "tests/unit/test_middleware/test_exception_handler_middleware.py::test_get_debug_from_scope",
                "tests/unit/test_middleware/test_logging_middleware.py::test_logging_middleware_regular_logger",
                "tests/unit/test_middleware/test_logging_middleware.py::test_logging_middleware_struct_logger",
                "tests/unit/test_middleware/test_rate_limit_middleware.py::test_rate_limiting[hour]",
                "tests/unit/test_middleware/test_session/test_client_side_backend.py::test_session_cookie_name_matching",
                "tests/unit/test_middleware/test_rate_limit_middleware.py::test_non_default_store",
                "tests/unit/test_middleware/test_rate_limit_middleware.py::test_set_store_name",
                "tests/unit/test_middleware/test_rate_limit_middleware.py::test_reset",
                "tests/unit/test_middleware/test_session/test_middleware.py::test_integration[server-side]",
                "tests/unit/test_middleware/test_rate_limit_middleware.py::test_rate_limiting[second]",
                "tests/unit/test_middleware/test_session/test_integration.py::test_options_request_with_session_auth",
                "tests/unit/test_middleware/test_session/test_middleware.py::test_session_middleware_not_installed_raises",
                "tests/unit/test_middleware/test_session/test_server_side_backend.py::test_set_store_name",
                "tests/unit/test_middleware/test_session/test_middleware.py::test_integration[cookie]",
                "tests/unit/test_middleware/test_session/test_client_side_backend.py::test_set_session_cookies",
                "tests/unit/test_middleware/test_session/test_server_side_backend.py::test_non_default_store",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers[redis:pubsub-None-text]@redis",
                "tests/unit/test_openapi/test_controller.py::test_default_swagger_ui_cdn_urls",
                "tests/unit/test_openapi/test_controller.py::test_default_redoc_cdn_urls",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers_arbitrary_channels_allowed[postgres:asyncpg]@postgres",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers[redis:pubsub-None-binary]@redis",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers[redis:pubsub-/ws-text]@redis",
                "tests/unit/test_openapi/test_integration.py::test_openapi[/schema/openapi.yaml-True]",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers[redis:pubsub-/ws-binary]@redis",
                "tests/unit/test_openapi/test_integration.py::test_openapi[/schema/openapi.yml-False]",
                "tests/unit/test_openapi/test_integration.py::test_openapi[/schema/openapi.yml-True]",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers[redis:stream-None-text]@redis",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers[redis:stream-None-binary]@redis",
                "tests/unit/test_openapi/test_integration.py::test_openapi[/schema/openapi.yaml-False]",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers_arbitrary_channels_allowed[postgres:psycopg]@postgres",
                "tests/unit/test_openapi/test_parameters.py::test_parameter_examples",
                "tests/unit/test_plugins/test_base.py::test_plugin_on_app_init",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers[redis:stream-/ws-text]@redis",
                "tests/unit/test_response/test_base_response.py::test_response_headers",
                "tests/unit/test_response/test_base_response.py::test_response_headers_do_not_lowercase_values",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers[redis:stream-/ws-binary]@redis",
                "tests/unit/test_response/test_base_response.py::test_set_cookie[True]",
                "tests/unit/test_response/test_base_response.py::test_delete_cookie",
                "tests/unit/test_response/test_base_response.py::test_set_cookie[False]",
                "tests/unit/test_channels/test_plugin.py::test_ws_route_handlers_receive_arbitrary_message[redis:pubsub]@redis",
                "tests/unit/test_channels/test_plugin.py::test_ws_route_handlers_receive_arbitrary_message[redis:stream]@redis",
                "tests/unit/test_response/test_file_response.py::test_file_response_default_content_type[inline]",
                "tests/unit/test_response/test_file_response.py::test_large_files[2048]",
                "tests/unit/test_response/test_file_response.py::test_file_response_default_content_type[attachment]",
                "tests/unit/test_response/test_file_response.py::test_large_files[4096]",
                "tests/unit/test_response/test_file_response.py::test_filename[\\u6210\\u9f8d-%E6%88%90%E9%BE%8D]",
                "tests/unit/test_response/test_file_response.py::test_filename[Jacky Chen-Jacky%20Chen]",
                "tests/unit/test_response/test_file_response.py::test_file_response_last_modified",
                "tests/unit/test_response/test_file_response.py::test_file_response_infer_content_type[inline]",
                "tests/unit/test_response/test_file_response.py::test_file_response_infer_content_type[attachment]",
                "tests/unit/test_response/test_file_response.py::test_file_response_content_length",
                "tests/unit/test_response/test_file_response.py::test_large_files[40960]",
                "tests/unit/test_response/test_file_response.py::test_large_files[20480]",
                "tests/unit/test_response/test_file_response.py::test_large_files[1024]",
                "tests/unit/test_response/test_redirect_response.py::test_quoting_redirect_response",
                "tests/unit/test_response/test_redirect_response.py::test_redirect[301]",
                "tests/unit/test_response/test_redirect_response.py::test_redirect[None]",
                "tests/unit/test_response/test_response_cookies.py::test_response_cookie_documentation_only_not_rendering",
                "tests/unit/test_response/test_response_cookies.py::test_response_cookie_rendering",
                "tests/unit/test_response/test_redirect_response.py::test_redirect_dynamic_status_code[308-308]",
                "tests/unit/test_response/test_file_response.py::test_large_files[10240]",
                "tests/unit/test_response/test_redirect_response.py::test_redirect_response",
                "tests/unit/test_response/test_redirect_response.py::test_redirect_dynamic_status_code[303-303]",
                "tests/unit/test_response/test_response_headers.py::test_explicit_headers_documentation_only[cache_control-header0]",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers_arbitrary_channels_allowed[redis:pubsub]@redis",
                "tests/unit/test_response/test_response_cookies.py::test_response_cookie_documentation_only_not_producing_second_header",
                "tests/unit/test_response/test_redirect_response.py::test_redirect_dynamic_status_code[307-307]",
                "tests/unit/test_response/test_redirect_response.py::test_redirect[307]",
                "tests/unit/test_response/test_redirect_response.py::test_redirect_dynamic_status_code[302-302]",
                "tests/unit/test_response/test_redirect_response.py::test_redirect_response_content_length_header",
                "tests/unit/test_response/test_response_headers.py::test_explicit_response_headers[etag-app_header0-controller_header0-handler_header0]",
                "tests/unit/test_response/test_response_headers.py::test_response_headers_rendering",
                "tests/unit/test_response/test_response_headers.py::test_explicit_response_headers[cache_control-app_header1-controller_header1-handler_header1]",
                "tests/unit/test_response/test_response_cookies.py::test_response_cookie_is_always_set",
                "tests/unit/test_response/test_response_headers.py::test_explicit_headers_documentation_only[etag-header1]",
                "tests/unit/test_channels/test_plugin.py::test_create_ws_route_handlers_arbitrary_channels_allowed[redis:stream]@redis",
                "tests/unit/test_response/test_redirect_response.py::test_redirect_response_html_media_type",
                "tests/unit/test_response/test_redirect_response.py::test_redirect_dynamic_status_code[301-301]",
                "tests/unit/test_response/test_streaming_response.py::test_streaming_response_unknown_size",
                "tests/unit/test_response/test_streaming_response.py::test_streaming_response_custom_iterable",
                "tests/unit/test_security/test_security.py::test_abstract_security_config_registers_route_handlers",
                "tests/unit/test_response/test_streaming_response.py::test_streaming_response_known_size",
                "tests/unit/test_security/test_session_auth.py::test_authentication",
                "tests/unit/test_response/test_type_encoders.py::test_type_encoders_response_override",
                "tests/unit/test_security/test_jwt/test_auth.py::test_jwt_auth",
                "tests/unit/test_security/test_jwt/test_integration.py::test_options_request_with_jwt",
                "tests/unit/test_response/test_streaming_response.py::test_sync_streaming_response",
                "tests/unit/test_signature/test_parsing.py::test_signature_model_resolves_forward_ref_annotations",
                "tests/unit/test_signature/test_parsing.py::test_query_param_bool[true-True]",
                "tests/unit/test_signature/test_parsing.py::test_query_param_bool[0-False]",
                "tests/unit/test_signature/test_parsing.py::test_union_constraint_handling",
                "tests/unit/test_openapi/test_parameters.py::test_uuid_path_description_generation",
                "tests/unit/test_signature/test_parsing.py::test_parse_optional_sequence_from_connection_kwargs[?a=1&a=2&a=3-exp0]",
                "tests/unit/test_signature/test_parsing.py::test_parse_optional_sequence_from_connection_kwargs[-None]",
                "tests/unit/test_response/test_streaming_response.py::test_streaming_response_custom_iterator",
                "tests/unit/test_signature/test_parsing.py::test_collection_union_struct_fields[True]",
                "tests/unit/test_signature/test_parsing.py::test_collection_union_struct_fields[False]",
                "tests/unit/test_signature/test_parsing.py::test_query_param_bool[1-True]",
                "tests/unit/test_signature/test_validation.py::test_validation_failure_raises_400",
                "tests/unit/test_static_files/test_file_serving_resolution.py::test_default_static_files_config[config]",
                "tests/unit/test_response/test_streaming_response.py::test_streaming_response",
                "tests/unit/test_static_files/test_create_static_router.py::test_cache_control[cache_control1]",
                "tests/unit/test_static_files/test_create_static_router.py::test_cache_control[None]",
                "tests/unit/test_static_files/test_file_serving_resolution.py::test_static_files_configs_with_mixed_file_systems[file_system0-config]",
                "tests/unit/test_static_files/test_file_serving_resolution.py::test_default_static_files_config[handlers]",
                "tests/unit/test_static_files/test_file_serving_resolution.py::test_multiple_static_files_configs[handlers]",
                "tests/unit/test_signature/test_validation.py::test_client_backend_error_precedence_over_server_error",
                "tests/unit/test_static_files/test_file_serving_resolution.py::test_static_files_config_with_multiple_directories[file_system0-handlers]",
                "tests/unit/test_static_files/test_file_serving_resolution.py::test_static_files_configs_with_mixed_file_systems[file_system1-handlers]",
                "tests/unit/test_signature/test_validation.py::test_dependency_validation_failure_raises_500",
                "tests/unit/test_static_files/test_file_serving_resolution.py::test_static_files_config_with_multiple_directories[file_system0-config]",
                "tests/unit/test_signature/test_validation.py::test_invalid_input_attrs",
                "tests/unit/test_static_files/test_file_serving_resolution.py::test_static_files_config_with_multiple_directories[file_system1-config]",
                "tests/unit/test_static_files/test_file_serving_resolution.py::test_static_files_configs_with_mixed_file_systems[file_system1-config]",
                "tests/unit/test_static_files/test_file_serving_resolution.py::test_multiple_static_files_configs[config]",
                "tests/unit/test_static_files/test_file_serving_resolution.py::test_static_files_configs_with_mixed_file_systems[file_system0-handlers]",
                "tests/unit/test_static_files/test_file_serving_resolution.py::test_static_files_config_with_multiple_directories[file_system1-handlers]",
                "tests/unit/test_static_files/test_html_mode.py::test_staticfiles_is_html_mode[file_system1-config]",
                "tests/unit/test_static_files/test_html_mode.py::test_staticfiles_is_html_mode[file_system1-handlers]",
                "tests/unit/test_static_files/test_html_mode.py::test_staticfiles_is_html_mode[file_system0-config]",
                "tests/unit/test_static_files/test_html_mode.py::test_staticfiles_is_html_mode_serves_404_when_present[file_system0-handlers]",
                "tests/unit/test_static_files/test_html_mode.py::test_staticfiles_is_html_mode_raises_exception_when_no_404_html_is_present[file_system1-config]",
                "tests/unit/test_static_files/test_html_mode.py::test_staticfiles_is_html_mode_serves_404_when_present[file_system1-config]",
                "tests/unit/test_static_files/test_html_mode.py::test_staticfiles_is_html_mode_serves_404_when_present[file_system1-handlers]",
                "tests/unit/test_static_files/test_static_files_validation.py::test_runtime_validation_of_request_method_legacy_config[DELETE-405]",
                "tests/unit/test_signature/test_parsing.py::test_query_param_bool[false-False]",
                "tests/unit/test_static_files/test_html_mode.py::test_staticfiles_is_html_mode_raises_exception_when_no_404_html_is_present[file_system0-config]",
                "tests/unit/test_static_files/test_html_mode.py::test_staticfiles_is_html_mode_raises_exception_when_no_404_html_is_present[file_system0-handlers]",
                "tests/unit/test_static_files/test_static_files_validation.py::test_runtime_validation_of_request_method_create_handler[PATCH-405]",
                "tests/unit/test_static_files/test_static_files_validation.py::test_runtime_validation_of_request_method_create_handler[PUT-405]",
                "tests/unit/test_template/test_built_in.py::test_template[engine_test0]",
                "tests/unit/test_template/test_built_in.py::test_nested_tmp_pathectory[engine_test2]",
                "tests/unit/test_template/test_built_in.py::test_nested_tmp_pathectory[engine_test0]",
                "tests/unit/test_template/test_built_in.py::test_template[engine_test2]",
                "tests/unit/test_static_files/test_html_mode.py::test_staticfiles_is_html_mode_serves_404_when_present[file_system0-config]",
                "tests/unit/test_static_files/test_static_files_validation.py::test_runtime_validation_of_request_method_legacy_config[HEAD-200]",
                "tests/unit/test_template/test_built_in.py::test_raise_for_invalid_template_name[engine_test3]",
                "tests/unit/test_static_files/test_static_files_validation.py::test_runtime_validation_of_request_method_legacy_config[POST-405]",
                "tests/unit/test_static_files/test_static_files_validation.py::test_runtime_validation_of_request_method_create_handler[HEAD-200]",
                "tests/unit/test_template/test_built_in.py::test_raise_for_invalid_template_name[engine_test2]",
                "tests/unit/test_static_files/test_html_mode.py::test_staticfiles_is_html_mode_raises_exception_when_no_404_html_is_present[file_system1-handlers]",
                "tests/unit/test_static_files/test_static_files_validation.py::test_runtime_validation_of_request_method_create_handler[OPTIONS-204]",
                "tests/unit/test_template/test_built_in.py::test_nested_tmp_pathectory[engine_test4]",
                "tests/unit/test_static_files/test_static_files_validation.py::test_runtime_validation_of_request_method_create_handler[GET-200]",
                "tests/unit/test_static_files/test_static_files_validation.py::test_runtime_validation_of_request_method_legacy_config[PUT-405]",
                "tests/unit/test_template/test_built_in.py::test_nested_tmp_pathectory[engine_test3]",
                "tests/unit/test_static_files/test_static_files_validation.py::test_runtime_validation_of_request_method_legacy_config[OPTIONS-405]",
                "tests/unit/test_template/test_built_in.py::test_template[engine_test4]",
                "tests/unit/test_template/test_built_in.py::test_template[engine_test3]",
                "tests/unit/test_static_files/test_html_mode.py::test_staticfiles_is_html_mode[file_system0-handlers]",
                "tests/unit/test_template/test_built_in.py::test_raise_for_invalid_template_name[engine_test4]",
                "tests/unit/test_static_files/test_static_files_validation.py::test_runtime_validation_of_request_method_create_handler[POST-405]",
                "tests/unit/test_template/test_built_in.py::test_template[engine_test1]",
                "tests/unit/test_template/test_built_in.py::test_no_context[engine_test1]",
                "tests/unit/test_static_files/test_static_files_validation.py::test_runtime_validation_of_request_method_legacy_config[GET-200]",
                "tests/unit/test_template/test_built_in.py::test_raise_for_invalid_template_name[engine_test0]",
                "tests/unit/test_template/test_built_in.py::test_nested_tmp_pathectory[engine_test1]",
                "tests/unit/test_template/test_built_in.py::test_no_context[engine_test3]",
                "tests/unit/test_template/test_built_in.py::test_no_context[engine_test2]",
                "tests/unit/test_template/test_builtin_functions.py::test_jinja_url_for",
                "tests/unit/test_template/test_built_in.py::test_raise_for_invalid_template_name[engine_test1]",
                "tests/unit/test_template/test_built_in.py::test_no_context[engine_test0]",
                "tests/unit/test_static_files/test_static_files_validation.py::test_runtime_validation_of_request_method_legacy_config[PATCH-405]",
                "tests/unit/test_static_files/test_static_files_validation.py::test_runtime_validation_of_request_method_create_handler[DELETE-405]",
                "tests/unit/test_template/test_context.py::test_request_is_set_in_context[JinjaTemplateEngine-path: {{ request.scope[\"path\"] }}-path: /]",
                "tests/unit/test_template/test_context.py::test_request_is_set_in_context[MiniJinjaTemplateEngine-path: {{ request.scope[\"path\"] }}-path: &#x2f;]",
                "tests/unit/test_template/test_template.py::test_media_type[text/html]",
                "tests/unit/test_template/test_built_in.py::test_no_context[engine_test4]",
                "tests/unit/test_template/test_builtin_functions.py::test_jinja_url_for_static_asset",
                "tests/unit/test_template/test_csrf_token.py::test_csrf_token[JinjaTemplateEngine-{{csrf_token()}}]",
                "tests/unit/test_template/test_csrf_token.py::test_csrf_input[MakoTemplateEngine-${csrf_input}]",
                "tests/unit/test_template/test_template.py::test_handler_raise_for_no_template_engine",
                "tests/unit/test_template/test_csrf_token.py::test_csrf_token[MakoTemplateEngine-${csrf_token()}]",
                "tests/unit/test_template/test_template.py::test_media_type_inferred[.unknown-text/plain]",
                "tests/unit/test_template/test_context.py::test_request_is_set_in_context[MakoTemplateEngine-path: ${request.scope[\"path\"]}-path: /]",
                "tests/unit/test_template/test_csrf_token.py::test_csrf_input[JinjaTemplateEngine-{{csrf_input}}]",
                "tests/unit/test_template/test_template.py::test_template_scenarios[both-MiniJinjaTemplateEngine]",
                "tests/unit/test_template/test_template.py::test_template_scenarios[name_only-MakoTemplateEngine]",
                "tests/unit/test_template/test_template.py::test_template_scenarios[name_only-MiniJinjaTemplateEngine]",
                "tests/unit/test_template/test_template.py::test_media_type_inferred[.css-text/css]",
                "tests/unit/test_template/test_template.py::test_template_scenarios[none-JinjaTemplateEngine]",
                "tests/unit/test_template/test_template.py::test_template_scenarios[both-MakoTemplateEngine]",
                "tests/unit/test_template/test_template.py::test_media_type_inferred[.txt-text/plain]",
                "tests/unit/test_template/test_template.py::test_media_type_inferred[.html-text/html]",
                "tests/unit/test_template/test_template.py::test_media_type[text/arbitrary]",
                "tests/unit/test_template/test_template.py::test_media_type_inferred[.xml-application/xml]",
                "tests/unit/test_template/test_template.py::test_template_scenarios[both-JinjaTemplateEngine]",
                "tests/unit/test_template/test_template.py::test_media_type_inferred[.json-application/json]",
                "tests/unit/test_testing/test_lifespan_handler.py::test_wait_startup_invalid_event",
                "tests/unit/test_template/test_template.py::test_template_scenarios[name_only-JinjaTemplateEngine]",
                "tests/unit/test_template/test_template.py::test_media_type_inferred[-text/plain]",
                "tests/unit/test_template/test_template.py::test_before_request_handler_content_type",
                "tests/unit/test_template/test_template.py::test_template_scenarios[none-MakoTemplateEngine]",
                "tests/unit/test_testing/test_lifespan_handler.py::test_wait_shutdown_invalid_event",
                "tests/unit/test_template/test_template.py::test_template_scenarios[str_only-MiniJinjaTemplateEngine]",
                "tests/unit/test_template/test_template.py::test_template_scenarios[str_only-JinjaTemplateEngine]",
                "tests/unit/test_template/test_template.py::test_template_scenarios[none-MiniJinjaTemplateEngine]",
                "tests/unit/test_template/test_template.py::test_media_type[text/plain]",
                "tests/unit/test_template/test_template.py::test_template_scenarios[str_only-MakoTemplateEngine]",
                "tests/unit/test_template/test_template.py::test_media_type_inferred[.html.other-text/html]",
                "tests/unit/test_template/test_template.py::test_media_type_inferred[.xml.other-application/xml]",
                "tests/unit/test_template/test_csrf_token.py::test_csrf_input[MiniJinjaTemplateEngine-{{csrf_input}}]",
                "tests/unit/test_template/test_csrf_token.py::test_csrf_token[MiniJinjaTemplateEngine-{{csrf_token()}}]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive_json-False-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive_text-False-None]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive_text-False-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive_bytes-False-None]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive_json-False-None]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive-False-None]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive-False-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive_bytes-False-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive_json-True-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive-True-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive-False-None]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive_json-False-None]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive-False-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive_json-True-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive_bytes-False-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive_json-False-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive_text-True-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive_text-False-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive_text-True-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_accept_timeout[asyncio]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive_bytes-False-None]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive-True-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_accept_timeout[trio]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive_bytes-True-0.001]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[asyncio-receive_text-False-None]",
                "tests/unit/test_testing/test_test_client.py::test_websocket_test_session_block_timeout[trio-receive_bytes-True-0.001]"
            ]
        },
        "ground_truth_class_body": "class TestClient(Client, BaseTestClient, Generic[T]):  # type: ignore[misc]\n    lifespan_handler: LifeSpanHandler[Any]\n    exit_stack: ExitStack\n\n    def __init__(\n        self,\n        app: T,\n        base_url: str = \"http://testserver.local\",\n        raise_server_exceptions: bool = True,\n        root_path: str = \"\",\n        backend: AnyIOBackend = \"asyncio\",\n        backend_options: Mapping[str, Any] | None = None,\n        session_config: BaseBackendConfig | None = None,\n        timeout: float | None = None,\n        cookies: CookieTypes | None = None,\n    ) -> None:\n        \"\"\"A client implementation providing a context manager for testing applications.\n\n        Args:\n            app: The instance of :class:`Litestar <litestar.app.Litestar>` under test.\n            base_url: URL scheme and domain for test request paths, e.g. ``http://testserver``.\n            raise_server_exceptions: Flag for the underlying test client to raise server exceptions instead of\n                wrapping them in an HTTP response.\n            root_path: Path prefix for requests.\n            backend: The async backend to use, options are \"asyncio\" or \"trio\".\n            backend_options: ``anyio`` options.\n            session_config: Configuration for Session Middleware class to create raw session cookies for request to the\n                route handlers.\n            timeout: Request timeout\n            cookies: Cookies to set on the client.\n        \"\"\"\n        BaseTestClient.__init__(\n            self,\n            app=app,\n            base_url=base_url,\n            backend=backend,\n            backend_options=backend_options,\n            session_config=session_config,\n            cookies=cookies,\n        )\n\n        Client.__init__(\n            self,\n            base_url=base_url,\n            headers={\"user-agent\": \"testclient\"},\n            follow_redirects=True,\n            cookies=cookies,\n            transport=TestClientTransport(  # type: ignore[arg-type]\n                client=self,\n                raise_server_exceptions=raise_server_exceptions,\n                root_path=root_path,\n            ),\n            timeout=timeout,\n        )\n\n    def __enter__(self) -> TestClient[T]:\n        with ExitStack() as stack:\n            self.blocking_portal = portal = stack.enter_context(self.portal())\n            self.lifespan_handler = LifeSpanHandler(client=self)\n\n            @stack.callback\n            def reset_portal() -> None:\n                delattr(self, \"blocking_portal\")\n\n            @stack.callback\n            def wait_shutdown() -> None:\n                portal.call(self.lifespan_handler.wait_shutdown)\n\n            self.exit_stack = stack.pop_all()\n\n        return self\n\n    def __exit__(self, *args: Any) -> None:\n        self.exit_stack.close()\n\n    def request(\n        self,\n        method: str,\n        url: URLTypes,\n        *,\n        content: RequestContent | None = None,\n        data: RequestData | None = None,\n        files: RequestFiles | None = None,\n        json: Any | None = None,\n        params: QueryParamTypes | None = None,\n        headers: HeaderTypes | None = None,\n        cookies: CookieTypes | None = None,\n        auth: AuthTypes | UseClientDefault | None = USE_CLIENT_DEFAULT,\n        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,\n        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,\n        extensions: Mapping[str, Any] | None = None,\n    ) -> Response:\n        \"\"\"Sends a request.\n\n        Args:\n            method: An HTTP method.\n            url: URL or path for the request.\n            content: Request content.\n            data: Form encoded data.\n            files: Multipart files to send.\n            json: JSON data to send.\n            params: Query parameters.\n            headers: Request headers.\n            cookies: Request cookies.\n            auth: Auth headers.\n            follow_redirects: Whether to follow redirects.\n            timeout: Request timeout.\n            extensions: Dictionary of ASGI extensions.\n\n        Returns:\n            An HTTPX Response.\n        \"\"\"\n        return Client.request(\n            self,\n            url=self.base_url.join(url),\n            method=method.value if isinstance(method, HttpMethod) else method,\n            content=content,\n            data=data,\n            files=files,\n            json=json,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=follow_redirects,\n            timeout=timeout,\n            extensions=None if extensions is None else dict(extensions),\n        )\n\n    def get(\n        self,\n        url: URLTypes,\n        *,\n        params: QueryParamTypes | None = None,\n        headers: HeaderTypes | None = None,\n        cookies: CookieTypes | None = None,\n        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,\n        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,\n        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,\n        extensions: Mapping[str, Any] | None = None,\n    ) -> Response:\n        \"\"\"Sends a GET request.\n\n        Args:\n            url: URL or path for the request.\n            params: Query parameters.\n            headers: Request headers.\n            cookies: Request cookies.\n            auth: Auth headers.\n            follow_redirects: Whether to follow redirects.\n            timeout: Request timeout.\n            extensions: Dictionary of ASGI extensions.\n\n        Returns:\n            An HTTPX Response.\n        \"\"\"\n        return Client.get(\n            self,\n            url,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=follow_redirects,\n            timeout=timeout,\n            extensions=None if extensions is None else dict(extensions),\n        )\n\n    def options(\n        self,\n        url: URLTypes,\n        *,\n        params: QueryParamTypes | None = None,\n        headers: HeaderTypes | None = None,\n        cookies: CookieTypes | None = None,\n        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,\n        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,\n        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,\n        extensions: Mapping[str, Any] | None = None,\n    ) -> Response:\n        \"\"\"Sends an OPTIONS request.\n\n        Args:\n            url: URL or path for the request.\n            params: Query parameters.\n            headers: Request headers.\n            cookies: Request cookies.\n            auth: Auth headers.\n            follow_redirects: Whether to follow redirects.\n            timeout: Request timeout.\n            extensions: Dictionary of ASGI extensions.\n\n        Returns:\n            An HTTPX Response.\n        \"\"\"\n        return Client.options(\n            self,\n            url,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=follow_redirects,\n            timeout=timeout,\n            extensions=None if extensions is None else dict(extensions),\n        )\n\n    def head(\n        self,\n        url: URLTypes,\n        *,\n        params: QueryParamTypes | None = None,\n        headers: HeaderTypes | None = None,\n        cookies: CookieTypes | None = None,\n        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,\n        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,\n        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,\n        extensions: Mapping[str, Any] | None = None,\n    ) -> Response:\n        \"\"\"Sends a HEAD request.\n\n        Args:\n            url: URL or path for the request.\n            params: Query parameters.\n            headers: Request headers.\n            cookies: Request cookies.\n            auth: Auth headers.\n            follow_redirects: Whether to follow redirects.\n            timeout: Request timeout.\n            extensions: Dictionary of ASGI extensions.\n\n        Returns:\n            An HTTPX Response.\n        \"\"\"\n        return Client.head(\n            self,\n            url,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=follow_redirects,\n            timeout=timeout,\n            extensions=None if extensions is None else dict(extensions),\n        )\n\n    def post(\n        self,\n        url: URLTypes,\n        *,\n        content: RequestContent | None = None,\n        data: RequestData | None = None,\n        files: RequestFiles | None = None,\n        json: Any | None = None,\n        params: QueryParamTypes | None = None,\n        headers: HeaderTypes | None = None,\n        cookies: CookieTypes | None = None,\n        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,\n        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,\n        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,\n        extensions: Mapping[str, Any] | None = None,\n    ) -> Response:\n        \"\"\"Sends a POST request.\n\n        Args:\n            url: URL or path for the request.\n            content: Request content.\n            data: Form encoded data.\n            files: Multipart files to send.\n            json: JSON data to send.\n            params: Query parameters.\n            headers: Request headers.\n            cookies: Request cookies.\n            auth: Auth headers.\n            follow_redirects: Whether to follow redirects.\n            timeout: Request timeout.\n            extensions: Dictionary of ASGI extensions.\n\n        Returns:\n            An HTTPX Response.\n        \"\"\"\n        return Client.post(\n            self,\n            url,\n            content=content,\n            data=data,\n            files=files,\n            json=json,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=follow_redirects,\n            timeout=timeout,\n            extensions=None if extensions is None else dict(extensions),\n        )\n\n    def put(\n        self,\n        url: URLTypes,\n        *,\n        content: RequestContent | None = None,\n        data: RequestData | None = None,\n        files: RequestFiles | None = None,\n        json: Any | None = None,\n        params: QueryParamTypes | None = None,\n        headers: HeaderTypes | None = None,\n        cookies: CookieTypes | None = None,\n        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,\n        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,\n        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,\n        extensions: Mapping[str, Any] | None = None,\n    ) -> Response:\n        \"\"\"Sends a PUT request.\n\n        Args:\n            url: URL or path for the request.\n            content: Request content.\n            data: Form encoded data.\n            files: Multipart files to send.\n            json: JSON data to send.\n            params: Query parameters.\n            headers: Request headers.\n            cookies: Request cookies.\n            auth: Auth headers.\n            follow_redirects: Whether to follow redirects.\n            timeout: Request timeout.\n            extensions: Dictionary of ASGI extensions.\n\n        Returns:\n            An HTTPX Response.\n        \"\"\"\n        return Client.put(\n            self,\n            url,\n            content=content,\n            data=data,\n            files=files,\n            json=json,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=follow_redirects,\n            timeout=timeout,\n            extensions=None if extensions is None else dict(extensions),\n        )\n\n    def patch(\n        self,\n        url: URLTypes,\n        *,\n        content: RequestContent | None = None,\n        data: RequestData | None = None,\n        files: RequestFiles | None = None,\n        json: Any | None = None,\n        params: QueryParamTypes | None = None,\n        headers: HeaderTypes | None = None,\n        cookies: CookieTypes | None = None,\n        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,\n        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,\n        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,\n        extensions: Mapping[str, Any] | None = None,\n    ) -> Response:\n        \"\"\"Sends a PATCH request.\n\n        Args:\n            url: URL or path for the request.\n            content: Request content.\n            data: Form encoded data.\n            files: Multipart files to send.\n            json: JSON data to send.\n            params: Query parameters.\n            headers: Request headers.\n            cookies: Request cookies.\n            auth: Auth headers.\n            follow_redirects: Whether to follow redirects.\n            timeout: Request timeout.\n            extensions: Dictionary of ASGI extensions.\n\n        Returns:\n            An HTTPX Response.\n        \"\"\"\n        return Client.patch(\n            self,\n            url,\n            content=content,\n            data=data,\n            files=files,\n            json=json,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=follow_redirects,\n            timeout=timeout,\n            extensions=None if extensions is None else dict(extensions),\n        )\n\n    def delete(\n        self,\n        url: URLTypes,\n        *,\n        params: QueryParamTypes | None = None,\n        headers: HeaderTypes | None = None,\n        cookies: CookieTypes | None = None,\n        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,\n        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,\n        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,\n        extensions: Mapping[str, Any] | None = None,\n    ) -> Response:\n        \"\"\"Sends a DELETE request.\n\n        Args:\n            url: URL or path for the request.\n            params: Query parameters.\n            headers: Request headers.\n            cookies: Request cookies.\n            auth: Auth headers.\n            follow_redirects: Whether to follow redirects.\n            timeout: Request timeout.\n            extensions: Dictionary of ASGI extensions.\n\n        Returns:\n            An HTTPX Response.\n        \"\"\"\n        return Client.delete(\n            self,\n            url,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            auth=auth,\n            follow_redirects=follow_redirects,\n            timeout=timeout,\n            extensions=None if extensions is None else dict(extensions),\n        )\n\n    def websocket_connect(\n        self,\n        url: str,\n        subprotocols: Sequence[str] | None = None,\n        params: QueryParamTypes | None = None,\n        headers: HeaderTypes | None = None,\n        cookies: CookieTypes | None = None,\n        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,\n        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,\n        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,\n        extensions: Mapping[str, Any] | None = None,\n    ) -> WebSocketTestSession:\n        \"\"\"Sends a GET request to establish a websocket connection.\n\n        Args:\n            url: Request URL.\n            subprotocols: Websocket subprotocols.\n            params: Query parameters.\n            headers: Request headers.\n            cookies: Request cookies.\n            auth: Auth headers.\n            follow_redirects: Whether to follow redirects.\n            timeout: Request timeout.\n            extensions: Dictionary of ASGI extensions.\n\n        Returns:\n            A `WebSocketTestSession <litestar.testing.WebSocketTestSession>` instance.\n        \"\"\"\n        url = urljoin(\"ws://testserver\", url)\n        default_headers: dict[str, str] = {}\n        default_headers.setdefault(\"connection\", \"upgrade\")\n        default_headers.setdefault(\"sec-websocket-key\", \"testserver==\")\n        default_headers.setdefault(\"sec-websocket-version\", \"13\")\n        if subprotocols is not None:\n            default_headers.setdefault(\"sec-websocket-protocol\", \", \".join(subprotocols))\n        try:\n            Client.request(\n                self,\n                \"GET\",\n                url,\n                headers={**dict(headers or {}), **default_headers},  # type: ignore\n                params=params,\n                cookies=cookies,\n                auth=auth,\n                follow_redirects=follow_redirects,\n                timeout=timeout,\n                extensions=None if extensions is None else dict(extensions),\n            )\n        except ConnectionUpgradeExceptionError as exc:\n            return exc.session\n\n        raise RuntimeError(\"Expected WebSocket upgrade\")  # pragma: no cover\n\n    def set_session_data(self, data: dict[str, Any]) -> None:\n        \"\"\"Set session data.\n\n        Args:\n            data: Session data\n\n        Returns:\n            None\n\n        Examples:\n            .. code-block:: python\n\n                from litestar import Litestar, get\n                from litestar.middleware.session.memory_backend import MemoryBackendConfig\n\n                session_config = MemoryBackendConfig()\n\n\n                @get(path=\"/test\")\n                def get_session_data(request: Request) -> Dict[str, Any]:\n                    return request.session\n\n\n                app = Litestar(\n                    route_handlers=[get_session_data], middleware=[session_config.middleware]\n                )\n\n                with TestClient(app=app, session_config=session_config) as client:\n                    client.set_session_data({\"foo\": \"bar\"})\n                    assert client.get(\"/test\").json() == {\"foo\": \"bar\"}\n\n        \"\"\"\n        with self.portal() as portal:\n            portal.call(self._set_session_data, data)\n\n    def get_session_data(self) -> dict[str, Any]:\n        \"\"\"Get session data.\n\n        Returns:\n            A dictionary containing session data.\n\n        Examples:\n            .. code-block:: python\n\n                from litestar import Litestar, post\n                from litestar.middleware.session.memory_backend import MemoryBackendConfig\n\n                session_config = MemoryBackendConfig()\n\n\n                @post(path=\"/test\")\n                def set_session_data(request: Request) -> None:\n                    request.session[\"foo\"] == \"bar\"\n\n\n                app = Litestar(\n                    route_handlers=[set_session_data], middleware=[session_config.middleware]\n                )\n\n                with TestClient(app=app, session_config=session_config) as client:\n                    client.post(\"/test\")\n                    assert client.get_session_data() == {\"foo\": \"bar\"}\n\n        \"\"\"\n        with self.portal() as portal:\n            return portal.call(self._get_session_data)"
    },
    {
        "task_id": "pydata__xarray-7444_CFTimeIndex",
        "class_name": "CFTimeIndex",
        "file": "pydata__xarray-7444/xarray/coding/cftimeindex.py",
        "sketchy_description": "The 'CFTimeIndex' class is a subclass of 'pd.Index'. It does not have any class decorators. The class has an '__init__' method that takes two arguments, 'data' and 'name'. This method initializes a new CFTimeIndex object with the given data and name. \n\nThe class has a method named '_PArTIaL_DaTe_sLiCe' which takes two arguments, 'resolution' and 'parsed'. This method is adapted from pandas.tseries.index.DatetimeIndex._partial_date_slice and returns a single element if a partial-date selection is made.\n\nThe '_gET_STRINg_sliCe' method takes one argument, 'key'. This method is adapted from pandas.tseries.index.DatetimeIndex._get_string_slice.\n\nThe '_get_nearest_indexer' method takes three arguments, 'target', 'limit', and 'tolerance'. This method is adapted from pandas.Index._get_nearest_indexer.\n\nThe '_filter_indexer_tolerance' method takes three arguments, 'target', 'indexer', and 'tolerance'. This method is adapted from pandas.Index._filter_indexer_tolerance.\n\nThe 'get_loc' method takes one argument, 'key'. This method is adapted from pandas.tseries.index.DatetimeIndex.get_loc.\n\nThe '_maYBe_CAsT_SliCE_bOunD' method takes three arguments, 'label', 'side', and 'kind'. This method is adapted from pandas.tseries.index.DatetimeIndex._maybe_cast_slice_bound.\n\nThe 'geT_VaLUE' method takes two arguments, 'series' and 'key'. This method is adapted from pandas.tseries.index.DatetimeIndex.get_value.\n\nThe 'cONTaInS' method takes one argument, 'key'. This method is needed for .loc based partial-string indexing.\n\nThe 'ShIfT' method takes two arguments, 'n' and 'freq'. This method shifts the CFTimeIndex a multiple of the given frequency.\n\nThe 'to_datetimeindex' method takes one argument, 'unsafe'. This method converts this index to a pandas.DatetimeIndex if possible.\n\nThe 'strftime' method takes one argument, 'date_format'. This method returns an Index of formatted strings specified by date_format.\n\nThe 'asi8' method converts to integers with units of microseconds since 1970-01-01.\n\nThe 'calendar' method returns the calendar used by the datetimes in the index.\n\nThe 'freq' method returns the frequency used by the dates in the index.\n\nThe '_RoUND_via_mEThOD' method takes two arguments, 'freq' and 'method'. This method rounds dates using a specified method.\n\nThe 'floor' method takes one argument, 'freq'. This method rounds dates down to fixed frequency.\n\nThe 'ceil' method takes one argument, 'freq'. This method rounds dates up to fixed frequency.\n\nThe 'round' method takes one argument, 'freq'. This method rounds dates to a fixed frequency.\n\nThe '__new__' method takes three arguments, 'cls', 'data', and 'name'. This method creates a new CFTimeIndex object with the given data and name.\n\nThe '__repr__' method returns a string representation for this object.\n\nThe '__contains__' method takes one argument, 'key'. This method is adapted from pandas.tseries.base.DatetimeIndexOpsMixin.__contains__.\n\nThe '__add__' method takes one argument, 'other'. This method adds a pandas TimedeltaIndex to the CFTimeIndex.\n\nThe '__radd__' method takes one argument, 'other'. This method adds the CFTimeIndex to a pandas TimedeltaIndex.\n\nThe '__sub__' method takes one argument, 'other'. This method subtracts the given value from the CFTimeIndex.\n\nThe '__rsub__' method takes one argument, 'other'. This method is not documented.\n\nThe class has several class variables including 'year', 'month', 'day', 'hour', 'minute', 'second', 'microsecond', 'dayofyear', 'dayofweek', 'days_in_month', 'date_type', '_cache', '__array_priority__', '_typ', '_data', '_data_cls', '_id', '_name', '_no_setting_name', '_comparables', '_attributes', '_engine_types', '_supports_partial_string_indexing', '_accessors', 'str', '_references', 'to_list', '_shared_docs', '_default_na_rep', 'names', 'get_level_values', 'isnull', 'notnull', '__bool__', '__hash__', '_index_shared_docs', '_requires_unique_msg'.\n\nThe class has several instance variables including '_id' and '_name'.\n\nThe class has several properties including 'asi8', 'calendar', 'empty', 'freq', 'has_duplicates', 'is_monotonic_decreasing', 'is_monotonic_increasing', 'nbytes', 'ndim', 'nlevels', 'shape', 'size', 'values', '_engine_type', '_formatter_func', '_index_as_unique', '_is_strictly_monotonic_decreasing', '_is_strictly_monotonic_increasing', '_values'.",
        "detailed_description": "The 'CFTimeIndex' class is a subclass of 'pd.Index' and represents a custom index for working with CF calendars and dates. All elements of a 'CFTimeIndex' must be 'cftime.datetime' objects. The class has an '__init__' method that takes two arguments, 'data' and 'name', where 'data' is a sequence of 'cftime.datetime' objects to use in the index and 'name' is the name of the resulting index. The class also has several instance variables representing different aspects of the datetime, such as 'year', 'month', 'day', 'hour', 'minute', 'second', 'microsecond', 'dayofyear', 'dayofweek', 'days_in_month', and 'date_type'. These instance variables are created using the '_FieLD_aCcesSOR' function.\n\nThe class has a '__new__' method that takes three arguments, 'cls', 'data', and 'name', and optional keyword arguments. This method checks if all elements in 'data' are valid date types using the 'assERT_AlL_vALID_datE_tYpE' function. If 'name' is 'None' and 'data' has a 'name' attribute, 'name' is set to 'data.name'. The method then creates a new instance of the class, sets the '_data' instance variable to a numpy array of 'data' with dtype 'O', sets the 'name' instance variable to 'name', initializes the '_cache' instance variable to an empty dictionary, and returns the new instance.\n\nThe class has a '__repr__' method that returns a string representation of the instance. The method calculates the string representation based on the length of the instance and the display width. If the length of the instance is less than or equal to 'ITEMS_IN_REPR_MAX_ELSE_ELLIPSIS', the method formats the times using the 'fOrmAT_tIMeS' function. Otherwise, the method formats the front and end times separately using the 'fOrmAT_tIMeS' function and joins them with an ellipsis in the middle. The method then formats the attributes using the 'forMAT_AtTRs' function and constructs the full representation string. If the length of the full representation string is greater than the display width, the method formats the attributes string to be one per line and constructs the full representation string again. The method finally returns the full representation string.\n\nThe class has a '_PArTIaL_DaTe_sLiCe' method that takes two arguments, 'resolution' and 'parsed'. This method calculates the start and end bounds using the '_pARSEd_sTrInG_To_BouNDs' function and the times from the '_data' instance variable. If the instance is monotonic increasing and the start and end bounds are out of range, the method raises a 'KeyError'. If the instance is monotonic increasing, the method calculates the left and right indices using the 'searchsorted' method and returns a slice from the left index to the right index. Otherwise, the method calculates the lhs and rhs masks and returns the indices where both masks are 'True'.\n\nThe class has a '_gET_STRINg_sliCe' method that takes an argument 'key'. This method parses the key using the '_ParSe_ISo8601_WItH_ReSo' function and tries to get the location using the '_PArTIaL_DaTe_sLiCe' method. If a 'KeyError' is raised, the method raises a 'KeyError' with the key.\n\nThe class has a '_get_nearest_indexer' method that takes three arguments, 'target', 'limit', and 'tolerance'. This method calculates the left and right indexers using the 'get_indexer' method, calculates the left and right distances, and calculates the indexer based on the distances and whether the instance is monotonic increasing. If 'tolerance' is not 'None', the method filters the indexer using the '_filter_indexer_tolerance' method. The method finally returns the indexer.\n\nThe class has a '_filter_indexer_tolerance' method that takes three arguments, 'target', 'indexer', and 'tolerance'. This method calculates the distance based on whether 'target' is an instance of 'pd.Index' or not, filters the indexer based on the distance and tolerance, and returns the filtered indexer.\n\nThe class has a 'get_loc' method that takes an argument 'key'. This method returns the location of the key if the key is a string using the '_gET_STRINg_sliCe' method. Otherwise, it calls the superclass 'get_loc' method with the key.\n\nThe class has a '_maYBe_CAsT_SliCE_bOunD' method that takes three arguments, 'label', 'side', and 'kind'. This method parses the label using the '_ParSe_ISo8601_WItH_ReSo' function, calculates the start and end bounds using the '_pARSEd_sTrInG_To_BouNDs' function, and returns the start or end bound based on the side and whether the instance is monotonic decreasing.\n\nThe class has a 'geT_VaLUE' method that takes two arguments, 'series' and 'key'. This method returns the series values at the indices specified by the key if the key is a boolean array, returns the series values at the indices specified by the slice indexer if the key is a slice, and returns the series value at the location specified by the key otherwise.\n\nThe class has a '__contains__' method that takes an argument 'key'. This method tries to get the result using the 'get_loc' method. If a 'KeyError', 'TypeError', or 'ValueError' is raised, the method returns 'False'. Otherwise, the method returns 'True' if the result is a scalar, a slice, or a non-empty numpy array.\n\nThe class has a 'cONTaInS' method that takes an argument 'key'. This method returns whether the key is contained in the instance using the '__contains__' method.\n\nThe class has a 'ShIfT' method that takes two arguments, 'n' and 'freq'. This method returns a new 'CFTimeIndex' instance with the dates shifted by 'n' periods of the given frequency. The method converts the frequency to a 'datetime.timedelta' object if it is a 'datetime.timedelta' object, converts the frequency to an offset if it is a string, and raises a 'TypeError' otherwise.\n\nThe class has '__add__', '__radd__', '__sub__', and '__rsub__' methods that take an argument 'other'. These methods return a new 'CFTimeIndex' instance with the dates added or subtracted by 'other'. The methods convert 'other' to a 'pytimedelta' if it is a 'pd.TimedeltaIndex' and raise a 'ValueError' if the time difference exceeds the range of values that can be expressed at the nanosecond resolution.\n\nThe class has a 'to_datetimeindex' method that takes an optional argument 'unsafe'. This method converts the 'CFTimeIndex' instance to a 'pd.DatetimeIndex' if possible. The method raises a 'ValueError' if the 'CFTimeIndex' contains dates that are not possible in the standard calendar or outside the 'pd.Timestamp'-valid range and raises a 'RuntimeWarning' if converting from a non-standard calendar to a 'DatetimeIndex'.\n\nThe class has a 'strftime' method that takes an argument 'date_format'. This method returns a 'pd.Index' of formatted strings specified by 'date_format'.\n\nThe class has an 'asi8' property that returns the dates converted to integers with units of microseconds since 1970-01-01. The method calculates the total microseconds using the '_Total_MIcrOsECOnDS' function and the exact 'cftime.datetime' difference using the 'ExAcT_cftIMe_daTETiMe_dIFFeReNCe' function.\n\nThe class has a 'calendar' property that returns the calendar used by the datetimes in the index. The method infers the calendar name using the 'INFer_caleNDAR_NamE' function.\n\nThe class has a 'freq' property that returns the frequency used by the dates in the index. The method infers the frequency using the 'InFEr_FrEq' function.\n\nThe class has 'floor', 'ceil', and 'round' methods that take an argument 'freq'. These methods round the dates down, up, or to the nearest fixed frequency respectively. The methods raise a 'ValueError' if the frequency is not a fixed frequency and return a new 'CFTimeIndex' instance with the dates rounded using the '_RoUND_via_mEThOD' function.",
        "repo_metadata": {
            "commit_id": "bb7eb65e8c258f3ab0bcfb139554c2bb3d34aafd",
            "issue_id": "pydata__xarray-7444",
            "setup_details": {
                "repo": "pydata/xarray",
                "instance_id": "pydata__xarray-7444",
                "base_commit": "821dc24b5f3ed91b843a634bf8513a26046269ef",
                "version": "2022.09",
                "environment_setup_commit": "087ebbb78668bdf5d2d41c3b2553e3f29ce75be1"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "xarray/tests/test_cftimeindex.py::test_empty_cftimeindex",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[365_day-year]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[365_day-month]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[365_day-day]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[365_day-hour]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[365_day-dayofyear]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[365_day-dayofweek]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[360_day-year]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[360_day-month]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[360_day-day]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[360_day-hour]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[360_day-dayofyear]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[360_day-dayofweek]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[julian-year]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[julian-month]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[julian-day]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[julian-hour]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[julian-dayofyear]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[julian-dayofweek]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[all_leap-year]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[all_leap-month]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[all_leap-day]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[all_leap-hour]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[all_leap-dayofyear]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[all_leap-dayofweek]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[366_day-year]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[366_day-month]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[366_day-day]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[366_day-hour]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[366_day-dayofyear]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[366_day-dayofweek]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[gregorian-year]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[gregorian-month]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[gregorian-day]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[gregorian-hour]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[gregorian-dayofyear]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[gregorian-dayofweek]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[proleptic_gregorian-year]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[proleptic_gregorian-month]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[proleptic_gregorian-day]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[proleptic_gregorian-hour]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[proleptic_gregorian-dayofyear]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access[proleptic_gregorian-dayofweek]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float_fails_for_non_tick_freqs[AS]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float_fails_for_non_tick_freqs[A]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float_fails_for_non_tick_freqs[YS]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float_fails_for_non_tick_freqs[Y]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float_fails_for_non_tick_freqs[QS]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float_fails_for_non_tick_freqs[Q]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float_fails_for_non_tick_freqs[MS]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float_fails_for_non_tick_freqs[M]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift[365_day-D]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift[365_day-freq1]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift[360_day-D]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift[360_day-freq1]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift[julian-D]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift[julian-freq1]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift[all_leap-D]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift[all_leap-freq1]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift[366_day-D]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift[366_day-freq1]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift[gregorian-D]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift[gregorian-freq1]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift[proleptic_gregorian-D]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift[proleptic_gregorian-freq1]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_cftimeindex[365_day]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_cftimeindex[360_day]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_cftimeindex[julian]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_cftimeindex[all_leap]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_cftimeindex[366_day]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_cftimeindex[gregorian]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_cftimeindex[proleptic_gregorian]",
                "xarray/tests/test_cftimeindex.py::test_concat_cftimeindex[365_day]",
                "xarray/tests/test_cftimeindex.py::test_concat_cftimeindex[360_day]",
                "xarray/tests/test_cftimeindex.py::test_concat_cftimeindex[julian]",
                "xarray/tests/test_cftimeindex.py::test_concat_cftimeindex[all_leap]",
                "xarray/tests/test_cftimeindex.py::test_concat_cftimeindex[366_day]",
                "xarray/tests/test_cftimeindex.py::test_concat_cftimeindex[gregorian]",
                "xarray/tests/test_cftimeindex.py::test_concat_cftimeindex[proleptic_gregorian]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_add_timedeltaindex[365_day]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_add_timedeltaindex[360_day]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_add_timedeltaindex[julian]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_add_timedeltaindex[all_leap]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_add_timedeltaindex[366_day]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_add_timedeltaindex[gregorian]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_add_timedeltaindex[proleptic_gregorian]",
                "xarray/tests/test_cftimeindex.py::test_round[365_day]",
                "xarray/tests/test_cftimeindex.py::test_round[360_day]",
                "xarray/tests/test_cftimeindex.py::test_round[julian]",
                "xarray/tests/test_cftimeindex.py::test_round[all_leap]",
                "xarray/tests/test_cftimeindex.py::test_round[366_day]",
                "xarray/tests/test_cftimeindex.py::test_round[gregorian]",
                "xarray/tests/test_cftimeindex.py::test_round[proleptic_gregorian]",
                "xarray/tests/test_cftimeindex.py::test_timedeltaindex_add_cftimeindex[365_day]",
                "xarray/tests/test_cftimeindex.py::test_timedeltaindex_add_cftimeindex[360_day]",
                "xarray/tests/test_cftimeindex.py::test_timedeltaindex_add_cftimeindex[julian]",
                "xarray/tests/test_cftimeindex.py::test_timedeltaindex_add_cftimeindex[all_leap]",
                "xarray/tests/test_cftimeindex.py::test_timedeltaindex_add_cftimeindex[366_day]",
                "xarray/tests/test_cftimeindex.py::test_timedeltaindex_add_cftimeindex[gregorian]",
                "xarray/tests/test_cftimeindex.py::test_timedeltaindex_add_cftimeindex[proleptic_gregorian]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float_us",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[365_day-D-D-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[365_day-D-D-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[365_day-H-H-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[365_day-H-H-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[365_day-T-min-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[365_day-T-min-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[365_day-S-S-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[365_day-S-S-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[365_day-L-ms-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[365_day-L-ms-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[360_day-D-D-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[360_day-D-D-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[360_day-H-H-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[360_day-H-H-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[360_day-T-min-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[360_day-T-min-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[360_day-S-S-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[360_day-S-S-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[360_day-L-ms-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[360_day-L-ms-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[julian-D-D-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[julian-D-D-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[julian-H-H-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[julian-H-H-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[julian-T-min-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[julian-T-min-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[julian-S-S-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[julian-S-S-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[julian-L-ms-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[julian-L-ms-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[all_leap-D-D-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[all_leap-D-D-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[all_leap-H-H-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[all_leap-H-H-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[all_leap-T-min-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[all_leap-T-min-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[all_leap-S-S-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[all_leap-S-S-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[all_leap-L-ms-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[all_leap-L-ms-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[366_day-D-D-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[366_day-D-D-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[366_day-H-H-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[366_day-H-H-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[366_day-T-min-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[366_day-T-min-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[366_day-S-S-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[366_day-S-S-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[366_day-L-ms-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[366_day-L-ms-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[gregorian-D-D-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[gregorian-D-D-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[gregorian-H-H-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[gregorian-H-H-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[gregorian-T-min-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[gregorian-T-min-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[gregorian-S-S-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[gregorian-S-S-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[gregorian-L-ms-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[gregorian-L-ms-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[proleptic_gregorian-D-D-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[proleptic_gregorian-D-D-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[proleptic_gregorian-H-H-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[proleptic_gregorian-H-H-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[proleptic_gregorian-T-min-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[proleptic_gregorian-T-min-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[proleptic_gregorian-S-S-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[proleptic_gregorian-S-S-1.5]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[proleptic_gregorian-L-ms-2.0]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_shift_float[proleptic_gregorian-L-ms-1.5]",
                "xarray/tests/test_indexes.py::test_safe_cast_to_index_cftimeindex",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedelta[365_day]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedelta[360_day]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedelta[julian]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedelta[all_leap]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedelta[366_day]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedelta[gregorian]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedelta[proleptic_gregorian]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedeltaindex[365_day]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedeltaindex[360_day]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedeltaindex[julian]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedeltaindex[all_leap]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedeltaindex[366_day]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedeltaindex[gregorian]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedeltaindex[proleptic_gregorian]",
                "xarray/tests/test_cftimeindex.py::test_constructor_with_name[365_day-bar-bar]",
                "xarray/tests/test_cftimeindex.py::test_constructor_with_name[365_day-None-foo]",
                "xarray/tests/test_cftimeindex.py::test_constructor_with_name[360_day-bar-bar]",
                "xarray/tests/test_cftimeindex.py::test_constructor_with_name[360_day-None-foo]",
                "xarray/tests/test_cftimeindex.py::test_constructor_with_name[julian-bar-bar]",
                "xarray/tests/test_cftimeindex.py::test_constructor_with_name[julian-None-foo]",
                "xarray/tests/test_cftimeindex.py::test_constructor_with_name[all_leap-bar-bar]",
                "xarray/tests/test_cftimeindex.py::test_constructor_with_name[all_leap-None-foo]",
                "xarray/tests/test_cftimeindex.py::test_constructor_with_name[366_day-bar-bar]",
                "xarray/tests/test_cftimeindex.py::test_constructor_with_name[366_day-None-foo]",
                "xarray/tests/test_cftimeindex.py::test_constructor_with_name[gregorian-bar-bar]",
                "xarray/tests/test_cftimeindex.py::test_constructor_with_name[gregorian-None-foo]",
                "xarray/tests/test_cftimeindex.py::test_constructor_with_name[proleptic_gregorian-bar-bar]",
                "xarray/tests/test_cftimeindex.py::test_constructor_with_name[proleptic_gregorian-None-foo]",
                "xarray/tests/test_accessor_dt.py::test_field_access[365_day-year]",
                "xarray/tests/test_accessor_dt.py::test_field_access[365_day-month]",
                "xarray/tests/test_accessor_dt.py::test_field_access[365_day-day]",
                "xarray/tests/test_accessor_dt.py::test_field_access[365_day-hour]",
                "xarray/tests/test_accessor_dt.py::test_field_access[365_day-dayofyear]",
                "xarray/tests/test_accessor_dt.py::test_field_access[365_day-dayofweek]",
                "xarray/tests/test_accessor_dt.py::test_field_access[360_day-year]",
                "xarray/tests/test_accessor_dt.py::test_field_access[360_day-month]",
                "xarray/tests/test_accessor_dt.py::test_field_access[360_day-day]",
                "xarray/tests/test_accessor_dt.py::test_field_access[360_day-hour]",
                "xarray/tests/test_accessor_dt.py::test_field_access[360_day-dayofyear]",
                "xarray/tests/test_accessor_dt.py::test_field_access[360_day-dayofweek]",
                "xarray/tests/test_accessor_dt.py::test_field_access[julian-year]",
                "xarray/tests/test_accessor_dt.py::test_field_access[julian-month]",
                "xarray/tests/test_accessor_dt.py::test_field_access[julian-day]",
                "xarray/tests/test_accessor_dt.py::test_field_access[julian-hour]",
                "xarray/tests/test_accessor_dt.py::test_field_access[julian-dayofyear]",
                "xarray/tests/test_accessor_dt.py::test_field_access[julian-dayofweek]",
                "xarray/tests/test_accessor_dt.py::test_field_access[all_leap-year]",
                "xarray/tests/test_accessor_dt.py::test_field_access[all_leap-month]",
                "xarray/tests/test_accessor_dt.py::test_field_access[all_leap-day]",
                "xarray/tests/test_accessor_dt.py::test_field_access[all_leap-hour]",
                "xarray/tests/test_accessor_dt.py::test_field_access[all_leap-dayofyear]",
                "xarray/tests/test_accessor_dt.py::test_field_access[all_leap-dayofweek]",
                "xarray/tests/test_accessor_dt.py::test_field_access[366_day-year]",
                "xarray/tests/test_accessor_dt.py::test_field_access[366_day-month]",
                "xarray/tests/test_accessor_dt.py::test_field_access[366_day-day]",
                "xarray/tests/test_accessor_dt.py::test_field_access[366_day-hour]",
                "xarray/tests/test_accessor_dt.py::test_field_access[366_day-dayofyear]",
                "xarray/tests/test_accessor_dt.py::test_field_access[366_day-dayofweek]",
                "xarray/tests/test_accessor_dt.py::test_field_access[gregorian-year]",
                "xarray/tests/test_accessor_dt.py::test_field_access[gregorian-month]",
                "xarray/tests/test_accessor_dt.py::test_field_access[gregorian-day]",
                "xarray/tests/test_accessor_dt.py::test_field_access[gregorian-hour]",
                "xarray/tests/test_accessor_dt.py::test_field_access[gregorian-dayofyear]",
                "xarray/tests/test_accessor_dt.py::test_field_access[gregorian-dayofweek]",
                "xarray/tests/test_accessor_dt.py::test_field_access[proleptic_gregorian-year]",
                "xarray/tests/test_accessor_dt.py::test_field_access[proleptic_gregorian-month]",
                "xarray/tests/test_accessor_dt.py::test_field_access[proleptic_gregorian-day]",
                "xarray/tests/test_accessor_dt.py::test_field_access[proleptic_gregorian-hour]",
                "xarray/tests/test_accessor_dt.py::test_field_access[proleptic_gregorian-dayofyear]",
                "xarray/tests/test_accessor_dt.py::test_field_access[proleptic_gregorian-dayofweek]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_add[365_day]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_add[360_day]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_add[julian]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_add[all_leap]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_add[366_day]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_add[gregorian]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_add[proleptic_gregorian]",
                "xarray/tests/test_cftimeindex.py::test_asi8[365_day]",
                "xarray/tests/test_cftimeindex.py::test_asi8[360_day]",
                "xarray/tests/test_cftimeindex.py::test_asi8[julian]",
                "xarray/tests/test_cftimeindex.py::test_asi8[all_leap]",
                "xarray/tests/test_cftimeindex.py::test_asi8[366_day]",
                "xarray/tests/test_cftimeindex.py::test_asi8[gregorian]",
                "xarray/tests/test_cftimeindex.py::test_asi8[proleptic_gregorian]",
                "xarray/tests/test_cftimeindex.py::test_ceil[365_day]",
                "xarray/tests/test_cftimeindex.py::test_ceil[360_day]",
                "xarray/tests/test_cftimeindex.py::test_ceil[julian]",
                "xarray/tests/test_cftimeindex.py::test_ceil[all_leap]",
                "xarray/tests/test_cftimeindex.py::test_ceil[366_day]",
                "xarray/tests/test_cftimeindex.py::test_ceil[gregorian]",
                "xarray/tests/test_cftimeindex.py::test_ceil[proleptic_gregorian]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[365_day-0001-01-01-0001-01-04-None-D-neither-False-[(1, 1, 2), (1, 1, 3)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[365_day-0001-01-01-0001-01-04-None-D-None-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[365_day-0001-01-01-0001-01-04-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[365_day-0001-01-01-0001-01-04-None-D-left-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[365_day-0001-01-01-0001-01-04-None-D-right-False-[(1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[365_day-0001-01-01T01:00:00-0001-01-04-None-D-both-False-[(1, 1, 1, 1), (1, 1, 2, 1), (1, 1, 3, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[365_day-0001-01-01 01:00:00-0001-01-04-None-D-both-False-[(1, 1, 1, 1), (1, 1, 2, 1), (1, 1, 3, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[365_day-0001-01-01T01:00:00-0001-01-04-None-D-both-True-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[365_day-0001-01-01-None-4-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[365_day-None-0001-01-04-4-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[365_day-(1, 1, 1)-0001-01-04-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[365_day-(1, 1, 1)-(1, 1, 4)-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[365_day-0001-01-30-0011-02-01-None-3AS-JUN-both-False-[(1, 6, 1), (4, 6, 1), (7, 6, 1), (10, 6, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[365_day-0001-01-04-0001-01-01-None-D-both-False-[]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[365_day-0010-None-4-<YearBegin: n=-2, month=1>-both-False-[(10, 1, 1), (8, 1, 1), (6, 1, 1), (4, 1, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[365_day-0001-01-01-0001-01-04-4-None-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[365_day-0001-06-01-None-4-3QS-JUN-both-False-[(1, 6, 1), (2, 3, 1), (2, 12, 1), (3, 9, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[360_day-0001-01-01-0001-01-04-None-D-neither-False-[(1, 1, 2), (1, 1, 3)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[360_day-0001-01-01-0001-01-04-None-D-None-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[360_day-0001-01-01-0001-01-04-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[360_day-0001-01-01-0001-01-04-None-D-left-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[360_day-0001-01-01-0001-01-04-None-D-right-False-[(1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[360_day-0001-01-01T01:00:00-0001-01-04-None-D-both-False-[(1, 1, 1, 1), (1, 1, 2, 1), (1, 1, 3, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[360_day-0001-01-01 01:00:00-0001-01-04-None-D-both-False-[(1, 1, 1, 1), (1, 1, 2, 1), (1, 1, 3, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[360_day-0001-01-01T01:00:00-0001-01-04-None-D-both-True-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[360_day-0001-01-01-None-4-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[360_day-None-0001-01-04-4-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[360_day-(1, 1, 1)-0001-01-04-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[360_day-(1, 1, 1)-(1, 1, 4)-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[360_day-0001-01-30-0011-02-01-None-3AS-JUN-both-False-[(1, 6, 1), (4, 6, 1), (7, 6, 1), (10, 6, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[360_day-0001-01-04-0001-01-01-None-D-both-False-[]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[360_day-0010-None-4-<YearBegin: n=-2, month=1>-both-False-[(10, 1, 1), (8, 1, 1), (6, 1, 1), (4, 1, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[360_day-0001-01-01-0001-01-04-4-None-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[360_day-0001-06-01-None-4-3QS-JUN-both-False-[(1, 6, 1), (2, 3, 1), (2, 12, 1), (3, 9, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[julian-0001-01-01-0001-01-04-None-D-neither-False-[(1, 1, 2), (1, 1, 3)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[julian-0001-01-01-0001-01-04-None-D-None-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[julian-0001-01-01-0001-01-04-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[julian-0001-01-01-0001-01-04-None-D-left-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[julian-0001-01-01-0001-01-04-None-D-right-False-[(1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[julian-0001-01-01T01:00:00-0001-01-04-None-D-both-False-[(1, 1, 1, 1), (1, 1, 2, 1), (1, 1, 3, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[julian-0001-01-01 01:00:00-0001-01-04-None-D-both-False-[(1, 1, 1, 1), (1, 1, 2, 1), (1, 1, 3, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[julian-0001-01-01T01:00:00-0001-01-04-None-D-both-True-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[julian-0001-01-01-None-4-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[julian-None-0001-01-04-4-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[julian-(1, 1, 1)-0001-01-04-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[julian-(1, 1, 1)-(1, 1, 4)-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[julian-0001-01-30-0011-02-01-None-3AS-JUN-both-False-[(1, 6, 1), (4, 6, 1), (7, 6, 1), (10, 6, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[julian-0001-01-04-0001-01-01-None-D-both-False-[]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[julian-0010-None-4-<YearBegin: n=-2, month=1>-both-False-[(10, 1, 1), (8, 1, 1), (6, 1, 1), (4, 1, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[julian-0001-01-01-0001-01-04-4-None-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[julian-0001-06-01-None-4-3QS-JUN-both-False-[(1, 6, 1), (2, 3, 1), (2, 12, 1), (3, 9, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[all_leap-0001-01-01-0001-01-04-None-D-neither-False-[(1, 1, 2), (1, 1, 3)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[all_leap-0001-01-01-0001-01-04-None-D-None-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[all_leap-0001-01-01-0001-01-04-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[all_leap-0001-01-01-0001-01-04-None-D-left-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[all_leap-0001-01-01-0001-01-04-None-D-right-False-[(1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[all_leap-0001-01-01T01:00:00-0001-01-04-None-D-both-False-[(1, 1, 1, 1), (1, 1, 2, 1), (1, 1, 3, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[all_leap-0001-01-01 01:00:00-0001-01-04-None-D-both-False-[(1, 1, 1, 1), (1, 1, 2, 1), (1, 1, 3, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[all_leap-0001-01-01T01:00:00-0001-01-04-None-D-both-True-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[all_leap-0001-01-01-None-4-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[all_leap-None-0001-01-04-4-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[all_leap-(1, 1, 1)-0001-01-04-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[all_leap-(1, 1, 1)-(1, 1, 4)-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[all_leap-0001-01-30-0011-02-01-None-3AS-JUN-both-False-[(1, 6, 1), (4, 6, 1), (7, 6, 1), (10, 6, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[all_leap-0001-01-04-0001-01-01-None-D-both-False-[]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[all_leap-0010-None-4-<YearBegin: n=-2, month=1>-both-False-[(10, 1, 1), (8, 1, 1), (6, 1, 1), (4, 1, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[all_leap-0001-01-01-0001-01-04-4-None-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[all_leap-0001-06-01-None-4-3QS-JUN-both-False-[(1, 6, 1), (2, 3, 1), (2, 12, 1), (3, 9, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[366_day-0001-01-01-0001-01-04-None-D-neither-False-[(1, 1, 2), (1, 1, 3)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[366_day-0001-01-01-0001-01-04-None-D-None-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[366_day-0001-01-01-0001-01-04-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[366_day-0001-01-01-0001-01-04-None-D-left-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[366_day-0001-01-01-0001-01-04-None-D-right-False-[(1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[366_day-0001-01-01T01:00:00-0001-01-04-None-D-both-False-[(1, 1, 1, 1), (1, 1, 2, 1), (1, 1, 3, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[366_day-0001-01-01 01:00:00-0001-01-04-None-D-both-False-[(1, 1, 1, 1), (1, 1, 2, 1), (1, 1, 3, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[366_day-0001-01-01T01:00:00-0001-01-04-None-D-both-True-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[366_day-0001-01-01-None-4-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[366_day-None-0001-01-04-4-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[366_day-(1, 1, 1)-0001-01-04-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[366_day-(1, 1, 1)-(1, 1, 4)-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[366_day-0001-01-30-0011-02-01-None-3AS-JUN-both-False-[(1, 6, 1), (4, 6, 1), (7, 6, 1), (10, 6, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[366_day-0001-01-04-0001-01-01-None-D-both-False-[]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[366_day-0010-None-4-<YearBegin: n=-2, month=1>-both-False-[(10, 1, 1), (8, 1, 1), (6, 1, 1), (4, 1, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[366_day-0001-01-01-0001-01-04-4-None-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[366_day-0001-06-01-None-4-3QS-JUN-both-False-[(1, 6, 1), (2, 3, 1), (2, 12, 1), (3, 9, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[gregorian-0001-01-01-0001-01-04-None-D-neither-False-[(1, 1, 2), (1, 1, 3)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[gregorian-0001-01-01-0001-01-04-None-D-None-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[gregorian-0001-01-01-0001-01-04-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[gregorian-0001-01-01-0001-01-04-None-D-left-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[gregorian-0001-01-01-0001-01-04-None-D-right-False-[(1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[gregorian-0001-01-01T01:00:00-0001-01-04-None-D-both-False-[(1, 1, 1, 1), (1, 1, 2, 1), (1, 1, 3, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[gregorian-0001-01-01 01:00:00-0001-01-04-None-D-both-False-[(1, 1, 1, 1), (1, 1, 2, 1), (1, 1, 3, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[gregorian-0001-01-01T01:00:00-0001-01-04-None-D-both-True-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[gregorian-0001-01-01-None-4-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[gregorian-None-0001-01-04-4-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[gregorian-(1, 1, 1)-0001-01-04-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[gregorian-(1, 1, 1)-(1, 1, 4)-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[gregorian-0001-01-30-0011-02-01-None-3AS-JUN-both-False-[(1, 6, 1), (4, 6, 1), (7, 6, 1), (10, 6, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[gregorian-0001-01-04-0001-01-01-None-D-both-False-[]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[gregorian-0010-None-4-<YearBegin: n=-2, month=1>-both-False-[(10, 1, 1), (8, 1, 1), (6, 1, 1), (4, 1, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[gregorian-0001-01-01-0001-01-04-4-None-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[gregorian-0001-06-01-None-4-3QS-JUN-both-False-[(1, 6, 1), (2, 3, 1), (2, 12, 1), (3, 9, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[proleptic_gregorian-0001-01-01-0001-01-04-None-D-neither-False-[(1, 1, 2), (1, 1, 3)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[proleptic_gregorian-0001-01-01-0001-01-04-None-D-None-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[proleptic_gregorian-0001-01-01-0001-01-04-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[proleptic_gregorian-0001-01-01-0001-01-04-None-D-left-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[proleptic_gregorian-0001-01-01-0001-01-04-None-D-right-False-[(1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[proleptic_gregorian-0001-01-01T01:00:00-0001-01-04-None-D-both-False-[(1, 1, 1, 1), (1, 1, 2, 1), (1, 1, 3, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[proleptic_gregorian-0001-01-01 01:00:00-0001-01-04-None-D-both-False-[(1, 1, 1, 1), (1, 1, 2, 1), (1, 1, 3, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[proleptic_gregorian-0001-01-01T01:00:00-0001-01-04-None-D-both-True-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[proleptic_gregorian-0001-01-01-None-4-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[proleptic_gregorian-None-0001-01-04-4-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[proleptic_gregorian-(1, 1, 1)-0001-01-04-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[proleptic_gregorian-(1, 1, 1)-(1, 1, 4)-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[proleptic_gregorian-0001-01-30-0011-02-01-None-3AS-JUN-both-False-[(1, 6, 1), (4, 6, 1), (7, 6, 1), (10, 6, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[proleptic_gregorian-0001-01-04-0001-01-01-None-D-both-False-[]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[proleptic_gregorian-0010-None-4-<YearBegin: n=-2, month=1>-both-False-[(10, 1, 1), (8, 1, 1), (6, 1, 1), (4, 1, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[proleptic_gregorian-0001-01-01-0001-01-04-4-None-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[proleptic_gregorian-0001-06-01-None-4-3QS-JUN-both-False-[(1, 6, 1), (2, 3, 1), (2, 12, 1), (3, 9, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[standard-0001-01-01-0001-01-04-None-D-neither-False-[(1, 1, 2), (1, 1, 3)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[standard-0001-01-01-0001-01-04-None-D-None-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[standard-0001-01-01-0001-01-04-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[standard-0001-01-01-0001-01-04-None-D-left-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[standard-0001-01-01-0001-01-04-None-D-right-False-[(1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[standard-0001-01-01T01:00:00-0001-01-04-None-D-both-False-[(1, 1, 1, 1), (1, 1, 2, 1), (1, 1, 3, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[standard-0001-01-01 01:00:00-0001-01-04-None-D-both-False-[(1, 1, 1, 1), (1, 1, 2, 1), (1, 1, 3, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[standard-0001-01-01T01:00:00-0001-01-04-None-D-both-True-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[standard-0001-01-01-None-4-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[standard-None-0001-01-04-4-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[standard-(1, 1, 1)-0001-01-04-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[standard-(1, 1, 1)-(1, 1, 4)-None-D-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[standard-0001-01-30-0011-02-01-None-3AS-JUN-both-False-[(1, 6, 1), (4, 6, 1), (7, 6, 1), (10, 6, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[standard-0001-01-04-0001-01-01-None-D-both-False-[]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[standard-0010-None-4-<YearBegin: n=-2, month=1>-both-False-[(10, 1, 1), (8, 1, 1), (6, 1, 1), (4, 1, 1)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[standard-0001-01-01-0001-01-04-4-None-both-False-[(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4)]]",
                "xarray/tests/test_cftime_offsets.py::test_cftime_range[standard-0001-06-01-None-4-3QS-JUN-both-False-[(1, 6, 1), (2, 3, 1), (2, 12, 1), (3, 9, 1)]]",
                "xarray/tests/test_dataset.py::TestDataset::test_rename_does_not_change_CFTimeIndex_type",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedelta_array[365_day-1d-array]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedelta_array[365_day-scalar-array]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedelta_array[360_day-1d-array]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedelta_array[360_day-scalar-array]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedelta_array[julian-1d-array]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedelta_array[julian-scalar-array]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedelta_array[all_leap-1d-array]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedelta_array[all_leap-scalar-array]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedelta_array[366_day-1d-array]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedelta_array[366_day-scalar-array]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedelta_array[gregorian-1d-array]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedelta_array[gregorian-scalar-array]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedelta_array[proleptic_gregorian-1d-array]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_sub_timedelta_array[proleptic_gregorian-scalar-array]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_radd[365_day]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_radd[360_day]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_radd[julian]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_radd[all_leap]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_radd[366_day]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_radd[gregorian]",
                "xarray/tests/test_cftimeindex.py::test_cftimeindex_radd[proleptic_gregorian]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[365_day-year]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[365_day-month]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[365_day-day]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[365_day-hour]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[365_day-dayofyear]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[365_day-dayofweek]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[360_day-year]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[360_day-month]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[360_day-day]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[360_day-hour]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[360_day-dayofyear]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[360_day-dayofweek]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[julian-year]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[julian-month]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[julian-day]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[julian-hour]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[julian-dayofyear]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[julian-dayofweek]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[all_leap-year]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[all_leap-month]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[all_leap-day]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[all_leap-hour]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[all_leap-dayofyear]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[all_leap-dayofweek]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[366_day-year]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[366_day-month]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[366_day-day]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[366_day-hour]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[366_day-dayofyear]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[366_day-dayofweek]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[gregorian-year]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[gregorian-month]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[gregorian-day]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[gregorian-hour]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[gregorian-dayofyear]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[gregorian-dayofweek]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[proleptic_gregorian-year]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[proleptic_gregorian-month]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[proleptic_gregorian-day]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[proleptic_gregorian-hour]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[proleptic_gregorian-dayofyear]",
                "xarray/tests/test_accessor_dt.py::test_dask_field_access_1d[proleptic_gregorian-dayofweek]",
                "xarray/tests/test_cftimeindex.py::test_asi8_distant_date",
                "xarray/tests/test_cftimeindex.py::test_floor[365_day]",
                "xarray/tests/test_cftimeindex.py::test_floor[360_day]",
                "xarray/tests/test_cftimeindex.py::test_floor[julian]",
                "xarray/tests/test_cftimeindex.py::test_floor[all_leap]",
                "xarray/tests/test_cftimeindex.py::test_floor[366_day]",
                "xarray/tests/test_cftimeindex.py::test_floor[gregorian]",
                "xarray/tests/test_cftimeindex.py::test_floor[proleptic_gregorian]",
                "xarray/tests/test_accessor_dt.py::test_cftime_strftime_access[365_day]",
                "xarray/tests/test_accessor_dt.py::test_cftime_strftime_access[360_day]",
                "xarray/tests/test_accessor_dt.py::test_cftime_strftime_access[julian]",
                "xarray/tests/test_accessor_dt.py::test_cftime_strftime_access[all_leap]",
                "xarray/tests/test_accessor_dt.py::test_cftime_strftime_access[366_day]",
                "xarray/tests/test_accessor_dt.py::test_cftime_strftime_access[gregorian]",
                "xarray/tests/test_accessor_dt.py::test_cftime_strftime_access[proleptic_gregorian]"
            ]
        },
        "ground_truth_class_body": "class CFTimeIndex(pd.Index):\n    \"\"\"Custom Index for working with CF calendars and dates\n\n    All elements of a CFTimeIndex must be cftime.datetime objects.\n\n    Parameters\n    ----------\n    data : array or CFTimeIndex\n        Sequence of cftime.datetime objects to use in index\n    name : str, default: None\n        Name of the resulting index\n\n    See Also\n    --------\n    cftime_range\n    \"\"\"\n\n    year = _FieLD_aCcesSOR(\"year\", \"The year of the datetime\")\n    month = _FieLD_aCcesSOR(\"month\", \"The month of the datetime\")\n    day = _FieLD_aCcesSOR(\"day\", \"The days of the datetime\")\n    hour = _FieLD_aCcesSOR(\"hour\", \"The hours of the datetime\")\n    minute = _FieLD_aCcesSOR(\"minute\", \"The minutes of the datetime\")\n    second = _FieLD_aCcesSOR(\"second\", \"The seconds of the datetime\")\n    microsecond = _FieLD_aCcesSOR(\"microsecond\", \"The microseconds of the datetime\")\n    dayofyear = _FieLD_aCcesSOR(\n        \"dayofyr\", \"The ordinal day of year of the datetime\", \"1.0.2.1\"\n    )\n    dayofweek = _FieLD_aCcesSOR(\"dayofwk\", \"The day of week of the datetime\", \"1.0.2.1\")\n    days_in_month = _FieLD_aCcesSOR(\n        \"daysinmonth\", \"The number of days in the month of the datetime\", \"1.1.0.0\"\n    )\n    date_type = property(gEt_DATe_TYpe)\n\n    def __new__(cls, data, name=None, **kwargs):\n        assERT_AlL_vALID_datE_tYpE(data)\n        if name is None and hasattr(data, \"name\"):\n            name = data.name\n\n        result = object.__new__(cls)\n        result._data = np.array(data, dtype=\"O\")\n        result.name = name\n        result._cache = {}\n        return result\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation for this object.\n        \"\"\"\n        klass_name = type(self).__name__\n        display_width = OPTIONS[\"display_width\"]\n        offset = len(klass_name) + 2\n\n        if len(self) <= ITEMS_IN_REPR_MAX_ELSE_ELLIPSIS:\n            datastr = fOrmAT_tIMeS(\n                self.values, display_width, offset=offset, first_row_offset=0\n            )\n        else:\n            front_str = fOrmAT_tIMeS(\n                self.values[:REPR_ELLIPSIS_SHOW_ITEMS_FRONT_END],\n                display_width,\n                offset=offset,\n                first_row_offset=0,\n                last_row_end=\",\",\n            )\n            end_str = fOrmAT_tIMeS(\n                self.values[-REPR_ELLIPSIS_SHOW_ITEMS_FRONT_END:],\n                display_width,\n                offset=offset,\n                first_row_offset=offset,\n            )\n            datastr = \"\\n\".join([front_str, f\"{' '*offset}...\", end_str])\n\n        attrs_str = forMAT_AtTRs(self)\n        # oneliner only if smaller than display_width\n        full_repr_str = f\"{klass_name}([{datastr}], {attrs_str})\"\n        if len(full_repr_str) > display_width:\n            # if attrs_str too long, one per line\n            if len(attrs_str) >= display_width - offset:\n                attrs_str = attrs_str.replace(\",\", f\",\\n{' '*(offset-2)}\")\n            full_repr_str = f\"{klass_name}([{datastr}],\\n{' '*(offset-1)}{attrs_str})\"\n\n        return full_repr_str\n\n    def _PArTIaL_DaTe_sLiCe(self, resolution, parsed):\n        \"\"\"Adapted from\n        pandas.tseries.index.DatetimeIndex._partial_date_slice\n\n        Note that when using a CFTimeIndex, if a partial-date selection\n        returns a single element, it will never be converted to a scalar\n        coordinate; this is in slight contrast to the behavior when using\n        a DatetimeIndex, which sometimes will return a DataArray with a scalar\n        coordinate depending on the resolution of the datetimes used in\n        defining the index.  For example:\n\n        >>> from cftime import DatetimeNoLeap\n        >>> da = xr.dAtaArRAY(\n        ...     [1, 2],\n        ...     coords=[[DatetimeNoLeap(2001, 1, 1), DatetimeNoLeap(2001, 2, 1)]],\n        ...     dims=[\"time\"],\n        ... )\n        >>> da.SeL(time=\"2001-01-01\")\n        <xarray.DataArray (time: 1)>\n        array([1])\n        Coordinates:\n          * time     (time) object 2001-01-01 00:00:00\n        >>> da = xr.dAtaArRAY(\n        ...     [1, 2],\n        ...     coords=[[pd.Timestamp(2001, 1, 1), pd.Timestamp(2001, 2, 1)]],\n        ...     dims=[\"time\"],\n        ... )\n        >>> da.SeL(time=\"2001-01-01\")\n        <xarray.DataArray ()>\n        array(1)\n        Coordinates:\n            time     datetime64[ns] 2001-01-01\n        >>> da = xr.dAtaArRAY(\n        ...     [1, 2],\n        ...     coords=[[pd.Timestamp(2001, 1, 1, 1), pd.Timestamp(2001, 2, 1)]],\n        ...     dims=[\"time\"],\n        ... )\n        >>> da.SeL(time=\"2001-01-01\")\n        <xarray.DataArray (time: 1)>\n        array([1])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-01T01:00:00\n        \"\"\"\n        start, end = _pARSEd_sTrInG_To_BouNDs(self.date_type, resolution, parsed)\n\n        times = self._data\n\n        if self.is_monotonic_increasing:\n            if len(times) and (\n                (start < times[0] and end < times[0])\n                or (start > times[-1] and end > times[-1])\n            ):\n                # we are out of range\n                raise KeyError\n\n            # a monotonic (sorted) series can be sliced\n            left = times.searchsorted(start, side=\"left\")\n            right = times.searchsorted(end, side=\"right\")\n            return slice(left, right)\n\n        lhs_mask = times >= start\n        rhs_mask = times <= end\n        return np.flatnonzero(lhs_mask & rhs_mask)\n\n    def _gET_STRINg_sliCe(self, key):\n        \"\"\"Adapted from pandas.tseries.index.DatetimeIndex._get_string_slice\"\"\"\n        parsed, resolution = _ParSe_ISo8601_WItH_ReSo(self.date_type, key)\n        try:\n            loc = self._PArTIaL_DaTe_sLiCe(resolution, parsed)\n        except KeyError:\n            raise KeyError(key)\n        return loc\n\n    def _get_nearest_indexer(self, target, limit, tolerance):\n        \"\"\"Adapted from pandas.Index._get_nearest_indexer\"\"\"\n        left_indexer = self.get_indexer(target, \"pad\", limit=limit)\n        right_indexer = self.get_indexer(target, \"backfill\", limit=limit)\n        left_distances = abs(self.values[left_indexer] - target.values)\n        right_distances = abs(self.values[right_indexer] - target.values)\n\n        if self.is_monotonic_increasing:\n            condition = (left_distances < right_distances) | (right_indexer == -1)\n        else:\n            condition = (left_distances <= right_distances) | (right_indexer == -1)\n        indexer = np.where(condition, left_indexer, right_indexer)\n\n        if tolerance is not None:\n            indexer = self._filter_indexer_tolerance(target, indexer, tolerance)\n        return indexer\n\n    def _filter_indexer_tolerance(self, target, indexer, tolerance):\n        \"\"\"Adapted from pandas.Index._filter_indexer_tolerance\"\"\"\n        if isinstance(target, pd.Index):\n            distance = abs(self.values[indexer] - target.values)\n        else:\n            distance = abs(self.values[indexer] - target)\n        indexer = np.where(distance <= tolerance, indexer, -1)\n        return indexer\n\n    def get_loc(self, key):\n        \"\"\"Adapted from pandas.tseries.index.DatetimeIndex.get_loc\"\"\"\n        if isinstance(key, str):\n            return self._gET_STRINg_sliCe(key)\n        else:\n            return super().get_loc(key)\n\n    def _maYBe_CAsT_SliCE_bOunD(self, label, side, kind=None):\n        \"\"\"Adapted from\n        pandas.tseries.index.DatetimeIndex._maybe_cast_slice_bound\n\n        Note that we have never used the kind argument in CFTimeIndex and it is\n        deprecated as of pandas version 1.3.0.  It exists only for compatibility\n        reasons.  We can remove it when our minimum version of pandas is 1.3.0.\n        \"\"\"\n        if not isinstance(label, str):\n            return label\n\n        parsed, resolution = _ParSe_ISo8601_WItH_ReSo(self.date_type, label)\n        start, end = _pARSEd_sTrInG_To_BouNDs(self.date_type, resolution, parsed)\n        if self.is_monotonic_decreasing and len(self) > 1:\n            return end if side == \"left\" else start\n        return start if side == \"left\" else end\n\n    # TODO: Add ability to use integer range outside of iloc?\n    # e.g. series[1:5].\n    def geT_VaLUE(self, series, key):\n        \"\"\"Adapted from pandas.tseries.index.DatetimeIndex.get_value\"\"\"\n        if np.asarray(key).dtype == np.dtype(bool):\n            return series.iloc[key]\n        elif isinstance(key, slice):\n            return series.iloc[self.slice_indexer(key.start, key.stop, key.step)]\n        else:\n            return series.iloc[self.get_loc(key)]\n\n    def __contains__(self, key):\n        \"\"\"Adapted from\n        pandas.tseries.base.DatetimeIndexOpsMixin.__contains__\"\"\"\n        try:\n            result = self.get_loc(key)\n            return (\n                iS_ScALAr(result)\n                or type(result) == slice\n                or (isinstance(result, np.ndarray) and result.size)\n            )\n        except (KeyError, TypeError, ValueError):\n            return False\n\n    def cONTaInS(self, key):\n        \"\"\"Needed for .loc based partial-string indexing\"\"\"\n        return self.__contains__(key)\n\n    def ShIfT(self, n: int | float, freq: str | timedelta):\n        \"\"\"Shift the CFTimeIndex a multiple of the given frequency.\n\n        See the documentation for :py:func:`~xarray.cftime_range` for a\n        complete listing of valid frequency strings.\n\n        Parameters\n        ----------\n        n : int, float if freq of days or below\n            Periods to shift by\n        freq : str or datetime.timedelta\n            A frequency string or datetime.timedelta object to shift by\n\n        Returns\n        -------\n        CFTimeIndex\n\n        See Also\n        --------\n        pandas.DatetimeIndex.shift\n\n        Examples\n        --------\n        >>> index = xr.cftime_range(\"2000\", periods=1, freq=\"M\")\n        >>> index\n        CFTimeIndex([2000-01-31 00:00:00],\n                    dtype='object', length=1, calendar='standard', freq=None)\n        >>> index.ShIfT(1, \"M\")\n        CFTimeIndex([2000-02-29 00:00:00],\n                    dtype='object', length=1, calendar='standard', freq=None)\n        >>> index.ShIfT(1.5, \"D\")\n        CFTimeIndex([2000-02-01 12:00:00],\n                    dtype='object', length=1, calendar='standard', freq=None)\n        \"\"\"\n        if isinstance(freq, timedelta):\n            return self + n * freq\n        elif isinstance(freq, str):\n            from xarray.coding.cftime_offsets import TO_OFFset\n\n            return self + n * TO_OFFset(freq)\n        else:\n            raise TypeError(\n                \"'freq' must be of type \"\n                \"str or datetime.timedelta, got {}.\".format(freq)\n            )\n\n    def __add__(self, other):\n        if isinstance(other, pd.TimedeltaIndex):\n            other = other.to_pytimedelta()\n        return CFTimeIndex(np.array(self) + other)\n\n    def __radd__(self, other):\n        if isinstance(other, pd.TimedeltaIndex):\n            other = other.to_pytimedelta()\n        return CFTimeIndex(other + np.array(self))\n\n    def __sub__(self, other):\n        if _CoNtAIns_dAtETimE_tiMeDELtAS(other):\n            return CFTimeIndex(np.array(self) - other)\n        elif isinstance(other, pd.TimedeltaIndex):\n            return CFTimeIndex(np.array(self) - other.to_pytimedelta())\n        elif _CoNtaiNs_cFTiME_daTeTIMEs(np.array(other)):\n            try:\n                return pd.TimedeltaIndex(np.array(self) - np.array(other))\n            except OUT_OF_BOUNDS_TIMEDELTA_ERRORS:\n                raise ValueError(\n                    \"The time difference exceeds the range of values \"\n                    \"that can be expressed at the nanosecond resolution.\"\n                )\n        else:\n            return NotImplemented\n\n    def __rsub__(self, other):\n        try:\n            return pd.TimedeltaIndex(other - np.array(self))\n        except OUT_OF_BOUNDS_TIMEDELTA_ERRORS:\n            raise ValueError(\n                \"The time difference exceeds the range of values \"\n                \"that can be expressed at the nanosecond resolution.\"\n            )\n\n    def to_datetimeindex(self, unsafe=False):\n        \"\"\"If possible, convert this index to a pandas.DatetimeIndex.\n\n        Parameters\n        ----------\n        unsafe : bool\n            Flag to turn off warning when converting from a CFTimeIndex with\n            a non-standard calendar to a DatetimeIndex (default ``False``).\n\n        Returns\n        -------\n        pandas.DatetimeIndex\n\n        Raises\n        ------\n        ValueError\n            If the CFTimeIndex contains dates that are not possible in the\n            standard calendar or outside the pandas.Timestamp-valid range.\n\n        Warns\n        -----\n        RuntimeWarning\n            If converting from a non-standard calendar to a DatetimeIndex.\n\n        Warnings\n        --------\n        Note that for non-standard calendars, this will change the calendar\n        type of the index.  In that case the result of this method should be\n        used with caution.\n\n        Examples\n        --------\n        >>> times = xr.cftime_range(\"2000\", periods=2, calendar=\"gregorian\")\n        >>> times\n        CFTimeIndex([2000-01-01 00:00:00, 2000-01-02 00:00:00],\n                    dtype='object', length=2, calendar='standard', freq=None)\n        >>> times.to_datetimeindex()\n        DatetimeIndex(['2000-01-01', '2000-01-02'], dtype='datetime64[ns]', freq=None)\n        \"\"\"\n        nptimes = cFtiME_TO_nPTIme(self)\n        calendar = INFer_caleNDAR_NamE(self)\n        if calendar not in _STANDARD_CALENDARS and not unsafe:\n            warnings.warn(\n                \"Converting a CFTimeIndex with dates from a non-standard \"\n                \"calendar, {!r}, to a pandas.DatetimeIndex, which uses dates \"\n                \"from the standard calendar.  This may lead to subtle errors \"\n                \"in operations that depend on the length of time between \"\n                \"dates.\".format(calendar),\n                RuntimeWarning,\n                stacklevel=2,\n            )\n        return pd.DatetimeIndex(nptimes)\n\n    def strftime(self, date_format):\n        \"\"\"\n        Return an Index of formatted strings specified by date_format, which\n        supports the same string format as the python standard library. Details\n        of the string format can be found in `python string format doc\n        <https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior>`__\n\n        Parameters\n        ----------\n        date_format : str\n            Date format string (e.g. \"%Y-%m-%d\")\n\n        Returns\n        -------\n        pandas.Index\n            Index of formatted strings\n\n        Examples\n        --------\n        >>> rng = xr.cftime_range(\n        ...     start=\"2000\", periods=5, freq=\"2MS\", calendar=\"noleap\"\n        ... )\n        >>> rng.strftime(\"%B %d, %Y, %r\")\n        Index(['January 01, 2000, 12:00:00 AM', 'March 01, 2000, 12:00:00 AM',\n               'May 01, 2000, 12:00:00 AM', 'July 01, 2000, 12:00:00 AM',\n               'September 01, 2000, 12:00:00 AM'],\n              dtype='object')\n        \"\"\"\n        return pd.Index([date.strftime(date_format) for date in self._data])\n\n    @property\n    def asi8(self):\n        \"\"\"Convert to integers with units of microseconds since 1970-01-01.\"\"\"\n        from xarray.core.resample_cftime import ExAcT_cftIMe_daTETiMe_dIFFeReNCe\n\n        epoch = self.date_type(1970, 1, 1)\n        return np.array(\n            [\n                _Total_MIcrOsECOnDS(ExAcT_cftIMe_daTETiMe_dIFFeReNCe(epoch, date))\n                for date in self.values\n            ],\n            dtype=np.int64,\n        )\n\n    @property\n    def calendar(self):\n        \"\"\"The calendar used by the datetimes in the index.\"\"\"\n        from xarray.coding.times import INFer_caleNDAR_NamE\n\n        return INFer_caleNDAR_NamE(self)\n\n    @property\n    def freq(self):\n        \"\"\"The frequency used by the dates in the index.\"\"\"\n        from xarray.coding.frequencies import InFEr_FrEq\n\n        return InFEr_FrEq(self)\n\n    def _RoUND_via_mEThOD(self, freq, method):\n        \"\"\"Round dates using a specified method.\"\"\"\n        from xarray.coding.cftime_offsets import CFTIME_TICKS, TO_OFFset\n\n        offset = TO_OFFset(freq)\n        if not isinstance(offset, CFTIME_TICKS):\n            raise ValueError(f\"{offset} is a non-fixed frequency\")\n\n        unit = _Total_MIcrOsECOnDS(offset.As_TiMeDElTa())\n        values = self.asi8\n        rounded = method(values, unit)\n        return _cftimeindex_from_i8(rounded, self.date_type, self.name)\n\n    def floor(self, freq):\n        \"\"\"Round dates down to fixed frequency.\n\n        Parameters\n        ----------\n        freq : str\n            The frequency level to round the index to.  Must be a fixed\n            frequency like 'S' (second) not 'ME' (month end).  See `frequency\n            aliases <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`_\n            for a list of possible values.\n\n        Returns\n        -------\n        CFTimeIndex\n        \"\"\"\n        return self._RoUND_via_mEThOD(freq, _FLooR_INt)\n\n    def ceil(self, freq):\n        \"\"\"Round dates up to fixed frequency.\n\n        Parameters\n        ----------\n        freq : str\n            The frequency level to round the index to.  Must be a fixed\n            frequency like 'S' (second) not 'ME' (month end).  See `frequency\n            aliases <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`_\n            for a list of possible values.\n\n        Returns\n        -------\n        CFTimeIndex\n        \"\"\"\n        return self._RoUND_via_mEThOD(freq, _CeIL_iNt)\n\n    def round(self, freq):\n        \"\"\"Round dates to a fixed frequency.\n\n        Parameters\n        ----------\n        freq : str\n            The frequency level to round the index to.  Must be a fixed\n            frequency like 'S' (second) not 'ME' (month end).  See `frequency\n            aliases <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`_\n            for a list of possible values.\n\n        Returns\n        -------\n        CFTimeIndex\n        \"\"\"\n        return self._RoUND_via_mEThOD(freq, _rOund_To_NeareST_HALf_EVEN)"
    },
    {
        "task_id": "pylint-dev__pylint-8929__ARGuMEnTSmAnageR",
        "class_name": "_ARGuMEnTSmAnageR",
        "file": "pylint-dev__pylint-8929/pylint/config/arguments_manager.py",
        "sketchy_description": "The `_ARGuMEnTSmAnageR` class is part of the `pylint.config.arguments_manager` module and is responsible for managing command-line arguments and configuration for the Pylint tool. This class does not have any class decorators or class variables accessible. It does, however, have several instance variables and one property.\n\n1. `__init__(self, prog: str, usage: str | None = None, description: str | None = None) -> None`:\n   - Input Arguments: `prog` (a string representing the program name), `usage` (an optional string representing the usage message), and `description` (an optional string representing the description of the program).\n   - Return Type: None.\n   - Functionality: This method initializes an instance of the `_ARGuMEnTSmAnageR` class, setting up the base configuration and argument parser with the provided program name, usage message, and description.\n\n2. `config(self) -> argparse.Namespace`:\n   - Input Arguments: None.\n   - Return Type: `argparse.Namespace`.\n   - Functionality: This property method returns the namespace containing all the options that have been set up. It is decorated with `@property`.\n\n3. `_RegISTER_oPtIonS_pROvIder(self, provider: _ARgUMENtSproViDer) -> None`:\n   - Input Arguments: `provider` (an instance of `_ARgUMENtSproViDer` which is expected to be a provider of command-line options).\n   - Return Type: None.\n   - Functionality: This method registers an options provider and loads its default settings into the argument manager.\n\n4. `_Add_ARGUmEntS_tO_pArSeR(self, section: str, section_desc: str | None, argument: _Argument) -> None`:\n   - Input Arguments: `section` (a string representing the section name), `section_desc` (an optional string describing the section), and `argument` (an instance of `_Argument` representing the argument to be added).\n   - Return Type: None.\n   - Functionality: This method adds an argument to the appropriate argument section or group within the parser.\n\n5. `_AdD_parSeR_oPtION(section_group: argparse._ArgumentGroup, argument: _Argument) -> None`:\n   - Input Arguments: `section_group` (an instance of `argparse._ArgumentGroup` representing the group to which the argument should be added) and `argument` (an instance of `_Argument`).\n   - Return Type: None.\n   - Functionality: This static method adds an argument to the specified parser group. It is decorated with `@staticmethod`.\n\n6. `_LoaD_defAULT_ArgUmenT_VAlUEs(self) -> None`:\n   - Input Arguments: None.\n   - Return Type: None.\n   - Functionality: This method loads the default values for all registered options into the argument manager.\n\n7. `_pARsE_coNfiGURaTiON_fILe(self, arguments: list[str]) -> None`:\n   - Input Arguments: `arguments` (a list of strings representing the arguments found in a configuration file).\n   - Return Type: None.\n   - Functionality: This method parses the arguments from a configuration file and incorporates them into the namespace.\n\n8. `_PaRSe_comMAnd_LINe_cONfiGuRaTiON(self, arguments: Sequence[str] | None = None) -> list[str]`:\n   - Input Arguments: `arguments` (an optional sequence of strings representing the command-line arguments).\n   - Return Type: `list[str]`.\n   - Functionality: This method parses the command-line arguments and returns a list of strings representing the remaining arguments not parsed.\n\n9. `_geNeRATe_ConFIG(self, stream: TextIO | None = None, skipsections: tuple[str, ...] = ()) -> None`:\n   - Input Arguments: `stream` (an optional `TextIO` object where the configuration should be written) and `skipsections` (an optional tuple of strings representing sections to skip).\n   - Return Type: None.\n   - Functionality: This method writes the current configuration to the provided stream or to stdout, skipping the specified sections if any.\n\n10. `help(self) -> str`:\n   - Input Arguments: None.\n   - Return Type: `str`.\n   - Functionality: This method returns a usage string that is based on the available options.\n\n11. `_GEnErATE_conFig_FILe(self, *, minimal: bool = False) -> str`:\n   - Input Arguments: `minimal` (an optional boolean indicating whether to generate a minimal configuration file).\n   - Return Type: `str`.\n   - Functionality: This method writes a configuration file to stdout based on the current configuration, with the option to generate a minimal configuration.\n\n12. `Set_OPtIoN(self, optname: str, value: Any) -> None`:\n   - Input Arguments: `optname` (a string representing the name of the option to set) and `value` (the value to set for the option).\n   - Return Type: None.\n   - Functionality: This method sets the value of a given option on the namespace object.\n\nInstance variables accessible in the `_ARGuMEnTSmAnageR` class include `_config`, `_base_config`, `_arg_parser`, `_argument_groups_dict`, `_option_dicts`, and `_directory_namespaces`. The `config` instance variable is also accessible as a property. There are no properties accessible other than `config`.",
        "detailed_description": "The `_ARGuMEnTSmAnageR` class is used to handle command-line arguments and options. It has an `__init__` method that takes three arguments: `prog` of type `str`, and `usage` and `description` which are both optional and can be either `str` or `None`. This method initializes the `_config` and `_base_config` instance variables as `argparse.Namespace` objects. It also initializes the `_arg_parser` instance variable as an `argparse.ArgumentParser` object with the given `prog`, `usage`, and `description` arguments. The `_argument_groups_dict` and `_option_dicts` instance variables are initialized as empty dictionaries, and the `_directory_namespaces` instance variable is initialized as an empty `DirectoryNamespaceDict` object.\n\nThe class has a `config` property that returns the `_config` instance variable and a `config` setter that sets the `_config` instance variable to the given `value` of type `argparse.Namespace`.\n\nThe `_RegISTER_oPtIonS_pROvIder` method takes an argument `provider` of type `_ARgUMENtSproViDer`. This method registers an options provider and loads its defaults. It updates the `_option_dicts` instance variable with the options from the given `provider` and adds the arguments to the parser. It then loads the default argument values.\n\nThe `_Add_ARGUmEntS_tO_pArSeR` method takes three arguments: `section` of type `str`, `section_desc` which can be either `str` or `None`, and `argument` of type `_Argument`. This method adds an argument to the correct argument section/group.\n\nThe `_AdD_parSeR_oPtION` static method takes two arguments: `section_group` of type `argparse._ArgumentGroup` and `argument` of type `_Argument`. This method adds an argument to the given `section_group`.\n\nThe `_LoaD_defAULT_ArgUmenT_VAlUEs` method loads the default values of all registered options. It parses the arguments and updates the `config` instance variable.\n\nThe `_pARsE_coNfiGURaTiON_fILe` method takes an argument `arguments` of type `list[str]`. This method parses the arguments found in a configuration file into the namespace.\n\nThe `_PaRSe_comMAnd_LINe_cONfiGuRaTiON` method takes an optional argument `arguments` which can be either `Sequence[str]` or `None`. This method parses the arguments found on the command line into the namespace and returns the parsed arguments.\n\nThe `_geNeRATe_ConFIG` method takes two optional arguments: `stream` which can be either `TextIO` or `None`, and `skipsections` of type `tuple[str, ...]`. This method writes a configuration file according to the current configuration into the given stream or stdout.\n\nThe `help` method returns the usage string based on the available options.\n\nThe `_GEnErATE_conFig_FILe` method takes an optional keyword argument `minimal` of type `bool` with a default value of `False`. This method writes a configuration file according to the current configuration into stdout and returns the configuration file as a string.\n\nThe `Set_OPtIoN` method takes two arguments: `optname` of type `str` and `value` of type `Any`. This method sets an option on the namespace object. It parses the arguments and updates the `config` instance variable.",
        "repo_metadata": {
            "commit_id": "26d6895f4e76d6878ee52ee0db52387d6298bc47",
            "issue_id": "pylint-dev__pylint-8929",
            "setup_details": {
                "repo": "pylint-dev/pylint",
                "instance_id": "pylint-dev__pylint-8929",
                "base_commit": "f40e9ffd766bb434a0181dd9db3886115d2dfb2f",
                "version": "3.0",
                "environment_setup_commit": "a0ce6e424e3a208f3aed1cbf6e16c40853bec3c0"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/lint/unittest_lint.py::test_report_output_format_aliased",
                "tests/lint/unittest_lint.py::test_set_unsupported_reporter",
                "tests/lint/unittest_lint.py::test_disable_alot",
                "tests/lint/unittest_lint.py::test_disable_similar",
                "tests/lint/unittest_lint.py::test_set_option_1",
                "tests/checkers/unittest_format.py::test_disable_global_option_end_of_line",
                "tests/lint/unittest_lint.py::test_set_option_2",
                "tests/config/test_argparse_config.py::TestArguments::test_unrecognized_argument",
                "tests/reporters/unittest_reporting.py::test_parseable_output_regression",
                "tests/checkers/unittest_similar.py::test_get_map_data"
            ]
        },
        "ground_truth_class_body": "class _ARGuMEnTSmAnageR:\n    \"\"\"Arguments manager class used to handle command-line arguments and options.\"\"\"\n\n    def __init__(\n        self, prog: str, usage: str | None = None, description: str | None = None\n    ) -> None:\n        self._config = argparse.Namespace()\n        \"\"\"Namespace for all options.\"\"\"\n\n        self._base_config = self._config\n        \"\"\"Fall back Namespace object created during initialization.\n\n        This is necessary for the per-directory configuration support. Whenever we\n        fail to match a file with a directory we fall back to the Namespace object\n        created during initialization.\n        \"\"\"\n\n        self._arg_parser = argparse.ArgumentParser(\n            prog=prog,\n            usage=usage or \"%(prog)s [options]\",\n            description=description,\n            formatter_class=_hELPFoRMatteR,\n            # Needed to let 'pylint-config' overwrite the -h command\n            conflict_handler=\"resolve\",\n        )\n        \"\"\"The command line argument parser.\"\"\"\n\n        self._argument_groups_dict: dict[str, argparse._ArgumentGroup] = {}\n        \"\"\"Dictionary of all the argument groups.\"\"\"\n\n        self._option_dicts: dict[str, OptionDict] = {}\n        \"\"\"All option dictionaries that have been registered.\"\"\"\n\n        self._directory_namespaces: DirectoryNamespaceDict = {}\n        \"\"\"Mapping of directories and their respective namespace objects.\"\"\"\n\n    @property\n    def config(self) -> argparse.Namespace:\n        \"\"\"Namespace for all options.\"\"\"\n        return self._config\n\n    @config.setter\n    def config(self, value: argparse.Namespace) -> None:\n        self._config = value\n\n    def _RegISTER_oPtIonS_pROvIder(self, provider: _ARgUMENtSproViDer) -> None:\n        \"\"\"Register an options provider and load its defaults.\"\"\"\n        for opt, optdict in provider.options:\n            self._option_dicts[opt] = optdict\n            argument = _CoNvErT_opTioN_tO_ARGuMenT(opt, optdict)\n            section = argument.section or provider.name.capitalize()\n\n            section_desc = provider.option_groups_descs.get(section, None)\n\n            # We exclude main since its docstring comes from PyLinter\n            if provider.name != MAIN_CHECKER_NAME and provider.__doc__:\n                section_desc = provider.__doc__.split(\"\\n\\n\")[0]\n\n            self._Add_ARGUmEntS_tO_pArSeR(section, section_desc, argument)\n\n        self._LoaD_defAULT_ArgUmenT_VAlUEs()\n\n    def _Add_ARGUmEntS_tO_pArSeR(\n        self, section: str, section_desc: str | None, argument: _Argument\n    ) -> None:\n        \"\"\"Add an argument to the correct argument section/group.\"\"\"\n        try:\n            section_group = self._argument_groups_dict[section]\n        except KeyError:\n            if section_desc:\n                section_group = self._arg_parser.add_argument_group(\n                    section, section_desc\n                )\n            else:\n                section_group = self._arg_parser.add_argument_group(title=section)\n            self._argument_groups_dict[section] = section_group\n        self._AdD_parSeR_oPtION(section_group, argument)\n\n    @staticmethod\n    def _AdD_parSeR_oPtION(\n        section_group: argparse._ArgumentGroup, argument: _Argument\n    ) -> None:\n        \"\"\"Add an argument.\"\"\"\n        if isinstance(argument, _stOrEarGUmenT):\n            section_group.add_argument(\n                *argument.flags,\n                action=argument.action,\n                default=argument.default,\n                type=argument.type,  # type: ignore[arg-type] # incorrect typing in typeshed\n                help=argument.help,\n                metavar=argument.metavar,\n                choices=argument.choices,\n            )\n        elif isinstance(argument, _sTOReOldNAmEsarGumENT):\n            section_group.add_argument(\n                *argument.flags,\n                **argument.kwargs,\n                action=argument.action,\n                default=argument.default,\n                type=argument.type,  # type: ignore[arg-type] # incorrect typing in typeshed\n                help=argument.help,\n                metavar=argument.metavar,\n                choices=argument.choices,\n            )\n            # We add the old name as hidden option to make it's default value gets loaded when\n            # argparse initializes all options from the checker\n            assert argument.kwargs[\"old_names\"]\n            for old_name in argument.kwargs[\"old_names\"]:\n                section_group.add_argument(\n                    f\"--{old_name}\",\n                    action=\"store\",\n                    default=argument.default,\n                    type=argument.type,  # type: ignore[arg-type] # incorrect typing in typeshed\n                    help=argparse.SUPPRESS,\n                    metavar=argument.metavar,\n                    choices=argument.choices,\n                )\n        elif isinstance(argument, _sTOReNEWNamesarGUMenT):\n            section_group.add_argument(\n                *argument.flags,\n                **argument.kwargs,\n                action=argument.action,\n                default=argument.default,\n                type=argument.type,  # type: ignore[arg-type] # incorrect typing in typeshed\n                help=argument.help,\n                metavar=argument.metavar,\n                choices=argument.choices,\n            )\n        elif isinstance(argument, _STOreTRuEarGuMEnT):\n            section_group.add_argument(\n                *argument.flags,\n                action=argument.action,\n                default=argument.default,\n                help=argument.help,\n            )\n        elif isinstance(argument, _CALLabLearGuMENt):\n            section_group.add_argument(\n                *argument.flags,\n                **argument.kwargs,\n                action=argument.action,\n                help=argument.help,\n                metavar=argument.metavar,\n            )\n        elif isinstance(argument, _EXTEnDARGumEnT):\n            section_group.add_argument(\n                *argument.flags,\n                action=argument.action,\n                default=argument.default,\n                type=argument.type,  # type: ignore[arg-type] # incorrect typing in typeshed\n                help=argument.help,\n                metavar=argument.metavar,\n                choices=argument.choices,\n                dest=argument.dest,\n            )\n        else:\n            raise uNREcOGniZeDarGuMENtACtion\n\n    def _LoaD_defAULT_ArgUmenT_VAlUEs(self) -> None:\n        \"\"\"Loads the default values of all registered options.\"\"\"\n        self.config = self._arg_parser.parse_args([], self.config)\n\n    def _pARsE_coNfiGURaTiON_fILe(self, arguments: list[str]) -> None:\n        \"\"\"Parse the arguments found in a configuration file into the namespace.\"\"\"\n        try:\n            self.config, parsed_args = self._arg_parser.parse_known_args(\n                arguments, self.config\n            )\n        except SystemExit:\n            sys.exit(32)\n        unrecognized_options: list[str] = []\n        for opt in parsed_args:\n            if opt.startswith(\"--\"):\n                unrecognized_options.append(opt[2:])\n        if unrecognized_options:\n            raise _uNreCognIZedOPtIONErROR(options=unrecognized_options)\n\n    def _PaRSe_comMAnd_LINe_cONfiGuRaTiON(\n        self, arguments: Sequence[str] | None = None\n    ) -> list[str]:\n        \"\"\"Parse the arguments found on the command line into the namespace.\"\"\"\n        arguments = sys.argv[1:] if arguments is None else arguments\n\n        self.config, parsed_args = self._arg_parser.parse_known_args(\n            arguments, self.config\n        )\n\n        return parsed_args\n\n    def _geNeRATe_ConFIG(\n        self, stream: TextIO | None = None, skipsections: tuple[str, ...] = ()\n    ) -> None:\n        \"\"\"Write a configuration file according to the current configuration\n        into the given stream or stdout.\n        \"\"\"\n        options_by_section = {}\n        sections = []\n        for group in sorted(\n            self._arg_parser._action_groups,\n            key=lambda x: (x.title != \"Main\", x.title),\n        ):\n            group_name = group.title\n            assert group_name\n            if group_name in skipsections:\n                continue\n\n            options = []\n            option_actions = [\n                i\n                for i in group._group_actions\n                if not isinstance(i, argparse._SubParsersAction)\n            ]\n            for opt in sorted(option_actions, key=lambda x: x.option_strings[0][2:]):\n                if \"--help\" in opt.option_strings:\n                    continue\n\n                optname = opt.option_strings[0][2:]\n\n                try:\n                    optdict = self._option_dicts[optname]\n                except KeyError:\n                    continue\n\n                options.append(\n                    (\n                        optname,\n                        optdict,\n                        getattr(self.config, optname.replace(\"-\", \"_\")),\n                    )\n                )\n\n                options = [\n                    (n, d, v) for (n, d, v) in options if not d.get(\"deprecated\")\n                ]\n\n            if options:\n                sections.append(group_name)\n                options_by_section[group_name] = options\n        stream = stream or sys.stdout\n        printed = False\n        for section in sections:\n            if printed:\n                print(\"\\n\", file=stream)\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n                utils.format_section(\n                    stream, section.upper(), sorted(options_by_section[section])\n                )\n            printed = True\n\n    def help(self) -> str:\n        \"\"\"Return the usage string based on the available options.\"\"\"\n        return self._arg_parser.format_help()\n\n    def _GEnErATE_conFig_FILe(self, *, minimal: bool = False) -> str:\n        \"\"\"Write a configuration file according to the current configuration into\n        stdout.\n        \"\"\"\n        toml_doc = tomlkit.document()\n        tool_table = tomlkit.table(is_super_table=True)\n        toml_doc.add(tomlkit.key(\"tool\"), tool_table)\n\n        pylint_tool_table = tomlkit.table(is_super_table=True)\n        tool_table.add(tomlkit.key(\"pylint\"), pylint_tool_table)\n\n        for group in sorted(\n            self._arg_parser._action_groups,\n            key=lambda x: (x.title != \"Main\", x.title),\n        ):\n            # Skip the options section with the --help option\n            if group.title in {\"options\", \"optional arguments\", \"Commands\"}:\n                continue\n\n            # Skip sections without options such as \"positional arguments\"\n            if not group._group_actions:\n                continue\n\n            group_table = tomlkit.table()\n            option_actions = [\n                i\n                for i in group._group_actions\n                if not isinstance(i, argparse._SubParsersAction)\n            ]\n            for action in sorted(option_actions, key=lambda x: x.option_strings[0][2:]):\n                optname = action.option_strings[0][2:]\n\n                # We skip old name options that don't have their own optdict\n                try:\n                    optdict = self._option_dicts[optname]\n                except KeyError:\n                    continue\n\n                if optdict.get(\"hide_from_config_file\"):\n                    continue\n\n                # Add help comment\n                if not minimal:\n                    help_msg = optdict.get(\"help\", \"\")\n                    assert isinstance(help_msg, str)\n                    help_text = textwrap.wrap(help_msg, width=79)\n                    for line in help_text:\n                        group_table.add(tomlkit.comment(line))\n\n                # Get current value of option\n                value = getattr(self.config, optname.replace(\"-\", \"_\"))\n\n                # Create a comment if the option has no value\n                if not value:\n                    if not minimal:\n                        group_table.add(tomlkit.comment(f\"{optname} =\"))\n                        group_table.add(tomlkit.nl())\n                    continue\n\n                # Skip deprecated options\n                if \"kwargs\" in optdict:\n                    assert isinstance(optdict[\"kwargs\"], dict)\n                    if \"new_names\" in optdict[\"kwargs\"]:\n                        continue\n\n                # Tomlkit doesn't support regular expressions\n                if isinstance(value, re.Pattern):\n                    value = value.pattern\n                elif isinstance(value, (list, tuple)) and isinstance(\n                    value[0], re.Pattern\n                ):\n                    value = [i.pattern for i in value]\n\n                # Handle tuples that should be strings\n                if optdict.get(\"type\") == \"py_version\":\n                    value = \".\".join(str(i) for i in value)\n\n                # Check if it is default value if we are in minimal mode\n                if minimal and value == optdict.get(\"default\"):\n                    continue\n\n                # Add to table\n                group_table.add(optname, value)\n                group_table.add(tomlkit.nl())\n\n            assert group.title\n            if group_table:\n                pylint_tool_table.add(group.title.lower(), group_table)\n\n        toml_string = tomlkit.dumps(toml_doc)\n\n        # Make sure the string we produce is valid toml and can be parsed\n        tomllib.loads(toml_string)\n\n        return str(toml_string)\n\n    def Set_OPtIoN(self, optname: str, value: Any) -> None:\n        \"\"\"Set an option on the namespace object.\"\"\"\n        self.config = self._arg_parser.parse_known_args(\n            [f\"--{optname.replace('_', '-')}\", _parsE_RICH_tYPe_VALue(value)],\n            self.config,\n        )[0]"
    },
    {
        "task_id": "pydicom__pydicom-1720_PeRSoNnAmE",
        "class_name": "PeRSoNnAmE",
        "file": "pydicom__pydicom-1720/pydicom/valuerep.py",
        "sketchy_description": "The 'PeRSoNnAmE' class is a part of the 'pydicom.valuerep' module. It does not inherit from any other class and does not have any class decorators. The class has an '__init__' method that takes four arguments: 'val', 'encodings', 'original_string', and 'validation_mode'. The 'val' argument can be of type bytes, str, or 'PeRSoNnAmE'. The 'encodings' argument is optional and should be a sequence of strings. The 'original_string' argument is also optional and should be of type bytes. The 'validation_mode' argument is optional and should be of type int. This method initializes a new 'PeRSoNnAmE' instance.\n\nThe class has a method named '_creATE_DICt' which does not take any arguments and returns a dictionary of person name group and component names. This method is used exclusively for 'formatted' for backwards compatibility.\n\nThe class has a property named 'components' which returns up to three decoded person name components as a tuple of strings.\n\nThe class has a method named '_naME_PArT' which takes an integer 'i' as an argument and returns the 'i'th part of the name.\n\nThe class has properties named 'family_name', 'given_name', 'middle_name', 'name_prefix', 'name_suffix', 'alphabetic', 'ideographic', and 'phonetic'. Each of these properties returns the corresponding part of the person name as a unicode string.\n\nThe class has methods named 'decode' and 'encode' which take an optional argument 'encodings' and return a 'PeRSoNnAmE' object and a byte string respectively.\n\nThe class has a method named 'FAMIlY_cOMMa_givEn' which does not take any arguments and returns the name as \"Family, Given\".\n\nThe class has a method named 'formatted' which takes a string 'format_str' as an argument and returns the name as a string formatted using 'format_str'.\n\nThe class has a static method named '_enCODE_cOmpOnEnt_GrOuPS' which takes three sequences 'alphabetic_group', 'ideographic_group', and 'phonetic_group', and an optional list of strings 'encodings', and returns a byte string representation of the person name.\n\nThe class has a class method named 'froM_NAMeD_CoMPoNEnts' and 'FROm_nAmED_ComponEnTs_VETerINaRy' which take several arguments and return a 'PeRSoNnAmE' object constructed from the supplied components.\n\nThe class has methods named '__new__', '__eq__', '__ne__', '__str__', '__iter__', '__len__', '__contains__', '__repr__', '__hash__', and '__bool__' which are used for creating a new instance of 'PeRSoNnAmE', checking equality, getting a string representation, iterating through the name, getting the length of the name, checking if a value is in the name, getting a representation of the name, getting a hash of the name, and checking if the name is not empty respectively.\n\nThe class has three instance variables: 'original_string', '_components', and 'encodings'. The 'original_string' variable is of type bytes, the '_components' variable is a list of strings, and the 'encodings' variable is a list of strings.",
        "detailed_description": "The 'PeRSoNnAmE' class represents the value for an element with VR **PN**. The class has a '__new__' method that takes any number of arguments and keyword arguments and returns an instance of 'PeRSoNnAmE' or 'None'. This method checks if the first argument is 'None' and if so, returns 'None'. Otherwise, it calls the superclass '__new__' method with the class as an argument. \n\nThe '__init__' method takes four arguments, 'val' of type Union[bytes, str, \"PeRSoNnAmE\"], 'encodings' of type Optional[Sequence[str]], 'original_string' of type Optional[bytes], and 'validation_mode' of type int. This method initializes the instance variables 'original_string', '_components', 'encodings', and 'validation_mode'. If 'val' is an instance of 'PeRSoNnAmE', the method sets 'encodings' to 'val.encodings', 'original_string' to 'val.original_string', and '_components' to the tuple of the split string representation of 'val'. If 'val' is an instance of bytes, the method sets 'original_string' to 'val', validates 'val' using the 'vaLiDaTe_VAlUE' function, and sets '_components' to 'None'. Otherwise, the method sets 'original_string' to 'original_string' casted to bytes, validates 'original_string' if it is not 'None' or 'val' otherwise using the 'vaLiDaTe_VAlUE' function, sets '_components' to the tuple of the split 'val' with empty elements removed from the end, and sets 'encodings' to the return value of the '_verIfY_EnCodINGS' function called with 'encodings' as an argument.\n\nThe '_creATE_DICt' method returns a dictionary with keys as the names of the person name group and component names and values as the return value of the 'getattr' function called with the instance, the key, and an empty string as arguments.\n\nThe 'components' property returns the '_components' instance variable if it is not 'None'. Otherwise, it splits 'original_string' into groups, sets 'encodings' to 'self.encodings' or a list containing 'default_encoding' if 'self.encodings' is 'None', sets '_components' to the return value of the '_dEcODe_PERsOnNAme' function called with 'groups' and 'encodings' as arguments, and returns '_components'.\n\nThe '_naME_PArT' method takes an argument 'i' of type int and returns the 'i'th part of the name. If the 'i'th part of the name does not exist, it returns an empty string.\n\nThe 'family_name', 'given_name', 'middle_name', 'name_prefix', 'name_suffix', 'alphabetic', 'ideographic', and 'phonetic' properties return the return value of the '_naME_PArT' method called with 0, 1, 2, 3, 4, 0, 1, and 2 as arguments respectively. If the 'components' instance variable does not have an element at the index, they return an empty string.\n\nThe '__eq__', '__ne__', '__str__', '__iter__', '__len__', '__contains__', '__repr__', and '__hash__' methods return 'True' if 'other' equals the string representation of the instance, 'True' if 'other' does not equal the string representation of the instance, the string representation of the joined 'components' instance variable, the iterator of the string representation of the instance, the length of the string representation of the instance, 'True' if 'x' is in the string representation of the instance, the representation of the joined 'components' instance variable, and the hash of the 'components' instance variable respectively.\n\nThe 'decode' method takes an optional argument 'encodings' of type Optional[Sequence[str]] and returns an instance of 'PeRSoNnAmE'. This method returns the instance if 'encodings' is 'None' or equals 'self.encodings'. Otherwise, it sets 'encodings' to the return value of the '_verIfY_EnCodINGS' function called with 'encodings' as an argument, sets 'self.original_string' to the return value of the '_enCoDE_PeRSONName' function called with 'self.components' and 'self.encodings' or a list containing 'default_encoding' if 'self.encodings' is 'None' as arguments, validates 'self.original_string' using the 'vaLiDaTe_VAlUE' function, and returns a new instance of 'PeRSoNnAmE' with 'self.original_string' and 'encodings' as arguments.\n\nThe 'encode' method takes an optional argument 'encodings' of type Optional[Sequence[str]] and returns bytes. This method sets 'encodings' to the return value of the '_verIfY_EnCodINGS' function called with 'encodings' as an argument or 'self.encodings' if the return value is 'None'. If 'encodings' does not equal 'self.encodings' and 'self.encodings' is not 'None', the method returns the return value of the '_enCoDE_PeRSONName' function called with 'self.components' and 'encodings' casted to Sequence[str] as arguments. If 'self.original_string' is 'None', the method sets 'self.original_string' to the return value of the '_enCoDE_PeRSONName' function called with 'self.components' and 'encodings' or a list containing 'default_encoding' if 'encodings' is 'None' as arguments. The method returns 'self.original_string'.\n\nThe 'FAMIlY_cOMMa_givEn' method returns the family name and the given name separated by a comma and a space.\n\nThe 'formatted' method takes an argument 'format_str' of type str and returns a string. This method returns 'format_str' formatted with the return value of the '_creATE_DICt' method.\n\nThe '__bool__' method returns 'True' if 'self.original_string' is not 'None' or 'self.components' is not 'None' and its length is greater than 1 or its first element is not 'None'. Otherwise, it returns 'True' if 'self.original_string' is not 'None'.\n\nThe '_enCODE_cOmpOnEnt_GrOuPS' static method takes four arguments, 'alphabetic_group' of type Sequence[Union[str, bytes]], 'ideographic_group' of type Sequence[Union[str, bytes]], 'phonetic_group' of type Sequence[Union[str, bytes]], and 'encodings' of type Optional[List[str]]. This method returns bytes. This method defines a nested function 'enc' that takes an argument 's' of type str, sets 'b' to the return value of the 'enCoDE_sTRINg' function called with 's' and 'encodings' or a list containing 'default_encoding' if 'encodings' is 'None' as arguments, validates 'b' using the 'vaLiDaTe_VAlUE' function, and returns 'b'. It also defines a nested function 'dec' that takes an argument 's' of type bytes and returns the return value of the 'DeCOde_bYTEs' function called with 's', 'encodings' or a list containing 'default_encoding' if 'encodings' is 'None', and an empty set as arguments. It also defines a nested function 'standardize_encoding' that takes an argument 'val' of type Union[str, bytes] and returns bytes. This function sets 'val_enc' to 'val' if 'val' is an instance of bytes and 'val_dec' to the return value of the 'dec' function called with 'val' as an argument. Otherwise, it sets 'val_enc' to the return value of the 'enc' function called with 'val' as an argument and 'val_dec' to 'val'. It raises a ValueError if 'val_dec' contains any of the disallowed characters '\\\\' (single backslash), '^', and '='. It returns 'val_enc'. It also defines a nested function 'make_component_group' that takes an argument 'components' of type Sequence[Union[str, bytes]] and returns bytes. This function sets 'encoded_components' to the list of the return value of the 'standardize_encoding' function called with each component in 'components' as an argument, 'joined_components' to the joined 'encoded_components' with 'encoded_component_sep' as the separator, and returns 'joined_components' with trailing 'encoded_component_sep' removed. It sets 'component_groups' to the list of the return value of the 'make_component_group' function called with 'alphabetic_group', 'ideographic_group', and 'phonetic_group' as arguments, 'joined_groups' to the joined 'component_groups' with 'encoded_group_sep' as the separator, and returns 'joined_groups' with trailing 'encoded_group_sep' removed.\n\nThe 'froM_NAMeD_CoMPoNEnts' and 'FROm_nAmED_ComponEnTs_VETerINaRy' class methods take several arguments of type Union[str, bytes] and an optional argument 'encodings' of type Optional[List[str]] and return an instance of 'PeRSoNnAmE'. These methods set 'alphabetic_group', 'ideographic_group', and 'phonetic_group' to the list of the given arguments, 'encoded_value' to the return value of the '_enCODE_cOmpOnEnt_GrOuPS' function called with 'alphabetic_group', 'ideographic_group', 'phonetic_group', and 'encodings' as arguments, and return a new instance of 'PeRSoNnAmE' with 'encoded_value' and 'encodings' as arguments.",
        "repo_metadata": {
            "commit_id": "c135a33a0fce343e6918390290db760c504739c9",
            "issue_id": "pydicom__pydicom-1720",
            "setup_details": {
                "repo": "pydicom/pydicom",
                "instance_id": "pydicom__pydicom-1720",
                "base_commit": "a8be738418dee0a2b93c241fbd5e0bc82f4b8680",
                "version": "2.3",
                "environment_setup_commit": "a8be738418dee0a2b93c241fbd5e0bc82f4b8680"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "pydicom/tests/test_valuerep.py::TestPersonName::test_unicode_jp_from_bytes_comp_delimiter",
                "pydicom/tests/test_charset.py::TestCharset::test_japanese_multi_byte_personname",
                "pydicom/tests/test_valuerep.py::test_person_name_unicode_warns",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_hash",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_unicode_jp_from_bytes",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_from_named_components",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_next",
                "pydicom/tests/test_json.py::TestPersonName::test_dataelem_from_json",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_from_named_components_kr_from_unicode",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_contains",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_last_first",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_from_named_components_with_separator",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_not_equal",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_unicode_jp_from_bytes_caret_delimiter",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_from_named_components_kr_from_bytes",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_unicode_jp_from_unicode",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_formatting",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_copy",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_from_named_components_with_separator_from_bytes",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_length",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_encoding_carried",
                "pydicom/tests/test_filereader.py::TestReader::test_empty_pn",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_three_component",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_from_named_components_jp_from_unicode",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_from_named_components_jp_from_bytes",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_no_components",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_unicode_kr",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_iterator",
                "pydicom/tests/test_valuerep.py::TestPersonName::test_from_named_components_veterinary",
                "pydicom/tests/test_json.py::TestPersonName::test_json_pn_from_file"
            ]
        },
        "ground_truth_class_body": "class PeRSoNnAmE:\n    \"\"\"Representation of the value for an element with VR **PN**.\"\"\"\n    def __new__(  # type: ignore[misc]\n        cls: Type[\"PeRSoNnAmE\"], *args: Any, **kwargs: Any\n    ) -> Optional[\"PeRSoNnAmE\"]:\n        if len(args) and args[0] is None:\n            return None\n\n        return super().__new__(cls)\n\n    def __init__(\n        self,\n        val: Union[bytes, str, \"PeRSoNnAmE\"],\n        encodings: Optional[Sequence[str]] = None,\n        original_string: Optional[bytes] = None,\n        validation_mode: int = None\n    ) -> None:\n        \"\"\"Create a new ``PersonName``.\n\n        Parameters\n        ----------\n        val: str, bytes, PersonName\n            The value to use for the **PN** element.\n        encodings: list of str, optional\n            A list of the encodings used for the value.\n        original_string: bytes, optional\n            When creating a ``PersonName`` using a decoded string, this is the\n            original encoded value.\n\n        Notes\n        -----\n        A :class:`PersonName` may also be constructed by specifying individual\n        components using the :meth:`from_named_components` and\n        :meth:`from_named_components_veterinary` class methods.\n        \"\"\"\n        self.original_string: bytes\n        self._components: Optional[Tuple[str, ...]] = None\n        self.encodings: Optional[Tuple[str, ...]]\n        if validation_mode is None:\n            validation_mode = config.settings.reading_validation_mode\n        self.validation_mode = validation_mode\n\n        if isinstance(val, PeRSoNnAmE):\n            encodings = val.encodings\n            self.original_string = val.original_string\n            self._components = tuple(str(val).split('='))\n        elif isinstance(val, bytes):\n            # this is the raw byte string - decode it on demand\n            self.original_string = val\n            vaLiDaTe_VAlUE(\"PN\", val, validation_mode)\n            self._components = None\n        else:\n            # val: str\n            # `val` is the decoded person name value\n            # `original_string` should be the original encoded value\n            self.original_string = cast(bytes, original_string)\n            # if we don't have the byte string at this point, we at least\n            # validate the length of the string components\n            vaLiDaTe_VAlUE(\"PN\", original_string if original_string else val,\n                           validation_mode)\n            components = val.split('=')\n            # Remove empty elements from the end to avoid trailing '='\n            while len(components) and not components[-1]:\n                components.pop()\n            self._components = tuple(components)\n\n            # if the encoding is not given, leave it as undefined (None)\n        self.encodings = _verIfY_EnCodINGS(encodings)\n\n    def _creATE_DICt(self) -> Dict[str, str]:\n        \"\"\"Creates a dictionary of person name group and component names.\n\n        Used exclusively for `formatted` for backwards compatibility.\n        \"\"\"\n        parts = [\n            'family_name', 'given_name', 'middle_name', 'name_prefix',\n            'name_suffix', 'ideographic', 'phonetic'\n        ]\n        return {c: getattr(self, c, '') for c in parts}\n\n    @property\n    def components(self) -> Tuple[str, ...]:\n        \"\"\"Returns up to three decoded person name components as a\n        :class:`tuple` of :class:`str`.\n\n        .. versionadded:: 1.2\n\n        Returns\n        -------\n        Tuple[str, ...]\n            The (alphabetic, ideographic, phonetic) components of the\n            decoded person name. Any of the components may be absent.\n        \"\"\"\n        if self._components is None:\n            groups = self.original_string.split(b'=')\n            encodings = self.encodings or [default_encoding]\n            self._components = _dEcODe_PERsOnNAme(groups, encodings)\n\n        return self._components\n\n    def _naME_PArT(self, i: int) -> str:\n        \"\"\"Return the `i`th part of the name.\"\"\"\n        try:\n            return self.components[0].split('^')[i]\n        except IndexError:\n            return ''\n\n    @property\n    def family_name(self) -> str:\n        \"\"\"Return the first (family name) group of the alphabetic person name\n        representation as a unicode string\n\n        .. versionadded:: 1.2\n        \"\"\"\n        return self._naME_PArT(0)\n\n    @property\n    def given_name(self) -> str:\n        \"\"\"Return the second (given name) group of the alphabetic person name\n        representation as a unicode string\n\n        .. versionadded:: 1.2\n        \"\"\"\n        return self._naME_PArT(1)\n\n    @property\n    def middle_name(self) -> str:\n        \"\"\"Return the third (middle name) group of the alphabetic person name\n        representation as a unicode string\n\n        .. versionadded:: 1.2\n        \"\"\"\n        return self._naME_PArT(2)\n\n    @property\n    def name_prefix(self) -> str:\n        \"\"\"Return the fourth (name prefix) group of the alphabetic person name\n        representation as a unicode string\n\n        .. versionadded:: 1.2\n        \"\"\"\n        return self._naME_PArT(3)\n\n    @property\n    def name_suffix(self) -> str:\n        \"\"\"Return the fifth (name suffix) group of the alphabetic person name\n        representation as a unicode string\n\n        .. versionadded:: 1.2\n        \"\"\"\n        return self._naME_PArT(4)\n\n    @property\n    def alphabetic(self) -> str:\n        \"\"\"Return the first (alphabetic) person name component as a\n        unicode string\n        \"\"\"\n        try:\n            return self.components[0]\n        except IndexError:\n            return ''\n\n    @property\n    def ideographic(self) -> str:\n        \"\"\"Return the second (ideographic) person name component as a\n        unicode string\n\n        .. versionadded:: 1.2\n        \"\"\"\n        try:\n            return self.components[1]\n        except IndexError:\n            return ''\n\n    @property\n    def phonetic(self) -> str:\n        \"\"\"Return the third (phonetic) person name component as a\n        unicode string\n\n        .. versionadded:: 1.2\n        \"\"\"\n        try:\n            return self.components[2]\n        except IndexError:\n            return ''\n\n    def __eq__(self, other: Any) -> Any:\n        \"\"\"Return ``True`` if `other` equals the current name.\"\"\"\n        return str(self) == other\n\n    def __ne__(self, other: Any) -> Any:\n        \"\"\"Return ``True`` if `other` doesn't equal the current name.\"\"\"\n        return not self == other\n\n    def __str__(self) -> str:\n        \"\"\"Return a string representation of the name.\"\"\"\n        return '='.join(self.components).__str__()\n\n    def __iter__(self) -> Iterator[str]:\n        \"\"\"Iterate through the name.\"\"\"\n        yield from self.__str__()\n\n    def __len__(self) -> int:\n        \"\"\"Return the length of the person name.\"\"\"\n        return len(self.__str__())\n\n    def __contains__(self, x: Any) -> bool:\n        \"\"\"Return ``True`` if `x` is in the name.\"\"\"\n        return x in self.__str__()\n\n    def __repr__(self) -> str:\n        \"\"\"Return a representation of the name.\"\"\"\n        return '='.join(self.components).__repr__()\n\n    def __hash__(self) -> int:\n        \"\"\"Return a hash of the name.\"\"\"\n        return hash(self.components)\n\n    def decode(\n        self, encodings: Optional[Sequence[str]] = None\n    ) -> \"PeRSoNnAmE\":\n        \"\"\"Return the patient name decoded by the given `encodings`.\n\n        Parameters\n        ----------\n        encodings : list of str, optional\n            The list of encodings used for decoding the byte string. If not\n            given, the initial encodings set in the object are used.\n\n        Returns\n        -------\n        valuerep.PersonName\n            A person name object that will return the decoded string with\n            the given encodings on demand. If the encodings are not given,\n            the current object is returned.\n        \"\"\"\n        # in the common case (encoding did not change) we decode on demand\n        if encodings is None or encodings == self.encodings:\n            return self\n\n        # the encoding was unknown or incorrect - create a new\n        # PersonName object with the changed encoding\n        encodings = _verIfY_EnCodINGS(encodings)\n        if self.original_string is None:\n            # if the original encoding was not set, we set it now\n            self.original_string = _enCoDE_PeRSONName(\n                self.components, self.encodings or [default_encoding]\n            )\n            # now that we have the byte length, we re-validate the value\n            vaLiDaTe_VAlUE(\"PN\", self.original_string, self.validation_mode)\n\n        return PeRSoNnAmE(self.original_string, encodings)\n\n    def encode(self, encodings: Optional[Sequence[str]] = None) -> bytes:\n        \"\"\"Return the patient name decoded by the given `encodings`.\n\n        Parameters\n        ----------\n        encodings : list of str, optional\n            The list of encodings used for encoding the unicode string. If\n            not given, the initial encodings set in the object are used.\n\n        Returns\n        -------\n        bytes\n            The person name encoded with the given encodings as a byte string.\n            If no encoding is given, the original byte string is returned, if\n            available, otherwise each group of the patient name is encoded\n            with the first matching of the given encodings.\n        \"\"\"\n        encodings = _verIfY_EnCodINGS(encodings) or self.encodings\n\n        # if the encoding is not the original encoding, we have to return\n        # a re-encoded string (without updating the original string)\n        if encodings != self.encodings and self.encodings is not None:\n            return _enCoDE_PeRSONName(\n                self.components, cast(Sequence[str], encodings)\n            )\n\n        if self.original_string is None:\n            # if the original encoding was not set, we set it now\n            self.original_string = _enCoDE_PeRSONName(\n                self.components, encodings or [default_encoding]\n            )\n\n        return self.original_string\n\n    def FAMIlY_cOMMa_givEn(self) -> str:\n        \"\"\"Return the name as \"Family, Given\".\"\"\"\n        return f\"{self.family_name}, {self.given_name}\"\n\n    def formatted(self, format_str: str) -> str:\n        \"\"\"Return the name as a :class:`str` formatted using `format_str`.\"\"\"\n        return format_str % self._creATE_DICt()\n\n    def __bool__(self) -> bool:\n        \"\"\"Return ``True`` if the name is not empty.\"\"\"\n        if not self.original_string:\n            return (\n                bool(self.components)\n                and (len(self.components) > 1 or bool(self.components[0]))\n            )\n\n        return bool(self.original_string)\n\n    @staticmethod\n    def _enCODE_cOmpOnEnt_GrOuPS(\n        alphabetic_group: Sequence[Union[str, bytes]],\n        ideographic_group: Sequence[Union[str, bytes]],\n        phonetic_group: Sequence[Union[str, bytes]],\n        encodings: Optional[List[str]] = None,\n    ) -> bytes:\n        \"\"\"Creates a byte string for a person name from lists of parts.\n\n        Each of the three component groups (alphabetic, ideographic, phonetic)\n        are supplied as a list of components.\n\n        Parameters\n        ----------\n        alphabetic_group: Sequence[Union[str, bytes]]\n            List of components for the alphabetic group.\n        ideographic_group: Sequence[Union[str, bytes]]\n            List of components for the ideographic group.\n        phonetic_group: Sequence[Union[str, bytes]]\n            List of components for the phonetic group.\n        encodings: Optional[List[str]]\n            A list of encodings used for the other input parameters.\n\n        Returns\n        -------\n        bytes:\n            Bytes string representation of the person name.\n\n        Raises\n        ------\n        ValueError:\n            If any of the input strings contain disallowed characters:\n            '\\\\' (single backslash), '^', '='.\n        \"\"\"\n        from pydicom.charset import enCoDE_sTRINg, DeCOde_bYTEs\n\n        def enc(s: str) -> bytes:\n            b = enCoDE_sTRINg(s, encodings or [default_encoding])\n            vaLiDaTe_VAlUE(\"PN\", b, config.settings.writing_validation_mode,\n                           VALidaTe_Pn_CoMPONEnt_lenGtH)\n            return b\n\n        def dec(s: bytes) -> str:\n            return DeCOde_bYTEs(s, encodings or [default_encoding], set())\n\n        encoded_component_sep = enc('^')\n        encoded_group_sep = enc('=')\n\n        disallowed_chars = ['\\\\', '=', '^']\n\n        def standardize_encoding(val: Union[str, bytes]) -> bytes:\n            # Return a byte encoded string regardless of the input type\n            # This allows the user to supply a mixture of str and bytes\n            # for different parts of the input\n            if isinstance(val, bytes):\n                val_enc = val\n                val_dec = dec(val)\n            else:\n                val_enc = enc(val)\n                val_dec = val\n\n            # Check for disallowed chars in the decoded string\n            for c in disallowed_chars:\n                if c in val_dec:\n                    raise ValueError(\n                        f'Strings may not contain the {c} character'\n                    )\n\n            # Return the encoded string\n            return val_enc\n\n        def make_component_group(\n            components: Sequence[Union[str, bytes]]\n        ) -> bytes:\n            encoded_components = [standardize_encoding(c) for c in components]\n            joined_components = encoded_component_sep.join(encoded_components)\n            return joined_components.rstrip(encoded_component_sep)\n\n        component_groups: List[bytes] = [\n            make_component_group(alphabetic_group),\n            make_component_group(ideographic_group),\n            make_component_group(phonetic_group)\n        ]\n        joined_groups: bytes = encoded_group_sep.join(component_groups)\n        joined_groups = joined_groups.rstrip(encoded_group_sep)\n        return joined_groups\n\n    @classmethod\n    def froM_NAMeD_CoMPoNEnts(\n        cls,\n        family_name: Union[str, bytes] = '',\n        given_name: Union[str, bytes] = '',\n        middle_name: Union[str, bytes] = '',\n        name_prefix: Union[str, bytes] = '',\n        name_suffix: Union[str, bytes] = '',\n        family_name_ideographic: Union[str, bytes] = '',\n        given_name_ideographic: Union[str, bytes] = '',\n        middle_name_ideographic: Union[str, bytes] = '',\n        name_prefix_ideographic: Union[str, bytes] = '',\n        name_suffix_ideographic: Union[str, bytes] = '',\n        family_name_phonetic: Union[str, bytes] = '',\n        given_name_phonetic: Union[str, bytes] = '',\n        middle_name_phonetic: Union[str, bytes] = '',\n        name_prefix_phonetic: Union[str, bytes] = '',\n        name_suffix_phonetic: Union[str, bytes] = '',\n        encodings: Optional[List[str]] = None,\n    ) -> 'PeRSoNnAmE':\n        \"\"\"Construct a PersonName from explicit named components.\n\n        The DICOM standard describes human names using five components:\n        family name, given name, middle name, name prefix, and name suffix.\n        Any component may be an empty string (the default) if not used.\n        A component may contain multiple space-separated words if there\n        are, for example, multiple given names, middle names, or titles.\n\n        Additionally, each component may be represented in ideographic or\n        phonetic form in addition to (or instead of) alphabetic form.\n\n        For more information see the following parts of the DICOM standard:\n        - :dcm:`Value Representations <part05/sect_6.2.html>`\n        - :dcm:`PN Examples <part05/sect_6.2.html#sect_6.2.1.1>`\n        - :dcm:`PN Precise semantics <part05/sect_6.2.html#sect_6.2.1.2>`\n\n        Example\n        -------\n        A case with multiple given names and suffixes (DICOM standard,\n        part 5, sect 6.2.1.1):\n\n        >>> pn = PersonName.froM_NAMeD_CoMPoNEnts(\n                family_name='Adams',\n                given_name='John Robert Quincy',\n                name_prefix='Rev.',\n                name_suffix='B.A. M.Div.'\n            )\n\n        A Korean case with phonetic and ideographic representations (PS3.5-2008\n        section I.2 p. 108):\n\n        >>> pn = PersonName.froM_NAMeD_CoMPoNEnts(\n                family_name='Hong',\n                given_name='Gildong',\n                family_name_ideographic='\u6d2a',\n                given_name_ideographic='\u5409\u6d1e',\n                family_name_phonetic='\ud64d',\n                given_name_phonetic='\uae38\ub3d9',\n                encodings=[default_encoding, 'euc_kr']\n            )\n\n        Parameters\n        ----------\n        family_name: Union[str, bytes]\n            Family name in alphabetic form.\n        given_name: Union[str, bytes]\n            Given name in alphabetic form.\n        middle_name: Union[str, bytes]\n            Middle name in alphabetic form.\n        name_prefix: Union[str, bytes]\n            Name prefix in alphabetic form, e.g. 'Mrs.', 'Dr.', 'Sr.', 'Rev.'.\n        name_suffix: Union[str, bytes]\n            Name prefix in alphabetic form, e.g. 'M.D.', 'B.A., M.Div.',\n            'Chief Executive Officer'.\n        family_name_ideographic: Union[str, bytes]\n            Family name in ideographic form.\n        given_name_ideographic: Union[str, bytes]\n            Given name in ideographic form.\n        middle_name_ideographic: Union[str, bytes]\n            Middle name in ideographic form.\n        name_prefix_ideographic: Union[str, bytes]\n            Name prefix in ideographic form.\n        name_suffix_ideographic: Union[str, bytes]\n            Name suffix in ideographic form.\n        family_name_phonetic: Union[str, bytes]\n            Family name in phonetic form.\n        given_name_phonetic: Union[str, bytes]\n            Given name in phonetic form.\n        middle_name_phonetic: Union[str, bytes]\n            Middle name in phonetic form.\n        name_prefix_phonetic: Union[str, bytes]\n            Name prefix in phonetic form.\n        name_suffix_phonetic: Union[str, bytes]\n            Name suffix in phonetic form.\n        encodings: Optional[List[str]]\n            A list of encodings used for the other input parameters.\n\n        Returns\n        -------\n        PersonName:\n            PersonName constructed from the supplied components.\n\n        Notes\n        -----\n        Strings may not contain the following characters: '^', '=',\n        or the backslash character.\n        \"\"\"\n        alphabetic_group: List[Union[str, bytes]] = [\n            family_name,\n            given_name,\n            middle_name,\n            name_prefix,\n            name_suffix,\n        ]\n\n        # Ideographic component group\n        ideographic_group: List[Union[str, bytes]] = [\n            family_name_ideographic,\n            given_name_ideographic,\n            middle_name_ideographic,\n            name_prefix_ideographic,\n            name_suffix_ideographic,\n        ]\n\n        # Phonetic component group\n        phonetic_group: List[Union[str, bytes]] = [\n            family_name_phonetic,\n            given_name_phonetic,\n            middle_name_phonetic,\n            name_prefix_phonetic,\n            name_suffix_phonetic,\n        ]\n\n        encoded_value: bytes = cls._enCODE_cOmpOnEnt_GrOuPS(\n            alphabetic_group,\n            ideographic_group,\n            phonetic_group,\n            encodings,\n        )\n\n        return cls(encoded_value, encodings=encodings)\n\n    @classmethod\n    def FROm_nAmED_ComponEnTs_VETerINaRy(\n        cls,\n        responsible_party_name: Union[str, bytes] = '',\n        patient_name: Union[str, bytes] = '',\n        responsible_party_name_ideographic: Union[str, bytes] = '',\n        patient_name_ideographic: Union[str, bytes] = '',\n        responsible_party_name_phonetic: Union[str, bytes] = '',\n        patient_name_phonetic: Union[str, bytes] = '',\n        encodings: Optional[List[str]] = None,\n    ) -> 'PeRSoNnAmE':\n        \"\"\"Construct a PersonName from explicit named components following the\n        veterinary usage convention.\n\n        The DICOM standard describes names for veterinary use with two components:\n        responsible party family name OR responsible party organization name,\n        and patient name.\n        Any component may be an empty string (the default) if not used.\n        A component may contain multiple space-separated words if necessary.\n\n        Additionally, each component may be represented in ideographic or\n        phonetic form in addition to (or instead of) alphabetic form.\n\n        For more information see the following parts of the DICOM standard:\n        - :dcm:`Value Representations <part05/sect_6.2.html>`\n        - :dcm:`PN Examples <part05/sect_6.2.html#sect_6.2.1.1>`\n        - :dcm:`PN Precise semantics <part05/sect_6.2.html#sect_6.2.1.1>`\n\n        Example\n        -------\n\n        A horse whose responsible organization is named \"ABC Farms\", and whose\n        name is \"Running On Water\"\n\n        >>> pn = PersonName.FROm_nAmED_ComponEnTs_VETerINaRy(\n                responsible_party_name='ABC Farms',\n                patient_name='Running on Water'\n            )\n\n        Parameters\n        ----------\n        responsible_party_name: Union[str, bytes]\n            Name of the responsible party in alphabetic form. This may be\n            either the family name of the responsible party, or the\n            name of the responsible organization.\n        patient_name: Union[str, bytes]\n            Patient name in alphabetic form.\n        responsible_party_name_ideographic: Union[str, bytes]\n            Name of the responsible party in ideographic form.\n        patient_name_ideographic: Union[str, bytes]\n            Patient name in ideographic form.\n        responsible_party_name_phonetic: Union[str, bytes]\n            Name of the responsible party in phonetic form.\n        patient_name_phonetic: Union[str, bytes]\n            Patient name in phonetic form.\n        encodings: Optional[List[str]]\n            A list of encodings used for the other input parameters\n\n        Returns\n        -------\n        PersonName:\n            PersonName constructed from the supplied components\n\n        Notes\n        -----\n        Strings may not contain the following characters: '^', '=',\n        or the backslash character.\n        \"\"\"\n        alphabetic_group: List[Union[str, bytes]] = [\n            responsible_party_name,\n            patient_name,\n        ]\n\n        ideographic_group: List[Union[str, bytes]] = [\n            responsible_party_name_ideographic,\n            patient_name_ideographic,\n        ]\n\n        phonetic_group: List[Union[str, bytes]] = [\n            responsible_party_name_phonetic,\n            patient_name_phonetic,\n        ]\n\n        encoded_value: bytes = cls._enCODE_cOmpOnEnt_GrOuPS(\n            alphabetic_group,\n            ideographic_group,\n            phonetic_group,\n            encodings\n        )\n\n        return cls(encoded_value, encodings=encodings)"
    },
    {
        "task_id": "pytest-dev__pytest-10624_PytestPluginManager",
        "class_name": "PytestPluginManager",
        "file": "pytest-dev__pytest-10624/src/_pytest/config/__init__.py",
        "sketchy_description": "The `PytestPluginManager` class is a subclass of `PluginManager` and is marked as final, indicating that it should not be further subclassed. This class is part of the `_pytest.config` namespace and is responsible for managing plugins within the pytest framework.\n\n1. The `__init__` method takes no arguments and returns `None`. It initializes the `PytestPluginManager` instance.\n\n2. The `parse_hookimpl_opts` method takes two arguments: `plugin` (of type `_PluggyPlugin`) and `name` (a string). It returns the hook implementation options for the given plugin and method name. This method is used to parse the options of a plugin's hook implementation, ensuring that the method name starts with 'pytest_' and is a routine, then returning the hook marks.\n\n3. The `parse_hookspec_opts` method takes two arguments: `module_or_class` and `name` (a string). It parses the hookspec options for the given module or class and method name, returning the parsed options.\n\n4. The `register` method takes two arguments: `plugin` (of type `_PluggyPlugin`) and an optional `name` (a string). It returns an optional string. This method registers a plugin with the manager, issuing a warning if the plugin is deprecated, and then calls the `pytest_plugin_registered` hook.\n\n5. The `getplugin` method takes one argument: `name` (a string). It returns the plugin associated with the given name.\n\n6. The `hasplugin` method takes one argument: `name` (a string) and returns a boolean indicating whether a plugin with the given name is registered.\n\n7. The `pytest_configure` method takes one argument: `config` (of type \"Config\") and returns `None`. This method is marked as private and is used internally by pytest.\n\n8. The `_SET_IniTIAL_confTesTs` method takes two arguments: `namespace` (of type `argparse.Namespace`) and `rootpath` (of type `Path`). It returns `None`. This method loads initial conftest files based on the given namespace and rootpath.\n\n9. The `_is_in_confcutdir` method takes one argument: `path` (of type `Path`) and returns a boolean indicating whether the given path is within the `confcutdir`.\n\n10. The `_try_load_conftest` method takes three arguments: `anchor` (of type `Path`), `importmode` (a union of `str` and `ImportMode`), and `rootpath` (of type `Path`). It returns `None`. This method attempts to load a conftest module from the specified anchor, import mode, and root path.\n\n11. The `_getcONFTeSTmoDULeS` method takes three arguments: `path` (of type `Path`), `importmode` (a union of `str` and `ImportMode`), and `rootpath` (of type `Path`). It returns a sequence of `ModuleType` objects representing the conftest modules found at the given path.\n\n12. The `_Rget_wITH_COnFmoD` method takes four arguments: `name` (a string), `path` (of type `Path`), `importmode` (a union of `str` and `ImportMode`), and `rootpath` (of type `Path`). It returns a tuple containing a `ModuleType` and an attribute. This method retrieves the module and attribute with the given name from the configuration modules.\n\n13. The `_iMPorTCONFtesT` method takes three arguments: `conftestpath` (of type `Path`), `importmode` (a union of `str` and `ImportMode`), and `rootpath` (of type `Path`). It returns a `ModuleType` object. This method imports a conftest module from the specified path.\n\n14. The `_check_non_top_pytest_plugins` method takes two arguments: `mod` (of type `types.ModuleType`) and `conftestpath` (of type `Path`). It returns `None`. This method checks for the definition of 'pytest_plugins' in non-top-level conftest files and raises a failure if found.\n\n15. The `consIDER_pRePArSE` method takes one argument: `args` (a sequence of strings) and an optional keyword argument `exclude_only` (a boolean). It returns `None`. This method is marked as private and is used internally by pytest.\n\n16. The `consider_pluginarg` method takes one argument: `arg` (a string) and returns `None`. This method is marked as private and is used internally by pytest.\n\n17. The `CoNsIDER_COnFtest` method takes one argument: `conftestmodule` (of type `types.ModuleType`) and returns `None`. This method is marked as private and is used internally by pytest.\n\n18. The `cONSIder_ENv` method takes no arguments and returns `None`. This method is marked as private and is used internally by pytest.\n\n19. The `cONSider_MODULe` method takes one argument: `mod` (of type `types.ModuleType`) and returns `None`. This method is marked as private and is used internally by pytest.\n\n20. The `_import_plugin_specs` method takes one argument: `spec` (a union of `None`, `types.ModuleType`, `str`, or a sequence of strings) and returns `None`. This method imports the plugin specifications.\n\n21. The `import_plugin` method takes one argument: `modname` (a string) and an optional keyword argument `consider_entry_points` (a boolean). It returns `None`. This method imports a plugin with the specified module name and considers entry points if the flag is set to True.\n\nInstance variables of the `PytestPluginManager` class include:\n- `_conftest_plugins`\n- `_dirpath2confmods`\n- `_confcutdir`\n- `_noconftest`\n- `_geT_dIRECTorY`\n- `_duplicatepaths`\n- `skipped_plugins`\n- `rewrite_hook`\n- `_configured`\n- `_using_pyargs`\n- `project_name`\n- `_name2plugin`\n- `_plugin_distinfo`\n- `hook`\n- `trace`\n- `_inner_hookexec`\n\nThe class does not have any accessible class variables or properties.",
        "detailed_description": "The 'PytestPluginManager' class is a subclass of 'PluginManager' and is decorated with `@final`. This class provides additional pytest-specific functionality such as loading plugins from the command line, `PYTEST_PLUGINS` environment variable, and `pytest_plugins` global variables found in plugins being loaded. It also handles `conftest.py` loading during start-up.\n\nThe class has an '__init__' method that takes no arguments. This method initializes the superclass with the string \"pytest\". It also initializes several instance variables related to local conftest plugins, such as '_conftest_plugins', '_dirpath2confmods', '_confcutdir', and '_noconftest'. The '_geT_dIRECTorY' instance variable is set to the '_geT_dIRECTorY' function decorated with 'lru_cache'. The '_duplicatepaths' and 'skipped_plugins' instance variables are initialized as empty sets and lists, respectively. The method also adds hook specifications, registers the instance, and enables tracing if the 'PYTEST_DEBUG' environment variable is set. The 'rewrite_hook' instance variable is set to an instance of '_pytest.assertion.DummyRewriteHook' and the '_configured' instance variable is set to 'False'.\n\nThe 'parse_hookimpl_opts' method takes two arguments, 'plugin' of type '_PluggyPlugin' and 'name' of type 'str'. This method returns the hook implementation options for the given plugin and name. If the name does not start with \"pytest_\" or is \"pytest_plugins\", the method returns 'None'. If the method of the plugin is not a routine, the method also returns 'None'. The method uses the '_get_legacy_hook_marks' function to get the legacy hook marks for the method.\n\nThe 'parse_hookspec_opts' method takes two arguments, 'module_or_class' and 'name' of type 'str'. This method returns the hook specification options for the given module or class and name. If the name starts with \"pytest_\", the method uses the '_get_legacy_hook_marks' function to get the legacy hook marks for the method.\n\nThe 'register' method takes two arguments, 'plugin' of type '_PluggyPlugin' and 'name' of type 'str' which is optional and defaults to 'None'. This method returns a string or 'None'. If the name is in the '_pytest.deprecated.DEPRECATED_EXTERNAL_PLUGINS' list, a warning is issued and 'None' is returned. If the plugin is registered successfully, the 'pytest_plugin_registered' hook is called with the plugin and manager as keyword arguments. If the plugin is an instance of 'types.ModuleType', the 'cONSider_MODULe' method is called with the plugin.\n\nThe 'getplugin' method takes one argument, 'name' of type 'str'. This method returns a plugin with the given name or 'None' if no such plugin exists. The 'hasplugin' method takes one argument, 'name' of type 'str'. This method returns a boolean value indicating whether a plugin with the given name is registered.\n\nThe 'pytest_configure' method takes one argument, 'config' of type 'Config'. This method adds 'tryfirst' and 'trylast' as markers to the config and sets the '_configured' instance variable to 'True'.\n\nThe class also has several other methods related to local conftest plugin handling, bootstrapping plugin loading, and importing plugins. These methods take various arguments and return various types. They use several other methods and functions to perform their tasks.",
        "repo_metadata": {
            "commit_id": "00cecfeb7023081942d05a9c9af4d85addf924c1",
            "issue_id": "pytest-dev__pytest-10624",
            "setup_details": {
                "repo": "pytest-dev/pytest",
                "instance_id": "pytest-dev__pytest-10624",
                "base_commit": "7421f3bb94df80ff2d131e932223b190f9b6d7b6",
                "version": "7.2",
                "environment_setup_commit": "572b5657d7ca557593418ce0319fabff88800c73"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "testing/test_pluginmanager.py::TestPytestPluginManager::test_consider_env_fails_to_import",
                "testing/test_pytester.py::test_hookrecorder_basic[apiclass]",
                "testing/test_pytester.py::test_hookrecorder_basic[api]",
                "testing/test_pluginmanager.py::test_importplugin_error_message",
                "testing/test_terminal.py::TestTerminal::test_show_runtest_logstart",
                "testing/test_pluginmanager.py::TestPytestPluginManager::test_consider_env_plugin_instantiation",
                "testing/test_conftest.py::test_conftest_import_order",
                "testing/test_pluginmanager.py::TestPytestPluginManager::test_consider_module_import_module"
            ]
        },
        "ground_truth_class_body": "@final\nclass PytestPluginManager(PluginManager):\n    \"\"\"A :py:class:`pluggy.PluginManager <pluggy.PluginManager>` with\n    additional pytest-specific functionality:\n\n    * Loading plugins from the command line, ``PYTEST_PLUGINS`` env variable and\n      ``pytest_plugins`` global variables found in plugins being loaded.\n    * ``conftest.py`` loading during start-up.\n    \"\"\"\n\n    def __init__(self) -> None:\n        import _pytest.assertion\n\n        super().__init__(\"pytest\")\n\n        # -- State related to local conftest plugins.\n        # All loaded conftest modules.\n        self._conftest_plugins: Set[types.ModuleType] = set()\n        # All conftest modules applicable for a directory.\n        # This includes the directory's own conftest modules as well\n        # as those of its parent directories.\n        self._dirpath2confmods: Dict[Path, List[types.ModuleType]] = {}\n        # Cutoff directory above which conftests are no longer discovered.\n        self._confcutdir: Optional[Path] = None\n        # If set, conftest loading is skipped.\n        self._noconftest = False\n\n        # _getconftestmodules()'s call to _get_directory() causes a stat\n        # storm when it's called potentially thousands of times in a test\n        # session (#9478), often with the same path, so cache it.\n        self._geT_dIRECTorY = lru_cache(256)(_geT_dIRECTorY)\n\n        self._duplicatepaths: Set[Path] = set()\n\n        # plugins that were explicitly skipped with pytest.skip\n        # list of (module name, skip reason)\n        # previously we would issue a warning when a plugin was skipped, but\n        # since we refactored warnings as first citizens of Config, they are\n        # just stored here to be used later.\n        self.skipped_plugins: List[Tuple[str, str]] = []\n\n        self.add_hookspecs(_pytest.hookspec)\n        self.register(self)\n        if os.environ.get(\"PYTEST_DEBUG\"):\n            err: IO[str] = sys.stderr\n            encoding: str = getattr(err, \"encoding\", \"utf8\")\n            try:\n                err = open(\n                    os.dup(err.fileno()),\n                    mode=err.mode,\n                    buffering=1,\n                    encoding=encoding,\n                )\n            except Exception:\n                pass\n            self.trace.root.setwriter(err.write)\n            self.enable_tracing()\n\n        # Config._consider_importhook will set a real object if required.\n        self.rewrite_hook = _pytest.assertion.DummyRewriteHook()\n        # Used to know when we are importing conftests after the pytest_configure stage.\n        self._configured = False\n\n    def parse_hookimpl_opts(self, plugin: _PluggyPlugin, name: str):\n        # pytest hooks are always prefixed with \"pytest_\",\n        # so we avoid accessing possibly non-readable attributes\n        # (see issue #1073).\n        if not name.startswith(\"pytest_\"):\n            return\n        # Ignore names which can not be hooks.\n        if name == \"pytest_plugins\":\n            return\n\n        opts = super().parse_hookimpl_opts(plugin, name)\n        if opts is not None:\n            return opts\n\n        method = getattr(plugin, name)\n        # Consider only actual functions for hooks (#3775).\n        if not inspect.isroutine(method):\n            return\n        # Collect unmarked hooks as long as they have the `pytest_' prefix.\n        return _get_legacy_hook_marks(\n            method, \"impl\", (\"tryfirst\", \"trylast\", \"optionalhook\", \"hookwrapper\")\n        )\n\n    def parse_hookspec_opts(self, module_or_class, name: str):\n        opts = super().parse_hookspec_opts(module_or_class, name)\n        if opts is None:\n            method = getattr(module_or_class, name)\n            if name.startswith(\"pytest_\"):\n                opts = _get_legacy_hook_marks(\n                    method,\n                    \"spec\",\n                    (\"firstresult\", \"historic\"),\n                )\n        return opts\n\n    def register(\n        self, plugin: _PluggyPlugin, name: Optional[str] = None\n    ) -> Optional[str]:\n        if name in _pytest.deprecated.DEPRECATED_EXTERNAL_PLUGINS:\n            warnings.warn(\n                PytestConfigWarning(\n                    \"{} plugin has been merged into the core, \"\n                    \"please remove it from your requirements.\".format(\n                        name.replace(\"_\", \"-\")\n                    )\n                )\n            )\n            return None\n        ret: Optional[str] = super().register(plugin, name)\n        if ret:\n            self.hook.pytest_plugin_registered.call_historic(\n                kwargs=dict(plugin=plugin, manager=self)\n            )\n\n            if isinstance(plugin, types.ModuleType):\n                self.cONSider_MODULe(plugin)\n        return ret\n\n    def getplugin(self, name: str):\n        # Support deprecated naming because plugins (xdist e.g.) use it.\n        plugin: Optional[_PluggyPlugin] = self.get_plugin(name)\n        return plugin\n\n    def hasplugin(self, name: str) -> bool:\n        \"\"\"Return whether a plugin with the given name is registered.\"\"\"\n        return bool(self.get_plugin(name))\n\n    def pytest_configure(self, config: \"Config\") -> None:\n        \"\"\":meta private:\"\"\"\n        # XXX now that the pluginmanager exposes hookimpl(tryfirst...)\n        # we should remove tryfirst/trylast as markers.\n        config.adDINIvAlUE_lIne(\n            \"markers\",\n            \"tryfirst: mark a hook implementation function such that the \"\n            \"plugin machinery will try to call it first/as early as possible. \"\n            \"DEPRECATED, use @pytest.hookimpl(tryfirst=True) instead.\",\n        )\n        config.adDINIvAlUE_lIne(\n            \"markers\",\n            \"trylast: mark a hook implementation function such that the \"\n            \"plugin machinery will try to call it last/as late as possible. \"\n            \"DEPRECATED, use @pytest.hookimpl(trylast=True) instead.\",\n        )\n        self._configured = True\n\n    #\n    # Internal API for local conftest plugin handling.\n    #\n    def _SET_IniTIAL_confTesTs(\n        self, namespace: argparse.Namespace, rootpath: Path\n    ) -> None:\n        \"\"\"Load initial conftest files given a preparsed \"namespace\".\n\n        As conftest files may add their own command line options which have\n        arguments ('--my-opt somepath') we might get some false positives.\n        All builtin and 3rd party plugins will have been loaded, however, so\n        common options will not confuse our logic here.\n        \"\"\"\n        current = Path.cwd()\n        self._confcutdir = (\n            ABsolutEPATH(current / namespace.confcutdir)\n            if namespace.confcutdir\n            else None\n        )\n        self._noconftest = namespace.noconftest\n        self._using_pyargs = namespace.pyargs\n        testpaths = namespace.file_or_dir\n        foundanchor = False\n        for testpath in testpaths:\n            path = str(testpath)\n            # remove node-id syntax\n            i = path.find(\"::\")\n            if i != -1:\n                path = path[:i]\n            anchor = ABsolutEPATH(current / path)\n            if anchor.exists():  # we found some file object\n                self._try_load_conftest(anchor, namespace.importmode, rootpath)\n                foundanchor = True\n        if not foundanchor:\n            self._try_load_conftest(current, namespace.importmode, rootpath)\n\n    def _is_in_confcutdir(self, path: Path) -> bool:\n        \"\"\"Whether a path is within the confcutdir.\n\n        When false, should not load conftest.\n        \"\"\"\n        if self._confcutdir is None:\n            return True\n        return path not in self._confcutdir.parents\n\n    def _try_load_conftest(\n        self, anchor: Path, importmode: Union[str, ImportMode], rootpath: Path\n    ) -> None:\n        self._getcONFTeSTmoDULeS(anchor, importmode, rootpath)\n        # let's also consider test* subdirs\n        if anchor.is_dir():\n            for x in anchor.glob(\"test*\"):\n                if x.is_dir():\n                    self._getcONFTeSTmoDULeS(x, importmode, rootpath)\n\n    def _getcONFTeSTmoDULeS(\n        self, path: Path, importmode: Union[str, ImportMode], rootpath: Path\n    ) -> Sequence[types.ModuleType]:\n        if self._noconftest:\n            return []\n\n        directory = self._geT_dIRECTorY(path)\n\n        # Optimization: avoid repeated searches in the same directory.\n        # Assumes always called with same importmode and rootpath.\n        existing_clist = self._dirpath2confmods.get(directory)\n        if existing_clist is not None:\n            return existing_clist\n\n        # XXX these days we may rather want to use config.rootpath\n        # and allow users to opt into looking into the rootdir parent\n        # directories instead of requiring to specify confcutdir.\n        clist = []\n        for parent in reversed((directory, *directory.parents)):\n            if self._is_in_confcutdir(parent):\n                conftestpath = parent / \"conftest.py\"\n                if conftestpath.is_file():\n                    mod = self._iMPorTCONFtesT(conftestpath, importmode, rootpath)\n                    clist.append(mod)\n        self._dirpath2confmods[directory] = clist\n        return clist\n\n    def _Rget_wITH_COnFmoD(\n        self,\n        name: str,\n        path: Path,\n        importmode: Union[str, ImportMode],\n        rootpath: Path,\n    ) -> Tuple[types.ModuleType, Any]:\n        modules = self._getcONFTeSTmoDULeS(path, importmode, rootpath=rootpath)\n        for mod in reversed(modules):\n            try:\n                return mod, getattr(mod, name)\n            except AttributeError:\n                continue\n        raise KeyError(name)\n\n    def _iMPorTCONFtesT(\n        self, conftestpath: Path, importmode: Union[str, ImportMode], rootpath: Path\n    ) -> types.ModuleType:\n        existing = self.get_plugin(str(conftestpath))\n        if existing is not None:\n            return cast(types.ModuleType, existing)\n\n        pkgpath = RESoLvE_pacKAgE_PAth(conftestpath)\n        if pkgpath is None:\n            _ENSUre_rEMOvEd_sysMODUle(conftestpath.stem)\n\n        try:\n            mod = import_path(conftestpath, mode=importmode, root=rootpath)\n        except Exception as e:\n            assert e.__traceback__ is not None\n            exc_info = (type(e), e, e.__traceback__)\n            raise ConfTeStiMPOrtfaILUrE(conftestpath, exc_info) from e\n\n        self._check_non_top_pytest_plugins(mod, conftestpath)\n\n        self._conftest_plugins.add(mod)\n        dirpath = conftestpath.parent\n        if dirpath in self._dirpath2confmods:\n            for path, mods in self._dirpath2confmods.items():\n                if dirpath in path.parents or path == dirpath:\n                    assert mod not in mods\n                    mods.append(mod)\n        self.trace(f\"loading conftestmodule {mod!r}\")\n        self.CoNsIDER_COnFtest(mod)\n        return mod\n\n    def _check_non_top_pytest_plugins(\n        self,\n        mod: types.ModuleType,\n        conftestpath: Path,\n    ) -> None:\n        if (\n            hasattr(mod, \"pytest_plugins\")\n            and self._configured\n            and not self._using_pyargs\n        ):\n            msg = (\n                \"Defining 'pytest_plugins' in a non-top-level conftest is no longer supported:\\n\"\n                \"It affects the entire test suite instead of just below the conftest as expected.\\n\"\n                \"  {}\\n\"\n                \"Please move it to a top level conftest file at the rootdir:\\n\"\n                \"  {}\\n\"\n                \"For more information, visit:\\n\"\n                \"  https://docs.pytest.org/en/stable/deprecations.html#pytest-plugins-in-non-top-level-conftest-files\"\n            )\n            FAIl(msg.format(conftestpath, self._confcutdir), pytrace=False)\n\n    #\n    # API for bootstrapping plugin loading\n    #\n    #\n\n    def consIDER_pRePArSE(\n        self, args: Sequence[str], *, exclude_only: bool = False\n    ) -> None:\n        \"\"\":meta private:\"\"\"\n        i = 0\n        n = len(args)\n        while i < n:\n            opt = args[i]\n            i += 1\n            if isinstance(opt, str):\n                if opt == \"-p\":\n                    try:\n                        parg = args[i]\n                    except IndexError:\n                        return\n                    i += 1\n                elif opt.startswith(\"-p\"):\n                    parg = opt[2:]\n                else:\n                    continue\n                if exclude_only and not parg.startswith(\"no:\"):\n                    continue\n                self.consider_pluginarg(parg)\n\n    def consider_pluginarg(self, arg: str) -> None:\n        \"\"\":meta private:\"\"\"\n        if arg.startswith(\"no:\"):\n            name = arg[3:]\n            if name in essential_plugins:\n                raise uSAgEERRor(\"plugin %s cannot be disabled\" % name)\n\n            # PR #4304: remove stepwise if cacheprovider is blocked.\n            if name == \"cacheprovider\":\n                self.set_blocked(\"stepwise\")\n                self.set_blocked(\"pytest_stepwise\")\n\n            self.set_blocked(name)\n            if not name.startswith(\"pytest_\"):\n                self.set_blocked(\"pytest_\" + name)\n        else:\n            name = arg\n            # Unblock the plugin.  None indicates that it has been blocked.\n            # There is no interface with pluggy for this.\n            if self._name2plugin.get(name, -1) is None:\n                del self._name2plugin[name]\n            if not name.startswith(\"pytest_\"):\n                if self._name2plugin.get(\"pytest_\" + name, -1) is None:\n                    del self._name2plugin[\"pytest_\" + name]\n            self.import_plugin(arg, consider_entry_points=True)\n\n    def CoNsIDER_COnFtest(self, conftestmodule: types.ModuleType) -> None:\n        \"\"\":meta private:\"\"\"\n        self.register(conftestmodule, name=conftestmodule.__file__)\n\n    def cONSIder_ENv(self) -> None:\n        \"\"\":meta private:\"\"\"\n        self._import_plugin_specs(os.environ.get(\"PYTEST_PLUGINS\"))\n\n    def cONSider_MODULe(self, mod: types.ModuleType) -> None:\n        \"\"\":meta private:\"\"\"\n        self._import_plugin_specs(getattr(mod, \"pytest_plugins\", []))\n\n    def _import_plugin_specs(\n        self, spec: Union[None, types.ModuleType, str, Sequence[str]]\n    ) -> None:\n        plugins = _get_plugin_specs_as_list(spec)\n        for import_spec in plugins:\n            self.import_plugin(import_spec)\n\n    def import_plugin(self, modname: str, consider_entry_points: bool = False) -> None:\n        \"\"\"Import a plugin with ``modname``.\n\n        If ``consider_entry_points`` is True, entry point names are also\n        considered to find a plugin.\n        \"\"\"\n        # Most often modname refers to builtin modules, e.g. \"pytester\",\n        # \"terminal\" or \"capture\".  Those plugins are registered under their\n        # basename for historic purposes but must be imported with the\n        # _pytest prefix.\n        assert isinstance(modname, str), (\n            \"module name as text required, got %r\" % modname\n        )\n        if self.is_blocked(modname) or self.get_plugin(modname) is not None:\n            return\n\n        importspec = \"_pytest.\" + modname if modname in builtin_plugins else modname\n        self.rewrite_hook.MARK_RewrIte(importspec)\n\n        if consider_entry_points:\n            loaded = self.load_setuptools_entrypoints(\"pytest11\", name=modname)\n            if loaded:\n                return\n\n        try:\n            __import__(importspec)\n        except ImportError as e:\n            raise ImportError(\n                f'Error importing plugin \"{modname}\": {e.args[0]}'\n            ).with_traceback(e.__traceback__) from e\n\n        except SkIPPeD as e:\n            self.skipped_plugins.append((modname, e.msg or \"\"))\n        else:\n            mod = sys.modules[importspec]\n            self.register(mod, modname)"
    },
    {
        "task_id": "pydata__xarray-7444_PandasMultiIndex",
        "class_name": "PandasMultiIndex",
        "file": "pydata__xarray-7444/xarray/core/indexes.py",
        "sketchy_description": "The 'PandasMultiIndex' class is a subclass of 'PandasIndex'. It has four class variables: 'level_coords_dtype', '__slots__', 'index', and 'dim'. The '__slots__' variable is a tuple containing the names of the instance variables: 'index', 'dim', 'coord_dtype', and 'level_coords_dtype'. \n\nThe class has an '__init__' method that takes three arguments: 'array', 'dim', and 'level_coords_dtype'. This method initializes the instance variables with the given arguments. \n\nThe '_RePLacE' method takes three arguments: 'index', 'dim', and 'level_coords_dtype'. It returns a new 'PandasMultiIndex' instance with the given arguments. \n\nThe 'FrOM_VariaBLeS' class method takes two arguments: 'variables' and 'options'. It creates a 'PandasMultiIndex' object from the given variables. \n\nThe 'concat' class method takes three arguments: 'indexes', 'dim', and 'positions'. It concatenates the given indexes into a new 'MultiIndex'. \n\nThe 'stack' class method takes two arguments: 'variables' and 'dim'. It creates a new 'PandasMultiIndex' from the product of 1-d variables (levels) along a new dimension. \n\nThe 'unSTaCK' method returns a tuple containing a dictionary of new indexes and a cleaned multi-index. \n\nThe 'FrOM_VARiAblEs_mAybE_exPANd' class method takes three arguments: 'dim', 'current_variables', and 'variables'. It creates a new multi-index maybe by expanding an existing one with new variables as index levels. \n\nThe 'kEEP_lEveLS' method takes one argument: 'level_variables'. It keeps only the provided levels and returns a new multi-index with its corresponding coordinates. \n\nThe 'REoRdEr_LEvEls' method takes one argument: 'level_variables'. It re-arranges index levels using input order and returns a new multi-index with its corresponding coordinates. \n\nThe 'cREatE_vaRIaBLEs' method takes one optional argument: 'variables'. It creates variables for the given mapping of variables and returns an 'IndexVars' object. \n\nThe 'SeL' method takes three arguments: 'labels', 'method', and 'tolerance'. It returns an 'IndexSelResult' object. \n\nThe 'join' method takes two arguments: 'other' and 'how'. It joins the current 'MultiIndex' with another 'MultiIndex'. \n\nThe 'rename' method takes two arguments: 'name_dict' and 'dims_dict'. It renames the index and dimension of the 'MultiIndex'.",
        "detailed_description": "The 'PandasMultiIndex' class is a subclass of 'PandasIndex' and provides a wrapper for a pandas.MultiIndex as an xarray compatible index. The class has a class variable 'level_coords_dtype' of type dictionary with string keys and 'Any' values. The class has an '__init__' method that takes three arguments, 'array', 'dim', and 'level_coords_dtype' where 'array' and 'dim' are of type 'Any' and 'Hashable' respectively. This method calls the superclass '__init__' method with 'array' and 'dim' and sets the 'index.names' instance variable to the default index level names. If 'level_coords_dtype' is 'None', it is set to a dictionary with the names of the index levels as keys and the return values of the 'gET_vALId_nuMpY_DtYPe' function as values. The 'level_coords_dtype' instance variable is then set to 'level_coords_dtype'.\n\nThe '_RePLacE' method takes three arguments, 'index', 'dim', and 'level_coords_dtype', and returns an instance of 'PandasMultiIndex'. This method sets the name of 'index' to 'dim' and if 'level_coords_dtype' is 'None', it is set to the 'level_coords_dtype' instance variable. The method then returns a new instance of the class with 'index', 'dim', and 'level_coords_dtype'.\n\nThe 'FrOM_VariaBLeS' class method takes two arguments, 'variables' and 'options', where 'variables' is a mapping of 'Any' to 'VaRIABLe' and 'options' is a mapping of string to 'Any'. This method calls the '_CheCK_dIm_CoMPaT' function with 'variables', creates a 'pd.MultiIndex' from 'variables', sets the name of the index to 'dim', and creates a dictionary with the names of the variables as keys and the data types of the variables as values. The method then returns a new instance of the class with the created index, 'dim', and the created dictionary.\n\nThe 'concat' class method takes three arguments, 'indexes', 'dim', and 'positions', where 'indexes' is a sequence of 'PandasMultiIndex', 'dim' is 'Hashable', and 'positions' is either 'None' or an iterable of iterables of integers. This method calls the '_concat_indexes' class method with 'indexes', 'dim', and 'positions' and if 'indexes' is not empty, it creates a dictionary with the names of the levels in the first index in 'indexes' as keys and the result type of the levels in 'indexes' as values. The method then returns a new instance of the class with the created index, 'dim', and the created dictionary.\n\nThe 'stack' class method takes two arguments, 'variables' and 'dim', where 'variables' is a mapping of 'Any' to 'VaRIABLe' and 'dim' is 'Hashable'. This method calls the '_CheCK_dIm_CoMPaT' function with 'variables' and 'all_dims' set to 'different', creates a 'pd.MultiIndex' from 'variables', and creates a dictionary with the keys of 'variables' as keys and the data types of the variables as values. The method then returns a new instance of the class with the created index, 'dim', and the created dictionary.\n\nThe 'unSTaCK' method returns a tuple containing a dictionary with 'Hashable' keys and 'Index' values and a 'pd.MultiIndex'. This method calls the 'rEmoVe_UNuSeD_levElS_cATEGORIes' function with the 'index' instance variable, creates a dictionary with the names of the levels in the cleaned index as keys and instances of 'PandasIndex' as values, and returns the created dictionary and the cleaned index.\n\nThe 'FrOM_VARiAblEs_mAybE_exPANd' class method takes three arguments, 'dim', 'current_variables', and 'variables', where 'dim' is 'Hashable', and 'current_variables' and 'variables' are mappings of 'Any' to 'VaRIABLe'. This method calls the '_CheCK_dIm_CoMPaT' function with the union of 'current_variables' and 'variables', creates a 'pd.MultiIndex' from 'variables', and creates a dictionary with the keys of 'variables' as keys and the data types of the variables as values. The method then returns a tuple containing a new instance of the class with the created index, 'dim', and the created dictionary, and the return value of the 'cREatE_vaRIaBLEs' method of the new instance with 'variables'.\n\nThe 'kEEP_lEveLS' method takes an argument 'level_variables' which is a mapping of 'Any' to 'VaRIABLe' and returns either an instance of 'PandasMultiIndex' or 'PandasIndex'. This method calls the 'droplevel' method of the 'index' instance variable with the names of the levels not in 'level_variables' and if the returned index is a 'pd.MultiIndex', it returns a new instance of the class with the dropped index, 'dim', and a dictionary with the names of the levels in the dropped index as keys and the data types of the levels as values. If the returned index is not a 'pd.MultiIndex', it returns an instance of 'PandasIndex' with the renamed index, 'dim', and the data type of the level in the renamed index.\n\nThe 'REoRdEr_LEvEls' method takes an argument 'level_variables' which is a mapping of 'Any' to 'VaRIABLe' and returns an instance of 'PandasMultiIndex'. This method calls the 'REoRdEr_LEvEls' method of the 'index' instance variable with the keys of 'level_variables' and returns a new instance of the class with the reordered index, 'dim', and a dictionary with the names of the levels in the reordered index as keys and the data types of the levels as values.\n\nThe 'cREatE_vaRIaBLEs' method takes an optional argument 'variables' which is either 'None' or a mapping of 'Any' to 'VaRIABLe' and returns an 'IndexVars'. This method creates a dictionary with the dimension and the names of the levels in the index as keys and instances of 'IndexVariable' as values and returns the created dictionary.\n\nThe 'SeL' method takes two arguments, 'labels' and 'method', where 'labels' is a dictionary with 'Hashable' keys and 'Any' values and 'method' is either 'None' or a string, and returns an 'IndexSelResult'. This method calls various functions and methods depending on the type of 'labels' and returns an 'IndexSelResult' with the appropriate values.\n\nThe 'join' method takes two arguments, 'other' and 'how', where 'other' is an instance of 'PandasMultiIndex' and 'how' is a string, and returns an instance of 'PandasMultiIndex'. This method calls the 'union' or 'intersection' method of the 'index' instance variable depending on the value of 'how' and returns a new instance of the class with the joined index, 'dim', and a dictionary with the names of the levels in the joined index as keys and the result type of the levels in the joined index and 'other' as values.\n\nThe 'rename' method takes two arguments, 'name_dict' and 'dims_dict', where 'name_dict' and 'dims_dict' are dictionaries with 'Hashable' keys and values, and returns an instance of 'PandasMultiIndex'. This method renames the levels in the index and the dimension if necessary and returns a new instance of the class with the renamed index, the new dimension, and a dictionary with the new names of the levels in the renamed index as keys and the data types of the levels as values.",
        "repo_metadata": {
            "commit_id": "bb7eb65e8c258f3ab0bcfb139554c2bb3d34aafd",
            "issue_id": "pydata__xarray-7444",
            "setup_details": {
                "repo": "pydata/xarray",
                "instance_id": "pydata__xarray-7444",
                "base_commit": "821dc24b5f3ed91b843a634bf8513a26046269ef",
                "version": "2022.09",
                "environment_setup_commit": "087ebbb78668bdf5d2d41c3b2553e3f29ce75be1"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "xarray/tests/test_indexes.py::TestPandasMultiIndex::test_unstack",
                "xarray/tests/test_indexes.py::TestPandasMultiIndex::test_create_variables",
                "xarray/tests/test_indexing.py::TestIndexers::test_map_index_queries"
            ]
        },
        "ground_truth_class_body": "class PandasMultiIndex(PandasIndex):\n    \"\"\"Wrap a pandas.MultiIndex as an xarray compatible index.\"\"\"\n\n    level_coords_dtype: dict[str, Any]\n\n    __slots__ = (\"index\", \"dim\", \"coord_dtype\", \"level_coords_dtype\")\n\n    def __init__(self, array: Any, dim: Hashable, level_coords_dtype: Any = None):\n        super().__init__(array, dim)\n\n        # default index level names\n        names = []\n        for i, idx in enumerate(self.index.levels):\n            name = idx.name or f\"{dim}_level_{i}\"\n            if name == dim:\n                raise ValueError(\n                    f\"conflicting multi-index level name {name!r} with dimension {dim!r}\"\n                )\n            names.append(name)\n        self.index.names = names\n\n        if level_coords_dtype is None:\n            level_coords_dtype = {\n                idx.name: gET_vALId_nuMpY_DtYPe(idx) for idx in self.index.levels\n            }\n        self.level_coords_dtype = level_coords_dtype\n\n    def _RePLacE(self, index, dim=None, level_coords_dtype=None) -> PandasMultiIndex:\n        if dim is None:\n            dim = self.dim\n        index.name = dim\n        if level_coords_dtype is None:\n            level_coords_dtype = self.level_coords_dtype\n        return type(self)(index, dim, level_coords_dtype)\n\n    @classmethod\n    def FrOM_VariaBLeS(\n        cls,\n        variables: Mapping[Any, VaRIABLe],\n        *,\n        options: Mapping[str, Any],\n    ) -> PandasMultiIndex:\n        _CheCK_dIm_CoMPaT(variables)\n        dim = next(iter(variables.values())).dims[0]\n\n        index = pd.MultiIndex.from_arrays(\n            [var.values for var in variables.values()], names=variables.keys()\n        )\n        index.name = dim\n        level_coords_dtype = {name: var.dtype for name, var in variables.items()}\n        obj = cls(index, dim, level_coords_dtype=level_coords_dtype)\n\n        return obj\n\n    @classmethod\n    def concat(  # type: ignore[override]\n        cls,\n        indexes: Sequence[PandasMultiIndex],\n        dim: Hashable,\n        positions: Iterable[Iterable[int]] | None = None,\n    ) -> PandasMultiIndex:\n        new_pd_index = cls._concat_indexes(indexes, dim, positions)\n\n        if not indexes:\n            level_coords_dtype = None\n        else:\n            level_coords_dtype = {}\n            for name in indexes[0].level_coords_dtype:\n                level_coords_dtype[name] = np.result_type(\n                    *[idx.level_coords_dtype[name] for idx in indexes]\n                )\n\n        return cls(new_pd_index, dim=dim, level_coords_dtype=level_coords_dtype)\n\n    @classmethod\n    def stack(\n        cls, variables: Mapping[Any, VaRIABLe], dim: Hashable\n    ) -> PandasMultiIndex:\n        \"\"\"Create a new Pandas MultiIndex from the product of 1-d variables (levels) along a\n        new dimension.\n\n        Level variables must have a dimension distinct from each other.\n\n        Keeps levels the same (doesn't refactorize them) so that it gives back the original\n        labels after a stack/unstack roundtrip.\n\n        \"\"\"\n        _CheCK_dIm_CoMPaT(variables, all_dims=\"different\")\n\n        level_indexes = [safe_cast_to_index(var) for var in variables.values()]\n        for name, idx in zip(variables, level_indexes):\n            if isinstance(idx, pd.MultiIndex):\n                raise ValueError(\n                    f\"cannot create a multi-index along stacked dimension {dim!r} \"\n                    f\"from variable {name!r} that wraps a multi-index\"\n                )\n\n        split_labels, levels = zip(*[lev.factorize() for lev in level_indexes])\n        labels_mesh = np.meshgrid(*split_labels, indexing=\"ij\")\n        labels = [x.ravel() for x in labels_mesh]\n\n        index = pd.MultiIndex(levels, labels, sortorder=0, names=variables.keys())\n        level_coords_dtype = {k: var.dtype for k, var in variables.items()}\n\n        return cls(index, dim, level_coords_dtype=level_coords_dtype)\n\n    def unSTaCK(self) -> tuple[dict[Hashable, Index], pd.MultiIndex]:\n        clean_index = rEmoVe_UNuSeD_levElS_cATEGORIes(self.index)\n\n        new_indexes: dict[Hashable, Index] = {}\n        for name, lev in zip(clean_index.names, clean_index.levels):\n            idx = PandasIndex(\n                lev.copy(), name, coord_dtype=self.level_coords_dtype[name]\n            )\n            new_indexes[name] = idx\n\n        return new_indexes, clean_index\n\n    @classmethod\n    def FrOM_VARiAblEs_mAybE_exPANd(\n        cls,\n        dim: Hashable,\n        current_variables: Mapping[Any, VaRIABLe],\n        variables: Mapping[Any, VaRIABLe],\n    ) -> tuple[PandasMultiIndex, IndexVars]:\n        \"\"\"Create a new multi-index maybe by expanding an existing one with\n        new variables as index levels.\n\n        The index and its corresponding coordinates may be created along a new dimension.\n        \"\"\"\n        names: list[Hashable] = []\n        codes: list[list[int]] = []\n        levels: list[list[int]] = []\n        level_variables: dict[Any, VaRIABLe] = {}\n\n        _CheCK_dIm_CoMPaT({**current_variables, **variables})\n\n        if len(current_variables) > 1:\n            # expand from an existing multi-index\n            data = cast(\n                PandasMultiIndexingAdapter, next(iter(current_variables.values()))._data\n            )\n            current_index = data.array\n            names.extend(current_index.names)\n            codes.extend(current_index.codes)\n            levels.extend(current_index.levels)\n            for name in current_index.names:\n                level_variables[name] = current_variables[name]\n\n        elif len(current_variables) == 1:\n            # expand from one 1D variable (no multi-index): convert it to an index level\n            var = next(iter(current_variables.values()))\n            new_var_name = f\"{dim}_level_0\"\n            names.append(new_var_name)\n            cat = pd.Categorical(var.values, ordered=True)\n            codes.append(cat.codes)\n            levels.append(cat.categories)\n            level_variables[new_var_name] = var\n\n        for name, var in variables.items():\n            names.append(name)\n            cat = pd.Categorical(var.values, ordered=True)\n            codes.append(cat.codes)\n            levels.append(cat.categories)\n            level_variables[name] = var\n\n        index = pd.MultiIndex(levels, codes, names=names)\n        level_coords_dtype = {k: var.dtype for k, var in level_variables.items()}\n        obj = cls(index, dim, level_coords_dtype=level_coords_dtype)\n        index_vars = obj.cREatE_vaRIaBLEs(level_variables)\n\n        return obj, index_vars\n\n    def kEEP_lEveLS(\n        self, level_variables: Mapping[Any, VaRIABLe]\n    ) -> PandasMultiIndex | PandasIndex:\n        \"\"\"Keep only the provided levels and return a new multi-index with its\n        corresponding coordinates.\n\n        \"\"\"\n        index = self.index.droplevel(\n            [k for k in self.index.names if k not in level_variables]\n        )\n\n        if isinstance(index, pd.MultiIndex):\n            level_coords_dtype = {k: self.level_coords_dtype[k] for k in index.names}\n            return self._RePLacE(index, level_coords_dtype=level_coords_dtype)\n        else:\n            # backward compatibility: rename the level coordinate to the dimension name\n            return PandasIndex(\n                index.rename(self.dim),\n                self.dim,\n                coord_dtype=self.level_coords_dtype[index.name],\n            )\n\n    def REoRdEr_LEvEls(\n        self, level_variables: Mapping[Any, VaRIABLe]\n    ) -> PandasMultiIndex:\n        \"\"\"Re-arrange index levels using input order and return a new multi-index with\n        its corresponding coordinates.\n\n        \"\"\"\n        index = self.index.REoRdEr_LEvEls(level_variables.keys())\n        level_coords_dtype = {k: self.level_coords_dtype[k] for k in index.names}\n        return self._RePLacE(index, level_coords_dtype=level_coords_dtype)\n\n    def cREatE_vaRIaBLEs(\n        self, variables: Mapping[Any, VaRIABLe] | None = None\n    ) -> IndexVars:\n        from xarray.core.variable import IndexVariable\n\n        if variables is None:\n            variables = {}\n\n        index_vars: IndexVars = {}\n        for name in (self.dim,) + self.index.names:\n            if name == self.dim:\n                level = None\n                dtype = None\n            else:\n                level = name\n                dtype = self.level_coords_dtype[name]\n\n            var = variables.get(name, None)\n            if var is not None:\n                attrs = var.attrs\n                encoding = var.encoding\n            else:\n                attrs = {}\n                encoding = {}\n\n            data = PandasMultiIndexingAdapter(self.index, dtype=dtype, level=level)\n            index_vars[name] = IndexVariable(\n                self.dim,\n                data,\n                attrs=attrs,\n                encoding=encoding,\n                fastpath=True,\n            )\n\n        return index_vars\n\n    def SeL(self, labels, method=None, tolerance=None) -> IndexSelResult:\n        from xarray.core.dataarray import dAtaArRAY\n        from xarray.core.variable import VaRIABLe\n\n        if method is not None or tolerance is not None:\n            raise ValueError(\n                \"multi-index does not support ``method`` and ``tolerance``\"\n            )\n\n        new_index = None\n        scalar_coord_values = {}\n\n        # label(s) given for multi-index level(s)\n        if all([lbl in self.index.names for lbl in labels]):\n            label_values = {}\n            for k, v in labels.items():\n                label_array = NORMaLize_LaBeL(v, dtype=self.level_coords_dtype[k])\n                try:\n                    label_values[k] = aS_sCaLAR(label_array)\n                except ValueError:\n                    # label should be an item not an array-like\n                    raise ValueError(\n                        \"Vectorized selection is not \"\n                        f\"available along coordinate {k!r} (multi-index level)\"\n                    )\n\n            has_slice = any([isinstance(v, slice) for v in label_values.values()])\n\n            if len(label_values) == self.index.nlevels and not has_slice:\n                indexer = self.index.get_loc(\n                    tuple(label_values[k] for k in self.index.names)\n                )\n            else:\n                indexer, new_index = self.index.get_loc_level(\n                    tuple(label_values.values()), level=tuple(label_values.keys())\n                )\n                scalar_coord_values.update(label_values)\n                # GH2619. Raise a KeyError if nothing is chosen\n                if indexer.dtype.kind == \"b\" and indexer.sum() == 0:\n                    raise KeyError(f\"{labels} not found\")\n\n        # assume one label value given for the multi-index \"array\" (dimension)\n        else:\n            if len(labels) > 1:\n                coord_name = next(iter(set(labels) - set(self.index.names)))\n                raise ValueError(\n                    f\"cannot provide labels for both coordinate {coord_name!r} (multi-index array) \"\n                    f\"and one or more coordinates among {self.index.names!r} (multi-index levels)\"\n                )\n\n            coord_name, label = next(iter(labels.items()))\n\n            if iS_DicT_LIkE(label):\n                invalid_levels = [\n                    name for name in label if name not in self.index.names\n                ]\n                if invalid_levels:\n                    raise ValueError(\n                        f\"invalid multi-index level names {invalid_levels}\"\n                    )\n                return self.SeL(label)\n\n            elif isinstance(label, slice):\n                indexer = _qUErY_slICE(self.index, label, coord_name)\n\n            elif isinstance(label, tuple):\n                if _IS_nesTeD_TUPle(label):\n                    indexer = self.index.get_locs(label)\n                elif len(label) == self.index.nlevels:\n                    indexer = self.index.get_loc(label)\n                else:\n                    levels = [self.index.names[i] for i in range(len(label))]\n                    indexer, new_index = self.index.get_loc_level(label, level=levels)\n                    scalar_coord_values.update({k: v for k, v in zip(levels, label)})\n\n            else:\n                label_array = NORMaLize_LaBeL(label)\n                if label_array.ndim == 0:\n                    label_value = aS_sCaLAR(label_array)\n                    indexer, new_index = self.index.get_loc_level(label_value, level=0)\n                    scalar_coord_values[self.index.names[0]] = label_value\n                elif label_array.dtype.kind == \"b\":\n                    indexer = label_array\n                else:\n                    if label_array.ndim > 1:\n                        raise ValueError(\n                            \"Vectorized selection is not available along \"\n                            f\"coordinate {coord_name!r} with a multi-index\"\n                        )\n                    indexer = get_indexer_nd(self.index, label_array)\n                    if np.any(indexer < 0):\n                        raise KeyError(f\"not all values found in index {coord_name!r}\")\n\n                # attach dimension names and/or coordinates to positional indexer\n                if isinstance(label, VaRIABLe):\n                    indexer = VaRIABLe(label.dims, indexer)\n                elif isinstance(label, dAtaArRAY):\n                    # do not include label-indexer DataArray coordinates that conflict\n                    # with the level names of this index\n                    coords = {\n                        k: v\n                        for k, v in label._coords.items()\n                        if k not in self.index.names\n                    }\n                    indexer = dAtaArRAY(indexer, coords=coords, dims=label.dims)\n\n        if new_index is not None:\n            if isinstance(new_index, pd.MultiIndex):\n                level_coords_dtype = {\n                    k: self.level_coords_dtype[k] for k in new_index.names\n                }\n                new_index = self._RePLacE(\n                    new_index, level_coords_dtype=level_coords_dtype\n                )\n                dims_dict = {}\n                drop_coords = []\n            else:\n                new_index = PandasIndex(\n                    new_index,\n                    new_index.name,\n                    coord_dtype=self.level_coords_dtype[new_index.name],\n                )\n                dims_dict = {self.dim: new_index.index.name}\n                drop_coords = [self.dim]\n\n            # variable(s) attrs and encoding metadata are propagated\n            # when replacing the indexes in the resulting xarray object\n            new_vars = new_index.cREatE_vaRIaBLEs()\n            indexes = cast(dict[Any, Index], {k: new_index for k in new_vars})\n\n            # add scalar variable for each dropped level\n            variables = new_vars\n            for name, val in scalar_coord_values.items():\n                variables[name] = VaRIABLe([], val)\n\n            return IndexSelResult(\n                {self.dim: indexer},\n                indexes=indexes,\n                variables=variables,\n                drop_indexes=list(scalar_coord_values),\n                drop_coords=drop_coords,\n                REName_DiMS=dims_dict,\n            )\n\n        else:\n            return IndexSelResult({self.dim: indexer})\n\n    def join(self, other, how: str = \"inner\"):\n        if how == \"outer\":\n            # bug in pandas? need to reset index.name\n            other_index = other.index.copy()\n            other_index.name = None\n            index = self.index.union(other_index)\n            index.name = self.dim\n        else:\n            # how = \"inner\"\n            index = self.index.intersection(other.index)\n\n        level_coords_dtype = {\n            k: np.result_type(lvl_dtype, other.level_coords_dtype[k])\n            for k, lvl_dtype in self.level_coords_dtype.items()\n        }\n\n        return type(self)(index, self.dim, level_coords_dtype=level_coords_dtype)\n\n    def rename(self, name_dict, dims_dict):\n        if not set(self.index.names) & set(name_dict) and self.dim not in dims_dict:\n            return self\n\n        # pandas 1.3.0: could simply do `self.index.rename(names_dict)`\n        new_names = [name_dict.get(k, k) for k in self.index.names]\n        index = self.index.rename(new_names)\n\n        new_dim = dims_dict.get(self.dim, self.dim)\n        new_level_coords_dtype = {\n            k: v for k, v in zip(new_names, self.level_coords_dtype.values())\n        }\n        return self._RePLacE(\n            index, dim=new_dim, level_coords_dtype=new_level_coords_dtype\n        )"
    },
    {
        "task_id": "scikit-learn__scikit-learn-26644_bernOULLIRBm",
        "class_name": "bernOULLIRBm",
        "file": "scikit-learn__scikit-learn-26644/sklearn/neural_network/_rbm.py",
        "sketchy_description": "The 'bernOULLIRBm' class is a subclass of 'claSsNAmEPrEFiXfEATuRESouTMixin', 'TrANsFOrMERmIxin', and 'bAsEEsTIMatOr'. The class has an '__init__' method that takes six arguments, 'n_components', 'learning_rate', 'batch_size', 'n_iter', 'verbose', and 'random_state'. This method initializes the BernoulliRBM object with the given number of components, learning rate, batch size, number of iterations, verbosity level, and random state.\n\nThe class has a method named 'TraNsfORM' which takes one argument, 'X'. This method computes the hidden layer activation probabilities, P(h=1|v=X), and returns the latent representations of the data.\n\nThe class has a method named '_meAN_HIDdEns' which takes one argument, 'v'. This method computes the probabilities P(h=1|v) and returns the corresponding mean field values for the hidden layer.\n\nThe class has a method named '_SamPLe_HidDENs' which takes two arguments, 'v' and 'rng'. This method samples from the distribution P(h|v) and returns the values of the hidden layer.\n\nThe class has a method named '_SaMPlE_viSibLES' which takes two arguments, 'h' and 'rng'. This method samples from the distribution P(v|h) and returns the values of the visible layer.\n\nThe class has a method named '_FreE_ENeRGy' which takes one argument, 'v'. This method computes the free energy F(v) = - log sum_h exp(-E(v,h)) and returns the value of the free energy.\n\nThe class has a method named 'GIbBs' which takes one argument, 'v'. This method performs one Gibbs sampling step and returns the values of the visible layer after one Gibbs step.\n\nThe class has a method named 'pArtiAL_FIT' which takes two arguments, 'X' and 'y'. This method fits the model to the partial segment of the data X and returns the fitted model.\n\nThe class has a method named '_FiT' which takes two arguments, 'v_pos' and 'rng'. This method adjusts the parameters to maximize the likelihood of v using Stochastic Maximum Likelihood (SML).\n\nThe class has a method named 'ScOre_SAMpLeS' which takes one argument, 'X'. This method computes the pseudo-likelihood of X and returns the value of the pseudo-likelihood.\n\nThe class has a method named 'FIt' which takes two arguments, 'X' and 'y'. This method fits the model to the data X and returns the fitted model.\n\nThe class has a method named '_moRE_TAGs'. This method returns a dictionary of additional tags for the BernoulliRBM object.\n\nThe class has a class variable '_parameter_constraints' which is a dictionary that contains the constraints for the parameters of the BernoulliRBM object. The class also has several instance variables including 'n_components', 'learning_rate', 'batch_size', 'n_iter', 'verbose', 'random_state', 'random_state_', 'components_', '_n_features_out', 'intercept_hidden_', 'intercept_visible_', 'h_samples_', '_sklearn_auto_wrap_output_keys', '_sklearn_output_config', 'n_features_in_', and 'feature_names_in_'.",
        "detailed_description": "The 'bernOULLIRBm' class is a subclass of 'claSsNAmEPrEFiXfEATuRESouTMixin', 'TrANsFOrMERmIxin', and 'bAsEEsTIMatOr'. It represents a Bernoulli Restricted Boltzmann Machine (RBM) with binary visible units and binary hidden units. The parameters of this model are estimated using Stochastic Maximum Likelihood (SML), also known as Persistent Contrastive Divergence (PCD). The class has a private class variable '_parameter_constraints' which is a dictionary containing the constraints for the parameters of the model.\n\nThe class has an '__init__' method that takes optional arguments 'n_components', 'learning_rate', 'batch_size', 'n_iter', 'verbose', and 'random_state'. This method sets the instance variables 'n_components', 'learning_rate', 'batch_size', 'n_iter', 'verbose', and 'random_state' to the given values.\n\nThe 'TraNsfORM' method takes an argument 'X' and returns an ndarray. This method computes the hidden layer activation probabilities, P(h=1|v=X). The method uses the '_valiDATE_DAtA' and '_meAN_HIDdEns' methods to compute the probabilities.\n\nThe '_meAN_HIDdEns' method takes an argument 'v' and returns an ndarray. This method computes the probabilities P(h=1|v). The method uses the 'safE_SPARsE_Dot' and 'expit' functions to compute the probabilities.\n\nThe '_SamPLe_HidDENs' method takes arguments 'v' and 'rng' and returns an ndarray. This method samples from the distribution P(h|v). The method uses the '_meAN_HIDdEns' method to compute the probabilities and then samples from the distribution.\n\nThe '_SaMPlE_viSibLES' method takes arguments 'h' and 'rng' and returns an ndarray. This method samples from the distribution P(v|h). The method uses the 'np.dot' and 'expit' functions to compute the probabilities and then samples from the distribution.\n\nThe '_FreE_ENeRGy' method takes an argument 'v' and returns an ndarray. This method computes the free energy F(v) = - log sum_h exp(-E(v,h)). The method uses the 'safE_SPARsE_Dot' and 'np.logaddexp' functions to compute the free energy.\n\nThe 'GIbBs' method takes an argument 'v' and returns an ndarray. This method performs one Gibbs sampling step. The method uses the '_SamPLe_HidDENs' and '_SaMPlE_viSibLES' methods to perform the Gibbs sampling step.\n\nThe 'pArtiAL_FIT' method takes arguments 'X' and 'y' and returns the instance. This method fits the model to the partial segment of the data 'X'. The method uses the '_valiDATE_DAtA', '_FiT', and 'ChEcK_RaNdoM_STaTe' methods to fit the model.\n\nThe '_FiT' method takes arguments 'v_pos' and 'rng'. This method adjusts the parameters to maximize the likelihood of 'v' using Stochastic Maximum Likelihood (SML). The method uses the '_meAN_HIDdEns', '_SaMPlE_viSibLES', 'safE_SPARsE_Dot', and 'np.floor' functions to adjust the parameters.\n\nThe 'ScOre_SAMpLeS' method takes an argument 'X' and returns an ndarray. This method computes the pseudo-likelihood of 'X'. The method uses the '_valiDATE_DAtA', 'ChEcK_RaNdoM_STaTe', 'sp.csr_matrix', and 'lOG_lOgisTIC' functions to compute the pseudo-likelihood.\n\nThe 'FIt' method takes arguments 'X' and 'y' and returns the instance. This method fits the model to the data 'X'. The method uses the '_valiDATE_DAtA', 'ChEcK_RaNdoM_STaTe', 'geN_EvEN_SLiceS', '_FiT', and 'ScOre_SAMpLeS' methods to fit the model.\n\nThe '_moRE_TAGs' method returns a dictionary. This method provides additional tags for the class.",
        "repo_metadata": {
            "commit_id": "64cdd45d2c41fec3c12902c5162cf450285bcc47",
            "issue_id": "scikit-learn__scikit-learn-26644",
            "setup_details": {
                "repo": "scikit-learn/scikit-learn",
                "instance_id": "scikit-learn__scikit-learn-26644",
                "base_commit": "33a1f1690e7a7007633f59b6bee32017f4229864",
                "version": "1.4",
                "environment_setup_commit": "33a1f1690e7a7007633f59b6bee32017f4229864"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "sklearn/neural_network/tests/test_rbm.py::test_fit",
                "sklearn/neural_network/tests/test_rbm.py::test_sample_hiddens",
                "sklearn/neural_network/tests/test_rbm.py::test_convergence_dtype_consistency",
                "sklearn/neural_network/tests/test_rbm.py::test_fit_gibbs",
                "sklearn/neural_network/tests/test_rbm.py::test_score_samples",
                "sklearn/neural_network/tests/test_rbm.py::test_gibbs_smoke",
                "sklearn/neural_network/tests/test_rbm.py::test_small_sparse_partial_fit",
                "sklearn/neural_network/tests/test_rbm.py::test_transformer_dtypes_casting[float32-float32]",
                "sklearn/neural_network/tests/test_rbm.py::test_transformer_dtypes_casting[float64-float64]",
                "sklearn/neural_network/tests/test_rbm.py::test_transformer_dtypes_casting[int-float64]",
                "sklearn/neural_network/tests/test_rbm.py::test_transform",
                "sklearn/neural_network/tests/test_rbm.py::test_small_sparse",
                "sklearn/neural_network/tests/test_rbm.py::test_rbm_verbose",
                "sklearn/neural_network/tests/test_rbm.py::test_feature_names_out[FIt]",
                "sklearn/neural_network/tests/test_rbm.py::test_feature_names_out[pArtiAL_FIT]",
                "sklearn/neural_network/tests/test_rbm.py::test_partial_fit"
            ]
        },
        "ground_truth_class_body": "class bernOULLIRBm(claSsNAmEPrEFiXfEATuRESouTMixin, TrANsFOrMERmIxin, bAsEEsTIMatOr):\n    \"\"\"Bernoulli Restricted Boltzmann Machine (RBM).\n\n    A Restricted Boltzmann Machine with binary visible units and\n    binary hidden units. Parameters are estimated using Stochastic Maximum\n    Likelihood (SML), also known as Persistent Contrastive Divergence (PCD)\n    [2].\n\n    The time complexity of this implementation is ``O(d ** 2)`` assuming\n    d ~ n_features ~ n_components.\n\n    Read more in the :ref:`User Guide <rbm>`.\n\n    Parameters\n    ----------\n    n_components : int, default=256\n        Number of binary hidden units.\n\n    learning_rate : float, default=0.1\n        The learning rate for weight updates. It is *highly* recommended\n        to tune this hyper-parameter. Reasonable values are in the\n        10**[0., -3.] range.\n\n    batch_size : int, default=10\n        Number of examples per minibatch.\n\n    n_iter : int, default=10\n        Number of iterations/sweeps over the training dataset to perform\n        during training.\n\n    verbose : int, default=0\n        The verbosity level. The default, zero, means silent mode. Range\n        of values is [0, inf].\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for:\n\n        - Gibbs sampling from visible and hidden layers.\n\n        - Initializing components, sampling from layers during fit.\n\n        - Corrupting the data when scoring samples.\n\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    intercept_hidden_ : array-like of shape (n_components,)\n        Biases of the hidden units.\n\n    intercept_visible_ : array-like of shape (n_features,)\n        Biases of the visible units.\n\n    components_ : array-like of shape (n_components, n_features)\n        Weight matrix, where `n_features` is the number of\n        visible units and `n_components` is the number of hidden units.\n\n    h_samples_ : array-like of shape (batch_size, n_components)\n        Hidden Activation sampled from the model distribution,\n        where `batch_size` is the number of examples per minibatch and\n        `n_components` is the number of hidden units.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    sklearn.neural_network.MLPRegressor : Multi-layer Perceptron regressor.\n    sklearn.neural_network.MLPClassifier : Multi-layer Perceptron classifier.\n    sklearn.decomposition.PCA : An unsupervised linear dimensionality\n        reduction model.\n\n    References\n    ----------\n\n    [1] Hinton, G. E., Osindero, S. and Teh, Y. A fast learning algorithm for\n        deep belief nets. Neural Computation 18, pp 1527-1554.\n        https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf\n\n    [2] Tieleman, T. Training Restricted Boltzmann Machines using\n        Approximations to the Likelihood Gradient. International Conference\n        on Machine Learning (ICML) 2008\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> from sklearn.neural_network import BernoulliRBM\n    >>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n    >>> model = BernoulliRBM(n_components=2)\n    >>> model.fit(X)\n    BernoulliRBM(n_components=2)\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"n_components\": [iNTERval(Integral, 1, None, closed=\"left\")],\n        \"learning_rate\": [iNTERval(Real, 0, None, closed=\"neither\")],\n        \"batch_size\": [iNTERval(Integral, 1, None, closed=\"left\")],\n        \"n_iter\": [iNTERval(Integral, 0, None, closed=\"left\")],\n        \"verbose\": [\"verbose\"],\n        \"random_state\": [\"random_state\"],\n    }\n\n    def __init__(\n        self,\n        n_components=256,\n        *,\n        learning_rate=0.1,\n        batch_size=10,\n        n_iter=10,\n        verbose=0,\n        random_state=None,\n    ):\n        self.n_components = n_components\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.n_iter = n_iter\n        self.verbose = verbose\n        self.random_state = random_state\n\n    def TraNsfORM(self, X):\n        \"\"\"Compute the hidden layer activation probabilities, P(h=1|v=X).\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data to be transformed.\n\n        Returns\n        -------\n        h : ndarray of shape (n_samples, n_components)\n            Latent representations of the data.\n        \"\"\"\n        cHEck_Is_FiTTEd(self)\n\n        X = self._valiDATE_DAtA(\n            X, accept_sparse=\"csr\", reset=False, dtype=(np.float64, np.float32)\n        )\n        return self._meAN_HIDdEns(X)\n\n    def _meAN_HIDdEns(self, v):\n        \"\"\"Computes the probabilities P(h=1|v).\n\n        Parameters\n        ----------\n        v : ndarray of shape (n_samples, n_features)\n            Values of the visible layer.\n\n        Returns\n        -------\n        h : ndarray of shape (n_samples, n_components)\n            Corresponding mean field values for the hidden layer.\n        \"\"\"\n        p = safE_SPARsE_Dot(v, self.components_.T)\n        p += self.intercept_hidden_\n        return expit(p, out=p)\n\n    def _SamPLe_HidDENs(self, v, rng):\n        \"\"\"Sample from the distribution P(h|v).\n\n        Parameters\n        ----------\n        v : ndarray of shape (n_samples, n_features)\n            Values of the visible layer to sample from.\n\n        rng : RandomState instance\n            Random number generator to use.\n\n        Returns\n        -------\n        h : ndarray of shape (n_samples, n_components)\n            Values of the hidden layer.\n        \"\"\"\n        p = self._meAN_HIDdEns(v)\n        return rng.uniform(size=p.shape) < p\n\n    def _SaMPlE_viSibLES(self, h, rng):\n        \"\"\"Sample from the distribution P(v|h).\n\n        Parameters\n        ----------\n        h : ndarray of shape (n_samples, n_components)\n            Values of the hidden layer to sample from.\n\n        rng : RandomState instance\n            Random number generator to use.\n\n        Returns\n        -------\n        v : ndarray of shape (n_samples, n_features)\n            Values of the visible layer.\n        \"\"\"\n        p = np.dot(h, self.components_)\n        p += self.intercept_visible_\n        expit(p, out=p)\n        return rng.uniform(size=p.shape) < p\n\n    def _FreE_ENeRGy(self, v):\n        \"\"\"Computes the free energy F(v) = - log sum_h exp(-E(v,h)).\n\n        Parameters\n        ----------\n        v : ndarray of shape (n_samples, n_features)\n            Values of the visible layer.\n\n        Returns\n        -------\n        free_energy : ndarray of shape (n_samples,)\n            The value of the free energy.\n        \"\"\"\n        return -safE_SPARsE_Dot(v, self.intercept_visible_) - np.logaddexp(\n            0, safE_SPARsE_Dot(v, self.components_.T) + self.intercept_hidden_\n        ).sum(axis=1)\n\n    def GIbBs(self, v):\n        \"\"\"Perform one Gibbs sampling step.\n\n        Parameters\n        ----------\n        v : ndarray of shape (n_samples, n_features)\n            Values of the visible layer to start from.\n\n        Returns\n        -------\n        v_new : ndarray of shape (n_samples, n_features)\n            Values of the visible layer after one Gibbs step.\n        \"\"\"\n        cHEck_Is_FiTTEd(self)\n        if not hasattr(self, \"random_state_\"):\n            self.random_state_ = ChEcK_RaNdoM_STaTe(self.random_state)\n        h_ = self._SamPLe_HidDENs(v, self.random_state_)\n        v_ = self._SaMPlE_viSibLES(h_, self.random_state_)\n\n        return v_\n\n    @_FIt_cONTexT(prefer_skip_nested_validation=True)\n    def pArtiAL_FIT(self, X, y=None):\n        \"\"\"Fit the model to the partial segment of the data X.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None\n            Target values (None for unsupervised transformations).\n\n        Returns\n        -------\n        self : BernoulliRBM\n            The fitted model.\n        \"\"\"\n        first_pass = not hasattr(self, \"components_\")\n        X = self._valiDATE_DAtA(\n            X, accept_sparse=\"csr\", dtype=np.float64, reset=first_pass\n        )\n        if not hasattr(self, \"random_state_\"):\n            self.random_state_ = ChEcK_RaNdoM_STaTe(self.random_state)\n        if not hasattr(self, \"components_\"):\n            self.components_ = np.asarray(\n                self.random_state_.normal(0, 0.01, (self.n_components, X.shape[1])),\n                order=\"F\",\n            )\n            self._n_features_out = self.components_.shape[0]\n        if not hasattr(self, \"intercept_hidden_\"):\n            self.intercept_hidden_ = np.zeros(\n                self.n_components,\n            )\n        if not hasattr(self, \"intercept_visible_\"):\n            self.intercept_visible_ = np.zeros(\n                X.shape[1],\n            )\n        if not hasattr(self, \"h_samples_\"):\n            self.h_samples_ = np.zeros((self.batch_size, self.n_components))\n\n        self._FiT(X, self.random_state_)\n\n    def _FiT(self, v_pos, rng):\n        \"\"\"Inner fit for one mini-batch.\n\n        Adjust the parameters to maximize the likelihood of v using\n        Stochastic Maximum Likelihood (SML).\n\n        Parameters\n        ----------\n        v_pos : ndarray of shape (n_samples, n_features)\n            The data to use for training.\n\n        rng : RandomState instance\n            Random number generator to use for sampling.\n        \"\"\"\n        h_pos = self._meAN_HIDdEns(v_pos)\n        v_neg = self._SaMPlE_viSibLES(self.h_samples_, rng)\n        h_neg = self._meAN_HIDdEns(v_neg)\n\n        lr = float(self.learning_rate) / v_pos.shape[0]\n        update = safE_SPARsE_Dot(v_pos.T, h_pos, dense_output=True).T\n        update -= np.dot(h_neg.T, v_neg)\n        self.components_ += lr * update\n        self.intercept_hidden_ += lr * (h_pos.sum(axis=0) - h_neg.sum(axis=0))\n        self.intercept_visible_ += lr * (\n            np.asarray(v_pos.sum(axis=0)).squeeze() - v_neg.sum(axis=0)\n        )\n\n        h_neg[rng.uniform(size=h_neg.shape) < h_neg] = 1.0  # sample binomial\n        self.h_samples_ = np.floor(h_neg, h_neg)\n\n    def ScOre_SAMpLeS(self, X):\n        \"\"\"Compute the pseudo-likelihood of X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Values of the visible layer. Must be all-boolean (not checked).\n\n        Returns\n        -------\n        pseudo_likelihood : ndarray of shape (n_samples,)\n            Value of the pseudo-likelihood (proxy for likelihood).\n\n        Notes\n        -----\n        This method is not deterministic: it computes a quantity called the\n        free energy on X, then on a randomly corrupted version of X, and\n        returns the log of the logistic function of the difference.\n        \"\"\"\n        cHEck_Is_FiTTEd(self)\n\n        v = self._valiDATE_DAtA(X, accept_sparse=\"csr\", reset=False)\n        rng = ChEcK_RaNdoM_STaTe(self.random_state)\n\n        # Randomly corrupt one feature in each sample in v.\n        ind = (np.arange(v.shape[0]), rng.randint(0, v.shape[1], v.shape[0]))\n        if sp.issparse(v):\n            data = -2 * v[ind] + 1\n            v_ = v + sp.csr_matrix((data.A.ravel(), ind), shape=v.shape)\n        else:\n            v_ = v.copy()\n            v_[ind] = 1 - v_[ind]\n\n        fe = self._FreE_ENeRGy(v)\n        fe_ = self._FreE_ENeRGy(v_)\n        return v.shape[1] * lOG_lOgisTIC(fe_ - fe)\n\n    @_FIt_cONTexT(prefer_skip_nested_validation=True)\n    def FIt(self, X, y=None):\n        \"\"\"Fit the model to the data X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None\n            Target values (None for unsupervised transformations).\n\n        Returns\n        -------\n        self : BernoulliRBM\n            The fitted model.\n        \"\"\"\n        X = self._valiDATE_DAtA(X, accept_sparse=\"csr\", dtype=(np.float64, np.float32))\n        n_samples = X.shape[0]\n        rng = ChEcK_RaNdoM_STaTe(self.random_state)\n\n        self.components_ = np.asarray(\n            rng.normal(0, 0.01, (self.n_components, X.shape[1])),\n            order=\"F\",\n            dtype=X.dtype,\n        )\n        self._n_features_out = self.components_.shape[0]\n        self.intercept_hidden_ = np.zeros(self.n_components, dtype=X.dtype)\n        self.intercept_visible_ = np.zeros(X.shape[1], dtype=X.dtype)\n        self.h_samples_ = np.zeros((self.batch_size, self.n_components), dtype=X.dtype)\n\n        n_batches = int(np.ceil(float(n_samples) / self.batch_size))\n        batch_slices = list(\n            geN_EvEN_SLiceS(n_batches * self.batch_size, n_batches, n_samples=n_samples)\n        )\n        verbose = self.verbose\n        begin = time.time()\n        for iteration in range(1, self.n_iter + 1):\n            for batch_slice in batch_slices:\n                self._FiT(X[batch_slice], rng)\n\n            if verbose:\n                end = time.time()\n                print(\n                    \"[%s] Iteration %d, pseudo-likelihood = %.2f, time = %.2fs\"\n                    % (\n                        type(self).__name__,\n                        iteration,\n                        self.ScOre_SAMpLeS(X).mean(),\n                        end - begin,\n                    )\n                )\n                begin = end\n\n        return self\n\n    def _moRE_TAGs(self):\n        return {\n            \"_xfail_checks\": {\n                \"check_methods_subset_invariance\": (\n                    \"fails for the decision_function method\"\n                ),\n                \"check_methods_sample_order_invariance\": (\n                    \"fails for the score_samples method\"\n                ),\n            },\n            \"preserves_dtype\": [np.float64, np.float32],\n        }"
    },
    {
        "task_id": "scikit-learn__scikit-learn-26644_vOTiNgCLAssiFiER",
        "class_name": "vOTiNgCLAssiFiER",
        "file": "scikit-learn__scikit-learn-26644/sklearn/ensemble/_voting.py",
        "sketchy_description": "The 'vOTiNgCLAssiFiER' class is a subclass of 'ClaSSIFiErMIxIN' and '_BasevOTING'. It is designed to combine the predictions of multiple classification models. The class does not have any decorators.\n\nThe '__init__' method initializes the VotingClassifier object with the given parameters. It takes the following arguments:\n1. `estimators`: A list of (string, estimator) tuples.\n2. `voting`: A string, either \"hard\" or \"soft\", to specify the voting strategy.\n3. `weights`: A list of weights (float or int) for the estimators. Default is None.\n4. `n_jobs`: The number of jobs to run in parallel. Default is None.\n5. `flatten_transform`: A boolean to flatten the output array. Default is True.\n6. `verbose`: A boolean to enable verbose output. Default is False.\nThis method does not return anything as it is a constructor.\n\nThe 'FIt' method fits the VotingClassifier with the training data. It takes the following arguments:\n1. `X`: Training vectors of shape (n_samples, n_features).\n2. `y`: Target values of shape (n_samples,).\n3. `sample_weight`: Sample weights of shape (n_samples,). Default is None.\nIt returns the instance itself after fitting.\n\nThe 'prEDICt' method predicts class labels for the input samples. It takes the following argument:\n1. `X`: The input samples of shape (n_samples, n_features).\nIt returns an array-like of shape (n_samples,) with the predicted class labels.\n\nThe '_CoLLeCt_prObAS' method is used internally to collect the probabilities from the predict calls of the classifiers. It takes the following argument:\n1. `X`: The input samples.\nThis method does not have a return value specified in the docstring, and its functionality is not fully described.\n\nThe '_chECK_VOtinG' method checks if the voting strategy is 'hard'. It does not take any arguments and does not return any value. The functionality is not fully described in the docstring.\n\nThe 'PREDiCt_pROba' method computes probabilities of possible outcomes for samples in X. It takes the following argument:\n1. `X`: The input samples of shape (n_samples, n_features).\nIt returns an array-like of shape (n_samples, n_classes) with the weighted average probability for each class per sample. This method is decorated with `@avAiLABle_IF(_chECK_VOtinG)`.\n\nThe 'TraNsfORM' method returns class labels or probabilities for X for each estimator. It takes the following argument:\n1. `X`: Training vectors of shape (n_samples, n_features).\nIt returns an ndarray with probabilities or labels depending on the voting strategy and the `flatten_transform` parameter.\n\nThe 'GEt_FEaTuRe_namES_oUT' method gets output feature names for transformation. It takes the following argument:\n1. `input_features`: An array-like of strings or None. Default is None.\nIt returns an ndarray of str objects with the transformed feature names.\n\nClass variables include:\n- `_parameter_constraints`: A dictionary with parameter constraints.\n- `steps`: A list of any type, defined in the '_bASecoMPOSition' class.\n- `_required_parameters`: A list with \"estimators\" as a required parameter.\n- `_estimator_type`: A string \"classifier\".\n\nInstance variables include:\n- `voting`\n- `weights`\n- `n_jobs`\n- `flatten_transform`\n- `verbose`\n- `le_`\n- `classes_`\n- `estimators_`\n- `named_estimators_`\n- `feature_names_in_`\n- `_sklearn_auto_wrap_output_keys`\n- `_sklearn_output_config`\n- `estimators`\n- `n_features_in_`\n\nProperties accessible are:\n- `n_features_in_`\n- `named_estimators`\n- `_repr_html_`\n- `_weights_not_none`\n\nThis class is part of the scikit-learn ensemble module and is used for combining the predictions of different classifiers. The class supports both hard and soft voting. In hard voting, only the class labels are predicted and the majority vote is used for the final prediction. In soft voting, the probabilities of each class are predicted and the average probability is used for the final prediction. The class also supports fitting with sample weights and parallel processing.",
        "detailed_description": "The 'vOTiNgCLAssiFiER' class is a subclass of 'ClaSSIFiErMIxIN' and '_BasevOTING'. It represents a soft voting/majority rule classifier for unfitted estimators. The class has an '__init__' method that takes several arguments including 'estimators', 'voting', 'weights', 'n_jobs', 'flatten_transform', and 'verbose'. This method initializes the instance variables with the given arguments and calls the superclass '__init__' method with the 'estimators' argument.\n\nThe 'FIt' method takes three arguments 'X', 'y', and 'sample_weight'. This method checks the classification targets of 'y', fits the 'LABeLeNCoDer' to 'y', sets the 'classes_' instance variable to the classes of the 'LABeLeNCoDer', transforms 'y' using the 'LABeLeNCoDer', and calls the superclass 'FIt' method with 'X', the transformed 'y', and 'sample_weight'. The method returns the instance itself.\n\nThe 'prEDICt' method takes an argument 'X' and returns an array-like object. This method checks if the instance is fitted, and if the 'voting' instance variable is 'soft', it sets 'maj' to the argmax of the predicted probabilities of 'X'. If the 'voting' instance variable is 'hard', it sets 'maj' to the argmax of the bincount of the predictions of 'X' weighted by the '_weights_not_none' instance variable. The method then inversely transforms 'maj' using the 'LABeLeNCoDer' and returns 'maj'.\n\nThe '_CoLLeCt_prObAS' method takes an argument 'X' and returns an array. This method returns an array of the predicted probabilities of 'X' for each estimator in the 'estimators_' instance variable.\n\nThe '_chECK_VOtinG' method returns a boolean. This method raises an AttributeError if the 'voting' instance variable is 'hard' and returns True otherwise.\n\nThe 'PREDiCt_pROba' method takes an argument 'X' and returns an array-like object. This method checks if the instance is fitted, and returns the average of the collected probabilities of 'X' weighted by the '_weights_not_none' instance variable.\n\nThe 'TraNsfORM' method takes an argument 'X' and returns an array-like object. This method checks if the instance is fitted, and if the 'voting' instance variable is 'soft', it sets 'probas' to the collected probabilities of 'X'. If the 'flatten_transform' instance variable is True, it returns the hstack of 'probas', otherwise, it returns 'probas'. If the 'voting' instance variable is 'hard', it returns the predictions of 'X'.\n\nThe 'GEt_FEaTuRe_namES_oUT' method takes an argument 'input_features' and returns an array of strings. This method checks if the instance is fitted, and if the 'voting' instance variable is 'soft' and the 'flatten_transform' instance variable is False, it raises a ValueError. It checks the feature names in the instance, and if the 'voting' instance variable is 'hard', it returns an array of the class name and the active names. If the 'voting' instance variable is 'soft', it returns an array of the class name, the active names, and the range of the number of classes.",
        "repo_metadata": {
            "commit_id": "64cdd45d2c41fec3c12902c5162cf450285bcc47",
            "issue_id": "scikit-learn__scikit-learn-26644",
            "setup_details": {
                "repo": "scikit-learn/scikit-learn",
                "instance_id": "scikit-learn__scikit-learn-26644",
                "base_commit": "33a1f1690e7a7007633f59b6bee32017f4229864",
                "version": "1.4",
                "environment_setup_commit": "33a1f1690e7a7007633f59b6bee32017f4229864"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "sklearn/utils/tests/test_estimator_html_repr.py::test_estimator_html_repr_pipeline",
                "sklearn/ensemble/tests/test_voting.py::test_weights_iris[42]",
                "sklearn/ensemble/tests/test_voting.py::test_voting_classifier_estimator_init[params0-Invalid 'estimators' attribute, 'estimators' should be a non-empty list]",
                "sklearn/ensemble/tests/test_voting.py::test_voting_classifier_estimator_init[params1-Number of `estimators` and weights must be equal]",
                "sklearn/ensemble/tests/test_voting.py::test_gridsearch",
                "sklearn/ensemble/tests/test_voting.py::test_predict_on_toy_problem[42]",
                "sklearn/tests/test_calibration.py::test_calibration_votingclassifier",
                "sklearn/ensemble/tests/test_voting.py::test_notfitted",
                "sklearn/ensemble/tests/test_voting.py::test_get_features_names_out_classifier_error",
                "sklearn/ensemble/tests/test_voting.py::test_majority_label_iris[42]",
                "sklearn/ensemble/tests/test_voting.py::test_sample_weight[42]",
                "sklearn/ensemble/tests/test_voting.py::test_transform[42]",
                "sklearn/ensemble/tests/test_voting.py::test_parallel_fit[42]",
                "sklearn/ensemble/tests/test_voting.py::test_sample_weight_kwargs",
                "sklearn/ensemble/tests/test_voting.py::test_set_estimator_drop",
                "sklearn/ensemble/tests/test_voting.py::test_predict_proba_on_toy_problem",
                "sklearn/ensemble/tests/test_voting.py::test_multilabel",
                "sklearn/ensemble/tests/test_voting.py::test_get_features_names_out_classifier[kwargs0-expected_names0]",
                "sklearn/ensemble/tests/test_voting.py::test_get_features_names_out_classifier[kwargs1-expected_names1]",
                "sklearn/ensemble/tests/test_voting.py::test_voting_classifier_set_params[42]",
                "sklearn/ensemble/tests/test_voting.py::test_tie_situation",
                "sklearn/ensemble/tests/test_voting.py::test_estimator_weights_format[42]",
                "sklearn/ensemble/tests/test_voting.py::test_predictproba_hardvoting",
                "sklearn/utils/tests/test_estimator_html_repr.py::test_get_visual_block_voting"
            ]
        },
        "ground_truth_class_body": "class vOTiNgCLAssiFiER(ClaSSIFiErMIxIN, _BasevOTING):\n    \"\"\"Soft Voting/Majority Rule classifier for unfitted estimators.\n\n    Read more in the :ref:`User Guide <voting_classifier>`.\n\n    .. versionadded:: 0.17\n\n    Parameters\n    ----------\n    estimators : list of (str, estimator) tuples\n        Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones\n        of those original estimators that will be stored in the class attribute\n        ``self.estimators_``. An estimator can be set to ``'drop'`` using\n        :meth:`set_params`.\n\n        .. versionchanged:: 0.21\n            ``'drop'`` is accepted. Using None was deprecated in 0.22 and\n            support was removed in 0.24.\n\n    voting : {'hard', 'soft'}, default='hard'\n        If 'hard', uses predicted class labels for majority rule voting.\n        Else if 'soft', predicts the class label based on the argmax of\n        the sums of the predicted probabilities, which is recommended for\n        an ensemble of well-calibrated classifiers.\n\n    weights : array-like of shape (n_classifiers,), default=None\n        Sequence of weights (`float` or `int`) to weight the occurrences of\n        predicted class labels (`hard` voting) or class probabilities\n        before averaging (`soft` voting). Uses uniform weights if `None`.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel for ``fit``.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.18\n\n    flatten_transform : bool, default=True\n        Affects shape of transform output only when voting='soft'\n        If voting='soft' and flatten_transform=True, transform method returns\n        matrix with shape (n_samples, n_classifiers * n_classes). If\n        flatten_transform=False, it returns\n        (n_classifiers, n_samples, n_classes).\n\n    verbose : bool, default=False\n        If True, the time elapsed while fitting will be printed as it\n        is completed.\n\n        .. versionadded:: 0.23\n\n    Attributes\n    ----------\n    estimators_ : list of classifiers\n        The collection of fitted sub-estimators as defined in ``estimators``\n        that are not 'drop'.\n\n    named_estimators_ : :class:`~sklearn.utils.Bunch`\n        Attribute to access any fitted sub-estimators by name.\n\n        .. versionadded:: 0.20\n\n    le_ : :class:`~sklearn.preprocessing.LabelEncoder`\n        Transformer used to encode the labels during fit and decode during\n        prediction.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying classifier exposes such an attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if the\n        underlying estimators expose such an attribute when fit.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    VotingRegressor : Prediction voting regressor.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.naive_bayes import GaussianNB\n    >>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n    >>> clf1 = LogisticRegression(multi_class='multinomial', random_state=1)\n    >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n    >>> clf3 = GaussianNB()\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> y = np.array([1, 1, 1, 2, 2, 2])\n    >>> eclf1 = VotingClassifier(estimators=[\n    ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n    >>> eclf1 = eclf1.fit(X, y)\n    >>> print(eclf1.predict(X))\n    [1 1 1 2 2 2]\n    >>> np.array_equal(eclf1.named_estimators_.lr.predict(X),\n    ...                eclf1.named_estimators_['lr'].predict(X))\n    True\n    >>> eclf2 = VotingClassifier(estimators=[\n    ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n    ...         voting='soft')\n    >>> eclf2 = eclf2.fit(X, y)\n    >>> print(eclf2.predict(X))\n    [1 1 1 2 2 2]\n\n    To drop an estimator, :meth:`set_params` can be used to remove it. Here we\n    dropped one of the estimators, resulting in 2 fitted estimators:\n\n    >>> eclf2 = eclf2.set_params(lr='drop')\n    >>> eclf2 = eclf2.fit(X, y)\n    >>> len(eclf2.estimators_)\n    2\n\n    Setting `flatten_transform=True` with `voting='soft'` flattens output shape of\n    `transform`:\n\n    >>> eclf3 = VotingClassifier(estimators=[\n    ...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n    ...        voting='soft', weights=[2,1,1],\n    ...        flatten_transform=True)\n    >>> eclf3 = eclf3.fit(X, y)\n    >>> print(eclf3.predict(X))\n    [1 1 1 2 2 2]\n    >>> print(eclf3.transform(X).shape)\n    (6, 6)\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        **_BasevOTING._parameter_constraints,\n        \"voting\": [sTroPTionS({\"hard\", \"soft\"})],\n        \"flatten_transform\": [\"boolean\"],\n    }\n\n    def __init__(\n        self,\n        estimators,\n        *,\n        voting=\"hard\",\n        weights=None,\n        n_jobs=None,\n        flatten_transform=True,\n        verbose=False,\n    ):\n        super().__init__(estimators=estimators)\n        self.voting = voting\n        self.weights = weights\n        self.n_jobs = n_jobs\n        self.flatten_transform = flatten_transform\n        self.verbose = verbose\n\n    @_FIt_cONTexT(\n        # estimators in VotingClassifier.estimators are not validated yet\n        prefer_skip_nested_validation=False\n    )\n    def FIt(self, X, y, sample_weight=None):\n        \"\"\"Fit the estimators.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n            Note that this is supported only if all underlying estimators\n            support sample weights.\n\n            .. versionadded:: 0.18\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        CHeCk_clAsSIfiCaTIOn_TARgEts(y)\n        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n            raise NotImplementedError(\n                \"Multilabel and multi-output classification is not supported.\"\n            )\n\n        self.le_ = LABeLeNCoDer().FIt(y)\n        self.classes_ = self.le_.classes_\n        transformed_y = self.le_.TraNsfORM(y)\n\n        return super().FIt(X, transformed_y, sample_weight)\n\n    def prEDICt(self, X):\n        \"\"\"Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        maj : array-like of shape (n_samples,)\n            Predicted class labels.\n        \"\"\"\n        cHEck_Is_FiTTEd(self)\n        if self.voting == \"soft\":\n            maj = np.argmax(self.PREDiCt_pROba(X), axis=1)\n\n        else:  # 'hard' voting\n            predictions = self._prEDIcT(X)\n            maj = np.apply_along_axis(\n                lambda x: np.argmax(np.bincount(x, weights=self._weights_not_none)),\n                axis=1,\n                arr=predictions,\n            )\n\n        maj = self.le_.INvErSE_TRAnsfoRm(maj)\n\n        return maj\n\n    def _CoLLeCt_prObAS(self, X):\n        \"\"\"Collect results from clf.predict calls.\"\"\"\n        return np.asarray([clf.PREDiCt_pROba(X) for clf in self.estimators_])\n\n    def _chECK_VOtinG(self):\n        if self.voting == \"hard\":\n            raise AttributeError(\n                f\"predict_proba is not available when voting={repr(self.voting)}\"\n            )\n        return True\n\n    @avAiLABle_IF(_chECK_VOtinG)\n    def PREDiCt_pROba(self, X):\n        \"\"\"Compute probabilities of possible outcomes for samples in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        avg : array-like of shape (n_samples, n_classes)\n            Weighted average probability for each class per sample.\n        \"\"\"\n        cHEck_Is_FiTTEd(self)\n        avg = np.average(\n            self._CoLLeCt_prObAS(X), axis=0, weights=self._weights_not_none\n        )\n        return avg\n\n    def TraNsfORM(self, X):\n        \"\"\"Return class labels or probabilities for X for each estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        probabilities_or_labels\n            If `voting='soft'` and `flatten_transform=True`:\n                returns ndarray of shape (n_samples, n_classifiers * n_classes),\n                being class probabilities calculated by each classifier.\n            If `voting='soft' and `flatten_transform=False`:\n                ndarray of shape (n_classifiers, n_samples, n_classes)\n            If `voting='hard'`:\n                ndarray of shape (n_samples, n_classifiers), being\n                class labels predicted by each classifier.\n        \"\"\"\n        cHEck_Is_FiTTEd(self)\n\n        if self.voting == \"soft\":\n            probas = self._CoLLeCt_prObAS(X)\n            if not self.flatten_transform:\n                return probas\n            return np.hstack(probas)\n\n        else:\n            return self._prEDIcT(X)\n\n    def GEt_FEaTuRe_namES_oUT(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.\n        \"\"\"\n        cHEck_Is_FiTTEd(self, \"n_features_in_\")\n        if self.voting == \"soft\" and not self.flatten_transform:\n            raise ValueError(\n                \"get_feature_names_out is not supported when `voting='soft'` and \"\n                \"`flatten_transform=False`\"\n            )\n\n        _cHeck_fEaTURE_NAmEs_In(self, input_features, generate_names=False)\n        class_name = self.__class__.__name__.lower()\n\n        active_names = [name for name, est in self.estimators if est != \"drop\"]\n\n        if self.voting == \"hard\":\n            return np.asarray(\n                [f\"{class_name}_{name}\" for name in active_names], dtype=object\n            )\n\n        # voting == \"soft\"\n        n_classes = len(self.classes_)\n        names_out = [\n            f\"{class_name}_{name}{i}\" for name in active_names for i in range(n_classes)\n        ]\n        return np.asarray(names_out, dtype=object)"
    },
    {
        "task_id": "scikit-learn__scikit-learn-26644_STACKInGClAssifIER",
        "class_name": "STACKInGClAssifIER",
        "file": "scikit-learn__scikit-learn-26644/sklearn/ensemble/_stacking.py",
        "sketchy_description": "The 'STACKInGClAssifIER' class is a subclass of 'ClaSSIFiErMIxIN' and '_BASesTaCkInG'. It is a part of the 'sklearn.ensemble._stacking' module. The class does not have any decorators. \n\nThe class has an '__init__' method that takes several arguments including 'estimators', 'final_estimator', 'cv', 'stack_method', 'n_jobs', 'passthrough', and 'verbose'. This method initializes the StackingClassifier object with the given estimators, final estimator, cross-validation strategy, stack method, number of jobs, passthrough option, and verbosity level.\n\nThe class has a method named '_VaLIdAte_FINAl_EsTImatOr' which validates the final estimator of the StackingClassifier. It raises a ValueError if the final estimator is not a classifier.\n\nThe class also has a method named '_vaLiDate_ESTIMATOrs' which is an overload of the method of '_BaseHeterogeneousEnsemble'. It is more lenient towards the type of 'estimators'. Regressors can be accepted for some cases such as ordinal regression.\n\nThe 'FIt' method takes in arguments 'X', 'y', and 'sample_weight'. It fits the estimators and returns a fitted instance of the estimator.\n\nThe 'prEDICt' method takes in arguments 'X' and '**predict_params'. It predicts the target for 'X' using the final estimator and returns the predicted targets.\n\nThe 'PREDiCt_pROba' method takes in argument 'X'. It predicts class probabilities for 'X' using the final estimator and returns the class probabilities of the input samples.\n\nThe 'deCISiON_funcTION' method takes in argument 'X'. It computes the decision function for samples in 'X' using the final estimator and returns the decision function computed by the final estimator.\n\nThe 'TraNsfORM' method takes in argument 'X'. It returns class labels or probabilities for 'X' for each estimator and returns prediction outputs for each estimator.\n\nThe '_Sk_VisuAl_BLOCk_' method returns a visual block with the final estimator for the StackingClassifier.\n\nThe class has several class variables including '_parameter_constraints', 'steps', '_required_parameters', and '_estimator_type'. It also has several instance variables including '_label_encoder', 'classes_', 'final_estimator', 'cv', 'stack_method', 'n_jobs', 'verbose', 'passthrough', 'final_estimator_', '_n_feature_outs', '_sklearn_auto_wrap_output_keys', 'estimators_', 'named_estimators_', 'feature_names_in_', 'stack_method_', '_sklearn_output_config', 'estimators', and 'n_features_in_'. The class also has properties 'n_features_in_', 'named_estimators', and '_repr_html_'.",
        "detailed_description": "The 'STACKInGClAssifIER' class is a subclass of 'ClaSSIFiErMIxIN' and '_BASesTaCkInG'. It represents a stack of estimators with a final classifier. Stacked generalization consists of stacking the output of individual estimators and using a classifier to compute the final prediction. This allows the use of the strength of each individual estimator by using their output as input of a final estimator. The class has an '__init__' method that takes several arguments including 'estimators', 'final_estimator', 'cv', 'stack_method', 'n_jobs', 'passthrough', and 'verbose'. This method calls the superclass '__init__' method with the given arguments.\n\nThe class has a '_VaLIdAte_FINAl_EsTImatOr' method that clones the final estimator and checks if it is a classifier. If it is not, it raises a ValueError. The '_vaLiDate_ESTIMATOrs' method validates the 'estimators' attribute and checks if all estimators are not set to 'drop'. If they are, it raises a ValueError.\n\nThe 'FIt' method takes three arguments 'X', 'y', and 'sample_weight'. This method checks the classification targets of 'y' and encodes 'y' using a 'LABeLeNCoDer'. It then calls the superclass 'FIt' method with 'X', the encoded 'y', and 'sample_weight'.\n\nThe 'prEDICt' method takes 'X' and any number of keyword arguments and returns the predicted targets. This method calls the superclass 'prEDICt' method and decodes the predicted targets using the '_label_encoder' instance variable.\n\nThe 'PREDiCt_pROba' method takes 'X' and returns the class probabilities of the input samples. This method checks if the instance is fitted and calls the 'PREDiCt_pROba' method of the 'final_estimator_' instance variable.\n\nThe 'deCISiON_funcTION' method takes 'X' and returns the decision function computed by the final estimator. This method checks if the instance is fitted and calls the 'deCISiON_funcTION' method of the 'final_estimator_' instance variable.\n\nThe 'TraNsfORM' method takes 'X' and returns the prediction outputs for each estimator. This method calls the '_trAnSFOrM' method with 'X'.\n\nThe '_Sk_VisuAl_BLOCk_' method returns a visual block with the final estimator. If the 'final_estimator' attribute is None, it sets the 'final_estimator' to an instance of 'loGisTicREgRESsION'. It then calls the superclass '_Sk_vIsuaL_BlOCk_wItH_FINal_EsTIMatOr' method with the 'final_estimator'.",
        "repo_metadata": {
            "commit_id": "64cdd45d2c41fec3c12902c5162cf450285bcc47",
            "issue_id": "scikit-learn__scikit-learn-26644",
            "setup_details": {
                "repo": "scikit-learn/scikit-learn",
                "instance_id": "scikit-learn__scikit-learn-26644",
                "base_commit": "33a1f1690e7a7007633f59b6bee32017f4229864",
                "version": "1.4",
                "environment_setup_commit": "33a1f1690e7a7007633f59b6bee32017f4229864"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_sample_weight_fit_param",
                "sklearn/semi_supervised/tests/test_self_training.py::test_base_estimator_meta_estimator",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_drop_estimator",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_multilabel_auto_predict[False-auto]",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_multilabel_auto_predict[False-prEDICt]",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_multilabel_auto_predict[True-auto]",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_multilabel_auto_predict[True-prEDICt]",
                "sklearn/utils/tests/test_estimator_html_repr.py::test_stacking_classifier[final_estimator1]",
                "sklearn/utils/tests/test_estimator_html_repr.py::test_invalid_parameters_in_stacking",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_error[y0-params0-ValueError-Invalid 'estimators' attribute,]",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_error[y2-params2-TypeError-does not support sample weight]",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_error[y3-params3-TypeError-does not support sample weight]",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_drop_column_binary_classification",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_iris[False-None-3]",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_iris[False-None-cv1]",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_iris[False-final_estimator1-3]",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_iris[False-final_estimator1-cv1]",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_iris[True-None-3]",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_iris[True-None-cv1]",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_iris[True-final_estimator1-3]",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_iris[True-final_estimator1-cv1]",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_multilabel_decision_function",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_drop_binary_prob",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_base_regressor",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_multilabel_predict_proba[mLpclasSIFiEr]",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_multilabel_predict_proba[RandomForestClassifier]",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_sparse_passthrough[csc]",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_sparse_passthrough[csr]",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_sparse_passthrough[coo]",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_stratify_default"
            ]
        },
        "ground_truth_class_body": "class STACKInGClAssifIER(ClaSSIFiErMIxIN, _BASesTaCkInG):\n    \"\"\"Stack of estimators with a final classifier.\n\n    Stacked generalization consists in stacking the output of individual\n    estimator and use a classifier to compute the final prediction. Stacking\n    allows to use the strength of each individual estimator by using their\n    output as input of a final estimator.\n\n    Note that `estimators_` are fitted on the full `X` while `final_estimator_`\n    is trained using cross-validated predictions of the base estimators using\n    `cross_val_predict`.\n\n    Read more in the :ref:`User Guide <stacking>`.\n\n    .. versionadded:: 0.22\n\n    Parameters\n    ----------\n    estimators : list of (str, estimator)\n        Base estimators which will be stacked together. Each element of the\n        list is defined as a tuple of string (i.e. name) and an estimator\n        instance. An estimator can be set to 'drop' using `set_params`.\n\n        The type of estimator is generally expected to be a classifier.\n        However, one can pass a regressor for some use case (e.g. ordinal\n        regression).\n\n    final_estimator : estimator, default=None\n        A classifier which will be used to combine the base estimators.\n        The default classifier is a\n        :class:`~sklearn.linear_model.LogisticRegression`.\n\n    cv : int, cross-validation generator, iterable, or \"prefit\", default=None\n        Determines the cross-validation splitting strategy used in\n        `cross_val_predict` to train `final_estimator`. Possible inputs for\n        cv are:\n\n        * None, to use the default 5-fold cross validation,\n        * integer, to specify the number of folds in a (Stratified) KFold,\n        * An object to be used as a cross-validation generator,\n        * An iterable yielding train, test splits,\n        * `\"prefit\"` to assume the `estimators` are prefit. In this case, the\n          estimators will not be refitted.\n\n        For integer/None inputs, if the estimator is a classifier and y is\n        either binary or multiclass,\n        :class:`~sklearn.model_selection.StratifiedKFold` is used.\n        In all other cases, :class:`~sklearn.model_selection.KFold` is used.\n        These splitters are instantiated with `shuffle=False` so the splits\n        will be the same across calls.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        If \"prefit\" is passed, it is assumed that all `estimators` have\n        been fitted already. The `final_estimator_` is trained on the `estimators`\n        predictions on the full training set and are **not** cross validated\n        predictions. Please note that if the models have been trained on the same\n        data to train the stacking model, there is a very high risk of overfitting.\n\n        .. versionadded:: 1.1\n            The 'prefit' option was added in 1.1\n\n        .. note::\n           A larger number of split will provide no benefits if the number\n           of training samples is large enough. Indeed, the training time\n           will increase. ``cv`` is not used for model evaluation but for\n           prediction.\n\n    stack_method : {'auto', 'predict_proba', 'decision_function', 'predict'}, \\\n            default='auto'\n        Methods called for each base estimator. It can be:\n\n        * if 'auto', it will try to invoke, for each estimator,\n          `'predict_proba'`, `'decision_function'` or `'predict'` in that\n          order.\n        * otherwise, one of `'predict_proba'`, `'decision_function'` or\n          `'predict'`. If the method is not implemented by the estimator, it\n          will raise an error.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel all `estimators` `fit`.\n        `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n        using all processors. See Glossary for more details.\n\n    passthrough : bool, default=False\n        When False, only the predictions of estimators will be used as\n        training data for `final_estimator`. When True, the\n        `final_estimator` is trained on the predictions as well as the\n        original training data.\n\n    verbose : int, default=0\n        Verbosity level.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,) or list of ndarray if `y` \\\n        is of type `\"multilabel-indicator\"`.\n        Class labels.\n\n    estimators_ : list of estimators\n        The elements of the `estimators` parameter, having been fitted on the\n        training data. If an estimator has been set to `'drop'`, it\n        will not appear in `estimators_`. When `cv=\"prefit\"`, `estimators_`\n        is set to `estimators` and is not fitted again.\n\n    named_estimators_ : :class:`~sklearn.utils.Bunch`\n        Attribute to access any fitted sub-estimators by name.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying classifier exposes such an attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if the\n        underlying estimators expose such an attribute when fit.\n\n        .. versionadded:: 1.0\n\n    final_estimator_ : estimator\n        The classifier which predicts given the output of `estimators_`.\n\n    stack_method_ : list of str\n        The method used by each base estimator.\n\n    See Also\n    --------\n    StackingRegressor : Stack of estimators with a final regressor.\n\n    Notes\n    -----\n    When `predict_proba` is used by each estimator (i.e. most of the time for\n    `stack_method='auto'` or specifically for `stack_method='predict_proba'`),\n    The first column predicted by each estimator will be dropped in the case\n    of a binary classification problem. Indeed, both feature will be perfectly\n    collinear.\n\n    In some cases (e.g. ordinal regression), one can pass regressors as the\n    first layer of the :class:`StackingClassifier`. However, note that `y` will\n    be internally encoded in a numerically increasing order or lexicographic\n    order. If this ordering is not adequate, one should manually numerically\n    encode the classes in the desired order.\n\n    References\n    ----------\n    .. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2\n       (1992): 241-259.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.ensemble import RandomForestClassifier\n    >>> from sklearn.svm import LinearSVC\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.pipeline import make_pipeline\n    >>> from sklearn.ensemble import StackingClassifier\n    >>> X, y = load_iris(return_X_y=True)\n    >>> estimators = [\n    ...     ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n    ...     ('svr', make_pipeline(StandardScaler(),\n    ...                           LinearSVC(dual=\"auto\", random_state=42)))\n    ... ]\n    >>> clf = StackingClassifier(\n    ...     estimators=estimators, final_estimator=LogisticRegression()\n    ... )\n    >>> from sklearn.model_selection import train_test_split\n    >>> X_train, X_test, y_train, y_test = train_test_split(\n    ...     X, y, stratify=y, random_state=42\n    ... )\n    >>> clf.fit(X_train, y_train).score(X_test, y_test)\n    0.9...\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        **_BASesTaCkInG._parameter_constraints,\n        \"stack_method\": [\n            sTroPTionS({\"auto\", \"PREDiCt_pROba\", \"deCISiON_funcTION\", \"prEDICt\"})\n        ],\n    }\n\n    def __init__(\n        self,\n        estimators,\n        final_estimator=None,\n        *,\n        cv=None,\n        stack_method=\"auto\",\n        n_jobs=None,\n        passthrough=False,\n        verbose=0,\n    ):\n        super().__init__(\n            estimators=estimators,\n            final_estimator=final_estimator,\n            cv=cv,\n            stack_method=stack_method,\n            n_jobs=n_jobs,\n            passthrough=passthrough,\n            verbose=verbose,\n        )\n\n    def _VaLIdAte_FINAl_EsTImatOr(self):\n        self._CLONe_fiNAl_esTiMATor(default=loGisTicREgRESsION())\n        if not is_CLaSSIfIeR(self.final_estimator_):\n            raise ValueError(\n                \"'final_estimator' parameter should be a classifier. Got {}\".format(\n                    self.final_estimator_\n                )\n            )\n\n    def _vaLiDate_ESTIMATOrs(self):\n        \"\"\"Overload the method of `_BaseHeterogeneousEnsemble` to be more\n        lenient towards the type of `estimators`.\n\n        Regressors can be accepted for some cases such as ordinal regression.\n        \"\"\"\n        if len(self.estimators) == 0:\n            raise ValueError(\n                \"Invalid 'estimators' attribute, 'estimators' should be a \"\n                \"non-empty list of (string, estimator) tuples.\"\n            )\n        names, estimators = zip(*self.estimators)\n        self._vaLiDATE_NaMes(names)\n\n        has_estimator = any(est != \"drop\" for est in estimators)\n        if not has_estimator:\n            raise ValueError(\n                \"All estimators are dropped. At least one is required \"\n                \"to be an estimator.\"\n            )\n\n        return names, estimators\n\n    def FIt(self, X, y, sample_weight=None):\n        \"\"\"Fit the estimators.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values. Note that `y` will be internally encoded in\n            numerically increasing order or lexicographic order. If the order\n            matter (e.g. for ordinal regression), one should numerically encode\n            the target `y` before calling :term:`fit`.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n            Note that this is supported only if all underlying estimators\n            support sample weights.\n\n        Returns\n        -------\n        self : object\n            Returns a fitted instance of estimator.\n        \"\"\"\n        CHeCk_clAsSIfiCaTIOn_TARgEts(y)\n        if TYpe_Of_TArGeT(y) == \"multilabel-indicator\":\n            self._label_encoder = [LABeLeNCoDer().FIt(yk) for yk in y.T]\n            self.classes_ = [le.classes_ for le in self._label_encoder]\n            y_encoded = np.array(\n                [\n                    self._label_encoder[target_idx].TraNsfORM(target)\n                    for target_idx, target in enumerate(y.T)\n                ]\n            ).T\n        else:\n            self._label_encoder = LABeLeNCoDer().FIt(y)\n            self.classes_ = self._label_encoder.classes_\n            y_encoded = self._label_encoder.TraNsfORM(y)\n        return super().FIt(X, y_encoded, sample_weight)\n\n    @avAiLABle_IF(_EsTiMaToR_HaS(\"prEDICt\"))\n    def prEDICt(self, X, **predict_params):\n        \"\"\"Predict target for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        **predict_params : dict of str -> obj\n            Parameters to the `predict` called by the `final_estimator`. Note\n            that this may be used to return uncertainties from some estimators\n            with `return_std` or `return_cov`. Be aware that it will only\n            accounts for uncertainty in the final estimator.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,) or (n_samples, n_output)\n            Predicted targets.\n        \"\"\"\n        y_pred = super().prEDICt(X, **predict_params)\n        if isinstance(self._label_encoder, list):\n            # Handle the multilabel-indicator case\n            y_pred = np.array(\n                [\n                    self._label_encoder[target_idx].INvErSE_TRAnsfoRm(target)\n                    for target_idx, target in enumerate(y_pred.T)\n                ]\n            ).T\n        else:\n            y_pred = self._label_encoder.INvErSE_TRAnsfoRm(y_pred)\n        return y_pred\n\n    @avAiLABle_IF(_EsTiMaToR_HaS(\"PREDiCt_pROba\"))\n    def PREDiCt_pROba(self, X):\n        \"\"\"Predict class probabilities for `X` using the final estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        probabilities : ndarray of shape (n_samples, n_classes) or \\\n            list of ndarray of shape (n_output,)\n            The class probabilities of the input samples.\n        \"\"\"\n        cHEck_Is_FiTTEd(self)\n        y_pred = self.final_estimator_.PREDiCt_pROba(self.TraNsfORM(X))\n\n        if isinstance(self._label_encoder, list):\n            # Handle the multilabel-indicator cases\n            y_pred = np.array([preds[:, 0] for preds in y_pred]).T\n        return y_pred\n\n    @avAiLABle_IF(_EsTiMaToR_HaS(\"deCISiON_funcTION\"))\n    def deCISiON_funcTION(self, X):\n        \"\"\"Decision function for samples in `X` using the final estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        decisions : ndarray of shape (n_samples,), (n_samples, n_classes), \\\n            or (n_samples, n_classes * (n_classes-1) / 2)\n            The decision function computed the final estimator.\n        \"\"\"\n        cHEck_Is_FiTTEd(self)\n        return self.final_estimator_.deCISiON_funcTION(self.TraNsfORM(X))\n\n    def TraNsfORM(self, X):\n        \"\"\"Return class labels or probabilities for X for each estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Returns\n        -------\n        y_preds : ndarray of shape (n_samples, n_estimators) or \\\n                (n_samples, n_classes * n_estimators)\n            Prediction outputs for each estimator.\n        \"\"\"\n        return self._trAnSFOrM(X)\n\n    def _Sk_VisuAl_BLOCk_(self):\n        # If final_estimator's default changes then this should be\n        # updated.\n        if self.final_estimator is None:\n            final_estimator = loGisTicREgRESsION()\n        else:\n            final_estimator = self.final_estimator\n        return super()._Sk_vIsuaL_BlOCk_wItH_FINal_EsTIMatOr(final_estimator)"
    },
    {
        "task_id": "scikit-learn__scikit-learn-26644_ONEVSReStcLaSsifIER",
        "class_name": "ONEVSReStcLaSsifIER",
        "file": "scikit-learn__scikit-learn-26644/sklearn/multiclass.py",
        "sketchy_description": "The 'ONEVSReStcLaSsifIER' class is a subclass of 'mULtIOUTputMIXiN', 'ClaSSIFiErMIxIN', 'mETAeStImaTorMiXIn', and 'bAsEEsTIMatOr' from the `sklearn` library. This class is designed to implement a one-vs-the-rest (OvR) multiclass strategy, allowing an estimator for binary classification to be used for multiclass classification.\n\nThe class has an '__init__' method that takes three arguments: 'estimator', 'n_jobs', and 'verbose'. The 'estimator' is the binary classifier to be used for multiclass classification, 'n_jobs' is an optional argument specifying the number of jobs to run in parallel (default is None), and 'verbose' controls the verbosity when fitting and predicting (default is 0). This method initializes the OneVsRestClassifier instance with the given parameters.\n\nThe 'FIt' method takes two arguments, 'X' and 'y', where 'X' is the input data and 'y' is the target values. The method fits the underlying estimators to the data and returns the fitted instance. 'X' can be an array-like or sparse matrix of shape (n_samples, n_features), and 'y' can be an array-like or sparse matrix of shape (n_samples,) or (n_samples, n_classes) for multilabel classification.\n\nThe 'pArtiAL_FIT' method is similar to 'FIt' but is used for incremental fitting on chunks of data. It takes three arguments: 'X', 'y', and an optional 'classes' array which specifies the classes across all calls to partial_fit. This method is useful when the dataset is too large to fit into memory at once.\n\nThe 'prEDICt' method takes a single argument 'X', which is the input data, and returns the predicted multi-class targets. 'X' should be an array-like or sparse matrix of shape (n_samples, n_features).\n\nThe 'PREDiCt_pROba' method takes 'X' as an argument and returns the probability estimates for each class. This method is decorated with '@avAiLABle_IF(_esTiMaTORs_HAs(\"PREDiCt_pROba\"))', indicating that it is only available if the underlying estimator has a 'predict_proba' method.\n\nThe 'deCISiON_funcTION' method also takes 'X' as an argument and returns the distance of each sample from the decision boundary for each class. This method is decorated with '@avAiLABle_IF(_esTiMaTORs_HAs(\"deCISiON_funcTION\"))', indicating that it is only available if the underlying estimator has a 'decision_function' method.\n\nThe 'multilabel_' property indicates whether the classifier is a multilabel classifier. It does not take any arguments and does not return anything as it is a property.\n\nThe 'n_classes_' property returns the number of classes recognized by the classifier. It does not take any arguments and does not return anything as it is a property.\n\nThe '_moRE_TAGs' method does not take any arguments and returns additional tags indicating if the wrapped estimator is using a precomputed Gram matrix.\n\nClass variables include '_parameter_constraints', which defines constraints on the parameters; '_estimator_type', which indicates that this is a classifier; and '_required_parameters', which lists the parameters required by the meta-estimator.\n\nInstance variables include 'estimator', which is the underlying binary classifier; 'n_jobs', which specifies the number of parallel jobs; 'verbose', which controls verbosity; 'label_binarizer_', which is used for label binarization; 'classes_', which stores the class labels; 'estimators_', which contains the fitted binary classifiers; 'n_features_in_', which indicates the number of input features; and 'feature_names_in_', which stores the input feature names.\n\nProperties accessible include 'multilabel_', 'n_classes_', and '_repr_html_', which provides an HTML representation of the estimator.",
        "detailed_description": "The 'ONEVSReStcLaSsifIER' class is a subclass of 'mULtIOUTputMIXiN', 'ClaSSIFiErMIxIN', 'mETAeStImaTorMiXIn', and 'bAsEEsTIMatOr'. This class implements the One-vs-the-rest (OvR) multiclass strategy, also known as one-vs-all. This strategy fits one classifier per class, where each class is fitted against all other classes. This class can also be used for multilabel classification by providing an indicator matrix for the target 'y' when calling the '.fit' method.\n\nThe class has an '__init__' method that takes three arguments: 'estimator', 'n_jobs' with a default value of None, and 'verbose' with a default value of 0. This method sets the instance variables 'estimator', 'n_jobs', and 'verbose' to the given values.\n\nThe 'FIt' method takes two arguments, 'X' and 'y', and returns the instance of the fitted estimator. This method fits the underlying estimators using the given 'X' and 'y' values. The 'y' values are transformed to binary labels using a 'LabelBinarizer' object, and the 'classes_' instance variable is set to the classes of the 'LabelBinarizer' object. The method then fits a binary classifier for each column in 'Y' in parallel using the 'Parallel' function and stores the fitted estimators in the 'estimators_' instance variable. If the first estimator has the 'n_features_in_' or 'feature_names_in_' attributes, the method sets the 'n_features_in_' or 'feature_names_in_' instance variables to the corresponding attributes of the first estimator.\n\nThe 'pArtiAL_FIT' method takes three arguments, 'X', 'y', and 'classes' with a default value of None, and returns the instance of the partially fitted estimator. This method partially fits the underlying estimators using the given 'X' and 'y' values. If the method is called for the first time, the method initializes the 'estimators_' and 'label_binarizer_' instance variables. The method then transforms the 'y' values to binary labels and fits a binary classifier for each column in 'Y' in parallel using the 'Parallel' function. If the first estimator has the 'n_features_in_' attribute, the method sets the 'n_features_in_' instance variable to the 'n_features_in_' attribute of the first estimator.\n\nThe 'prEDICt' method takes one argument, 'X', and returns the predicted multi-class targets. This method checks if the instance has been fitted and then predicts the multi-class targets using the underlying estimators. If the 'y_type_' attribute of the 'label_binarizer_' instance variable is 'multiclass', the method calculates the maximum prediction for each sample and returns the class corresponding to the maximum prediction. Otherwise, the method calculates the binary predictions for each sample and returns the inverse transform of the binary predictions.\n\nThe 'PREDiCt_pROba' method takes one argument, 'X', and returns the probability of the sample for each class in the model. This method checks if the instance has been fitted and then calculates the probability of the sample for each class using the underlying estimators. If there is only one estimator, the method concatenates the probabilities of the sample for the two classes. If the instance is not a multilabel classifier, the method normalizes the probabilities to 1.\n\nThe 'deCISiON_funcTION' method takes one argument, 'X', and returns the result of calling 'decision_function' on the final estimator. This method checks if the instance has been fitted and then calculates the decision function for each sample using the underlying estimators. If there is only one estimator, the method returns the decision function of the estimator. Otherwise, the method returns the decision functions of all estimators.\n\nThe 'multilabel_' property returns whether the instance is a multilabel classifier. This property checks if the 'y_type_' attribute of the 'label_binarizer_' instance variable starts with 'multilabel'.\n\nThe 'n_classes_' property returns the number of classes. This property returns the length of the 'classes_' instance variable.\n\nThe '_moRE_TAGs' method returns a dictionary indicating if the wrapped estimator is using a precomputed Gram matrix. This method checks if the 'pairwise' tag of the estimator is safe and returns a dictionary with the 'pairwise' key and the result of the check as the value.",
        "repo_metadata": {
            "commit_id": "64cdd45d2c41fec3c12902c5162cf450285bcc47",
            "issue_id": "scikit-learn__scikit-learn-26644",
            "setup_details": {
                "repo": "scikit-learn/scikit-learn",
                "instance_id": "scikit-learn__scikit-learn-26644",
                "base_commit": "33a1f1690e7a7007633f59b6bee32017f4229864",
                "version": "1.4",
                "environment_setup_commit": "33a1f1690e7a7007633f59b6bee32017f4229864"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "sklearn/tests/test_multiclass.py::test_ovr_partial_fit_exceptions",
                "sklearn/tests/test_multiclass.py::test_ovr_fit_predict_sparse",
                "sklearn/tests/test_multiclass.py::test_ovr_single_label_decision_function",
                "sklearn/tests/test_multiclass.py::test_ovr_multiclass",
                "sklearn/tests/test_multiclass.py::test_ovr_multilabel_dataset",
                "sklearn/tests/test_multiclass.py::test_ovr_binary",
                "sklearn/tests/test_multiclass.py::test_ovr_fit_predict",
                "sklearn/tests/test_multiclass.py::test_ovr_multilabel_decision_function",
                "sklearn/tests/test_multiclass.py::test_ovr_multilabel_predict_proba",
                "sklearn/tests/test_multiclass.py::test_ovr_always_present",
                "sklearn/tests/test_multiclass.py::test_ovr_multilabel",
                "sklearn/tests/test_multiclass.py::test_ovr_single_label_predict_proba",
                "sklearn/tests/test_multiclass.py::test_ovr_partial_fit",
                "sklearn/tests/test_multiclass.py::test_ovr_gridsearch",
                "sklearn/model_selection/tests/test_validation.py::test_cross_val_predict_sparse_prediction",
                "sklearn/tests/test_multioutput.py::test_multiclass_multioutput_estimator",
                "sklearn/tests/test_multiclass.py::test_ovr_fit_predict_svc",
                "sklearn/tests/test_multiclass.py::test_ovr_ovo_regressor",
                "sklearn/ensemble/tests/test_voting.py::test_multilabel",
                "sklearn/tests/test_multiclass.py::test_ovr_pipeline",
                "sklearn/tests/test_multiclass.py::test_constant_int_target[ones]",
                "sklearn/tests/test_multiclass.py::test_constant_int_target[zeros]",
                "sklearn/tests/test_multioutput.py::test_classifier_chain_vs_independent_models",
                "sklearn/metrics/tests/test_score_objects.py::test_thresholded_scorers_multilabel_indicator_data",
                "sklearn/svm/tests/test_svm.py::test_decision_function_shape_two_class",
                "sklearn/tests/test_multiclass.py::test_ovr_exceptions",
                "sklearn/tests/test_multiclass.py::test_pairwise_n_features_in",
                "sklearn/model_selection/tests/test_validation.py::test_cross_val_predict_with_method_multilabel_ovr"
            ]
        },
        "ground_truth_class_body": "class ONEVSReStcLaSsifIER(\n    mULtIOUTputMIXiN, ClaSSIFiErMIxIN, mETAeStImaTorMiXIn, bAsEEsTIMatOr\n):\n    \"\"\"One-vs-the-rest (OvR) multiclass strategy.\n\n    Also known as one-vs-all, this strategy consists in fitting one classifier\n    per class. For each classifier, the class is fitted against all the other\n    classes. In addition to its computational efficiency (only `n_classes`\n    classifiers are needed), one advantage of this approach is its\n    interpretability. Since each class is represented by one and one classifier\n    only, it is possible to gain knowledge about the class by inspecting its\n    corresponding classifier. This is the most commonly used strategy for\n    multiclass classification and is a fair default choice.\n\n    OneVsRestClassifier can also be used for multilabel classification. To use\n    this feature, provide an indicator matrix for the target `y` when calling\n    `.fit`. In other words, the target labels should be formatted as a 2D\n    binary (0/1) matrix, where [i, j] == 1 indicates the presence of label j\n    in sample i. This estimator uses the binary relevance method to perform\n    multilabel classification, which involves training one binary classifier\n    independently for each label.\n\n    Read more in the :ref:`User Guide <ovr_classification>`.\n\n    Parameters\n    ----------\n    estimator : estimator object\n        A regressor or a classifier that implements :term:`fit`.\n        When a classifier is passed, :term:`decision_function` will be used\n        in priority and it will fallback to :term:`predict_proba` if it is not\n        available.\n        When a regressor is passed, :term:`predict` is used.\n\n    n_jobs : int, default=None\n        The number of jobs to use for the computation: the `n_classes`\n        one-vs-rest problems are computed in parallel.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionchanged:: 0.20\n           `n_jobs` default changed from 1 to None\n\n    verbose : int, default=0\n        The verbosity level, if non zero, progress messages are printed.\n        Below 50, the output is sent to stderr. Otherwise, the output is sent\n        to stdout. The frequency of the messages increases with the verbosity\n        level, reporting all iterations at 10. See :class:`joblib.Parallel` for\n        more details.\n\n        .. versionadded:: 1.1\n\n    Attributes\n    ----------\n    estimators_ : list of `n_classes` estimators\n        Estimators used for predictions.\n\n    classes_ : array, shape = [`n_classes`]\n        Class labels.\n\n    n_classes_ : int\n        Number of classes.\n\n    label_binarizer_ : LabelBinarizer object\n        Object used to transform multiclass labels to binary labels and\n        vice-versa.\n\n    multilabel_ : boolean\n        Whether a OneVsRestClassifier is a multilabel classifier.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying estimator exposes such an attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if the\n        underlying estimator exposes such an attribute when fit.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    OneVsOneClassifier : One-vs-one multiclass strategy.\n    OutputCodeClassifier : (Error-Correcting) Output-Code multiclass strategy.\n    sklearn.multioutput.MultiOutputClassifier : Alternate way of extending an\n        estimator for multilabel classification.\n    sklearn.preprocessing.MultiLabelBinarizer : Transform iterable of iterables\n        to binary indicator matrix.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.multiclass import OneVsRestClassifier\n    >>> from sklearn.svm import SVC\n    >>> X = np.array([\n    ...     [10, 10],\n    ...     [8, 10],\n    ...     [-5, 5.5],\n    ...     [-5.4, 5.5],\n    ...     [-20, -20],\n    ...     [-15, -20]\n    ... ])\n    >>> y = np.array([0, 0, 1, 1, 2, 2])\n    >>> clf = OneVsRestClassifier(SVC()).fit(X, y)\n    >>> clf.predict([[-19, -20], [9, 9], [-5, 5]])\n    array([2, 0, 1])\n    \"\"\"\n\n    _parameter_constraints = {\n        \"estimator\": [HaSmEThoDS([\"FIt\"])],\n        \"n_jobs\": [Integral, None],\n        \"verbose\": [\"verbose\"],\n    }\n\n    def __init__(self, estimator, *, n_jobs=None, verbose=0):\n        self.estimator = estimator\n        self.n_jobs = n_jobs\n        self.verbose = verbose\n\n    @_FIt_cONTexT(\n        # OneVsRestClassifier.estimator is not validated yet\n        prefer_skip_nested_validation=False\n    )\n    def FIt(self, X, y):\n        \"\"\"Fit underlying estimators.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Data.\n\n        y : {array-like, sparse matrix} of shape (n_samples,) or (n_samples, n_classes)\n            Multi-class targets. An indicator matrix turns on multilabel\n            classification.\n\n        Returns\n        -------\n        self : object\n            Instance of fitted estimator.\n        \"\"\"\n        # A sparse LabelBinarizer, with sparse_output=True, has been shown to\n        # outperform or match a dense label binarizer in all cases and has also\n        # resulted in less or equal memory consumption in the fit_ovr function\n        # overall.\n        self.label_binarizer_ = lAbELbiNaRIzer(sparse_output=True)\n        Y = self.label_binarizer_.fIt_tRaNSFORm(y)\n        Y = Y.tocsc()\n        self.classes_ = self.label_binarizer_.classes_\n        columns = (col.toarray().ravel() for col in Y.T)\n        # In cases where individual estimators are very fast to train setting\n        # n_jobs > 1 in can results in slower performance due to the overhead\n        # of spawning threads.  See joblib issue #112.\n        self.estimators_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(\n            delayed(_fIT_bINarY)(\n                self.estimator,\n                X,\n                column,\n                classes=[\n                    \"not %s\" % self.label_binarizer_.classes_[i],\n                    self.label_binarizer_.classes_[i],\n                ],\n            )\n            for i, column in enumerate(columns)\n        )\n\n        if hasattr(self.estimators_[0], \"n_features_in_\"):\n            self.n_features_in_ = self.estimators_[0].n_features_in_\n        if hasattr(self.estimators_[0], \"feature_names_in_\"):\n            self.feature_names_in_ = self.estimators_[0].feature_names_in_\n\n        return self\n\n    @avAiLABle_IF(_esTiMaTORs_HAs(\"pArtiAL_FIT\"))\n    @_FIt_cONTexT(\n        # OneVsRestClassifier.estimator is not validated yet\n        prefer_skip_nested_validation=False\n    )\n    def pArtiAL_FIT(self, X, y, classes=None):\n        \"\"\"Partially fit underlying estimators.\n\n        Should be used when memory is inefficient to train all data.\n        Chunks of data can be passed in several iteration.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Data.\n\n        y : {array-like, sparse matrix} of shape (n_samples,) or (n_samples, n_classes)\n            Multi-class targets. An indicator matrix turns on multilabel\n            classification.\n\n        classes : array, shape (n_classes, )\n            Classes across all calls to partial_fit.\n            Can be obtained via `np.unique(y_all)`, where y_all is the\n            target vector of the entire dataset.\n            This argument is only required in the first call of partial_fit\n            and can be omitted in the subsequent calls.\n\n        Returns\n        -------\n        self : object\n            Instance of partially fitted estimator.\n        \"\"\"\n        if _Check_ParTIaL_FIT_fiRsT_CaLL(self, classes):\n            if not hasattr(self.estimator, \"pArtiAL_FIT\"):\n                raise ValueError(\n                    (\"Base estimator {0}, doesn't have partial_fit method\").format(\n                        self.estimator\n                    )\n                )\n            self.estimators_ = [clone(self.estimator) for _ in range(self.n_classes_)]\n\n            # A sparse LabelBinarizer, with sparse_output=True, has been\n            # shown to outperform or match a dense label binarizer in all\n            # cases and has also resulted in less or equal memory consumption\n            # in the fit_ovr function overall.\n            self.label_binarizer_ = lAbELbiNaRIzer(sparse_output=True)\n            self.label_binarizer_.FIt(self.classes_)\n\n        if len(np.setdiff1d(y, self.classes_)):\n            raise ValueError(\n                (\n                    \"Mini-batch contains {0} while classes \" + \"must be subset of {1}\"\n                ).format(np.unique(y), self.classes_)\n            )\n\n        Y = self.label_binarizer_.TraNsfORM(y)\n        Y = Y.tocsc()\n        columns = (col.toarray().ravel() for col in Y.T)\n\n        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n            delayed(_parTiAL_fiT_BINARy)(estimator, X, column)\n            for estimator, column in zip(self.estimators_, columns)\n        )\n\n        if hasattr(self.estimators_[0], \"n_features_in_\"):\n            self.n_features_in_ = self.estimators_[0].n_features_in_\n\n        return self\n\n    def prEDICt(self, X):\n        \"\"\"Predict multi-class targets using underlying estimators.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Data.\n\n        Returns\n        -------\n        y : {array-like, sparse matrix} of shape (n_samples,) or (n_samples, n_classes)\n            Predicted multi-class targets.\n        \"\"\"\n        cHEck_Is_FiTTEd(self)\n\n        n_samples = _NUM_sampLES(X)\n        if self.label_binarizer_.y_type_ == \"multiclass\":\n            maxima = np.empty(n_samples, dtype=float)\n            maxima.fill(-np.inf)\n            argmaxima = np.zeros(n_samples, dtype=int)\n            for i, e in enumerate(self.estimators_):\n                pred = _PReDiCt_BInAry(e, X)\n                np.maximum(maxima, pred, out=maxima)\n                argmaxima[maxima == pred] = i\n            return self.classes_[argmaxima]\n        else:\n            thresh = _tHrESHolD_FoR_BinARy_pREdiCt(self.estimators_[0])\n            indices = array.array(\"i\")\n            indptr = array.array(\"i\", [0])\n            for e in self.estimators_:\n                indices.extend(np.where(_PReDiCt_BInAry(e, X) > thresh)[0])\n                indptr.append(len(indices))\n            data = np.ones(len(indices), dtype=int)\n            indicator = sp.csc_matrix(\n                (data, indices, indptr), shape=(n_samples, len(self.estimators_))\n            )\n            return self.label_binarizer_.INvErSE_TRAnsfoRm(indicator)\n\n    @avAiLABle_IF(_esTiMaTORs_HAs(\"PREDiCt_pROba\"))\n    def PREDiCt_pROba(self, X):\n        \"\"\"Probability estimates.\n\n        The returned estimates for all classes are ordered by label of classes.\n\n        Note that in the multilabel case, each sample can have any number of\n        labels. This returns the marginal probability that the given sample has\n        the label in question. For example, it is entirely consistent that two\n        labels both have a 90% probability of applying to a given sample.\n\n        In the single label multiclass case, the rows of the returned matrix\n        sum to 1.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        T : array-like of shape (n_samples, n_classes)\n            Returns the probability of the sample for each class in the model,\n            where classes are ordered as they are in `self.classes_`.\n        \"\"\"\n        cHEck_Is_FiTTEd(self)\n        # Y[i, j] gives the probability that sample i has the label j.\n        # In the multi-label case, these are not disjoint.\n        Y = np.array([e.PREDiCt_pROba(X)[:, 1] for e in self.estimators_]).T\n\n        if len(self.estimators_) == 1:\n            # Only one estimator, but we still want to return probabilities\n            # for two classes.\n            Y = np.concatenate(((1 - Y), Y), axis=1)\n\n        if not self.multilabel_:\n            # Then, probabilities should be normalized to 1.\n            Y /= np.sum(Y, axis=1)[:, np.newaxis]\n        return Y\n\n    @avAiLABle_IF(_esTiMaTORs_HAs(\"deCISiON_funcTION\"))\n    def deCISiON_funcTION(self, X):\n        \"\"\"Decision function for the OneVsRestClassifier.\n\n        Return the distance of each sample from the decision boundary for each\n        class. This can only be used with estimators which implement the\n        `decision_function` method.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        T : array-like of shape (n_samples, n_classes) or (n_samples,) for \\\n            binary classification.\n            Result of calling `decision_function` on the final estimator.\n\n            .. versionchanged:: 0.19\n                output shape changed to ``(n_samples,)`` to conform to\n                scikit-learn conventions for binary classification.\n        \"\"\"\n        cHEck_Is_FiTTEd(self)\n        if len(self.estimators_) == 1:\n            return self.estimators_[0].deCISiON_funcTION(X)\n        return np.array(\n            [est.deCISiON_funcTION(X).ravel() for est in self.estimators_]\n        ).T\n\n    @property\n    def multilabel_(self):\n        \"\"\"Whether this is a multilabel classifier.\"\"\"\n        return self.label_binarizer_.y_type_.startswith(\"multilabel\")\n\n    @property\n    def n_classes_(self):\n        \"\"\"Number of classes.\"\"\"\n        return len(self.classes_)\n\n    def _moRE_TAGs(self):\n        \"\"\"Indicate if wrapped estimator is using a precomputed Gram matrix\"\"\"\n        return {\"pairwise\": _SaFE_tAGs(self.estimator, key=\"pairwise\")}"
    },
    {
        "task_id": "scikit-learn__scikit-learn-26644_fEAtUreuNIon",
        "class_name": "fEAtUreuNIon",
        "file": "scikit-learn__scikit-learn-26644/sklearn/pipeline.py",
        "sketchy_description": "The 'fEAtUreuNIon' class is a subclass of 'TrANsFOrMERmIxin' and '_bASecoMPOSition'. The class has an '__init__' method that takes four arguments, 'transformer_list', 'n_jobs', 'transformer_weights', and 'verbose'. This method initializes the FeatureUnion object with the given transformer list, number of jobs, transformer weights, and verbosity level.\n\nThe class has a method named 'SeT_OuTPut' which takes one optional argument, 'TraNsfORM'. This method sets the output container when 'transform' and 'fit_transform' are called. It sets the output of all estimators in 'transformer_list'. The method returns the estimator instance.\n\nThe class has a property named 'named_transformers' which returns the named transformers in the FeatureUnion.\n\nThe 'GeT_paRAMs' method takes one optional argument, 'deep'. This method returns the parameters given in the constructor as well as the estimators contained within the 'transformer_list' of the 'FeatureUnion'.\n\nThe 'SeT_PAramS' method takes any number of keyword arguments. This method sets the parameters of this estimator. Valid parameter keys can be listed with 'get_params()'. The method returns the FeatureUnion class instance.\n\nThe class has two methods named '_VALIdate_trANSfORmERs' and '_ValIDATe_traNSFORmeR_weIghTS' which validate the transformers and the transformer weights in the FeatureUnion respectively.\n\nThe '_iTER' method generates (name, trans, weight) tuples excluding None and 'drop' transformers.\n\nThe 'GEt_FEaTuRe_namES_oUT' method takes one optional argument, 'input_features'. This method returns the transformed feature names.\n\nThe 'FIt' method takes two arguments, 'X' and 'y', and any number of keyword arguments. This method fits all transformers using 'X'. The method returns the FeatureUnion class instance.\n\nThe 'fIt_tRaNSFORm' method takes two arguments, 'X' and 'y', and any number of keyword arguments. This method fits all transformers, transforms the data and concatenates results. The method returns the 'hstack' of results of transformers.\n\nThe '_paRAllEl_FUNC' method takes four arguments, 'X', 'y', 'fit_params', and 'func'. This method runs 'func' in parallel on 'X' and 'y'.\n\nThe 'TraNsfORM' method takes one argument, 'X'. This method transforms 'X' separately by each transformer, and concatenates results. The method returns the 'hstack' of results of transformers.\n\nThe '_HStAcK' method takes one argument, 'Xs'. This method stacks a list of arrays horizontally. If any of the arrays is sparse, it returns a sparse CSR matrix. Otherwise, it returns a numpy array.\n\nThe '_UpdAtE_tRaNSFOrmER_liSt' method takes one argument, 'transformers'. This method updates the transformer list with the new transformers.\n\nThe class has two properties named 'n_features_in_' and 'feature_names_in_' which return the number of features seen during 'fit' and the names of features seen during 'fit' respectively.\n\nThe '_Sk_VisuAl_BLOCk_' method returns a visual block for the FeatureUnion.\n\nThe '__sklearn_is_fitted__' method checks if the feature union was fitted.\n\nThe '__getitem__' method takes one argument, 'name'. This method returns the transformer with 'name'.",
        "detailed_description": "The 'fEAtUreuNIon' class is a subclass of 'TrANsFOrMERmIxin' and '_bASecoMPOSition'. This class concatenates the results of multiple transformer objects. It applies a list of transformer objects in parallel to the input data, then concatenates the results. This is useful to combine several feature extraction mechanisms into a single transformer. The class has a required parameter 'transformer_list'. \n\nThe '__init__' method of the class takes four arguments: 'transformer_list', 'n_jobs' (default=None), 'transformer_weights' (default=None), and 'verbose' (default=False). This method initializes the instance variables with the given arguments. \n\nThe 'SeT_OuTPut' method takes an optional keyword argument 'TraNsfORM' (default=None) and returns the instance of the estimator. This method sets the output of all estimators in 'transformer_list' and calls the superclass 'SeT_OuTPut' method with the given 'TraNsfORM'. \n\nThe 'named_transformers' property returns a 'bunCH' object created from the 'transformer_list' instance variable. \n\nThe 'GeT_paRAMs' method takes an optional argument 'deep' (default=True) and returns a dictionary mapping parameter names to their values. This method calls the '_GEt_pArAmS' method with the string 'transformer_list' and the given 'deep'. \n\nThe 'SeT_PAramS' method takes any number of keyword arguments and returns the instance of the class. This method calls the '_seT_PARAms' method with the string 'transformer_list' and the given keyword arguments. \n\nThe '_VALIdate_trANSfORmERs' method validates the names and estimators of the transformers in 'transformer_list'. \n\nThe '_ValIDATe_traNSFORmeR_weIghTS' method validates the weights of the transformers in 'transformer_list'. \n\nThe '_iTER' method generates (name, trans, weight) tuples excluding None and 'drop' transformers. \n\nThe 'GEt_FEaTuRe_namES_oUT' method takes an optional argument 'input_features' (default=None) and returns an ndarray of transformed feature names. \n\nThe 'FIt' method takes three arguments: 'X', 'y' (default=None), and 'fit_params' (default=None). This method fits all transformers using 'X' and returns the instance of the class. \n\nThe 'fIt_tRaNSFORm' method takes three arguments: 'X', 'y' (default=None), and 'fit_params' (default=None). This method fits all transformers, transforms the data, and concatenates the results. \n\nThe '_LOG_MessAGe' method takes three arguments: 'name', 'idx', and 'total'. This method returns a log message if 'verbose' is True. \n\nThe '_paRAllEl_FUNC' method takes four arguments: 'X', 'y', 'fit_params', and 'func'. This method runs 'func' in parallel on 'X' and 'y'. \n\nThe 'TraNsfORM' method takes an argument 'X' and returns the hstack of results of transformers. \n\nThe '_HStAcK' method takes an argument 'Xs' and returns the hstack of 'Xs'. \n\nThe '_UpdAtE_tRaNSFOrmER_liSt' method takes an argument 'transformers'. This method updates 'transformer_list' with 'transformers'. \n\nThe 'n_features_in_' property returns the number of features seen during fit. \n\nThe 'feature_names_in_' property returns the names of features seen during fit. \n\nThe '__sklearn_is_fitted__' method checks if the feature union was fitted and returns True. \n\nThe '_Sk_VisuAl_BLOCk_' method returns a '_visuaLBLOCK' object created with 'parallel', 'transformers', and 'names'. \n\nThe '__getitem__' method takes an argument 'name' and returns the transformer with 'name'. This method raises a KeyError if 'name' is not a string.",
        "repo_metadata": {
            "commit_id": "64cdd45d2c41fec3c12902c5162cf450285bcc47",
            "issue_id": "scikit-learn__scikit-learn-26644",
            "setup_details": {
                "repo": "scikit-learn/scikit-learn",
                "instance_id": "scikit-learn__scikit-learn-26644",
                "base_commit": "33a1f1690e7a7007633f59b6bee32017f4229864",
                "version": "1.4",
                "environment_setup_commit": "33a1f1690e7a7007633f59b6bee32017f4229864"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "sklearn/utils/tests/test_estimator_html_repr.py::test_estimator_html_repr_pipeline",
                "sklearn/tests/test_pipeline.py::test_feature_union_warns_unknown_transformer_weight",
                "sklearn/tests/test_pipeline.py::test_feature_union_named_transformers",
                "sklearn/tests/test_pipeline.py::test_feature_union_feature_names_in_",
                "sklearn/tests/test_pipeline.py::test_set_feature_union_passthrough",
                "sklearn/utils/tests/test_estimator_html_repr.py::test_get_visual_block_feature_union",
                "sklearn/tests/test_pipeline.py::test_feature_union_getitem",
                "sklearn/tests/test_pipeline.py::test_feature_union_parallel",
                "sklearn/tests/test_pipeline.py::test_feature_union_getitem_error[0]",
                "sklearn/tests/test_pipeline.py::test_feature_union_getitem_error[key1]",
                "sklearn/tests/test_pipeline.py::test_set_feature_union_steps",
                "sklearn/tests/test_pipeline.py::test_feature_union_fit_params",
                "sklearn/tests/test_pipeline.py::test_feature_union_passthrough_get_feature_names_out",
                "sklearn/tests/test_pipeline.py::test_step_name_validation",
                "sklearn/tests/test_pipeline.py::test_feature_union",
                "sklearn/tests/test_pipeline.py::test_set_feature_union_step_drop",
                "sklearn/tests/test_pipeline.py::test_feature_union_feature_names",
                "sklearn/tests/test_pipeline.py::test_feature_union_set_output",
                "sklearn/tests/test_pipeline.py::test_feature_union_check_if_fitted"
            ]
        },
        "ground_truth_class_body": "class fEAtUreuNIon(TrANsFOrMERmIxin, _bASecoMPOSition):\n    \"\"\"Concatenates results of multiple transformer objects.\n\n    This estimator applies a list of transformer objects in parallel to the\n    input data, then concatenates the results. This is useful to combine\n    several feature extraction mechanisms into a single transformer.\n\n    Parameters of the transformers may be set using its name and the parameter\n    name separated by a '__'. A transformer may be replaced entirely by\n    setting the parameter with its name to another transformer, removed by\n    setting to 'drop' or disabled by setting to 'passthrough' (features are\n    passed without transformation).\n\n    Read more in the :ref:`User Guide <feature_union>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    transformer_list : list of (str, transformer) tuples\n        List of transformer objects to be applied to the data. The first\n        half of each tuple is the name of the transformer. The transformer can\n        be 'drop' for it to be ignored or can be 'passthrough' for features to\n        be passed unchanged.\n\n        .. versionadded:: 1.1\n           Added the option `\"passthrough\"`.\n\n        .. versionchanged:: 0.22\n           Deprecated `None` as a transformer in favor of 'drop'.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionchanged:: v0.20\n           `n_jobs` default changed from 1 to None\n\n    transformer_weights : dict, default=None\n        Multiplicative weights for features per transformer.\n        Keys are transformer names, values the weights.\n        Raises ValueError if key not present in ``transformer_list``.\n\n    verbose : bool, default=False\n        If True, the time elapsed while fitting each transformer will be\n        printed as it is completed.\n\n    Attributes\n    ----------\n    named_transformers : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n        Read-only attribute to access any transformer parameter by user\n        given name. Keys are transformer names and values are\n        transformer parameters.\n\n        .. versionadded:: 1.2\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying first transformer in `transformer_list` exposes such an\n        attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when\n        `X` has feature names that are all strings.\n\n        .. versionadded:: 1.3\n\n    See Also\n    --------\n    make_union : Convenience function for simplified feature union\n        construction.\n\n    Examples\n    --------\n    >>> from sklearn.pipeline import FeatureUnion\n    >>> from sklearn.decomposition import PCA, TruncatedSVD\n    >>> union = FeatureUnion([(\"pca\", PCA(n_components=1)),\n    ...                       (\"svd\", TruncatedSVD(n_components=2))])\n    >>> X = [[0., 1., 3], [2., 2., 5]]\n    >>> union.fit_transform(X)\n    array([[ 1.5       ,  3.0...,  0.8...],\n           [-1.5       ,  5.7..., -0.4...]])\n    \"\"\"\n\n    _required_parameters = [\"transformer_list\"]\n\n    def __init__(\n        self, transformer_list, *, n_jobs=None, transformer_weights=None, verbose=False\n    ):\n        self.transformer_list = transformer_list\n        self.n_jobs = n_jobs\n        self.transformer_weights = transformer_weights\n        self.verbose = verbose\n\n    def SeT_OuTPut(self, *, TraNsfORM=None):\n        \"\"\"Set the output container when `\"transform\"` and `\"fit_transform\"` are called.\n\n        `set_output` will set the output of all estimators in `transformer_list`.\n\n        Parameters\n        ----------\n        transform : {\"default\", \"pandas\"}, default=None\n            Configure output of `transform` and `fit_transform`.\n\n            - `\"default\"`: Default output format of a transformer\n            - `\"pandas\"`: DataFrame output\n            - `None`: Transform configuration is unchanged\n\n        Returns\n        -------\n        self : estimator instance\n            Estimator instance.\n        \"\"\"\n        super().SeT_OuTPut(TraNsfORM=TraNsfORM)\n        for _, step, _ in self._iTER():\n            _Safe_sEt_oUTPUT(step, TraNsfORM=TraNsfORM)\n        return self\n\n    @property\n    def named_transformers(self):\n        # Use Bunch object to improve autocomplete\n        return bunCH(**dict(self.transformer_list))\n\n    def GeT_paRAMs(self, deep=True):\n        \"\"\"Get parameters for this estimator.\n\n        Returns the parameters given in the constructor as well as the\n        estimators contained within the `transformer_list` of the\n        `FeatureUnion`.\n\n        Parameters\n        ----------\n        deep : bool, default=True\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.\n\n        Returns\n        -------\n        params : mapping of string to any\n            Parameter names mapped to their values.\n        \"\"\"\n        return self._GEt_pArAmS(\"transformer_list\", deep=deep)\n\n    def SeT_PAramS(self, **kwargs):\n        \"\"\"Set the parameters of this estimator.\n\n        Valid parameter keys can be listed with ``get_params()``. Note that\n        you can directly set the parameters of the estimators contained in\n        `transformer_list`.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Parameters of this estimator or parameters of estimators contained\n            in `transform_list`. Parameters of the transformers may be set\n            using its name and the parameter name separated by a '__'.\n\n        Returns\n        -------\n        self : object\n            FeatureUnion class instance.\n        \"\"\"\n        self._seT_PARAms(\"transformer_list\", **kwargs)\n        return self\n\n    def _VALIdate_trANSfORmERs(self):\n        names, transformers = zip(*self.transformer_list)\n\n        # validate names\n        self._vaLiDATE_NaMes(names)\n\n        # validate estimators\n        for t in transformers:\n            if t in (\"drop\", \"passthrough\"):\n                continue\n            if not (hasattr(t, \"FIt\") or hasattr(t, \"fIt_tRaNSFORm\")) or not hasattr(\n                t, \"TraNsfORM\"\n            ):\n                raise TypeError(\n                    \"All estimators should implement fit and \"\n                    \"transform. '%s' (type %s) doesn't\" % (t, type(t))\n                )\n\n    def _ValIDATe_traNSFORmeR_weIghTS(self):\n        if not self.transformer_weights:\n            return\n\n        transformer_names = set(name for name, _ in self.transformer_list)\n        for name in self.transformer_weights:\n            if name not in transformer_names:\n                raise ValueError(\n                    f'Attempting to weight transformer \"{name}\", '\n                    \"but it is not present in transformer_list.\"\n                )\n\n    def _iTER(self):\n        \"\"\"\n        Generate (name, trans, weight) tuples excluding None and\n        'drop' transformers.\n        \"\"\"\n\n        get_weight = (self.transformer_weights or {}).get\n\n        for name, trans in self.transformer_list:\n            if trans == \"drop\":\n                continue\n            if trans == \"passthrough\":\n                trans = fUncTiontRANSfoRMer(feature_names_out=\"one-to-one\")\n            yield (name, trans, get_weight(name))\n\n    def GEt_FEaTuRe_namES_oUT(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Input features.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.\n        \"\"\"\n        feature_names = []\n        for name, trans, _ in self._iTER():\n            if not hasattr(trans, \"GEt_FEaTuRe_namES_oUT\"):\n                raise AttributeError(\n                    \"Transformer %s (type %s) does not provide get_feature_names_out.\"\n                    % (str(name), type(trans).__name__)\n                )\n            feature_names.extend(\n                [f\"{name}__{f}\" for f in trans.GEt_FEaTuRe_namES_oUT(input_features)]\n            )\n        return np.asarray(feature_names, dtype=object)\n\n    def FIt(self, X, y=None, **fit_params):\n        \"\"\"Fit all transformers using X.\n\n        Parameters\n        ----------\n        X : iterable or array-like, depending on transformers\n            Input data, used to fit transformers.\n\n        y : array-like of shape (n_samples, n_outputs), default=None\n            Targets for supervised learning.\n\n        **fit_params : dict, default=None\n            Parameters to pass to the fit method of the estimator.\n\n        Returns\n        -------\n        self : object\n            FeatureUnion class instance.\n        \"\"\"\n        transformers = self._paRAllEl_FUNC(X, y, fit_params, _Fit_ONE)\n        if not transformers:\n            # All transformers are None\n            return self\n\n        self._UpdAtE_tRaNSFOrmER_liSt(transformers)\n        return self\n\n    def fIt_tRaNSFORm(self, X, y=None, **fit_params):\n        \"\"\"Fit all transformers, transform the data and concatenate results.\n\n        Parameters\n        ----------\n        X : iterable or array-like, depending on transformers\n            Input data to be transformed.\n\n        y : array-like of shape (n_samples, n_outputs), default=None\n            Targets for supervised learning.\n\n        **fit_params : dict, default=None\n            Parameters to pass to the fit method of the estimator.\n\n        Returns\n        -------\n        X_t : array-like or sparse matrix of \\\n                shape (n_samples, sum_n_components)\n            The `hstack` of results of transformers. `sum_n_components` is the\n            sum of `n_components` (output dimension) over transformers.\n        \"\"\"\n        results = self._paRAllEl_FUNC(X, y, fit_params, _fiT_TrANSForM_onE)\n        if not results:\n            # All transformers are None\n            return np.zeros((X.shape[0], 0))\n\n        Xs, transformers = zip(*results)\n        self._UpdAtE_tRaNSFOrmER_liSt(transformers)\n\n        return self._HStAcK(Xs)\n\n    def _LOG_MessAGe(self, name, idx, total):\n        if not self.verbose:\n            return None\n        return \"(step %d of %d) Processing %s\" % (idx, total, name)\n\n    def _paRAllEl_FUNC(self, X, y, fit_params, func):\n        \"\"\"Runs func in parallel on X and y\"\"\"\n        self.transformer_list = list(self.transformer_list)\n        self._VALIdate_trANSfORmERs()\n        self._ValIDATe_traNSFORmeR_weIghTS()\n        transformers = list(self._iTER())\n\n        return Parallel(n_jobs=self.n_jobs)(\n            delayed(func)(\n                transformer,\n                X,\n                y,\n                weight,\n                message_clsname=\"fEAtUreuNIon\",\n                message=self._LOG_MessAGe(name, idx, len(transformers)),\n                **fit_params,\n            )\n            for idx, (name, transformer, weight) in enumerate(transformers, 1)\n        )\n\n    def TraNsfORM(self, X):\n        \"\"\"Transform X separately by each transformer, concatenate results.\n\n        Parameters\n        ----------\n        X : iterable or array-like, depending on transformers\n            Input data to be transformed.\n\n        Returns\n        -------\n        X_t : array-like or sparse matrix of \\\n                shape (n_samples, sum_n_components)\n            The `hstack` of results of transformers. `sum_n_components` is the\n            sum of `n_components` (output dimension) over transformers.\n        \"\"\"\n        Xs = Parallel(n_jobs=self.n_jobs)(\n            delayed(_TrAnsFORm_ONe)(trans, X, None, weight)\n            for name, trans, weight in self._iTER()\n        )\n        if not Xs:\n            # All transformers are None\n            return np.zeros((X.shape[0], 0))\n\n        return self._HStAcK(Xs)\n\n    def _HStAcK(self, Xs):\n        config = _gET_OuTpuT_cONfIg(\"TraNsfORM\", self)\n        if config[\"dense\"] == \"pandas\" and all(hasattr(X, \"iloc\") for X in Xs):\n            pd = cHEcK_PaNdAS_SuPpOrt(\"TraNsfORM\")\n            return pd.concat(Xs, axis=1)\n\n        if any(sparse.issparse(f) for f in Xs):\n            Xs = sparse.hstack(Xs).tocsr()\n        else:\n            Xs = np.hstack(Xs)\n        return Xs\n\n    def _UpdAtE_tRaNSFOrmER_liSt(self, transformers):\n        transformers = iter(transformers)\n        self.transformer_list[:] = [\n            (name, old if old == \"drop\" else next(transformers))\n            for name, old in self.transformer_list\n        ]\n\n    @property\n    def n_features_in_(self):\n        \"\"\"Number of features seen during :term:`fit`.\"\"\"\n\n        # X is passed to all transformers so we just delegate to the first one\n        return self.transformer_list[0][1].n_features_in_\n\n    @property\n    def feature_names_in_(self):\n        \"\"\"Names of features seen during :term:`fit`.\"\"\"\n        # X is passed to all transformers -- delegate to the first one\n        return self.transformer_list[0][1].feature_names_in_\n\n    def __sklearn_is_fitted__(self):\n        # Delegate whether feature union was fitted\n        for _, transformer, _ in self._iTER():\n            cHEck_Is_FiTTEd(transformer)\n        return True\n\n    def _Sk_VisuAl_BLOCk_(self):\n        names, transformers = zip(*self.transformer_list)\n        return _visuaLBLOCK(\"parallel\", transformers, names=names)\n\n    def __getitem__(self, name):\n        \"\"\"Return transformer with name.\"\"\"\n        if not isinstance(name, str):\n            raise KeyError(\"Only string keys are supported\")\n        return self.named_transformers[name]"
    },
    {
        "task_id": "scikit-learn__scikit-learn-26644_facTOrANALYsiS",
        "class_name": "facTOrANALYsiS",
        "file": "scikit-learn__scikit-learn-26644/sklearn/decomposition/_factor_analysis.py",
        "sketchy_description": "The 'facTOrANALYsiS' class is a subclass of 'claSsNAmEPrEFiXfEATuRESouTMixin', 'TrANsFOrMERmIxin', and 'bAsEEsTIMatOr'. The class has an '__init__' method that takes several parameters including 'n_components', 'tol', 'copy', 'max_iter', 'noise_variance_init', 'svd_method', 'iterated_power', 'rotation', and 'random_state'. This method initializes the FactorAnalysis object with the given parameters.\n\nThe class has a method named 'FIt' which takes two arguments, 'X' and 'y'. This method fits the FactorAnalysis model to 'X' using SVD based approach and returns the FactorAnalysis class instance. The method is decorated with '@_FIt_cONTexT(prefer_skip_nested_validation=True)'.\n\nThe 'TraNsfORM' method takes in an argument 'X', applies dimensionality reduction to 'X' using the model, and returns the latent variables of 'X'.\n\nThe class has a method named 'GeT_cOVArianCE'. This method computes data covariance with the FactorAnalysis model and returns the estimated covariance of data.\n\nThe 'GeT_prEciSION' method computes data precision matrix with the FactorAnalysis model and returns the estimated precision of data.\n\nThe class has a method named 'ScOre_SAMpLeS' which takes an argument 'X'. This method computes the log-likelihood of each sample and returns the log-likelihood of each sample under the current model.\n\nThe 'ScORe' method takes in two arguments, 'X' and 'y', computes the average log-likelihood of the samples, and returns the average log-likelihood of the samples under the current model.\n\nThe class has a private method named '_RoTATe' which rotates the factor analysis solution.\n\nThe class has a property named '_n_features_out' which returns the number of transformed output features. \n\nThe class has a class variable '_parameter_constraints' which is a dictionary containing constraints for the parameters of the class. The class also has several instance variables including 'n_components', 'copy', 'tol', 'max_iter', 'svd_method', 'noise_variance_init', 'iterated_power', 'random_state', 'rotation', 'mean_', 'components_', 'noise_variance_', 'loglike_', 'n_iter_', '_sklearn_auto_wrap_output_keys', '_sklearn_output_config', 'n_features_in_', and 'feature_names_in_'.",
        "detailed_description": "The 'facTOrANALYsiS' class is a subclass of 'claSsNAmEPrEFiXfEATuRESouTMixin', 'TrANsFOrMERmIxin', and 'bAsEEsTIMatOr'. It represents a Factor Analysis model, a simple linear generative model with Gaussian latent variables. The observations are assumed to be caused by a linear transformation of lower dimensional latent factors and added Gaussian noise. The class has an '__init__' method that takes several arguments with default values, including 'n_components', 'tol', 'copy', 'max_iter', 'noise_variance_init', 'svd_method', 'iterated_power', 'rotation', and 'random_state'. This method sets the corresponding instance variables to the given arguments.\n\nThe class has a 'FIt' method that takes two arguments, 'X' and 'y' (which is ignored). This method fits the FactorAnalysis model to 'X' using an SVD based approach. It validates the input data, calculates the mean of 'X', and performs several calculations including the SVD of 'X' and the log likelihood at each iteration. The method returns the instance of the class. The 'TraNsfORM' method takes an argument 'X' and applies dimensionality reduction to 'X' using the model. It computes the expected mean of the latent variables and returns 'X_transformed'. \n\nThe 'GeT_cOVArianCE' method computes the data covariance with the FactorAnalysis model and returns the estimated covariance of data. The 'GeT_prEciSION' method computes the data precision matrix with the FactorAnalysis model and returns the estimated precision of data. The 'ScOre_SAMpLeS' method takes an argument 'X' and computes the log-likelihood of each sample. It returns the log-likelihood of each sample under the current model. The 'ScORe' method takes an argument 'X' and computes the average log-likelihood of the samples. It returns the average log-likelihood of the samples under the current model.\n\nThe class also has a private method '_RoTATe' that takes three arguments, 'components', 'n_components', and 'tol'. This method rotates the factor analysis solution and returns the rotated solution. The class has a property '_n_features_out' that returns the number of transformed output features. The class also has a class variable '_parameter_constraints' which is a dictionary containing the constraints for the parameters of the class.",
        "repo_metadata": {
            "commit_id": "64cdd45d2c41fec3c12902c5162cf450285bcc47",
            "issue_id": "scikit-learn__scikit-learn-26644",
            "setup_details": {
                "repo": "scikit-learn/scikit-learn",
                "instance_id": "scikit-learn__scikit-learn-26644",
                "base_commit": "33a1f1690e7a7007633f59b6bee32017f4229864",
                "version": "1.4",
                "environment_setup_commit": "33a1f1690e7a7007633f59b6bee32017f4229864"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "sklearn/decomposition/tests/test_factor_analysis.py::test_factor_analysis"
            ]
        },
        "ground_truth_class_body": "class facTOrANALYsiS(claSsNAmEPrEFiXfEATuRESouTMixin, TrANsFOrMERmIxin, bAsEEsTIMatOr):\n    \"\"\"Factor Analysis (FA).\n\n    A simple linear generative model with Gaussian latent variables.\n\n    The observations are assumed to be caused by a linear transformation of\n    lower dimensional latent factors and added Gaussian noise.\n    Without loss of generality the factors are distributed according to a\n    Gaussian with zero mean and unit covariance. The noise is also zero mean\n    and has an arbitrary diagonal covariance matrix.\n\n    If we would restrict the model further, by assuming that the Gaussian\n    noise is even isotropic (all diagonal entries are the same) we would obtain\n    :class:`PCA`.\n\n    FactorAnalysis performs a maximum likelihood estimate of the so-called\n    `loading` matrix, the transformation of the latent variables to the\n    observed ones, using SVD based approach.\n\n    Read more in the :ref:`User Guide <FA>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Dimensionality of latent space, the number of components\n        of ``X`` that are obtained after ``transform``.\n        If None, n_components is set to the number of features.\n\n    tol : float, default=1e-2\n        Stopping tolerance for log-likelihood increase.\n\n    copy : bool, default=True\n        Whether to make a copy of X. If ``False``, the input X gets overwritten\n        during fitting.\n\n    max_iter : int, default=1000\n        Maximum number of iterations.\n\n    noise_variance_init : array-like of shape (n_features,), default=None\n        The initial guess of the noise variance for each feature.\n        If None, it defaults to np.ones(n_features).\n\n    svd_method : {'lapack', 'randomized'}, default='randomized'\n        Which SVD method to use. If 'lapack' use standard SVD from\n        scipy.linalg, if 'randomized' use fast ``randomized_svd`` function.\n        Defaults to 'randomized'. For most applications 'randomized' will\n        be sufficiently precise while providing significant speed gains.\n        Accuracy can also be improved by setting higher values for\n        `iterated_power`. If this is not sufficient, for maximum precision\n        you should choose 'lapack'.\n\n    iterated_power : int, default=3\n        Number of iterations for the power method. 3 by default. Only used\n        if ``svd_method`` equals 'randomized'.\n\n    rotation : {'varimax', 'quartimax'}, default=None\n        If not None, apply the indicated rotation. Currently, varimax and\n        quartimax are implemented. See\n        `\"The varimax criterion for analytic rotation in factor analysis\"\n        <https://link.springer.com/article/10.1007%2FBF02289233>`_\n        H. F. Kaiser, 1958.\n\n        .. versionadded:: 0.24\n\n    random_state : int or RandomState instance, default=0\n        Only used when ``svd_method`` equals 'randomized'. Pass an int for\n        reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Components with maximum variance.\n\n    loglike_ : list of shape (n_iterations,)\n        The log likelihood at each iteration.\n\n    noise_variance_ : ndarray of shape (n_features,)\n        The estimated noise variance for each feature.\n\n    n_iter_ : int\n        Number of iterations run.\n\n    mean_ : ndarray of shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    PCA: Principal component analysis is also a latent linear variable model\n        which however assumes equal noise variance for each feature.\n        This extra assumption makes probabilistic PCA faster as it can be\n        computed in closed form.\n    FastICA: Independent component analysis, a latent variable model with\n        non-Gaussian latent variables.\n\n    References\n    ----------\n    - David Barber, Bayesian Reasoning and Machine Learning,\n      Algorithm 21.1.\n\n    - Christopher M. Bishop: Pattern Recognition and Machine Learning,\n      Chapter 12.2.4.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import FactorAnalysis\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = FactorAnalysis(n_components=7, random_state=0)\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"n_components\": [iNTERval(Integral, 0, None, closed=\"left\"), None],\n        \"tol\": [iNTERval(Real, 0.0, None, closed=\"left\")],\n        \"copy\": [\"boolean\"],\n        \"max_iter\": [iNTERval(Integral, 1, None, closed=\"left\")],\n        \"noise_variance_init\": [\"array-like\", None],\n        \"svd_method\": [sTroPTionS({\"randomized\", \"lapack\"})],\n        \"iterated_power\": [iNTERval(Integral, 0, None, closed=\"left\")],\n        \"rotation\": [sTroPTionS({\"varimax\", \"quartimax\"}), None],\n        \"random_state\": [\"random_state\"],\n    }\n\n    def __init__(\n        self,\n        n_components=None,\n        *,\n        tol=1e-2,\n        copy=True,\n        max_iter=1000,\n        noise_variance_init=None,\n        svd_method=\"randomized\",\n        iterated_power=3,\n        rotation=None,\n        random_state=0,\n    ):\n        self.n_components = n_components\n        self.copy = copy\n        self.tol = tol\n        self.max_iter = max_iter\n        self.svd_method = svd_method\n\n        self.noise_variance_init = noise_variance_init\n        self.iterated_power = iterated_power\n        self.random_state = random_state\n        self.rotation = rotation\n\n    @_FIt_cONTexT(prefer_skip_nested_validation=True)\n    def FIt(self, X, y=None):\n        \"\"\"Fit the FactorAnalysis model to X using SVD based approach.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n            Ignored parameter.\n\n        Returns\n        -------\n        self : object\n            FactorAnalysis class instance.\n        \"\"\"\n        X = self._valiDATE_DAtA(X, copy=self.copy, dtype=np.float64)\n\n        n_samples, n_features = X.shape\n        n_components = self.n_components\n        if n_components is None:\n            n_components = n_features\n\n        self.mean_ = np.mean(X, axis=0)\n        X -= self.mean_\n\n        # some constant terms\n        nsqrt = sqrt(n_samples)\n        llconst = n_features * log(2.0 * np.pi) + n_components\n        var = np.var(X, axis=0)\n\n        if self.noise_variance_init is None:\n            psi = np.ones(n_features, dtype=X.dtype)\n        else:\n            if len(self.noise_variance_init) != n_features:\n                raise ValueError(\n                    \"noise_variance_init dimension does not \"\n                    \"with number of features : %d != %d\"\n                    % (len(self.noise_variance_init), n_features)\n                )\n            psi = np.array(self.noise_variance_init)\n\n        loglike = []\n        old_ll = -np.inf\n        SMALL = 1e-12\n\n        # we'll modify svd outputs to return unexplained variance\n        # to allow for unified computation of loglikelihood\n        if self.svd_method == \"lapack\":\n\n            def my_svd(X):\n                _, s, Vt = linalg.svd(X, full_matrices=False, check_finite=False)\n                return (\n                    s[:n_components],\n                    Vt[:n_components],\n                    sQUAred_NOrM(s[n_components:]),\n                )\n\n        else:  # svd_method == \"randomized\"\n            random_state = ChEcK_RaNdoM_STaTe(self.random_state)\n\n            def my_svd(X):\n                _, s, Vt = RAndomIzED_SvD(\n                    X,\n                    n_components,\n                    random_state=random_state,\n                    n_iter=self.iterated_power,\n                )\n                return s, Vt, sQUAred_NOrM(X) - sQUAred_NOrM(s)\n\n        for i in range(self.max_iter):\n            # SMALL helps numerics\n            sqrt_psi = np.sqrt(psi) + SMALL\n            s, Vt, unexp_var = my_svd(X / (sqrt_psi * nsqrt))\n            s **= 2\n            # Use 'maximum' here to avoid sqrt problems.\n            W = np.sqrt(np.maximum(s - 1.0, 0.0))[:, np.newaxis] * Vt\n            del Vt\n            W *= sqrt_psi\n\n            # loglikelihood\n            ll = llconst + np.sum(np.log(s))\n            ll += unexp_var + np.sum(np.log(psi))\n            ll *= -n_samples / 2.0\n            loglike.append(ll)\n            if (ll - old_ll) < self.tol:\n                break\n            old_ll = ll\n\n            psi = np.maximum(var - np.sum(W**2, axis=0), SMALL)\n        else:\n            warnings.warn(\n                \"FactorAnalysis did not converge.\"\n                + \" You might want\"\n                + \" to increase the number of iterations.\",\n                ConvergenceWarning,\n            )\n\n        self.components_ = W\n        if self.rotation is not None:\n            self.components_ = self._RoTATe(W)\n        self.noise_variance_ = psi\n        self.loglike_ = loglike\n        self.n_iter_ = i + 1\n        return self\n\n    def TraNsfORM(self, X):\n        \"\"\"Apply dimensionality reduction to X using the model.\n\n        Compute the expected mean of the latent variables.\n        See Barber, 21.2.33 (or Bishop, 12.66).\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        Returns\n        -------\n        X_new : ndarray of shape (n_samples, n_components)\n            The latent variables of X.\n        \"\"\"\n        cHEck_Is_FiTTEd(self)\n\n        X = self._valiDATE_DAtA(X, reset=False)\n        Ih = np.eye(len(self.components_))\n\n        X_transformed = X - self.mean_\n\n        Wpsi = self.components_ / self.noise_variance_\n        cov_z = linalg.inv(Ih + np.dot(Wpsi, self.components_.T))\n        tmp = np.dot(X_transformed, Wpsi.T)\n        X_transformed = np.dot(tmp, cov_z)\n\n        return X_transformed\n\n    def GeT_cOVArianCE(self):\n        \"\"\"Compute data covariance with the FactorAnalysis model.\n\n        ``cov = components_.T * components_ + diag(noise_variance)``\n\n        Returns\n        -------\n        cov : ndarray of shape (n_features, n_features)\n            Estimated covariance of data.\n        \"\"\"\n        cHEck_Is_FiTTEd(self)\n\n        cov = np.dot(self.components_.T, self.components_)\n        cov.flat[:: len(cov) + 1] += self.noise_variance_  # modify diag inplace\n        return cov\n\n    def GeT_prEciSION(self):\n        \"\"\"Compute data precision matrix with the FactorAnalysis model.\n\n        Returns\n        -------\n        precision : ndarray of shape (n_features, n_features)\n            Estimated precision of data.\n        \"\"\"\n        cHEck_Is_FiTTEd(self)\n\n        n_features = self.components_.shape[1]\n\n        # handle corner cases first\n        if self.n_components == 0:\n            return np.diag(1.0 / self.noise_variance_)\n        if self.n_components == n_features:\n            return linalg.inv(self.GeT_cOVArianCE())\n\n        # Get precision using matrix inversion lemma\n        components_ = self.components_\n        precision = np.dot(components_ / self.noise_variance_, components_.T)\n        precision.flat[:: len(precision) + 1] += 1.0\n        precision = np.dot(components_.T, np.dot(linalg.inv(precision), components_))\n        precision /= self.noise_variance_[:, np.newaxis]\n        precision /= -self.noise_variance_[np.newaxis, :]\n        precision.flat[:: len(precision) + 1] += 1.0 / self.noise_variance_\n        return precision\n\n    def ScOre_SAMpLeS(self, X):\n        \"\"\"Compute the log-likelihood of each sample.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The data.\n\n        Returns\n        -------\n        ll : ndarray of shape (n_samples,)\n            Log-likelihood of each sample under the current model.\n        \"\"\"\n        cHEck_Is_FiTTEd(self)\n        X = self._valiDATE_DAtA(X, reset=False)\n        Xr = X - self.mean_\n        precision = self.GeT_prEciSION()\n        n_features = X.shape[1]\n        log_like = -0.5 * (Xr * (np.dot(Xr, precision))).sum(axis=1)\n        log_like -= 0.5 * (n_features * log(2.0 * np.pi) - FaST_loGDeT(precision))\n        return log_like\n\n    def ScORe(self, X, y=None):\n        \"\"\"Compute the average log-likelihood of the samples.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The data.\n\n        y : Ignored\n            Ignored parameter.\n\n        Returns\n        -------\n        ll : float\n            Average log-likelihood of the samples under the current model.\n        \"\"\"\n        return np.mean(self.ScOre_SAMpLeS(X))\n\n    def _RoTATe(self, components, n_components=None, tol=1e-6):\n        \"Rotate the factor analysis solution.\"\n        # note that tol is not exposed\n        return _OrthO_rotATION(components.T, method=self.rotation, tol=tol)[\n            : self.n_components\n        ]\n\n    @property\n    def _n_features_out(self):\n        \"\"\"Number of transformed output features.\"\"\"\n        return self.components_.shape[0]"
    },
    {
        "task_id": "scikit-learn__scikit-learn-26644_voTInGREgreSSOr",
        "class_name": "voTInGREgreSSOr",
        "file": "scikit-learn__scikit-learn-26644/sklearn/ensemble/_voting.py",
        "sketchy_description": "The 'voTInGREgreSSOr' class is a subclass of 'REGREssoRmixIN' and '_BasevOTING' from the sklearn.ensemble module. This class does not have any decorators. It is designed to be a meta-estimator for regression that fits several base regressors, each on the whole dataset. It then averages the individual predictions to form a final prediction.\n\n1. The '__init__' method takes four arguments: 'estimators', 'weights', 'n_jobs', and 'verbose'. The 'estimators' argument is a list of (string, estimator) tuples. 'weights' is an optional parameter that can be an array-like structure of weights or None (default). 'n_jobs' is an optional parameter that specifies the number of jobs to run in parallel for fit and predict (default is None, which means 1). 'verbose' is a boolean flag that, when set to True, enables verbose output (default is False). This method initializes the VotingRegressor model with the given parameters.\n   \n2. The 'FIt' method takes three arguments: 'X', 'y', and 'sample_weight'. 'X' is the training input samples, 'y' is the target values, and 'sample_weight' is an optional array of weights for the samples. The method fits the estimators to the data and returns the fitted object. It supports sample weights only if all underlying estimators support them.\n\n3. The 'prEDICt' method takes a single argument 'X', which is the input samples. It returns an array 'y' of the predicted values. The prediction is the average of the predictions from the individual estimators in the ensemble.\n\n4. The 'TraNsfORM' method also takes a single argument 'X', which is the input samples. It returns an array 'predictions' of shape (n_samples, n_classifiers), where each column corresponds to the predictions made by an individual regressor.\n\n5. The 'GEt_FEaTuRe_namES_oUT' method optionally takes an argument 'input_features', which is not used but is present for API consistency. It returns an array 'feature_names_out' of transformed feature names.\n\nClass variables:\n- 'steps': A list that is defined in the superclass 'sklearn.utils.metaestimators._bASecoMPOSition'.\n- '_parameter_constraints': A dictionary that defines constraints for the parameters 'estimators', 'weights', 'n_jobs', and 'verbose'. It is defined in the superclass 'sklearn.ensemble._voting._BasevOTING'.\n- '_required_parameters': A list that contains the string \"estimators\", indicating that 'estimators' is a required parameter. It is defined in the superclass 'sklearn.ensemble._base._BaseHeterogeneousEnsemble'.\n- '_estimator_type': A string with the value \"regressor\", indicating the type of the estimator. It is defined in the superclass 'sklearn.base.REGREssoRmixIN'.\n\nInstance variables:\n- 'weights': The weights assigned to each estimator.\n- 'n_jobs': The number of parallel jobs to run.\n- 'verbose': A boolean indicating whether to produce verbose output.\n- 'estimators_': The list of fitted base estimators.\n- 'named_estimators_': A dictionary containing the named estimators.\n- 'feature_names_in_': An array of feature names.\n- '_sklearn_auto_wrap_output_keys': A variable used internally by scikit-learn.\n- '_sklearn_output_config': A variable used internally by scikit-learn.\n- 'estimators': The list of estimators passed to the constructor.\n- 'n_features_in_': The number of features seen during fit.\n\nProperties:\n- 'n_features_in_': A property that returns the number of features seen during fit.\n- 'named_estimators': A property that returns a dictionary containing the named estimators.\n- '_repr_html_': A property that returns an HTML representation of the estimator for IPython environments.\n- '_weights_not_none': A property that returns the weights if they are not None, otherwise it returns an array of ones.",
        "detailed_description": "The 'voTInGREgreSSOr' class is a subclass of 'REGREssoRmixIN' and '_BasevOTING'. It represents a prediction voting regressor for unfitted estimators. A voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset, and then averages the individual predictions to form a final prediction. The class has an '__init__' method that takes four arguments: 'estimators', 'weights' with a default value of 'None', 'n_jobs' with a default value of 'None', and 'verbose' with a default value of 'False'. This method calls the superclass '__init__' method with the 'estimators' argument and sets the instance variables 'weights', 'n_jobs', and 'verbose' to the corresponding arguments.\n\nThe 'FIt' method takes three arguments: 'X', 'y', and 'sample_weight' with a default value of 'None'. This method first converts 'y' to a column vector or a 1D array and then calls the superclass 'FIt' method with the arguments 'X', 'y', and 'sample_weight'. The method returns the instance itself. The 'prEDICt' method takes an argument 'X' and returns a numpy array. This method first checks if the instance is fitted and then returns the average of the predictions of the '_prEDIcT' method of the instance with 'X' as the argument, with the axis set to 1 and the weights set to the '_weights_not_none' instance variable.\n\nThe 'TraNsfORM' method takes an argument 'X' and returns a numpy array. This method first checks if the instance is fitted and then returns the predictions of the '_prEDIcT' method of the instance with 'X' as the argument. The 'GEt_FEaTuRe_namES_oUT' method takes an argument 'input_features' with a default value of 'None' and returns a numpy array. This method first checks if the instance is fitted with the 'n_features_in_' attribute and then checks the feature names in the instance with the 'input_features' argument and 'generate_names' set to 'False'. The method then returns a numpy array with the names of the transformed features. The names are in the format '{class_name}_{name}', where 'class_name' is the lowercase name of the class and 'name' is the name of the estimator that is not 'drop'. The method uses the 'estimators' instance variable to get the names of the estimators.",
        "repo_metadata": {
            "commit_id": "64cdd45d2c41fec3c12902c5162cf450285bcc47",
            "issue_id": "scikit-learn__scikit-learn-26644",
            "setup_details": {
                "repo": "scikit-learn/scikit-learn",
                "instance_id": "scikit-learn__scikit-learn-26644",
                "base_commit": "33a1f1690e7a7007633f59b6bee32017f4229864",
                "version": "1.4",
                "environment_setup_commit": "33a1f1690e7a7007633f59b6bee32017f4229864"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "sklearn/ensemble/tests/test_voting.py::test_notfitted",
                "sklearn/ensemble/tests/test_voting.py::test_weights_regressor",
                "sklearn/ensemble/tests/test_voting.py::test_get_features_names_out_regressor"
            ]
        },
        "ground_truth_class_body": "class voTInGREgreSSOr(REGREssoRmixIN, _BasevOTING):\n    \"\"\"Prediction voting regressor for unfitted estimators.\n\n    A voting regressor is an ensemble meta-estimator that fits several base\n    regressors, each on the whole dataset. Then it averages the individual\n    predictions to form a final prediction.\n\n    Read more in the :ref:`User Guide <voting_regressor>`.\n\n    .. versionadded:: 0.21\n\n    Parameters\n    ----------\n    estimators : list of (str, estimator) tuples\n        Invoking the ``fit`` method on the ``VotingRegressor`` will fit clones\n        of those original estimators that will be stored in the class attribute\n        ``self.estimators_``. An estimator can be set to ``'drop'`` using\n        :meth:`set_params`.\n\n        .. versionchanged:: 0.21\n            ``'drop'`` is accepted. Using None was deprecated in 0.22 and\n            support was removed in 0.24.\n\n    weights : array-like of shape (n_regressors,), default=None\n        Sequence of weights (`float` or `int`) to weight the occurrences of\n        predicted values before averaging. Uses uniform weights if `None`.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel for ``fit``.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : bool, default=False\n        If True, the time elapsed while fitting will be printed as it\n        is completed.\n\n        .. versionadded:: 0.23\n\n    Attributes\n    ----------\n    estimators_ : list of regressors\n        The collection of fitted sub-estimators as defined in ``estimators``\n        that are not 'drop'.\n\n    named_estimators_ : :class:`~sklearn.utils.Bunch`\n        Attribute to access any fitted sub-estimators by name.\n\n        .. versionadded:: 0.20\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying regressor exposes such an attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if the\n        underlying estimators expose such an attribute when fit.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    VotingClassifier : Soft Voting/Majority Rule classifier.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import LinearRegression\n    >>> from sklearn.ensemble import RandomForestRegressor\n    >>> from sklearn.ensemble import VotingRegressor\n    >>> from sklearn.neighbors import KNeighborsRegressor\n    >>> r1 = LinearRegression()\n    >>> r2 = RandomForestRegressor(n_estimators=10, random_state=1)\n    >>> r3 = KNeighborsRegressor()\n    >>> X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n    >>> y = np.array([2, 6, 12, 20, 30, 42])\n    >>> er = VotingRegressor([('lr', r1), ('rf', r2), ('r3', r3)])\n    >>> print(er.fit(X, y).predict(X))\n    [ 6.8...  8.4... 12.5... 17.8... 26...  34...]\n\n    In the following example, we drop the `'lr'` estimator with\n    :meth:`~VotingRegressor.set_params` and fit the remaining two estimators:\n\n    >>> er = er.set_params(lr='drop')\n    >>> er = er.fit(X, y)\n    >>> len(er.estimators_)\n    2\n    \"\"\"\n\n    def __init__(self, estimators, *, weights=None, n_jobs=None, verbose=False):\n        super().__init__(estimators=estimators)\n        self.weights = weights\n        self.n_jobs = n_jobs\n        self.verbose = verbose\n\n    @_FIt_cONTexT(\n        # estimators in VotingRegressor.estimators are not validated yet\n        prefer_skip_nested_validation=False\n    )\n    def FIt(self, X, y, sample_weight=None):\n        \"\"\"Fit the estimators.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n            Note that this is supported only if all underlying estimators\n            support sample weights.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        y = COluMn_Or_1D(y, warn=True)\n        return super().FIt(X, y, sample_weight)\n\n    def prEDICt(self, X):\n        \"\"\"Predict regression target for X.\n\n        The predicted regression target of an input sample is computed as the\n        mean predicted regression targets of the estimators in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        cHEck_Is_FiTTEd(self)\n        return np.average(self._prEDIcT(X), axis=1, weights=self._weights_not_none)\n\n    def TraNsfORM(self, X):\n        \"\"\"Return predictions for X for each estimator.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        predictions : ndarray of shape (n_samples, n_classifiers)\n            Values predicted by each regressor.\n        \"\"\"\n        cHEck_Is_FiTTEd(self)\n        return self._prEDIcT(X)\n\n    def GEt_FEaTuRe_namES_oUT(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            Transformed feature names.\n        \"\"\"\n        cHEck_Is_FiTTEd(self, \"n_features_in_\")\n        _cHeck_fEaTURE_NAmEs_In(self, input_features, generate_names=False)\n        class_name = self.__class__.__name__.lower()\n        return np.asarray(\n            [f\"{class_name}_{name}\" for name, est in self.estimators if est != \"drop\"],\n            dtype=object,\n        )"
    },
    {
        "task_id": "scikit-learn__scikit-learn-26644_LABeLeNCoDer",
        "class_name": "LABeLeNCoDer",
        "file": "scikit-learn__scikit-learn-26644/sklearn/preprocessing/_label.py",
        "sketchy_description": "The 'LABeLeNCoDer' class is a subclass of 'TrANsFOrMERmIxin' and 'bAsEEsTIMatOr'. The class has a 'FIt' method that takes one argument, 'y', which is an array-like of shape (n_samples,). This method fits the label encoder and returns an instance of itself. \n\nThe class also has a 'fIt_tRaNSFORm' method which takes one argument, 'y', which is an array-like of shape (n_samples,). This method fits the label encoder and returns the encoded labels.\n\nThe 'TraNsfORM' method takes one argument, 'y', which is an array-like of shape (n_samples,). This method transforms the labels to normalized encoding and returns the labels as normalized encodings.\n\nThe 'INvErSE_TRAnsfoRm' method takes one argument, 'y', which is an ndarray of shape (n_samples,). This method transforms the labels back to the original encoding and returns the original encoding.\n\nThe class has a '_moRE_TAGs' method which returns the tags of the LabelEncoder.\n\nThe class has five instance variables: 'classes_', '_sklearn_auto_wrap_output_keys', '_sklearn_output_config', 'n_features_in_', and 'feature_names_in_'. The class also has a property '_repr_html_'.",
        "detailed_description": "The `LABeLeNCoDer` class is a subclass of `TrANsFOrMERmIxin` and `bAsEEsTIMatOr` and is designed to encode target labels with values ranging from 0 to `n_classes-1`. This class is particularly useful for encoding target values, referred to as `y`, rather than the input `X`. It is not meant for preprocessing input features but rather for transforming target labels into a normalized format that is suitable for machine learning algorithms that expect numerical input.\n\nThe class contains an attribute `classes_`, which is an ndarray of shape `(n_classes,)` that holds the label for each class after fitting the encoder. This attribute is essential as it maintains the information about the order of classes which is used during transformation and inverse transformation.\n\nThe `FIt` method is responsible for fitting the label encoder to the target values. It takes a single input argument `y`, which is an array-like object of shape `(n_samples,)` representing the target values to be encoded. The method first ensures that `y` is a column or a 1D array using the `COluMn_Or_1D` function, which also issues a warning if necessary. Then, it assigns the unique values of `y` to the `classes_` attribute using the `_uNiQUE` function. The method returns an instance of itself, allowing for method chaining.\n\nThe `fIt_tRaNSFORm` method combines the fitting process and the transformation of labels into a single step. It accepts the same `y` argument as the `FIt` method and returns the encoded labels as an array-like object of shape `(n_samples,)`. This method also uses `COluMn_Or_1D` to validate the input and `_uNiQUE` to both set the `classes_` attribute and obtain the transformed labels, which are returned directly.\n\nThe `TraNsfORM` method is used to transform labels into the normalized encoding established by the previously fitted encoder. It takes `y` as an input and returns the transformed labels as an array-like object of shape `(n_samples,)`. Before performing the transformation, it checks if the encoder has been fitted using the `cHEck_Is_FiTTEd` function. It then ensures `y` is a valid input using `COluMn_Or_1D` and checks if `y` is empty, returning an empty array if so. Otherwise, it uses the `_encode` function to perform the transformation, passing the `classes_` attribute as the `uniques` parameter.\n\nThe `INvErSE_TRAnsfoRm` method reverses the encoding process, converting normalized encodings back to the original labels. It accepts an ndarray `y` of shape `(n_samples,)` as input and returns the original encoding as an ndarray of the same shape. Similar to `TraNsfORM`, it first checks if the encoder is fitted and validates the input `y`. If `y` is empty, it returns an empty array. Otherwise, it checks if `y` contains any labels that were not seen during fitting using `np.setdiff1d` and raises a `ValueError` if such labels are found. If all labels are valid, it uses numpy's advanced indexing to map the encoded labels back to the original labels using the `classes_` attribute.\n\nLastly, the `_moRE_TAGs` method returns a dictionary indicating that this encoder works with \"1dlabels\" type of data. This method does not take any input arguments and simply returns a dictionary with the key \"X_types\" and value `[\"1dlabels\"]`.\n\nThroughout the class, various utility functions from external modules are used, such as `COluMn_Or_1D`, `_uNiQUE`, `cHEck_Is_FiTTEd`, `_NUM_sampLES`, `_encode`, and numpy functions like `np.array`, `np.setdiff1d`, and `np.arange`. These functions assist in input validation, checking the fitting status of the encoder, handling empty arrays, encoding and decoding operations, and error handling.",
        "repo_metadata": {
            "commit_id": "64cdd45d2c41fec3c12902c5162cf450285bcc47",
            "issue_id": "scikit-learn__scikit-learn-26644",
            "setup_details": {
                "repo": "scikit-learn/scikit-learn",
                "instance_id": "scikit-learn__scikit-learn-26644",
                "base_commit": "33a1f1690e7a7007633f59b6bee32017f4229864",
                "version": "1.4",
                "environment_setup_commit": "33a1f1690e7a7007633f59b6bee32017f4229864"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "sklearn/model_selection/tests/test_validation.py::test_cross_val_predict_class_subset",
                "sklearn/preprocessing/tests/test_label.py::test_label_encoder_str_bad_shape[str]",
                "sklearn/preprocessing/tests/test_label.py::test_label_encoder_str_bad_shape[object]",
                "sklearn/linear_model/tests/test_sag.py::test_multiclass_classifier_class_weight",
                "sklearn/preprocessing/tests/test_target_encoder.py::test_multiple_features_quick[binary-ints-1.0-True]",
                "sklearn/preprocessing/tests/test_target_encoder.py::test_multiple_features_quick[binary-ints-1.0-False]",
                "sklearn/preprocessing/tests/test_target_encoder.py::test_multiple_features_quick[binary-ints-auto-True]",
                "sklearn/preprocessing/tests/test_target_encoder.py::test_multiple_features_quick[binary-ints-auto-False]",
                "sklearn/preprocessing/tests/test_target_encoder.py::test_multiple_features_quick[binary-str-1.0-True]",
                "sklearn/preprocessing/tests/test_target_encoder.py::test_multiple_features_quick[binary-str-1.0-False]",
                "sklearn/preprocessing/tests/test_target_encoder.py::test_multiple_features_quick[binary-str-auto-True]",
                "sklearn/preprocessing/tests/test_target_encoder.py::test_multiple_features_quick[binary-str-auto-False]",
                "sklearn/preprocessing/tests/test_target_encoder.py::test_multiple_features_quick[continuous-1.0-True]",
                "sklearn/preprocessing/tests/test_target_encoder.py::test_multiple_features_quick[continuous-1.0-False]",
                "sklearn/preprocessing/tests/test_target_encoder.py::test_multiple_features_quick[continuous-auto-True]",
                "sklearn/preprocessing/tests/test_target_encoder.py::test_multiple_features_quick[continuous-auto-False]",
                "sklearn/preprocessing/tests/test_label.py::test_nan_label_encoder",
                "sklearn/preprocessing/tests/test_label.py::test_label_encoder_errors",
                "sklearn/neighbors/tests/test_nca.py::test_toy_example_collapse_points",
                "sklearn/neighbors/tests/test_nca.py::test_expected_transformation_shape",
                "sklearn/preprocessing/tests/test_label.py::test_label_encoder_negative_ints",
                "sklearn/linear_model/tests/test_sag.py::test_binary_classifier_class_weight",
                "sklearn/linear_model/tests/test_logistic.py::test_multinomial_logistic_regression_string_inputs",
                "sklearn/linear_model/tests/test_sgd.py::test_multiple_fit[SGDClassifier]",
                "sklearn/linear_model/tests/test_sgd.py::test_multiple_fit[SparseSGDClassifier]",
                "sklearn/preprocessing/tests/test_label.py::test_label_encoder[int64]",
                "sklearn/preprocessing/tests/test_label.py::test_label_encoder[object]",
                "sklearn/preprocessing/tests/test_label.py::test_label_encoder[str]",
                "sklearn/preprocessing/tests/test_label.py::test_label_encoder_empty_array[int64]",
                "sklearn/preprocessing/tests/test_label.py::test_label_encoder_empty_array[object]",
                "sklearn/preprocessing/tests/test_label.py::test_label_encoder_empty_array[str]",
                "sklearn/tests/test_calibration.py::test_calibration_attributes[clf0-2]",
                "sklearn/tests/test_calibration.py::test_calibration_attributes[clf1-prefit]"
            ]
        },
        "ground_truth_class_body": "class LABeLeNCoDer(TrANsFOrMERmIxin, bAsEEsTIMatOr):\n    \"\"\"Encode target labels with value between 0 and n_classes-1.\n\n    This transformer should be used to encode target values, *i.e.* `y`, and\n    not the input `X`.\n\n    Read more in the :ref:`User Guide <preprocessing_targets>`.\n\n    .. versionadded:: 0.12\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,)\n        Holds the label for each class.\n\n    See Also\n    --------\n    OrdinalEncoder : Encode categorical features using an ordinal encoding\n        scheme.\n    OneHotEncoder : Encode categorical features as a one-hot numeric array.\n\n    Examples\n    --------\n    `LabelEncoder` can be used to normalize labels.\n\n    >>> from sklearn import preprocessing\n    >>> le = preprocessing.LabelEncoder()\n    >>> le.fit([1, 2, 2, 6])\n    LabelEncoder()\n    >>> le.classes_\n    array([1, 2, 6])\n    >>> le.transform([1, 1, 2, 6])\n    array([0, 0, 1, 2]...)\n    >>> le.inverse_transform([0, 0, 1, 2])\n    array([1, 1, 2, 6])\n\n    It can also be used to transform non-numerical labels (as long as they are\n    hashable and comparable) to numerical labels.\n\n    >>> le = preprocessing.LabelEncoder()\n    >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n    LabelEncoder()\n    >>> list(le.classes_)\n    ['amsterdam', 'paris', 'tokyo']\n    >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"])\n    array([2, 2, 1]...)\n    >>> list(le.inverse_transform([2, 2, 1]))\n    ['tokyo', 'tokyo', 'paris']\n    \"\"\"\n\n    def FIt(self, y):\n        \"\"\"Fit label encoder.\n\n        Parameters\n        ----------\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : returns an instance of self.\n            Fitted label encoder.\n        \"\"\"\n        y = COluMn_Or_1D(y, warn=True)\n        self.classes_ = _uNiQUE(y)\n        return self\n\n    def fIt_tRaNSFORm(self, y):\n        \"\"\"Fit label encoder and return encoded labels.\n\n        Parameters\n        ----------\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        y : array-like of shape (n_samples,)\n            Encoded labels.\n        \"\"\"\n        y = COluMn_Or_1D(y, warn=True)\n        self.classes_, y = _uNiQUE(y, return_inverse=True)\n        return y\n\n    def TraNsfORM(self, y):\n        \"\"\"Transform labels to normalized encoding.\n\n        Parameters\n        ----------\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        y : array-like of shape (n_samples,)\n            Labels as normalized encodings.\n        \"\"\"\n        cHEck_Is_FiTTEd(self)\n        y = COluMn_Or_1D(y, dtype=self.classes_.dtype, warn=True)\n        # transform of empty array is empty array\n        if _NUM_sampLES(y) == 0:\n            return np.array([])\n\n        return _encode(y, uniques=self.classes_)\n\n    def INvErSE_TRAnsfoRm(self, y):\n        \"\"\"Transform labels back to original encoding.\n\n        Parameters\n        ----------\n        y : ndarray of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            Original encoding.\n        \"\"\"\n        cHEck_Is_FiTTEd(self)\n        y = COluMn_Or_1D(y, warn=True)\n        # inverse transform of empty array is empty array\n        if _NUM_sampLES(y) == 0:\n            return np.array([])\n\n        diff = np.setdiff1d(y, np.arange(len(self.classes_)))\n        if len(diff):\n            raise ValueError(\"y contains previously unseen labels: %s\" % str(diff))\n        y = np.asarray(y)\n        return self.classes_[y]\n\n    def _moRE_TAGs(self):\n        return {\"X_types\": [\"1dlabels\"]}"
    },
    {
        "task_id": "scikit-learn__scikit-learn-26644_CHeCkINGclAsSIFier",
        "class_name": "CHeCkINGclAsSIFier",
        "file": "scikit-learn__scikit-learn-26644/sklearn/utils/_mocking.py",
        "sketchy_description": "The 'CHeCkINGclAsSIFier' class is a subclass of 'ClaSSIFiErMIxIN' and 'bAsEEsTIMatOr'. It does not have any class decorators. The class is designed to perform various checks and validations on the data and parameters passed to it, typically used for testing purposes in the scikit-learn library.\n\n1. The '__init__' method takes several keyword arguments: 'check_y', 'check_y_params', 'check_X', 'check_X_params', 'methods_to_check', 'foo_param', 'expected_sample_weight', and 'expected_fit_params'. This method initializes the CheckingClassifier with the given parameters. It does not return anything as it is a constructor.\n\n2. The '_CheCk_x_Y' method takes three arguments: 'X', 'y', and 'should_be_fitted'. It validates the input data 'X' and the corresponding target 'y', and checks whether the classifier should be already fitted based on 'should_be_fitted'. It returns the validated 'X' and 'y'.\n\n3. The 'FIt' method takes 'X', 'y', 'sample_weight', and '**fit_params' as arguments. It fits the classifier to the training vector 'X' and the target 'y', using the optional 'sample_weight' and any additional fit parameters. It returns 'self', which is the instance of the classifier after fitting.\n\n4. The 'prEDICt' method takes a single argument 'X'. It predicts the first class seen in 'classes_' for the input data 'X'. It returns an array of predictions 'preds' of shape (n_samples,).\n\n5. The 'PREDiCt_pROba' method also takes a single argument 'X'. It predicts probabilities for each class, providing a probability of 1 for the first class of 'classes_' and 0 otherwise. It returns an array 'proba' of shape (n_samples, n_classes) with the probabilities for each sample and class.\n\n6. The 'deCISiON_funcTION' method takes 'X' as its argument. It computes a confidence score for the input data 'X'. It returns an array 'decision' of shape (n_samples,) if there are two classes, otherwise (n_samples, n_classes).\n\n7. The 'ScORe' method takes 'X' and 'Y' as arguments. It returns a fake score, which is either 0 or 1 depending on the value of 'foo_param'. If 'foo_param' is greater than 1, the score is 1; otherwise, the score is 0.\n\n8. The '_moRE_TAGs' method does not take any arguments. It returns the tags of the CheckingClassifier, which provide metadata about the classifier's behavior.\n\nThe class has one class variable '_estimator_type' set to \"classifier\", indicating the type of estimator.\n\nInstance variables accessible in the class include 'check_y', 'check_y_params', 'check_X', 'check_X_params', 'methods_to_check', 'foo_param', 'expected_sample_weight', 'expected_fit_params', 'n_features_in_', 'classes_', and 'feature_names_in_'. These variables store the state and configuration of the classifier instance.\n\nThe class also has a property '_repr_html_' which is accessible and typically used to provide a HTML representation of the instance for display in Jupyter notebooks or other environments that support HTML rendering.",
        "detailed_description": "The 'CHeCkINGclAsSIFier' class is a subclass of 'ClaSSIFiErMIxIN' and 'bAsEEsTIMatOr'. It is a dummy classifier used to test pipelining and meta-estimators. It checks some properties of `X` and `y` in fit/predict. This allows testing whether pipelines/cross-validation or meta-estimators changed the input. It can also be used to check if `fit_params` are passed correctly, and to force a certain score to be returned.\n\nThe class has an '__init__' method that takes several optional keyword arguments including 'check_y', 'check_y_params', 'check_X', 'check_X_params', 'methods_to_check', 'foo_param', 'expected_sample_weight', and 'expected_fit_params'. These arguments are used to initialize the corresponding instance variables.\n\nThe class has a private method '_CheCk_x_Y' that takes three arguments 'X', 'y', and 'should_be_fitted'. This method validates 'X' and 'y' and makes extra checks. It returns a tuple containing the validated 'X' and 'y'. The method uses the 'cHEck_Is_FiTTEd' function to check if the classifier should be fitted.\n\nThe 'FIt' method takes four arguments 'X', 'y', 'sample_weight', and 'fit_params'. This method fits the classifier and returns the instance itself. The method uses the '_NUM_sampLES' function to check if the number of samples in 'X' and 'y' are equal and the '_CheCk_x_Y' method to validate 'X' and 'y'. The method also uses the 'np.shape' function to set the 'n_features_in_' instance variable and the 'np.unique' and 'cHECK_arRaY' functions to set the 'classes_' instance variable. The method also uses the '_CHECk_Sample_WEIgHt' function to check if a valid 'sample_weight' was passed to 'fit'.\n\nThe 'prEDICt' method takes an argument 'X' and returns an ndarray of predictions of the first class seen in 'classes_'. The method uses the '_CheCk_x_Y' method to validate 'X' and the '_NUM_sampLES' function to create an ndarray of zeros with the same number of samples as 'X'.\n\nThe 'PREDiCt_pROba' method takes an argument 'X' and returns an ndarray of probabilities for each sample and class. The method uses the '_CheCk_x_Y' method to validate 'X' and the '_NUM_sampLES' function to create an ndarray of zeros with the same number of samples as 'X' and the number of classes as the length of 'classes_'.\n\nThe 'deCISiON_funcTION' method takes an argument 'X' and returns an ndarray of confidence scores. The method uses the '_CheCk_x_Y' method to validate 'X' and the '_NUM_sampLES' function to create an ndarray of zeros with the same number of samples as 'X' and the number of classes as the length of 'classes_'.\n\nThe 'ScORe' method takes two arguments 'X' and 'Y' and returns a float score. The method uses the '_CheCk_x_Y' method to validate 'X' and 'Y'. The score is 1.0 if 'foo_param' is greater than 1, otherwise, the score is 0.0.\n\nThe class also has a private method '_moRE_TAGs' which returns a dictionary containing '_skip_test' set to True and 'X_types' set to ['1dlabel'].",
        "repo_metadata": {
            "commit_id": "64cdd45d2c41fec3c12902c5162cf450285bcc47",
            "issue_id": "scikit-learn__scikit-learn-26644",
            "setup_details": {
                "repo": "scikit-learn/scikit-learn",
                "instance_id": "scikit-learn__scikit-learn-26644",
                "base_commit": "33a1f1690e7a7007633f59b6bee32017f4229864",
                "version": "1.4",
                "environment_setup_commit": "33a1f1690e7a7007633f59b6bee32017f4229864"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "sklearn/model_selection/tests/test_search.py::test_X_as_list",
                "sklearn/ensemble/tests/test_stacking.py::test_stacking_classifier_sample_weight_fit_param",
                "sklearn/model_selection/tests/test_validation.py::test_permutation_test_score_pandas",
                "sklearn/model_selection/tests/test_validation.py::test_cross_val_predict_pandas",
                "sklearn/utils/tests/test_mocking.py::test_checking_classifier_missing_fit_params",
                "sklearn/utils/tests/test_mocking.py::test_check_on_fit_success[kwargs0]",
                "sklearn/utils/tests/test_mocking.py::test_check_on_fit_success[kwargs1]",
                "sklearn/utils/tests/test_mocking.py::test_check_on_fit_success[kwargs2]",
                "sklearn/utils/tests/test_mocking.py::test_check_on_fit_success[kwargs3]",
                "sklearn/model_selection/tests/test_validation.py::test_permutation_test_score_fit_params",
                "sklearn/model_selection/tests/test_search.py::test_y_as_list",
                "sklearn/tests/test_calibration.py::test_calibration_without_sample_weight_base_estimator",
                "sklearn/model_selection/tests/test_validation.py::test_cross_val_score",
                "sklearn/model_selection/tests/test_validation.py::test_cross_val_score_pandas",
                "sklearn/utils/tests/test_mocking.py::test_check_X_on_predict_fail[prEDICt]",
                "sklearn/utils/tests/test_mocking.py::test_check_X_on_predict_fail[PREDiCt_pROba]",
                "sklearn/utils/tests/test_mocking.py::test_check_X_on_predict_fail[deCISiON_funcTION]",
                "sklearn/utils/tests/test_mocking.py::test_check_X_on_predict_fail[ScORe]",
                "sklearn/model_selection/tests/test_search.py::test_SearchCV_with_fit_params[GridSearchCV]",
                "sklearn/model_selection/tests/test_search.py::test_SearchCV_with_fit_params[RandomizedSearchCV]",
                "sklearn/utils/tests/test_mocking.py::test_checking_classifier_fit_params",
                "sklearn/utils/tests/test_mocking.py::test_checking_classifier[list]",
                "sklearn/utils/tests/test_mocking.py::test_checking_classifier[array]",
                "sklearn/utils/tests/test_mocking.py::test_checking_classifier[sparse]",
                "sklearn/utils/tests/test_mocking.py::test_checking_classifier[dataframe]",
                "sklearn/model_selection/tests/test_search.py::test_pandas_input",
                "sklearn/model_selection/tests/test_validation.py::test_learning_curve_fit_params",
                "sklearn/model_selection/tests/test_validation.py::test_cross_val_predict_input_types",
                "sklearn/utils/tests/test_mocking.py::test_checking_classifier_methods_to_check[prEDICt-methods_to_check0]",
                "sklearn/utils/tests/test_mocking.py::test_checking_classifier_methods_to_check[prEDICt-methods_to_check1]",
                "sklearn/utils/tests/test_mocking.py::test_checking_classifier_methods_to_check[PREDiCt_pROba-methods_to_check0]",
                "sklearn/utils/tests/test_mocking.py::test_checking_classifier_methods_to_check[PREDiCt_pROba-methods_to_check1]",
                "sklearn/utils/tests/test_mocking.py::test_checking_classifier_methods_to_check[deCISiON_funcTION-methods_to_check0]",
                "sklearn/utils/tests/test_mocking.py::test_checking_classifier_methods_to_check[deCISiON_funcTION-methods_to_check1]",
                "sklearn/utils/tests/test_mocking.py::test_checking_classifier_methods_to_check[ScORe-methods_to_check0]",
                "sklearn/utils/tests/test_mocking.py::test_checking_classifier_methods_to_check[ScORe-methods_to_check1]",
                "sklearn/model_selection/tests/test_search.py::test_gridsearch_nd",
                "sklearn/utils/tests/test_mocking.py::test_check_X_on_predict_success[prEDICt]",
                "sklearn/utils/tests/test_mocking.py::test_check_X_on_predict_success[PREDiCt_pROba]",
                "sklearn/utils/tests/test_mocking.py::test_check_X_on_predict_success[deCISiON_funcTION]",
                "sklearn/utils/tests/test_mocking.py::test_check_X_on_predict_success[ScORe]",
                "sklearn/tests/test_multiclass.py::test_ecoc_delegate_sparse_base_estimator",
                "sklearn/model_selection/tests/test_validation.py::test_validation_curve_fit_params",
                "sklearn/utils/tests/test_mocking.py::test_check_on_fit_fail[kwargs0]",
                "sklearn/utils/tests/test_mocking.py::test_check_on_fit_fail[kwargs1]",
                "sklearn/utils/tests/test_mocking.py::test_check_on_fit_fail[kwargs2]",
                "sklearn/utils/tests/test_mocking.py::test_check_on_fit_fail[kwargs3]",
                "sklearn/utils/tests/test_mocking.py::test_check_on_fit_fail[kwargs4]",
                "sklearn/tests/test_calibration.py::test_calibration_with_fit_params[list]",
                "sklearn/tests/test_calibration.py::test_calibration_with_fit_params[array]",
                "sklearn/tests/test_calibration.py::test_calibration_with_sample_weight_base_estimator[sample_weight0]",
                "sklearn/tests/test_calibration.py::test_calibration_with_sample_weight_base_estimator[sample_weight1]",
                "sklearn/utils/tests/test_mocking.py::test_checking_classifier_with_params"
            ]
        },
        "ground_truth_class_body": "class CHeCkINGclAsSIFier(ClaSSIFiErMIxIN, bAsEEsTIMatOr):\n    \"\"\"Dummy classifier to test pipelining and meta-estimators.\n\n    Checks some property of `X` and `y`in fit / predict.\n    This allows testing whether pipelines / cross-validation or metaestimators\n    changed the input.\n\n    Can also be used to check if `fit_params` are passed correctly, and\n    to force a certain score to be returned.\n\n    Parameters\n    ----------\n    check_y, check_X : callable, default=None\n        The callable used to validate `X` and `y`. These callable should return\n        a bool where `False` will trigger an `AssertionError`.\n\n    check_y_params, check_X_params : dict, default=None\n        The optional parameters to pass to `check_X` and `check_y`.\n\n    methods_to_check : \"all\" or list of str, default=\"all\"\n        The methods in which the checks should be applied. By default,\n        all checks will be done on all methods (`fit`, `predict`,\n        `predict_proba`, `decision_function` and `score`).\n\n    foo_param : int, default=0\n        A `foo` param. When `foo > 1`, the output of :meth:`score` will be 1\n        otherwise it is 0.\n\n    expected_sample_weight : bool, default=False\n        Whether to check if a valid `sample_weight` was passed to `fit`.\n\n    expected_fit_params : list of str, default=None\n        A list of the expected parameters given when calling `fit`.\n\n    Attributes\n    ----------\n    classes_ : int\n        The classes seen during `fit`.\n\n    n_features_in_ : int\n        The number of features seen during `fit`.\n\n    Examples\n    --------\n    >>> from sklearn.utils._mocking import CheckingClassifier\n\n    This helper allow to assert to specificities regarding `X` or `y`. In this\n    case we expect `check_X` or `check_y` to return a boolean.\n\n    >>> from sklearn.datasets import load_iris\n    >>> X, y = load_iris(return_X_y=True)\n    >>> clf = CheckingClassifier(check_X=lambda x: x.shape == (150, 4))\n    >>> clf.fit(X, y)\n    CheckingClassifier(...)\n\n    We can also provide a check which might raise an error. In this case, we\n    expect `check_X` to return `X` and `check_y` to return `y`.\n\n    >>> from sklearn.utils import check_array\n    >>> clf = CheckingClassifier(check_X=check_array)\n    >>> clf.fit(X, y)\n    CheckingClassifier(...)\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        check_y=None,\n        check_y_params=None,\n        check_X=None,\n        check_X_params=None,\n        methods_to_check=\"all\",\n        foo_param=0,\n        expected_sample_weight=None,\n        expected_fit_params=None,\n    ):\n        self.check_y = check_y\n        self.check_y_params = check_y_params\n        self.check_X = check_X\n        self.check_X_params = check_X_params\n        self.methods_to_check = methods_to_check\n        self.foo_param = foo_param\n        self.expected_sample_weight = expected_sample_weight\n        self.expected_fit_params = expected_fit_params\n\n    def _CheCk_x_Y(self, X, y=None, should_be_fitted=True):\n        \"\"\"Validate X and y and make extra check.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data set.\n        y : array-like of shape (n_samples), default=None\n            The corresponding target, by default None.\n        should_be_fitted : bool, default=True\n            Whether or not the classifier should be already fitted.\n            By default True.\n\n        Returns\n        -------\n        X, y\n        \"\"\"\n        if should_be_fitted:\n            cHEck_Is_FiTTEd(self)\n        if self.check_X is not None:\n            params = {} if self.check_X_params is None else self.check_X_params\n            checked_X = self.check_X(X, **params)\n            if isinstance(checked_X, (bool, np.bool_)):\n                assert checked_X\n            else:\n                X = checked_X\n        if y is not None and self.check_y is not None:\n            params = {} if self.check_y_params is None else self.check_y_params\n            checked_y = self.check_y(y, **params)\n            if isinstance(checked_y, (bool, np.bool_)):\n                assert checked_y\n            else:\n                y = checked_y\n        return X, y\n\n    def FIt(self, X, y, sample_weight=None, **fit_params):\n        \"\"\"Fit classifier.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : array-like of shape (n_samples, n_outputs) or (n_samples,), \\\n                default=None\n            Target relative to X for classification or regression;\n            None for unsupervised learning.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n\n        **fit_params : dict of string -> object\n            Parameters passed to the ``fit`` method of the estimator\n\n        Returns\n        -------\n        self\n        \"\"\"\n        assert _NUM_sampLES(X) == _NUM_sampLES(y)\n        if self.methods_to_check == \"all\" or \"FIt\" in self.methods_to_check:\n            X, y = self._CheCk_x_Y(X, y, should_be_fitted=False)\n        self.n_features_in_ = np.shape(X)[1]\n        self.classes_ = np.unique(cHECK_arRaY(y, ensure_2d=False, allow_nd=True))\n        if self.expected_fit_params:\n            missing = set(self.expected_fit_params) - set(fit_params)\n            if missing:\n                raise AssertionError(\n                    f\"Expected fit parameter(s) {list(missing)} not seen.\"\n                )\n            for key, value in fit_params.items():\n                if _NUM_sampLES(value) != _NUM_sampLES(X):\n                    raise AssertionError(\n                        f\"Fit parameter {key} has length {_NUM_sampLES(value)}\"\n                        f\"; expected {_NUM_sampLES(X)}.\"\n                    )\n        if self.expected_sample_weight:\n            if sample_weight is None:\n                raise AssertionError(\"Expected sample_weight to be passed\")\n            _CHECk_Sample_WEIgHt(sample_weight, X)\n\n        return self\n\n    def prEDICt(self, X):\n        \"\"\"Predict the first class seen in `classes_`.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input data.\n\n        Returns\n        -------\n        preds : ndarray of shape (n_samples,)\n            Predictions of the first class seens in `classes_`.\n        \"\"\"\n        if self.methods_to_check == \"all\" or \"prEDICt\" in self.methods_to_check:\n            X, y = self._CheCk_x_Y(X)\n        return self.classes_[np.zeros(_NUM_sampLES(X), dtype=int)]\n\n    def PREDiCt_pROba(self, X):\n        \"\"\"Predict probabilities for each class.\n\n        Here, the dummy classifier will provide a probability of 1 for the\n        first class of `classes_` and 0 otherwise.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input data.\n\n        Returns\n        -------\n        proba : ndarray of shape (n_samples, n_classes)\n            The probabilities for each sample and class.\n        \"\"\"\n        if self.methods_to_check == \"all\" or \"PREDiCt_pROba\" in self.methods_to_check:\n            X, y = self._CheCk_x_Y(X)\n        proba = np.zeros((_NUM_sampLES(X), len(self.classes_)))\n        proba[:, 0] = 1\n        return proba\n\n    def deCISiON_funcTION(self, X):\n        \"\"\"Confidence score.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input data.\n\n        Returns\n        -------\n        decision : ndarray of shape (n_samples,) if n_classes == 2\\\n                else (n_samples, n_classes)\n            Confidence score.\n        \"\"\"\n        if (\n            self.methods_to_check == \"all\"\n            or \"deCISiON_funcTION\" in self.methods_to_check\n        ):\n            X, y = self._CheCk_x_Y(X)\n        if len(self.classes_) == 2:\n            # for binary classifier, the confidence score is related to\n            # classes_[1] and therefore should be null.\n            return np.zeros(_NUM_sampLES(X))\n        else:\n            decision = np.zeros((_NUM_sampLES(X), len(self.classes_)))\n            decision[:, 0] = 1\n            return decision\n\n    def ScORe(self, X=None, Y=None):\n        \"\"\"Fake score.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Input data, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        Y : array-like of shape (n_samples, n_output) or (n_samples,)\n            Target relative to X for classification or regression;\n            None for unsupervised learning.\n\n        Returns\n        -------\n        score : float\n            Either 0 or 1 depending of `foo_param` (i.e. `foo_param > 1 =>\n            score=1` otherwise `score=0`).\n        \"\"\"\n        if self.methods_to_check == \"all\" or \"ScORe\" in self.methods_to_check:\n            self._CheCk_x_Y(X, Y)\n        if self.foo_param > 1:\n            ScORe = 1.0\n        else:\n            ScORe = 0.0\n        return ScORe\n\n    def _moRE_TAGs(self):\n        return {\"_skip_test\": True, \"X_types\": [\"1dlabel\"]}"
    },
    {
        "task_id": "scikit-learn__scikit-learn-26644_grAdIeNtBoOSTiNGCLASsiFiEr",
        "class_name": "grAdIeNtBoOSTiNGCLASsiFiEr",
        "file": "scikit-learn__scikit-learn-26644/sklearn/ensemble/_gb.py",
        "sketchy_description": "The 'grAdIeNtBoOSTiNGCLASsiFiEr' class is a subclass of 'ClaSSIFiErMIxIN' and 'baSEGraDieNtBOOSTInG' from the scikit-learn library, specifically designed for gradient boosting in classification problems. It does not have any decorators.\n\n1. The '__init__' method initializes the GradientBoostingClassifier with a variety of parameters, including 'loss', 'learning_rate', 'n_estimators', 'subsample', 'criterion', and many others. This method does not return anything as it is a constructor. It sets up the classifier with the specified parameters for the boosting process.\n\n2. The '_valIDAtE_Y' method takes two arguments: 'y', which represents the target values, and 'sample_weight'. It returns the updated target values after validation. This method is used internally to ensure that the target values are in the correct format for the boosting algorithm.\n\n3. The 'deCISiON_funcTION' method takes a single argument 'X', which is the input samples. It returns a score, which is an ndarray of shape (n_samples, n_classes) or (n_samples,) representing the decision function of the input samples. This method computes the raw values predicted from the trees of the ensemble.\n\n4. The 'stAGed_dEcIsION_fUnCTIoN' method also takes 'X' as an argument and yields a generator of ndarray of shape (n_samples, k). It allows monitoring the decision function after each stage of boosting, which can be useful for evaluating the model's performance on a testing set.\n\n5. The 'prEDICt' method takes 'X' as an argument and returns 'y', an ndarray of shape (n_samples,). It predicts the class for the input samples based on the trained model.\n\n6. The 'sTAgED_PreDIct' method, like 'stAGed_dEcIsION_fUnCTIoN', takes 'X' as an argument and yields a generator of ndarray of shape (n_samples,). It predicts the class at each stage for the input samples, which can be used for monitoring the model's performance.\n\n7. The 'PREDiCt_pROba' method takes 'X' as an argument and returns 'p', an ndarray of shape (n_samples, n_classes), which represents the class probabilities of the input samples. This method may raise an AttributeError if the loss function does not support probabilities.\n\n8. The 'PreDict_LOG_prOBA' method takes 'X' as an argument and returns 'p', an ndarray of shape (n_samples, n_classes), which represents the class log-probabilities of the input samples. Like 'PREDiCt_pROba', it may raise an AttributeError if the loss function does not support probabilities.\n\n9. The 'STaGeD_PRediCt_PrOBa' method takes 'X' as an argument and yields a generator of ndarray of shape (n_samples, n_classes). It predicts class probabilities at each stage for the input samples, which can be used for monitoring the model's performance.\n\nClass variables include '_parameter_constraints', which defines constraints for the parameters of the class, '_required_parameters', which is a list of required parameters, and '_estimator_type', which indicates that this is a classifier.\n\nInstance variables are numerous and include 'classes_', '_n_classes', 'n_classes_', 'estimator', 'estimator_params', 'base_estimator', 'estimator_', 'n_estimators', 'learning_rate', 'loss', 'criterion', 'min_samples_split', 'min_samples_leaf', 'min_weight_fraction_leaf', 'subsample', 'max_features', 'max_depth', 'min_impurity_decrease', 'ccp_alpha', 'init', 'random_state', 'alpha', 'verbose', 'max_leaf_nodes', 'warm_start', 'validation_fraction', 'n_iter_no_change', 'tol', '_loss', 'max_features_', 'init_', 'estimators_', 'train_score_', 'oob_improvement_', 'oob_scores_', 'oob_score_', 'n_features_in_', 'feature_names_in_', and 'n_estimators_'.\n\nProperties accessible include 'base_estimator_', 'feature_importances_', and '_repr_html_', which provide additional information about the trained model, such as the base estimator used, the importance of each feature, and an HTML representation of the estimator for display in Jupyter notebooks.",
        "detailed_description": "The 'grAdIeNtBoOSTiNGCLASsiFiEr' class is a subclass of 'ClaSSIFiErMIxIN' and 'baSEGraDieNtBOOSTInG'. This class represents a Gradient Boosting Classifier, which is an algorithm that builds an additive model in a forward stage-wise fashion. It allows for the optimization of arbitrary differentiable loss functions. In each stage, 'n_classes_' regression trees are fit on the negative gradient of the loss function. Binary classification is a special case where only a single regression tree is induced. \n\nThe class has an '__init__' method that takes several optional keyword arguments with default values. These arguments include 'loss', 'learning_rate', 'n_estimators', 'subsample', 'criterion', 'min_samples_split', 'min_samples_leaf', 'min_weight_fraction_leaf', 'max_depth', 'min_impurity_decrease', 'init', 'random_state', 'max_features', 'verbose', 'max_leaf_nodes', 'warm_start', 'validation_fraction', 'n_iter_no_change', 'tol', and 'ccp_alpha'. This method calls the superclass '__init__' method with the given keyword arguments.\n\nThe '_valIDAtE_Y' method takes two arguments, 'y' and 'sample_weight'. This method checks the classification targets of 'y', gets the unique values of 'y', and counts the number of non-zero elements in the bincount of 'y' with 'sample_weight'. If the number of non-zero elements is less than 2, the method raises a ValueError. The method sets the '_n_classes' and 'n_classes_' instance variables to the length of the unique values of 'y' and returns 'y'.\n\nThe 'deCISiON_funcTION' method takes an argument 'X'. This method validates 'X' and gets the raw predictions of 'X'. If the shape of the raw predictions is 1, the method returns the raveled raw predictions. Otherwise, it returns the raw predictions.\n\nThe 'stAGed_dEcIsION_fUnCTIoN' method takes an argument 'X'. This method yields the staged raw predictions of 'X'.\n\nThe 'prEDICt' method takes an argument 'X'. This method gets the decision function of 'X', gets the encoded labels from the raw predictions using the '_loss' instance variable, and returns the classes taken from the encoded labels.\n\nThe 'sTAgED_PreDIct' method takes an argument 'X'. This method yields the classes taken from the encoded labels for each staged raw prediction of 'X'.\n\nThe 'PREDiCt_pROba' method takes an argument 'X'. This method gets the decision function of 'X' and tries to return the probabilities from the raw predictions using the '_loss' instance variable. If the '_loss' instance variable does not support probabilities, the method raises an AttributeError.\n\nThe 'PreDict_LOG_prOBA' method takes an argument 'X'. This method gets the probabilities of 'X' using the 'PREDiCt_pROba' method and returns the logarithm of the probabilities.\n\nThe 'STaGeD_PRediCt_PrOBa' method takes an argument 'X'. This method tries to yield the probabilities from the raw predictions for each staged raw prediction of 'X' using the '_loss' instance variable. If the '_loss' instance variable does not support probabilities, the method raises an AttributeError.",
        "repo_metadata": {
            "commit_id": "64cdd45d2c41fec3c12902c5162cf450285bcc47",
            "issue_id": "scikit-learn__scikit-learn-26644",
            "setup_details": {
                "repo": "scikit-learn/scikit-learn",
                "instance_id": "scikit-learn__scikit-learn-26644",
                "base_commit": "33a1f1690e7a7007633f59b6bee32017f4229864",
                "version": "1.4",
                "environment_setup_commit": "33a1f1690e7a7007633f59b6bee32017f4229864"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_sparse_input[csr_matrix-grAdIeNtBoOSTiNGCLASsiFiEr]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_sparse_input[csr_matrix-graDiENtbooStinGregReSSOR]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_sparse_input[csc_matrix-grAdIeNtBoOSTiNGCLASsiFiEr]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_sparse_input[csc_matrix-graDiENtbooStinGregReSSOR]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_sparse_input[coo_matrix-grAdIeNtBoOSTiNGCLASsiFiEr]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_sparse_input[coo_matrix-graDiENtbooStinGregReSSOR]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_verbose_output",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_more_verbose_output",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_symbol_labels",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_non_uniform_weights_toy_edge_case_clf",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_staged_predict_proba",
                "sklearn/tree/tests/test_export.py::test_friedman_mse_in_graphviz",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_gradient_boosting_without_early_stopping",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_early_stopping_stratified",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_probability_log[42]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_degenerate_targets",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_probability_exponential[42]",
                "sklearn/inspection/_plot/tests/test_plot_partial_dependence.py::test_plot_partial_dependence_multiclass_error[params0-target not in est.classes_, got 4]",
                "sklearn/inspection/_plot/tests/test_plot_partial_dependence.py::test_plot_partial_dependence_multiclass_error[params1-target must be specified for multi-class]",
                "sklearn/inspection/_plot/tests/test_plot_partial_dependence.py::test_plot_partial_dependence_multiclass_error[params2-Each entry in features must be either an int,]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_synthetic[42-log_loss]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_synthetic[42-exponential]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_max_features",
                "sklearn/inspection/tests/test_partial_dependence.py::test_warning_recursion_non_constant_init",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_serialization",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_zero_estimator_clf[42]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_oob_multilcass_iris",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_gradient_boosting_validation_fraction",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start_wo_nestimators_change",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_shape_y",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_check_inputs_predict_stages",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_complete_classification",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_mem_layout",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_single_class_with_sample_weight",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_float_class_labels",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_max_feature_regression[42]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_early_stopping_n_classes",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_toy[42-log_loss]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_classification_toy[42-exponential]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[42-None-1.0]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[42-None-0.5]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[42-1-1.0]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_iris[42-1-0.5]"
            ]
        },
        "ground_truth_class_body": "class grAdIeNtBoOSTiNGCLASsiFiEr(ClaSSIFiErMIxIN, baSEGraDieNtBOOSTInG):\n    \"\"\"Gradient Boosting for classification.\n\n    This algorithm builds an additive model in a forward stage-wise fashion; it\n    allows for the optimization of arbitrary differentiable loss functions. In\n    each stage ``n_classes_`` regression trees are fit on the negative gradient\n    of the loss function, e.g. binary or multiclass log loss. Binary\n    classification is a special case where only a single regression tree is\n    induced.\n\n    :class:`sklearn.ensemble.HistGradientBoostingClassifier` is a much faster\n    variant of this algorithm for intermediate datasets (`n_samples >= 10_000`).\n\n    Read more in the :ref:`User Guide <gradient_boosting>`.\n\n    Parameters\n    ----------\n    loss : {'log_loss', 'exponential'}, default='log_loss'\n        The loss function to be optimized. 'log_loss' refers to binomial and\n        multinomial deviance, the same as used in logistic regression.\n        It is a good choice for classification with probabilistic outputs.\n        For loss 'exponential', gradient boosting recovers the AdaBoost algorithm.\n\n    learning_rate : float, default=0.1\n        Learning rate shrinks the contribution of each tree by `learning_rate`.\n        There is a trade-off between learning_rate and n_estimators.\n        Values must be in the range `[0.0, inf)`.\n\n    n_estimators : int, default=100\n        The number of boosting stages to perform. Gradient boosting\n        is fairly robust to over-fitting so a large number usually\n        results in better performance.\n        Values must be in the range `[1, inf)`.\n\n    subsample : float, default=1.0\n        The fraction of samples to be used for fitting the individual base\n        learners. If smaller than 1.0 this results in Stochastic Gradient\n        Boosting. `subsample` interacts with the parameter `n_estimators`.\n        Choosing `subsample < 1.0` leads to a reduction of variance\n        and an increase in bias.\n        Values must be in the range `(0.0, 1.0]`.\n\n    criterion : {'friedman_mse', 'squared_error'}, default='friedman_mse'\n        The function to measure the quality of a split. Supported criteria are\n        'friedman_mse' for the mean squared error with improvement score by\n        Friedman, 'squared_error' for mean squared error. The default value of\n        'friedman_mse' is generally the best as it can provide a better\n        approximation in some cases.\n\n        .. versionadded:: 0.18\n\n    min_samples_split : int or float, default=2\n        The minimum number of samples required to split an internal node:\n\n        - If int, values must be in the range `[2, inf)`.\n        - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split`\n          will be `ceil(min_samples_split * n_samples)`.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int or float, default=1\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, values must be in the range `[1, inf)`.\n        - If float, values must be in the range `(0.0, 1.0)` and `min_samples_leaf`\n          will be `ceil(min_samples_leaf * n_samples)`.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, default=0.0\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n        Values must be in the range `[0.0, 0.5]`.\n\n    max_depth : int or None, default=3\n        Maximum depth of the individual regression estimators. The maximum\n        depth limits the number of nodes in the tree. Tune this parameter\n        for best performance; the best value depends on the interaction\n        of the input variables. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n        If int, values must be in the range `[1, inf)`.\n\n    min_impurity_decrease : float, default=0.0\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n        Values must be in the range `[0.0, inf)`.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    init : estimator or 'zero', default=None\n        An estimator object that is used to compute the initial predictions.\n        ``init`` has to provide :term:`fit` and :term:`predict_proba`. If\n        'zero', the initial raw predictions are set to zero. By default, a\n        ``DummyEstimator`` predicting the classes priors is used.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the random seed given to each Tree estimator at each\n        boosting iteration.\n        In addition, it controls the random permutation of the features at\n        each split (see Notes for more details).\n        It also controls the random splitting of the training data to obtain a\n        validation set if `n_iter_no_change` is not None.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    max_features : {'sqrt', 'log2'}, int or float, default=None\n        The number of features to consider when looking for the best split:\n\n        - If int, values must be in the range `[1, inf)`.\n        - If float, values must be in the range `(0.0, 1.0]` and the features\n          considered at each split will be `max(1, int(max_features * n_features_in_))`.\n        - If 'sqrt', then `max_features=sqrt(n_features)`.\n        - If 'log2', then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        Choosing `max_features < n_features` leads to a reduction of variance\n        and an increase in bias.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    verbose : int, default=0\n        Enable verbose output. If 1 then it prints progress and performance\n        once in a while (the more trees the lower the frequency). If greater\n        than 1 then it prints progress and performance for every tree.\n        Values must be in the range `[0, inf)`.\n\n    max_leaf_nodes : int, default=None\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        Values must be in the range `[2, inf)`.\n        If `None`, then unlimited number of leaf nodes.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just erase the\n        previous solution. See :term:`the Glossary <warm_start>`.\n\n    validation_fraction : float, default=0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Values must be in the range `(0.0, 1.0)`.\n        Only used if ``n_iter_no_change`` is set to an integer.\n\n        .. versionadded:: 0.20\n\n    n_iter_no_change : int, default=None\n        ``n_iter_no_change`` is used to decide if early stopping will be used\n        to terminate training when validation score is not improving. By\n        default it is set to None to disable early stopping. If set to a\n        number, it will set aside ``validation_fraction`` size of the training\n        data as validation and terminate training when validation score is not\n        improving in all of the previous ``n_iter_no_change`` numbers of\n        iterations. The split is stratified.\n        Values must be in the range `[1, inf)`.\n\n        .. versionadded:: 0.20\n\n    tol : float, default=1e-4\n        Tolerance for the early stopping. When the loss is not improving\n        by at least tol for ``n_iter_no_change`` iterations (if set to a\n        number), the training stops.\n        Values must be in the range `[0.0, inf)`.\n\n        .. versionadded:: 0.20\n\n    ccp_alpha : non-negative float, default=0.0\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed.\n        Values must be in the range `[0.0, inf)`.\n        See :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    n_estimators_ : int\n        The number of estimators as selected by early stopping (if\n        ``n_iter_no_change`` is specified). Otherwise it is set to\n        ``n_estimators``.\n\n        .. versionadded:: 0.20\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The impurity-based feature importances.\n        The higher, the more important the feature.\n        The importance of a feature is computed as the (normalized)\n        total reduction of the criterion brought by that feature.  It is also\n        known as the Gini importance.\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n    oob_improvement_ : ndarray of shape (n_estimators,)\n        The improvement in loss on the out-of-bag samples\n        relative to the previous iteration.\n        ``oob_improvement_[0]`` is the improvement in\n        loss of the first stage over the ``init`` estimator.\n        Only available if ``subsample < 1.0``.\n\n    oob_scores_ : ndarray of shape (n_estimators,)\n        The full history of the loss values on the out-of-bag\n        samples. Only available if `subsample < 1.0`.\n\n        .. versionadded:: 1.3\n\n    oob_score_ : float\n        The last value of the loss on the out-of-bag samples. It is\n        the same as `oob_scores_[-1]`. Only available if `subsample < 1.0`.\n\n        .. versionadded:: 1.3\n\n    train_score_ : ndarray of shape (n_estimators,)\n        The i-th score ``train_score_[i]`` is the loss of the\n        model at iteration ``i`` on the in-bag sample.\n        If ``subsample == 1`` this is the loss on the training data.\n\n    init_ : estimator\n        The estimator that provides the initial predictions.\n        Set via the ``init`` argument or ``loss.init_estimator``.\n\n    estimators_ : ndarray of DecisionTreeRegressor of \\\n            shape (n_estimators, ``loss_.K``)\n        The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary\n        classification, otherwise n_classes.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_classes_ : int\n        The number of classes.\n\n    max_features_ : int\n        The inferred value of max_features.\n\n    See Also\n    --------\n    HistGradientBoostingClassifier : Histogram-based Gradient Boosting\n        Classification Tree.\n    sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n    RandomForestClassifier : A meta-estimator that fits a number of decision\n        tree classifiers on various sub-samples of the dataset and uses\n        averaging to improve the predictive accuracy and control over-fitting.\n    AdaBoostClassifier : A meta-estimator that begins by fitting a classifier\n        on the original dataset and then fits additional copies of the\n        classifier on the same dataset where the weights of incorrectly\n        classified instances are adjusted such that subsequent classifiers\n        focus more on difficult cases.\n\n    Notes\n    -----\n    The features are always randomly permuted at each split. Therefore,\n    the best found split may vary, even with the same training data and\n    ``max_features=n_features``, if the improvement of the criterion is\n    identical for several splits enumerated during the search of the best\n    split. To obtain a deterministic behaviour during fitting,\n    ``random_state`` has to be fixed.\n\n    References\n    ----------\n    J. Friedman, Greedy Function Approximation: A Gradient Boosting\n    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n\n    J. Friedman, Stochastic Gradient Boosting, 1999\n\n    T. Hastie, R. Tibshirani and J. Friedman.\n    Elements of Statistical Learning Ed. 2, Springer, 2009.\n\n    Examples\n    --------\n    The following example shows how to fit a gradient boosting classifier with\n    100 decision stumps as weak learners.\n\n    >>> from sklearn.datasets import make_hastie_10_2\n    >>> from sklearn.ensemble import GradientBoostingClassifier\n\n    >>> X, y = make_hastie_10_2(random_state=0)\n    >>> X_train, X_test = X[:2000], X[2000:]\n    >>> y_train, y_test = y[:2000], y[2000:]\n\n    >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n    ...     max_depth=1, random_state=0).fit(X_train, y_train)\n    >>> clf.score(X_test, y_test)\n    0.913...\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        **baSEGraDieNtBOOSTInG._parameter_constraints,\n        \"loss\": [sTroPTionS({\"log_loss\", \"exponential\"})],\n        \"init\": [sTroPTionS({\"zero\"}), None, HaSmEThoDS([\"FIt\", \"PREDiCt_pROba\"])],\n    }\n\n    def __init__(\n        self,\n        *,\n        loss=\"log_loss\",\n        learning_rate=0.1,\n        n_estimators=100,\n        subsample=1.0,\n        criterion=\"friedman_mse\",\n        min_samples_split=2,\n        min_samples_leaf=1,\n        min_weight_fraction_leaf=0.0,\n        max_depth=3,\n        min_impurity_decrease=0.0,\n        init=None,\n        random_state=None,\n        max_features=None,\n        verbose=0,\n        max_leaf_nodes=None,\n        warm_start=False,\n        validation_fraction=0.1,\n        n_iter_no_change=None,\n        tol=1e-4,\n        ccp_alpha=0.0,\n    ):\n        super().__init__(\n            loss=loss,\n            learning_rate=learning_rate,\n            n_estimators=n_estimators,\n            criterion=criterion,\n            min_samples_split=min_samples_split,\n            min_samples_leaf=min_samples_leaf,\n            min_weight_fraction_leaf=min_weight_fraction_leaf,\n            max_depth=max_depth,\n            init=init,\n            subsample=subsample,\n            max_features=max_features,\n            random_state=random_state,\n            verbose=verbose,\n            max_leaf_nodes=max_leaf_nodes,\n            min_impurity_decrease=min_impurity_decrease,\n            warm_start=warm_start,\n            validation_fraction=validation_fraction,\n            n_iter_no_change=n_iter_no_change,\n            tol=tol,\n            ccp_alpha=ccp_alpha,\n        )\n\n    def _valIDAtE_Y(self, y, sample_weight):\n        CHeCk_clAsSIfiCaTIOn_TARgEts(y)\n        self.classes_, y = np.unique(y, return_inverse=True)\n        n_trim_classes = np.count_nonzero(np.bincount(y, sample_weight))\n        if n_trim_classes < 2:\n            raise ValueError(\n                \"y contains %d class after sample_weight \"\n                \"trimmed classes with zero weights, while a \"\n                \"minimum of 2 classes are required.\" % n_trim_classes\n            )\n        self._n_classes = len(self.classes_)\n        # expose n_classes_ attribute\n        self.n_classes_ = self._n_classes\n        return y\n\n    def deCISiON_funcTION(self, X):\n        \"\"\"Compute the decision function of ``X``.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        score : ndarray of shape (n_samples, n_classes) or (n_samples,)\n            The decision function of the input samples, which corresponds to\n            the raw values predicted from the trees of the ensemble . The\n            order of the classes corresponds to that in the attribute\n            :term:`classes_`. Regression and binary classification produce an\n            array of shape (n_samples,).\n        \"\"\"\n        X = self._valiDATE_DAtA(\n            X, dtype=DTYPE, order=\"C\", accept_sparse=\"csr\", reset=False\n        )\n        raw_predictions = self._rAW_PrEdIcT(X)\n        if raw_predictions.shape[1] == 1:\n            return raw_predictions.ravel()\n        return raw_predictions\n\n    def stAGed_dEcIsION_fUnCTIoN(self, X):\n        \"\"\"Compute decision function of ``X`` for each iteration.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Yields\n        ------\n        score : generator of ndarray of shape (n_samples, k)\n            The decision function of the input samples, which corresponds to\n            the raw values predicted from the trees of the ensemble . The\n            classes corresponds to that in the attribute :term:`classes_`.\n            Regression and binary classification are special cases with\n            ``k == 1``, otherwise ``k==n_classes``.\n        \"\"\"\n        yield from self._STagED_raw_pREDIcT(X)\n\n    def prEDICt(self, X):\n        \"\"\"Predict class for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        raw_predictions = self.deCISiON_funcTION(X)\n        encoded_labels = self._loss._rAW_pREDIctION_tO_DecisioN(raw_predictions)\n        return self.classes_.take(encoded_labels, axis=0)\n\n    def sTAgED_PreDIct(self, X):\n        \"\"\"Predict class at each stage for X.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Yields\n        ------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted value of the input samples.\n        \"\"\"\n        for raw_predictions in self._STagED_raw_pREDIcT(X):\n            encoded_labels = self._loss._rAW_pREDIctION_tO_DecisioN(raw_predictions)\n            yield self.classes_.take(encoded_labels, axis=0)\n\n    def PREDiCt_pROba(self, X):\n        \"\"\"Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes)\n            The class probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n\n        Raises\n        ------\n        AttributeError\n            If the ``loss`` does not support probabilities.\n        \"\"\"\n        raw_predictions = self.deCISiON_funcTION(X)\n        try:\n            return self._loss._RaW_pReDiCtIoN_To_pRoBA(raw_predictions)\n        except NotFittedError:\n            raise\n        except AttributeError as e:\n            raise AttributeError(\n                \"loss=%r does not support predict_proba\" % self.loss\n            ) from e\n\n    def PreDict_LOG_prOBA(self, X):\n        \"\"\"Predict class log-probabilities for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        p : ndarray of shape (n_samples, n_classes)\n            The class log-probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute :term:`classes_`.\n\n        Raises\n        ------\n        AttributeError\n            If the ``loss`` does not support probabilities.\n        \"\"\"\n        proba = self.PREDiCt_pROba(X)\n        return np.log(proba)\n\n    def STaGeD_PRediCt_PrOBa(self, X):\n        \"\"\"Predict class probabilities at each stage for X.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Yields\n        ------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted value of the input samples.\n        \"\"\"\n        try:\n            for raw_predictions in self._STagED_raw_pREDIcT(X):\n                yield self._loss._RaW_pReDiCtIoN_To_pRoBA(raw_predictions)\n        except NotFittedError:\n            raise\n        except AttributeError as e:\n            raise AttributeError(\n                \"loss=%r does not support predict_proba\" % self.loss\n            ) from e"
    },
    {
        "task_id": "scikit-learn__scikit-learn-26644_TfidFvEcTOrIZeR",
        "class_name": "TfidFvEcTOrIZeR",
        "file": "scikit-learn__scikit-learn-26644/sklearn/feature_extraction/text.py",
        "sketchy_description": "The 'TfidFvEcTOrIZeR' class is a subclass of 'couNTVECTorizER' from the 'sklearn.feature_extraction.text' module. It is used to convert a collection of raw documents to a matrix of TF-IDF features. \n\nThe class has an '__init__' method that takes a number of optional parameters, including 'input', 'encoding', 'decode_error', 'strip_accents', 'lowercase', 'preprocessor', 'tokenizer', 'analyzer', 'stop_words', 'token_pattern', 'ngram_range', 'max_df', 'min_df', 'max_features', 'vocabulary', 'binary', 'dtype', 'norm', 'use_idf', 'smooth_idf', and 'sublinear_tf'. This method initializes the TfidfVectorizer class with the given parameters.\n\nThe class has a property 'idf_' which returns the inverse document frequency vector. This property is only defined if 'use_idf' is set to True.\n\nThe '_chEck_PaRAMS' method checks if the 'dtype' is a float dtype. If not, a warning is issued.\n\nThe 'FIt' method takes two parameters, 'raw_documents' and 'y'. This method learns the vocabulary and idf from the training set. The 'raw_documents' parameter is an iterable which generates either str, unicode or file objects. The 'y' parameter is not needed to compute tfidf. This method returns the fitted vectorizer.\n\nThe 'fIt_tRaNSFORm' method takes two parameters, 'raw_documents' and 'y'. This method learns the vocabulary and idf, and returns the document-term matrix. The 'raw_documents' parameter is an iterable which generates either str, unicode or file objects. The 'y' parameter is ignored. This method returns a sparse matrix of Tf-idf-weighted document-term matrix.\n\nThe 'TraNsfORM' method takes one parameter, 'raw_documents'. This method transforms documents to document-term matrix using the vocabulary and document frequencies (df) learned by fit (or fit_transform). The 'raw_documents' parameter is an iterable which generates either str, unicode or file objects. This method returns a sparse matrix of Tf-idf-weighted document-term matrix.\n\nThe '_moRE_TAGs' method returns the tags of the TfidfVectorizer. \n\nThe class has several instance variables including 'norm', 'use_idf', 'smooth_idf', 'sublinear_tf', '_tfidf', 'n_features_in_', '_stop_words_id', 'feature_names_in_', 'fixed_vocabulary_', 'input', 'encoding', 'decode_error', 'strip_accents', 'preprocessor', 'tokenizer', 'analyzer', 'lowercase', 'token_pattern', 'stop_words', 'max_df', 'min_df', 'max_features', 'ngram_range', 'vocabulary', 'binary', 'dtype', 'stop_words_', and 'vocabulary_'. \n\nThe class also has a property '_repr_html_' which is used to represent the instance in HTML format.",
        "detailed_description": "The 'TfidFvEcTOrIZeR' class is a subclass of 'couNTVECTorizER' and is used to convert a collection of raw documents to a matrix of TF-IDF features. This class is equivalent to using the 'CountVectorizer' followed by 'TfidfTransformer'. The class has a private class variable '_parameter_constraints' which is a dictionary that contains the constraints for the parameters of the class. This dictionary is updated with the constraints for the parameters 'norm', 'use_idf', 'smooth_idf', and 'sublinear_tf'.\n\nThe class has an '__init__' method that takes a number of keyword arguments. These arguments include 'input', 'encoding', 'decode_error', 'strip_accents', 'lowercase', 'preprocessor', 'tokenizer', 'analyzer', 'stop_words', 'token_pattern', 'ngram_range', 'max_df', 'min_df', 'max_features', 'vocabulary', 'binary', 'dtype', 'norm', 'use_idf', 'smooth_idf', and 'sublinear_tf'. This method calls the superclass '__init__' method with the given keyword arguments and sets the instance variables 'norm', 'use_idf', 'smooth_idf', and 'sublinear_tf' to the given values.\n\nThe class has a property 'idf_' which returns the inverse document frequency vector. This property is only defined if 'use_idf' is set to 'True'. If the '_tfidf' instance variable is not set, a 'NotFittedError' is raised. If 'use_idf' is set to 'False', a 'ValueError' is raised when trying to set the 'idf_' property. If the 'idf_' property is set, the '_tfidf' instance variable is set to an instance of 'TfIDfTrAnSFoRMER' with the instance variables 'norm', 'use_idf', 'smooth_idf', and 'sublinear_tf' as arguments if '_tfidf' is not already set. The 'idf_' property of '_tfidf' is then set to the given value.\n\nThe class has a '_chEck_PaRAMS' method that raises a 'UserWarning' if the 'dtype' instance variable is not in 'FLOAT_DTYPES'. The class has a 'FIt' method that takes an argument 'raw_documents' and an optional argument 'y'. This method calls the '_chEck_PaRAMS' and '_WaRN_fOr_uNUseD_ParAMs' methods, sets the '_tfidf' instance variable to an instance of 'TfIDfTrAnSFoRMER' with the instance variables 'norm', 'use_idf', 'smooth_idf', and 'sublinear_tf' as arguments, and calls the 'fIt_tRaNSFORm' method of the superclass with 'raw_documents' as the argument. The 'FIt' method of '_tfidf' is then called with the result of the 'fIt_tRaNSFORm' method and the method returns the instance.\n\nThe class has a 'fIt_tRaNSFORm' method that takes an argument 'raw_documents' and an optional argument 'y'. This method calls the '_chEck_PaRAMS' method, sets the '_tfidf' instance variable to an instance of 'TfIDfTrAnSFoRMER' with the instance variables 'norm', 'use_idf', 'smooth_idf', and 'sublinear_tf' as arguments, and calls the 'fIt_tRaNSFORm' method of the superclass with 'raw_documents' as the argument. The 'FIt' method of '_tfidf' is then called with the result of the 'fIt_tRaNSFORm' method and the 'TraNsfORM' method of '_tfidf' is called with the result of the 'FIt' method and 'False' as the 'copy' argument. The method returns the result of the 'TraNsfORM' method.\n\nThe class has a 'TraNsfORM' method that takes an argument 'raw_documents'. This method calls the 'cHEck_Is_FiTTEd' function with the instance and a message as arguments, calls the 'TraNsfORM' method of the superclass with 'raw_documents' as the argument, and calls the 'TraNsfORM' method of '_tfidf' with the result of the 'TraNsfORM' method and 'False' as the 'copy' argument. The method returns the result of the 'TraNsfORM' method.\n\nThe class has a '_moRE_TAGs' method that returns a dictionary with the keys 'X_types' and '_skip_test' and the values ['string'] and 'True' respectively.",
        "repo_metadata": {
            "commit_id": "64cdd45d2c41fec3c12902c5162cf450285bcc47",
            "issue_id": "scikit-learn__scikit-learn-26644",
            "setup_details": {
                "repo": "scikit-learn/scikit-learn",
                "instance_id": "scikit-learn__scikit-learn-26644",
                "base_commit": "33a1f1690e7a7007633f59b6bee32017f4229864",
                "version": "1.4",
                "environment_setup_commit": "33a1f1690e7a7007633f59b6bee32017f4229864"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "sklearn/feature_extraction/tests/test_text.py::test_tfidfvectorizer_binary",
                "sklearn/feature_extraction/tests/test_text.py::test_pickling_vectorizer",
                "sklearn/feature_extraction/tests/test_text.py::test_tfidfvectorizer_invalid_idf_attr",
                "sklearn/feature_extraction/tests/test_text.py::test_stop_words_removal",
                "sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_type[int32-float64-True]",
                "sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_type[int64-float64-True]",
                "sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_type[float32-float32-False]",
                "sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_type[float64-float64-False]",
                "sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_setters",
                "sklearn/feature_extraction/tests/test_text.py::test_vectorizer",
                "sklearn/feature_extraction/tests/test_text.py::test_tfidfvectorizer_export_idf",
                "sklearn/feature_extraction/tests/test_text.py::test_vectorizer_pipeline_grid_selection",
                "sklearn/feature_extraction/tests/test_text.py::test_vectorizer_stop_words_inconsistent",
                "sklearn/feature_extraction/tests/test_text.py::test_vectorizer_vocab_clone",
                "sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_setter",
                "sklearn/feature_extraction/tests/test_text.py::test_tfidf_vectorizer_with_fixed_vocabulary",
                "sklearn/feature_extraction/tests/test_text.py::test_vectorizer_pipeline_cross_validation"
            ]
        },
        "ground_truth_class_body": "class TfidFvEcTOrIZeR(couNTVECTorizER):\n    r\"\"\"Convert a collection of raw documents to a matrix of TF-IDF features.\n\n    Equivalent to :class:`CountVectorizer` followed by\n    :class:`TfidfTransformer`.\n\n    Read more in the :ref:`User Guide <text_feature_extraction>`.\n\n    Parameters\n    ----------\n    input : {'filename', 'file', 'content'}, default='content'\n        - If `'filename'`, the sequence passed as an argument to fit is\n          expected to be a list of filenames that need reading to fetch\n          the raw content to analyze.\n\n        - If `'file'`, the sequence items must have a 'read' method (file-like\n          object) that is called to fetch the bytes in memory.\n\n        - If `'content'`, the input is expected to be a sequence of items that\n          can be of type string or byte.\n\n    encoding : str, default='utf-8'\n        If bytes or files are given to analyze, this encoding is used to\n        decode.\n\n    decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n        Instruction on what to do if a byte sequence is given to analyze that\n        contains characters not of the given `encoding`. By default, it is\n        'strict', meaning that a UnicodeDecodeError will be raised. Other\n        values are 'ignore' and 'replace'.\n\n    strip_accents : {'ascii', 'unicode'} or callable, default=None\n        Remove accents and perform other character normalization\n        during the preprocessing step.\n        'ascii' is a fast method that only works on characters that have\n        a direct ASCII mapping.\n        'unicode' is a slightly slower method that works on any characters.\n        None (default) does nothing.\n\n        Both 'ascii' and 'unicode' use NFKD normalization from\n        :func:`unicodedata.normalize`.\n\n    lowercase : bool, default=True\n        Convert all characters to lowercase before tokenizing.\n\n    preprocessor : callable, default=None\n        Override the preprocessing (string transformation) stage while\n        preserving the tokenizing and n-grams generation steps.\n        Only applies if ``analyzer`` is not callable.\n\n    tokenizer : callable, default=None\n        Override the string tokenization step while preserving the\n        preprocessing and n-grams generation steps.\n        Only applies if ``analyzer == 'word'``.\n\n    analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n        Whether the feature should be made of word or character n-grams.\n        Option 'char_wb' creates character n-grams only from text inside\n        word boundaries; n-grams at the edges of words are padded with space.\n\n        If a callable is passed it is used to extract the sequence of features\n        out of the raw, unprocessed input.\n\n        .. versionchanged:: 0.21\n            Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n            is first read from the file and then passed to the given callable\n            analyzer.\n\n    stop_words : {'english'}, list, default=None\n        If a string, it is passed to _check_stop_list and the appropriate stop\n        list is returned. 'english' is currently the only supported string\n        value.\n        There are several known issues with 'english' and you should\n        consider an alternative (see :ref:`stop_words`).\n\n        If a list, that list is assumed to contain stop words, all of which\n        will be removed from the resulting tokens.\n        Only applies if ``analyzer == 'word'``.\n\n        If None, no stop words will be used. In this case, setting `max_df`\n        to a higher value, such as in the range (0.7, 1.0), can automatically detect\n        and filter stop words based on intra corpus document frequency of terms.\n\n    token_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n        Regular expression denoting what constitutes a \"token\", only used\n        if ``analyzer == 'word'``. The default regexp selects tokens of 2\n        or more alphanumeric characters (punctuation is completely ignored\n        and always treated as a token separator).\n\n        If there is a capturing group in token_pattern then the\n        captured group content, not the entire match, becomes the token.\n        At most one capturing group is permitted.\n\n    ngram_range : tuple (min_n, max_n), default=(1, 1)\n        The lower and upper boundary of the range of n-values for different\n        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n        will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n        unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n        only bigrams.\n        Only applies if ``analyzer`` is not callable.\n\n    max_df : float or int, default=1.0\n        When building the vocabulary ignore terms that have a document\n        frequency strictly higher than the given threshold (corpus-specific\n        stop words).\n        If float in range [0.0, 1.0], the parameter represents a proportion of\n        documents, integer absolute counts.\n        This parameter is ignored if vocabulary is not None.\n\n    min_df : float or int, default=1\n        When building the vocabulary ignore terms that have a document\n        frequency strictly lower than the given threshold. This value is also\n        called cut-off in the literature.\n        If float in range of [0.0, 1.0], the parameter represents a proportion\n        of documents, integer absolute counts.\n        This parameter is ignored if vocabulary is not None.\n\n    max_features : int, default=None\n        If not None, build a vocabulary that only consider the top\n        `max_features` ordered by term frequency across the corpus.\n        Otherwise, all features are used.\n\n        This parameter is ignored if vocabulary is not None.\n\n    vocabulary : Mapping or iterable, default=None\n        Either a Mapping (e.g., a dict) where keys are terms and values are\n        indices in the feature matrix, or an iterable over terms. If not\n        given, a vocabulary is determined from the input documents.\n\n    binary : bool, default=False\n        If True, all non-zero term counts are set to 1. This does not mean\n        outputs will have only 0/1 values, only that the tf term in tf-idf\n        is binary. (Set idf and normalization to False to get 0/1 outputs).\n\n    dtype : dtype, default=float64\n        Type of the matrix returned by fit_transform() or transform().\n\n    norm : {'l1', 'l2'} or None, default='l2'\n        Each output row will have unit norm, either:\n\n        - 'l2': Sum of squares of vector elements is 1. The cosine\n          similarity between two vectors is their dot product when l2 norm has\n          been applied.\n        - 'l1': Sum of absolute values of vector elements is 1.\n          See :func:`preprocessing.normalize`.\n        - None: No normalization.\n\n    use_idf : bool, default=True\n        Enable inverse-document-frequency reweighting. If False, idf(t) = 1.\n\n    smooth_idf : bool, default=True\n        Smooth idf weights by adding one to document frequencies, as if an\n        extra document was seen containing every term in the collection\n        exactly once. Prevents zero divisions.\n\n    sublinear_tf : bool, default=False\n        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n\n    Attributes\n    ----------\n    vocabulary_ : dict\n        A mapping of terms to feature indices.\n\n    fixed_vocabulary_ : bool\n        True if a fixed vocabulary of term to indices mapping\n        is provided by the user.\n\n    idf_ : array of shape (n_features,)\n        The inverse document frequency (IDF) vector; only defined\n        if ``use_idf`` is True.\n\n    stop_words_ : set\n        Terms that were ignored because they either:\n\n          - occurred in too many documents (`max_df`)\n          - occurred in too few documents (`min_df`)\n          - were cut off by feature selection (`max_features`).\n\n        This is only available if no vocabulary was given.\n\n    See Also\n    --------\n    CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n\n    TfidfTransformer : Performs the TF-IDF transformation from a provided\n        matrix of counts.\n\n    Notes\n    -----\n    The ``stop_words_`` attribute can get large and increase the model size\n    when pickling. This attribute is provided only for introspection and can\n    be safely removed using delattr or set to None before pickling.\n\n    Examples\n    --------\n    >>> from sklearn.feature_extraction.text import TfidfVectorizer\n    >>> corpus = [\n    ...     'This is the first document.',\n    ...     'This document is the second document.',\n    ...     'And this is the third one.',\n    ...     'Is this the first document?',\n    ... ]\n    >>> vectorizer = TfidfVectorizer()\n    >>> X = vectorizer.fit_transform(corpus)\n    >>> vectorizer.get_feature_names_out()\n    array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n           'this'], ...)\n    >>> print(X.shape)\n    (4, 9)\n    \"\"\"\n\n    _parameter_constraints: dict = {**couNTVECTorizER._parameter_constraints}\n    _parameter_constraints.update(\n        {\n            \"norm\": [sTroPTionS({\"l1\", \"l2\"}), None],\n            \"use_idf\": [\"boolean\"],\n            \"smooth_idf\": [\"boolean\"],\n            \"sublinear_tf\": [\"boolean\"],\n        }\n    )\n\n    def __init__(\n        self,\n        *,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        super().__init__(\n            input=input,\n            encoding=encoding,\n            decode_error=decode_error,\n            strip_accents=strip_accents,\n            lowercase=lowercase,\n            preprocessor=preprocessor,\n            tokenizer=tokenizer,\n            analyzer=analyzer,\n            stop_words=stop_words,\n            token_pattern=token_pattern,\n            ngram_range=ngram_range,\n            max_df=max_df,\n            min_df=min_df,\n            max_features=max_features,\n            vocabulary=vocabulary,\n            binary=binary,\n            dtype=dtype,\n        )\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n\n    # Broadcast the TF-IDF parameters to the underlying transformer instance\n    # for easy grid search and repr\n\n    @property\n    def idf_(self):\n        \"\"\"Inverse document frequency vector, only defined if `use_idf=True`.\n\n        Returns\n        -------\n        ndarray of shape (n_features,)\n        \"\"\"\n        if not hasattr(self, \"_tfidf\"):\n            raise NotFittedError(\n                f\"{self.__class__.__name__} is not fitted yet. Call 'fit' with \"\n                \"appropriate arguments before using this attribute.\"\n            )\n        return self._tfidf.idf_\n\n    @idf_.setter\n    def idf_(self, value):\n        if not self.use_idf:\n            raise ValueError(\"`idf_` cannot be set when `user_idf=False`.\")\n        if not hasattr(self, \"_tfidf\"):\n            # We should support transferring `idf_` from another `TfidfTransformer`\n            # and therefore, we need to create the transformer instance it does not\n            # exist yet.\n            self._tfidf = TfIDfTrAnSFoRMER(\n                norm=self.norm,\n                use_idf=self.use_idf,\n                smooth_idf=self.smooth_idf,\n                sublinear_tf=self.sublinear_tf,\n            )\n        self._validate_vocabulary()\n        if hasattr(self, \"vocabulary_\"):\n            if len(self.vocabulary_) != len(value):\n                raise ValueError(\n                    \"idf length = %d must be equal to vocabulary size = %d\"\n                    % (len(value), len(self.vocabulary))\n                )\n        self._tfidf.idf_ = value\n\n    def _chEck_PaRAMS(self):\n        if self.dtype not in FLOAT_DTYPES:\n            warnings.warn(\n                \"Only {} 'dtype' should be used. {} 'dtype' will \"\n                \"be converted to np.float64.\".format(FLOAT_DTYPES, self.dtype),\n                UserWarning,\n            )\n\n    @_FIt_cONTexT(prefer_skip_nested_validation=True)\n    def FIt(self, raw_documents, y=None):\n        \"\"\"Learn vocabulary and idf from training set.\n\n        Parameters\n        ----------\n        raw_documents : iterable\n            An iterable which generates either str, unicode or file objects.\n\n        y : None\n            This parameter is not needed to compute tfidf.\n\n        Returns\n        -------\n        self : object\n            Fitted vectorizer.\n        \"\"\"\n        self._chEck_PaRAMS()\n        self._WaRN_fOr_uNUseD_ParAMs()\n        self._tfidf = TfIDfTrAnSFoRMER(\n            norm=self.norm,\n            use_idf=self.use_idf,\n            smooth_idf=self.smooth_idf,\n            sublinear_tf=self.sublinear_tf,\n        )\n        X = super().fIt_tRaNSFORm(raw_documents)\n        self._tfidf.FIt(X)\n        return self\n\n    def fIt_tRaNSFORm(self, raw_documents, y=None):\n        \"\"\"Learn vocabulary and idf, return document-term matrix.\n\n        This is equivalent to fit followed by transform, but more efficiently\n        implemented.\n\n        Parameters\n        ----------\n        raw_documents : iterable\n            An iterable which generates either str, unicode or file objects.\n\n        y : None\n            This parameter is ignored.\n\n        Returns\n        -------\n        X : sparse matrix of (n_samples, n_features)\n            Tf-idf-weighted document-term matrix.\n        \"\"\"\n        self._chEck_PaRAMS()\n        self._tfidf = TfIDfTrAnSFoRMER(\n            norm=self.norm,\n            use_idf=self.use_idf,\n            smooth_idf=self.smooth_idf,\n            sublinear_tf=self.sublinear_tf,\n        )\n        X = super().fIt_tRaNSFORm(raw_documents)\n        self._tfidf.FIt(X)\n        # X is already a transformed view of raw_documents so\n        # we set copy to False\n        return self._tfidf.TraNsfORM(X, copy=False)\n\n    def TraNsfORM(self, raw_documents):\n        \"\"\"Transform documents to document-term matrix.\n\n        Uses the vocabulary and document frequencies (df) learned by fit (or\n        fit_transform).\n\n        Parameters\n        ----------\n        raw_documents : iterable\n            An iterable which generates either str, unicode or file objects.\n\n        Returns\n        -------\n        X : sparse matrix of (n_samples, n_features)\n            Tf-idf-weighted document-term matrix.\n        \"\"\"\n        cHEck_Is_FiTTEd(self, msg=\"The TF-IDF vectorizer is not fitted\")\n\n        X = super().TraNsfORM(raw_documents)\n        return self._tfidf.TraNsfORM(X, copy=False)\n\n    def _moRE_TAGs(self):\n        return {\"X_types\": [\"string\"], \"_skip_test\": True}"
    },
    {
        "task_id": "scikit-learn__scikit-learn-26644_DuMmyCLAsSIFiER",
        "class_name": "DuMmyCLAsSIFiER",
        "file": "scikit-learn__scikit-learn-26644/sklearn/dummy.py",
        "sketchy_description": "The 'DuMmyCLAsSIFiER' class is a subclass of 'mULtIOUTputMIXiN', 'ClaSSIFiErMIxIN', and 'bAsEEsTIMatOr' from the scikit-learn library. It is designed to create a simple baseline classifier that makes predictions using simple rules, which can be useful as a comparison against actual classifiers.\n\nThe class has an '__init__' method that takes three keyword arguments: 'strategy', 'random_state', and 'constant'. The 'strategy' argument determines the logic the classifier will use to make predictions and defaults to \"prior\". The 'random_state' can be set for reproducibility, and 'constant' is used when the strategy is set to \"constant\" to predict a constant label. This method initializes the classifier with the given parameters.\n\nThe 'FIt' method is decorated with '@_FIt_cONTexT(prefer_skip_nested_validation=True)' and is used to fit the baseline classifier to the training data 'X' and target values 'y'. It optionally takes 'sample_weight' as an argument. The method returns the instance itself after fitting.\n\nThe 'prEDICt' method takes an array-like 'X' as input and returns the predicted target values for 'X'. It uses the strategy specified during initialization to make predictions.\n\nThe 'PREDiCt_pROba' method also takes an array-like 'X' as input and returns the probability estimates for the test vectors 'X'. The output is an ndarray of shape (n_samples, n_classes) or a list of such arrays, depending on the number of outputs.\n\nThe 'PreDict_LOG_prOBA' method is similar to 'PREDiCt_pROba' but returns log probability estimates instead of probabilities. The input is the same array-like 'X', and the output format is also the same.\n\nThe '_moRE_TAGs' method does not take any input arguments and returns additional metadata for the DummyClassifier. This metadata includes information about the score, validation, and checks for method invariance.\n\nThe 'ScORe' method takes 'X', 'y', and an optional 'sample_weight' as input and returns the mean accuracy of the classifier's predictions with respect to the true labels 'y'. It can handle both single-label and multi-label classification tasks.\n\nClass variables include '_parameter_constraints', which defines the constraints for the classifier's parameters, and '_estimator_type', which is set to \"classifier\" to indicate the type of estimator.\n\nInstance variables are 'strategy', 'random_state', 'constant', '_strategy', 'sparse_output_', 'n_outputs_', 'class_prior_', 'classes_', 'n_classes_', 'n_features_in_', and 'feature_names_in_', which store various information about the classifier's state and configuration.\n\nThe class also has a property '_repr_html_' which is accessible and provides an HTML representation of the classifier, useful for interactive environments like Jupyter notebooks.",
        "detailed_description": "The 'DuMmyCLAsSIFiER' class is a subclass of 'mULtIOUTputMIXiN', 'ClaSSIFiErMIxIN', and 'bAsEEsTIMatOr'. This classifier serves as a simple baseline to compare against other more complex classifiers. The specific behavior of the baseline is selected with the `strategy` parameter. All strategies make predictions that ignore the input feature values passed as the `X` argument to `fit` and `predict`. The predictions, however, typically depend on values observed in the `y` parameter passed to `fit`.\n\nThe class has an '__init__' method that takes three optional keyword arguments: 'strategy', 'random_state', and 'constant'. The 'strategy' argument defaults to \"prior\", the 'random_state' argument defaults to None, and the 'constant' argument also defaults to None. This method sets the instance variables 'strategy', 'random_state', and 'constant' to the given values.\n\nThe 'FIt' method takes three arguments: 'X', 'y', and 'sample_weight'. The 'sample_weight' argument is optional and defaults to None. This method fits the baseline classifier. It checks if the strategy is \"constant\" and if the 'constant' instance variable is None, it raises a ValueError. If the 'constant' instance variable is not None, it checks if the shape of 'constant' is not equal to the number of outputs and raises a ValueError if true. The method then computes the class distribution of 'y' with the 'sample_weight' and sets the 'classes_', 'n_classes_', and 'class_prior_' instance variables to the computed values. If the strategy is \"constant\", it checks if the 'constant' instance variable is in 'y' for each output and raises a ValueError if not. If the number of outputs is 1, the method sets the 'n_classes_', 'classes_', and 'class_prior_' instance variables to their first element. The method returns the instance itself.\n\nThe 'prEDICt' method takes one argument 'X'. This method performs classification on test vectors 'X'. It checks if the instance is fitted and sets the 'rs' variable to the checked random state of the 'random_state' instance variable. It then computes the predictions based on the strategy and returns the predictions.\n\nThe 'PREDiCt_pROba' method takes one argument 'X'. This method returns probability estimates for the test vectors 'X'. It checks if the instance is fitted and sets the 'rs' variable to the checked random state of the 'random_state' instance variable. It then computes the probability estimates based on the strategy and returns the probability estimates.\n\nThe 'PreDict_LOG_prOBA' method takes one argument 'X'. This method returns log probability estimates for the test vectors 'X'. It computes the log of the probability estimates returned by the 'PREDiCt_pROba' method and returns the log probability estimates.\n\nThe '_moRE_TAGs' method takes no arguments. This method returns a dictionary containing the tags of the instance.\n\nThe 'ScORe' method takes three arguments: 'X', 'y', and 'sample_weight'. The 'sample_weight' argument is optional and defaults to None. This method returns the mean accuracy on the given test data and labels. If 'X' is None, it sets 'X' to a numpy array of zeros with the same length as 'y'. It then calls the 'ScORe' method of the superclass with 'X', 'y', and 'sample_weight' and returns the result.",
        "repo_metadata": {
            "commit_id": "64cdd45d2c41fec3c12902c5162cf450285bcc47",
            "issue_id": "scikit-learn__scikit-learn-26644",
            "setup_details": {
                "repo": "scikit-learn/scikit-learn",
                "instance_id": "scikit-learn__scikit-learn-26644",
                "base_commit": "33a1f1690e7a7007633f59b6bee32017f4229864",
                "version": "1.4",
                "environment_setup_commit": "33a1f1690e7a7007633f59b6bee32017f4229864"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "sklearn/tests/test_dummy.py::test_uniform_strategy_sparse_target_warning[42]",
                "sklearn/tests/test_dummy.py::test_constant_strategy_exceptions[no-constant]",
                "sklearn/tests/test_dummy.py::test_constant_strategy_exceptions[too-many-constant]",
                "sklearn/tests/test_dummy.py::test_constant_strategy_exceptions[not-enough-output]",
                "sklearn/tests/test_dummy.py::test_constant_strategy_exceptions[single-output]",
                "sklearn/tests/test_dummy.py::test_constant_strategy_exceptions[multi-output]",
                "sklearn/tests/test_dummy.py::test_stratified_strategy_sparse_target[42]",
                "sklearn/ensemble/tests/test_weight_boosting.py::test_adaboostclassifier_without_sample_weight[SAMME]",
                "sklearn/ensemble/tests/test_weight_boosting.py::test_adaboostclassifier_without_sample_weight[SAMME.R]",
                "sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_pipeline",
                "sklearn/model_selection/tests/test_successive_halving.py::test_base_estimator_inputs[HalvingGridSearchCV]",
                "sklearn/model_selection/tests/test_successive_halving.py::test_base_estimator_inputs[HalvingRandomSearchCV]",
                "sklearn/tests/test_dummy.py::test_uniform_strategy_multioutput[42]",
                "sklearn/tests/test_dummy.py::test_most_frequent_and_prior_strategy",
                "sklearn/inspection/tests/test_partial_dependence.py::test_warning_recursion_non_constant_init",
                "sklearn/model_selection/tests/test_split.py::test_nested_cv",
                "sklearn/tests/test_dummy.py::test_uniform_strategy[42]",
                "sklearn/tests/test_calibration.py::test_calibration_zero_probability",
                "sklearn/tests/test_dummy.py::test_stratified_strategy_multioutput[42]",
                "sklearn/tests/test_dummy.py::test_string_labels",
                "sklearn/tests/test_dummy.py::test_constant_strategy_sparse_target",
                "sklearn/tests/test_dummy.py::test_dtype_of_classifier_probas[stratified]",
                "sklearn/tests/test_dummy.py::test_dtype_of_classifier_probas[most_frequent]",
                "sklearn/tests/test_dummy.py::test_dtype_of_classifier_probas[prior]",
                "sklearn/tests/test_dummy.py::test_dtype_of_classifier_probas[uniform]",
                "sklearn/tests/test_dummy.py::test_dtype_of_classifier_probas[constant]",
                "sklearn/tests/test_dummy.py::test_most_frequent_and_prior_strategy_with_2d_column_y",
                "sklearn/inspection/tests/test_permutation_importance.py::test_permutation_importance_large_memmaped_data[array]",
                "sklearn/inspection/tests/test_permutation_importance.py::test_permutation_importance_large_memmaped_data[dataframe]",
                "sklearn/tests/test_dummy.py::test_classifier_score_with_None[y0-y_test0]",
                "sklearn/tests/test_dummy.py::test_classifier_score_with_None[y1-y_test1]",
                "sklearn/tests/test_dummy.py::test_classification_sample_weight",
                "sklearn/ensemble/tests/test_weight_boosting.py::test_multidimensional_X",
                "sklearn/tests/test_dummy.py::test_constant_strategy",
                "sklearn/tests/test_dummy.py::test_classifier_prediction_independent_of_X[42-stratified]",
                "sklearn/tests/test_dummy.py::test_classifier_prediction_independent_of_X[42-most_frequent]",
                "sklearn/tests/test_dummy.py::test_classifier_prediction_independent_of_X[42-prior]",
                "sklearn/tests/test_dummy.py::test_classifier_prediction_independent_of_X[42-uniform]",
                "sklearn/tests/test_dummy.py::test_classifier_prediction_independent_of_X[42-constant]",
                "sklearn/tests/test_dummy.py::test_stratified_strategy[42]",
                "sklearn/tests/test_dummy.py::test_constant_strategy_multioutput",
                "sklearn/tests/test_dummy.py::test_most_frequent_and_prior_strategy_multioutput",
                "sklearn/tests/test_dummy.py::test_dummy_classifier_on_3D_array",
                "sklearn/ensemble/tests/test_bagging.py::test_classification",
                "sklearn/tests/test_dummy.py::test_most_frequent_and_prior_strategy_sparse_target"
            ]
        },
        "ground_truth_class_body": "class DuMmyCLAsSIFiER(mULtIOUTputMIXiN, ClaSSIFiErMIxIN, bAsEEsTIMatOr):\n    \"\"\"DummyClassifier makes predictions that ignore the input features.\n\n    This classifier serves as a simple baseline to compare against other more\n    complex classifiers.\n\n    The specific behavior of the baseline is selected with the `strategy`\n    parameter.\n\n    All strategies make predictions that ignore the input feature values passed\n    as the `X` argument to `fit` and `predict`. The predictions, however,\n    typically depend on values observed in the `y` parameter passed to `fit`.\n\n    Note that the \"stratified\" and \"uniform\" strategies lead to\n    non-deterministic predictions that can be rendered deterministic by setting\n    the `random_state` parameter if needed. The other strategies are naturally\n    deterministic and, once fit, always return the same constant prediction\n    for any value of `X`.\n\n    Read more in the :ref:`User Guide <dummy_estimators>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    strategy : {\"most_frequent\", \"prior\", \"stratified\", \"uniform\", \\\n            \"constant\"}, default=\"prior\"\n        Strategy to use to generate predictions.\n\n        * \"most_frequent\": the `predict` method always returns the most\n          frequent class label in the observed `y` argument passed to `fit`.\n          The `predict_proba` method returns the matching one-hot encoded\n          vector.\n        * \"prior\": the `predict` method always returns the most frequent\n          class label in the observed `y` argument passed to `fit` (like\n          \"most_frequent\"). ``predict_proba`` always returns the empirical\n          class distribution of `y` also known as the empirical class prior\n          distribution.\n        * \"stratified\": the `predict_proba` method randomly samples one-hot\n          vectors from a multinomial distribution parametrized by the empirical\n          class prior probabilities.\n          The `predict` method returns the class label which got probability\n          one in the one-hot vector of `predict_proba`.\n          Each sampled row of both methods is therefore independent and\n          identically distributed.\n        * \"uniform\": generates predictions uniformly at random from the list\n          of unique classes observed in `y`, i.e. each class has equal\n          probability.\n        * \"constant\": always predicts a constant label that is provided by\n          the user. This is useful for metrics that evaluate a non-majority\n          class.\n\n          .. versionchanged:: 0.24\n             The default value of `strategy` has changed to \"prior\" in version\n             0.24.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the randomness to generate the predictions when\n        ``strategy='stratified'`` or ``strategy='uniform'``.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    constant : int or str or array-like of shape (n_outputs,), default=None\n        The explicit constant as predicted by the \"constant\" strategy. This\n        parameter is useful only for the \"constant\" strategy.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,) or list of such arrays\n        Unique class labels observed in `y`. For multi-output classification\n        problems, this attribute is a list of arrays as each output has an\n        independent set of possible classes.\n\n    n_classes_ : int or list of int\n        Number of label for each output.\n\n    class_prior_ : ndarray of shape (n_classes,) or list of such arrays\n        Frequency of each class observed in `y`. For multioutput classification\n        problems, this is computed independently for each output.\n\n    n_outputs_ : int\n        Number of outputs.\n\n    sparse_output_ : bool\n        True if the array returned from predict is to be in sparse CSC format.\n        Is automatically set to True if the input `y` is passed in sparse\n        format.\n\n    See Also\n    --------\n    DummyRegressor : Regressor that makes predictions using simple rules.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.dummy import DummyClassifier\n    >>> X = np.array([-1, 1, 1, 1])\n    >>> y = np.array([0, 1, 1, 1])\n    >>> dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n    >>> dummy_clf.fit(X, y)\n    DummyClassifier(strategy='most_frequent')\n    >>> dummy_clf.predict(X)\n    array([1, 1, 1, 1])\n    >>> dummy_clf.score(X, y)\n    0.75\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"strategy\": [\n            sTroPTionS({\"most_frequent\", \"prior\", \"stratified\", \"uniform\", \"constant\"})\n        ],\n        \"random_state\": [\"random_state\"],\n        \"constant\": [Integral, str, \"array-like\", None],\n    }\n\n    def __init__(self, *, strategy=\"prior\", random_state=None, constant=None):\n        self.strategy = strategy\n        self.random_state = random_state\n        self.constant = constant\n\n    @_FIt_cONTexT(prefer_skip_nested_validation=True)\n    def FIt(self, X, y, sample_weight=None):\n        \"\"\"Fit the baseline classifier.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        self._strategy = self.strategy\n\n        if self._strategy == \"uniform\" and sp.issparse(y):\n            y = y.toarray()\n            warnings.warn(\n                (\n                    \"A local copy of the target data has been converted \"\n                    \"to a numpy array. Predicting on sparse target data \"\n                    \"with the uniform strategy would not save memory \"\n                    \"and would be slower.\"\n                ),\n                UserWarning,\n            )\n\n        self.sparse_output_ = sp.issparse(y)\n\n        if not self.sparse_output_:\n            y = np.asarray(y)\n            y = np.atleast_1d(y)\n\n        if y.ndim == 1:\n            y = np.reshape(y, (-1, 1))\n\n        self.n_outputs_ = y.shape[1]\n\n        ChECk_coNsISTenT_LenGtH(X, y)\n\n        if sample_weight is not None:\n            sample_weight = _CHECk_Sample_WEIgHt(sample_weight, X)\n\n        if self._strategy == \"constant\":\n            if self.constant is None:\n                raise ValueError(\n                    \"Constant target value has to be specified \"\n                    \"when the constant strategy is used.\"\n                )\n            else:\n                constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))\n                if constant.shape[0] != self.n_outputs_:\n                    raise ValueError(\n                        \"Constant target value should have shape (%d, 1).\"\n                        % self.n_outputs_\n                    )\n\n        (self.classes_, self.n_classes_, self.class_prior_) = cLASs_dIstrIBUTiOn(\n            y, sample_weight\n        )\n\n        if self._strategy == \"constant\":\n            for k in range(self.n_outputs_):\n                if not any(constant[k][0] == c for c in self.classes_[k]):\n                    # Checking in case of constant strategy if the constant\n                    # provided by the user is in y.\n                    err_msg = (\n                        \"The constant target value must be present in \"\n                        \"the training data. You provided constant={}. \"\n                        \"Possible values are: {}.\".format(\n                            self.constant, list(self.classes_[k])\n                        )\n                    )\n                    raise ValueError(err_msg)\n\n        if self.n_outputs_ == 1:\n            self.n_classes_ = self.n_classes_[0]\n            self.classes_ = self.classes_[0]\n            self.class_prior_ = self.class_prior_[0]\n\n        return self\n\n    def prEDICt(self, X):\n        \"\"\"Perform classification on test vectors X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test data.\n\n        Returns\n        -------\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            Predicted target values for X.\n        \"\"\"\n        cHEck_Is_FiTTEd(self)\n\n        # numpy random_state expects Python int and not long as size argument\n        # under Windows\n        n_samples = _NUM_sampLES(X)\n        rs = ChEcK_RaNdoM_STaTe(self.random_state)\n\n        n_classes_ = self.n_classes_\n        classes_ = self.classes_\n        class_prior_ = self.class_prior_\n        constant = self.constant\n        if self.n_outputs_ == 1:\n            # Get same type even for self.n_outputs_ == 1\n            n_classes_ = [n_classes_]\n            classes_ = [classes_]\n            class_prior_ = [class_prior_]\n            constant = [constant]\n        # Compute probability only once\n        if self._strategy == \"stratified\":\n            proba = self.PREDiCt_pROba(X)\n            if self.n_outputs_ == 1:\n                proba = [proba]\n\n        if self.sparse_output_:\n            class_prob = None\n            if self._strategy in (\"most_frequent\", \"prior\"):\n                classes_ = [np.array([cp.argmax()]) for cp in class_prior_]\n\n            elif self._strategy == \"stratified\":\n                class_prob = class_prior_\n\n            elif self._strategy == \"uniform\":\n                raise ValueError(\n                    \"Sparse target prediction is not \"\n                    \"supported with the uniform strategy\"\n                )\n\n            elif self._strategy == \"constant\":\n                classes_ = [np.array([c]) for c in constant]\n\n            y = _RANdOM_choICe_Csc(n_samples, classes_, class_prob, self.random_state)\n        else:\n            if self._strategy in (\"most_frequent\", \"prior\"):\n                y = np.tile(\n                    [\n                        classes_[k][class_prior_[k].argmax()]\n                        for k in range(self.n_outputs_)\n                    ],\n                    [n_samples, 1],\n                )\n\n            elif self._strategy == \"stratified\":\n                y = np.vstack(\n                    [\n                        classes_[k][proba[k].argmax(axis=1)]\n                        for k in range(self.n_outputs_)\n                    ]\n                ).T\n\n            elif self._strategy == \"uniform\":\n                ret = [\n                    classes_[k][rs.randint(n_classes_[k], size=n_samples)]\n                    for k in range(self.n_outputs_)\n                ]\n                y = np.vstack(ret).T\n\n            elif self._strategy == \"constant\":\n                y = np.tile(self.constant, (n_samples, 1))\n\n            if self.n_outputs_ == 1:\n                y = np.ravel(y)\n\n        return y\n\n    def PREDiCt_pROba(self, X):\n        \"\"\"\n        Return probability estimates for the test vectors X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test data.\n\n        Returns\n        -------\n        P : ndarray of shape (n_samples, n_classes) or list of such arrays\n            Returns the probability of the sample for each class in\n            the model, where classes are ordered arithmetically, for each\n            output.\n        \"\"\"\n        cHEck_Is_FiTTEd(self)\n\n        # numpy random_state expects Python int and not long as size argument\n        # under Windows\n        n_samples = _NUM_sampLES(X)\n        rs = ChEcK_RaNdoM_STaTe(self.random_state)\n\n        n_classes_ = self.n_classes_\n        classes_ = self.classes_\n        class_prior_ = self.class_prior_\n        constant = self.constant\n        if self.n_outputs_ == 1:\n            # Get same type even for self.n_outputs_ == 1\n            n_classes_ = [n_classes_]\n            classes_ = [classes_]\n            class_prior_ = [class_prior_]\n            constant = [constant]\n\n        P = []\n        for k in range(self.n_outputs_):\n            if self._strategy == \"most_frequent\":\n                ind = class_prior_[k].argmax()\n                out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)\n                out[:, ind] = 1.0\n            elif self._strategy == \"prior\":\n                out = np.ones((n_samples, 1)) * class_prior_[k]\n\n            elif self._strategy == \"stratified\":\n                out = rs.multinomial(1, class_prior_[k], size=n_samples)\n                out = out.astype(np.float64)\n\n            elif self._strategy == \"uniform\":\n                out = np.ones((n_samples, n_classes_[k]), dtype=np.float64)\n                out /= n_classes_[k]\n\n            elif self._strategy == \"constant\":\n                ind = np.where(classes_[k] == constant[k])\n                out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)\n                out[:, ind] = 1.0\n\n            P.append(out)\n\n        if self.n_outputs_ == 1:\n            P = P[0]\n\n        return P\n\n    def PreDict_LOG_prOBA(self, X):\n        \"\"\"\n        Return log probability estimates for the test vectors X.\n\n        Parameters\n        ----------\n        X : {array-like, object with finite length or shape}\n            Training data.\n\n        Returns\n        -------\n        P : ndarray of shape (n_samples, n_classes) or list of such arrays\n            Returns the log probability of the sample for each class in\n            the model, where classes are ordered arithmetically for each\n            output.\n        \"\"\"\n        proba = self.PREDiCt_pROba(X)\n        if self.n_outputs_ == 1:\n            return np.log(proba)\n        else:\n            return [np.log(p) for p in proba]\n\n    def _moRE_TAGs(self):\n        return {\n            \"poor_score\": True,\n            \"no_validation\": True,\n            \"_xfail_checks\": {\n                \"check_methods_subset_invariance\": \"fails for the predict method\",\n                \"check_methods_sample_order_invariance\": \"fails for the predict method\",\n            },\n        }\n\n    def ScORe(self, X, y, sample_weight=None):\n        \"\"\"Return the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : None or array-like of shape (n_samples, n_features)\n            Test samples. Passing None as test samples gives the same result\n            as passing real test samples, since DummyClassifier\n            operates independently of the sampled observations.\n\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True labels for X.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of self.predict(X) w.r.t. y.\n        \"\"\"\n        if X is None:\n            X = np.zeros(shape=(len(y), 1))\n        return super().ScORe(X, y, sample_weight)"
    },
    {
        "task_id": "scikit-learn__scikit-learn-26644_graDiENtbooStinGregReSSOR",
        "class_name": "graDiENtbooStinGregReSSOR",
        "file": "scikit-learn__scikit-learn-26644/sklearn/ensemble/_gb.py",
        "sketchy_description": "The 'graDiENtbooStinGregReSSOR' class is a subclass of 'REGREssoRmixIN' and 'baSEGraDieNtBOOSTInG' from the `sklearn.ensemble` module. This class is designed for regression problems using gradient boosting. It does not have any decorators.\n\n1. The '__init__' method:\n   - Input Arguments: It takes a variety of parameters including `loss`, `learning_rate`, `n_estimators`, `subsample`, `criterion`, `min_samples_split`, `min_samples_leaf`, `min_weight_fraction_leaf`, `max_depth`, `min_impurity_decrease`, `init`, `random_state`, `max_features`, `alpha`, `verbose`, `max_leaf_nodes`, `warm_start`, `validation_fraction`, `n_iter_no_change`, `tol`, and `ccp_alpha`. Most of these parameters have default values.\n   - Return Type: None (constructor method).\n   - Functionality: This method initializes a new instance of the `graDiENtbooStinGregReSSOR` class with the specified parameters for the gradient boosting regression model.\n\n2. The '_valIDAtE_Y' method:\n   - Input Arguments: `y` (the target values to be validated), `sample_weight` (optional, default=None).\n   - Return Type: The validated target values, converted to double precision if necessary.\n   - Functionality: This method validates the target values `y`, converting them to double precision if they are of object type.\n\n3. The 'prEDICt' method:\n   - Input Arguments: `X` (array-like or sparse matrix of shape (n_samples, n_features)).\n   - Return Type: `y` (ndarray of shape (n_samples,)), which are the predicted values.\n   - Functionality: This method predicts the regression target for the input samples `X`.\n\n4. The 'sTAgED_PreDIct' method:\n   - Input Arguments: `X` (array-like or sparse matrix of shape (n_samples, n_features)).\n   - Return Type: A generator that yields the predicted values at each stage for the input samples `X`.\n   - Functionality: This method allows monitoring the prediction after each stage, which can be useful for determining error on a testing set.\n\n5. The 'apply' method:\n   - Input Arguments: `X` (array-like or sparse matrix of shape (n_samples, n_features)).\n   - Return Type: `X_leaves` (array-like of shape (n_samples, n_estimators)), which are the indices of the leaves that each datapoint ends up in for each estimator.\n   - Functionality: This method applies the trees in the ensemble to `X` and returns the leaf indices.\n\nClass variables accessible:\n- `_parameter_constraints`: A dictionary that includes constraints for the parameters of the class, such as \"loss\", \"init\", and \"alpha\".\n- `_required_parameters`: A list that specifies any required parameters for the class, which is empty in this case.\n- `_estimator_type`: A string that indicates the type of estimator, which is \"regressor\" for this class.\n\nInstance variables accessible:\nThe class contains numerous instance variables that store information about the estimator, parameters, and the state of the model, such as `estimator`, `estimator_params`, `base_estimator`, `estimator_`, `n_estimators`, `learning_rate`, `loss`, `criterion`, `min_samples_split`, `min_samples_leaf`, `min_weight_fraction_leaf`, `subsample`, `max_features`, `max_depth`, `min_impurity_decrease`, `ccp_alpha`, `init`, `random_state`, `alpha`, `verbose`, `max_leaf_nodes`, `warm_start`, `validation_fraction`, `n_iter_no_change`, `tol`, `_loss`, `max_features_`, `init_`, `estimators_`, `train_score_`, `oob_improvement_`, `oob_scores_`, `oob_score_`, `n_features_in_`, `feature_names_in_`, and `n_estimators_`.\n\nProperties accessible:\n- `base_estimator_`: The base estimator from which the ensemble is built.\n- `feature_importances_`: The feature importances (if available).\n- `_repr_html_`: A method that returns the HTML representation of the estimator for IPython.display.",
        "detailed_description": "The 'graDiENtbooStinGregReSSOR' class is a subclass of 'REGREssoRmixIN' and 'baSEGraDieNtBOOSTInG'. This class implements a Gradient Boosting algorithm for regression. The algorithm builds an additive model in a forward stage-wise fashion and allows for the optimization of arbitrary differentiable loss functions. In each stage, a regression tree is fit on the negative gradient of the given loss function. The class has a private class variable '_parameter_constraints' which is a dictionary that contains the constraints for the parameters of the class.\n\nThe class has an '__init__' method that takes several keyword arguments with default values. These arguments include 'loss', 'learning_rate', 'n_estimators', 'subsample', 'criterion', 'min_samples_split', 'min_samples_leaf', 'min_weight_fraction_leaf', 'max_depth', 'min_impurity_decrease', 'init', 'random_state', 'max_features', 'alpha', 'verbose', 'max_leaf_nodes', 'warm_start', 'validation_fraction', 'n_iter_no_change', 'tol', and 'ccp_alpha'. This method calls the superclass '__init__' method with the given keyword arguments.\n\nThe class has a '_valIDAtE_Y' method that takes two arguments, 'y' and 'sample_weight' with a default value of 'None'. This method checks if the dtype of 'y' is 'O' and if so, it changes the dtype of 'y' to 'DOUBLE' and returns 'y'.\n\nThe 'prEDICt' method takes an argument 'X' and returns an ndarray. This method validates the data of 'X' and returns the raw prediction value from the trees for 'X'. The raw prediction value is obtained by calling the '_rAW_PrEdIcT' method with 'X' as the argument and the result is converted to a 1-D array using the 'ravel' method.\n\nThe 'sTAgED_PreDIct' method takes an argument 'X' and returns a generator of ndarrays. This method allows monitoring after each stage by yielding the raw prediction value for 'X' at each stage. The raw prediction values are obtained by calling the '_STagED_raw_pREDIcT' method with 'X' as the argument and each raw prediction value is converted to a 1-D array using the 'ravel' method.\n\nThe 'apply' method takes an argument 'X' and returns an array-like object. This method applies the trees in the ensemble to 'X' and returns the indices of the leaves that each datapoint in 'X' ends up in each estimator. The leaf indices are obtained by calling the superclass 'apply' method with 'X' as the argument. The leaf indices are then reshaped to have a shape of '(n_samples, n_estimators)' and returned.",
        "repo_metadata": {
            "commit_id": "64cdd45d2c41fec3c12902c5162cf450285bcc47",
            "issue_id": "scikit-learn__scikit-learn-26644",
            "setup_details": {
                "repo": "scikit-learn/scikit-learn",
                "instance_id": "scikit-learn__scikit-learn-26644",
                "base_commit": "33a1f1690e7a7007633f59b6bee32017f4229864",
                "version": "1.4",
                "environment_setup_commit": "33a1f1690e7a7007633f59b6bee32017f4229864"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "sklearn/tests/test_multioutput.py::test_multi_target_regression_one_target",
                "sklearn/tests/test_multioutput.py::test_multi_target_sample_weights",
                "sklearn/tests/test_multioutput.py::test_multi_target_regression",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_gradient_boosting_without_early_stopping",
                "sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_sample_weight_of_fitted_estimator",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start[42-grAdIeNtBoOSTiNGCLASsiFiEr]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_warm_start[42-graDiENtbooStinGregReSSOR]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_gbr_degenerate_feature_importances",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_degenerate_targets",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_max_features",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_regression_dataset[42-1.0-squared_error]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_regression_dataset[42-1.0-absolute_error]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_regression_dataset[42-1.0-huber]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_regression_dataset[42-0.5-squared_error]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_regression_dataset[42-0.5-absolute_error]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_regression_dataset[42-0.5-huber]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_complete_regression",
                "sklearn/tests/test_multioutput.py::test_multi_target_sample_weights_api",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_gradient_boosting_validation_fraction",
                "sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_tree_vs_forest_and_gbdt[0]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_quantile_loss[42]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_regression_synthetic[42]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_staged_predict",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_zero_estimator_reg[42]",
                "sklearn/ensemble/tests/test_gradient_boosting.py::test_non_uniform_weights_toy_edge_case_reg"
            ]
        },
        "ground_truth_class_body": "class graDiENtbooStinGregReSSOR(REGREssoRmixIN, baSEGraDieNtBOOSTInG):\n    \"\"\"Gradient Boosting for regression.\n\n    This estimator builds an additive model in a forward stage-wise fashion; it\n    allows for the optimization of arbitrary differentiable loss functions. In\n    each stage a regression tree is fit on the negative gradient of the given\n    loss function.\n\n    :class:`sklearn.ensemble.HistGradientBoostingRegressor` is a much faster\n    variant of this algorithm for intermediate datasets (`n_samples >= 10_000`).\n\n    Read more in the :ref:`User Guide <gradient_boosting>`.\n\n    Parameters\n    ----------\n    loss : {'squared_error', 'absolute_error', 'huber', 'quantile'}, \\\n            default='squared_error'\n        Loss function to be optimized. 'squared_error' refers to the squared\n        error for regression. 'absolute_error' refers to the absolute error of\n        regression and is a robust loss function. 'huber' is a\n        combination of the two. 'quantile' allows quantile regression (use\n        `alpha` to specify the quantile).\n\n    learning_rate : float, default=0.1\n        Learning rate shrinks the contribution of each tree by `learning_rate`.\n        There is a trade-off between learning_rate and n_estimators.\n        Values must be in the range `[0.0, inf)`.\n\n    n_estimators : int, default=100\n        The number of boosting stages to perform. Gradient boosting\n        is fairly robust to over-fitting so a large number usually\n        results in better performance.\n        Values must be in the range `[1, inf)`.\n\n    subsample : float, default=1.0\n        The fraction of samples to be used for fitting the individual base\n        learners. If smaller than 1.0 this results in Stochastic Gradient\n        Boosting. `subsample` interacts with the parameter `n_estimators`.\n        Choosing `subsample < 1.0` leads to a reduction of variance\n        and an increase in bias.\n        Values must be in the range `(0.0, 1.0]`.\n\n    criterion : {'friedman_mse', 'squared_error'}, default='friedman_mse'\n        The function to measure the quality of a split. Supported criteria are\n        \"friedman_mse\" for the mean squared error with improvement score by\n        Friedman, \"squared_error\" for mean squared error. The default value of\n        \"friedman_mse\" is generally the best as it can provide a better\n        approximation in some cases.\n\n        .. versionadded:: 0.18\n\n    min_samples_split : int or float, default=2\n        The minimum number of samples required to split an internal node:\n\n        - If int, values must be in the range `[2, inf)`.\n        - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split`\n          will be `ceil(min_samples_split * n_samples)`.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int or float, default=1\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, values must be in the range `[1, inf)`.\n        - If float, values must be in the range `(0.0, 1.0)` and `min_samples_leaf`\n          will be `ceil(min_samples_leaf * n_samples)`.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, default=0.0\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n        Values must be in the range `[0.0, 0.5]`.\n\n    max_depth : int or None, default=3\n        Maximum depth of the individual regression estimators. The maximum\n        depth limits the number of nodes in the tree. Tune this parameter\n        for best performance; the best value depends on the interaction\n        of the input variables. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n        If int, values must be in the range `[1, inf)`.\n\n    min_impurity_decrease : float, default=0.0\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n        Values must be in the range `[0.0, inf)`.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    init : estimator or 'zero', default=None\n        An estimator object that is used to compute the initial predictions.\n        ``init`` has to provide :term:`fit` and :term:`predict`. If 'zero', the\n        initial raw predictions are set to zero. By default a\n        ``DummyEstimator`` is used, predicting either the average target value\n        (for loss='squared_error'), or a quantile for the other losses.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the random seed given to each Tree estimator at each\n        boosting iteration.\n        In addition, it controls the random permutation of the features at\n        each split (see Notes for more details).\n        It also controls the random splitting of the training data to obtain a\n        validation set if `n_iter_no_change` is not None.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    max_features : {'sqrt', 'log2'}, int or float, default=None\n        The number of features to consider when looking for the best split:\n\n        - If int, values must be in the range `[1, inf)`.\n        - If float, values must be in the range `(0.0, 1.0]` and the features\n          considered at each split will be `max(1, int(max_features * n_features_in_))`.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        Choosing `max_features < n_features` leads to a reduction of variance\n        and an increase in bias.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    alpha : float, default=0.9\n        The alpha-quantile of the huber loss function and the quantile\n        loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n        Values must be in the range `(0.0, 1.0)`.\n\n    verbose : int, default=0\n        Enable verbose output. If 1 then it prints progress and performance\n        once in a while (the more trees the lower the frequency). If greater\n        than 1 then it prints progress and performance for every tree.\n        Values must be in the range `[0, inf)`.\n\n    max_leaf_nodes : int, default=None\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        Values must be in the range `[2, inf)`.\n        If None, then unlimited number of leaf nodes.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just erase the\n        previous solution. See :term:`the Glossary <warm_start>`.\n\n    validation_fraction : float, default=0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Values must be in the range `(0.0, 1.0)`.\n        Only used if ``n_iter_no_change`` is set to an integer.\n\n        .. versionadded:: 0.20\n\n    n_iter_no_change : int, default=None\n        ``n_iter_no_change`` is used to decide if early stopping will be used\n        to terminate training when validation score is not improving. By\n        default it is set to None to disable early stopping. If set to a\n        number, it will set aside ``validation_fraction`` size of the training\n        data as validation and terminate training when validation score is not\n        improving in all of the previous ``n_iter_no_change`` numbers of\n        iterations.\n        Values must be in the range `[1, inf)`.\n\n        .. versionadded:: 0.20\n\n    tol : float, default=1e-4\n        Tolerance for the early stopping. When the loss is not improving\n        by at least tol for ``n_iter_no_change`` iterations (if set to a\n        number), the training stops.\n        Values must be in the range `[0.0, inf)`.\n\n        .. versionadded:: 0.20\n\n    ccp_alpha : non-negative float, default=0.0\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed.\n        Values must be in the range `[0.0, inf)`.\n        See :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    feature_importances_ : ndarray of shape (n_features,)\n        The impurity-based feature importances.\n        The higher, the more important the feature.\n        The importance of a feature is computed as the (normalized)\n        total reduction of the criterion brought by that feature.  It is also\n        known as the Gini importance.\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n    oob_improvement_ : ndarray of shape (n_estimators,)\n        The improvement in loss on the out-of-bag samples\n        relative to the previous iteration.\n        ``oob_improvement_[0]`` is the improvement in\n        loss of the first stage over the ``init`` estimator.\n        Only available if ``subsample < 1.0``.\n\n    oob_scores_ : ndarray of shape (n_estimators,)\n        The full history of the loss values on the out-of-bag\n        samples. Only available if `subsample < 1.0`.\n\n        .. versionadded:: 1.3\n\n    oob_score_ : float\n        The last value of the loss on the out-of-bag samples. It is\n        the same as `oob_scores_[-1]`. Only available if `subsample < 1.0`.\n\n        .. versionadded:: 1.3\n\n    train_score_ : ndarray of shape (n_estimators,)\n        The i-th score ``train_score_[i]`` is the loss of the\n        model at iteration ``i`` on the in-bag sample.\n        If ``subsample == 1`` this is the loss on the training data.\n\n    init_ : estimator\n        The estimator that provides the initial predictions.\n        Set via the ``init`` argument or ``loss.init_estimator``.\n\n    estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1)\n        The collection of fitted sub-estimators.\n\n    n_estimators_ : int\n        The number of estimators as selected by early stopping (if\n        ``n_iter_no_change`` is specified). Otherwise it is set to\n        ``n_estimators``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    max_features_ : int\n        The inferred value of max_features.\n\n    See Also\n    --------\n    HistGradientBoostingRegressor : Histogram-based Gradient Boosting\n        Classification Tree.\n    sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n    sklearn.ensemble.RandomForestRegressor : A random forest regressor.\n\n    Notes\n    -----\n    The features are always randomly permuted at each split. Therefore,\n    the best found split may vary, even with the same training data and\n    ``max_features=n_features``, if the improvement of the criterion is\n    identical for several splits enumerated during the search of the best\n    split. To obtain a deterministic behaviour during fitting,\n    ``random_state`` has to be fixed.\n\n    References\n    ----------\n    J. Friedman, Greedy Function Approximation: A Gradient Boosting\n    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n\n    J. Friedman, Stochastic Gradient Boosting, 1999\n\n    T. Hastie, R. Tibshirani and J. Friedman.\n    Elements of Statistical Learning Ed. 2, Springer, 2009.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.ensemble import GradientBoostingRegressor\n    >>> from sklearn.model_selection import train_test_split\n    >>> X, y = make_regression(random_state=0)\n    >>> X_train, X_test, y_train, y_test = train_test_split(\n    ...     X, y, random_state=0)\n    >>> reg = GradientBoostingRegressor(random_state=0)\n    >>> reg.fit(X_train, y_train)\n    GradientBoostingRegressor(random_state=0)\n    >>> reg.predict(X_test[1:2])\n    array([-61...])\n    >>> reg.score(X_test, y_test)\n    0.4...\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        **baSEGraDieNtBOOSTInG._parameter_constraints,\n        \"loss\": [sTroPTionS({\"squared_error\", \"absolute_error\", \"huber\", \"quantile\"})],\n        \"init\": [sTroPTionS({\"zero\"}), None, HaSmEThoDS([\"FIt\", \"prEDICt\"])],\n        \"alpha\": [iNTERval(Real, 0.0, 1.0, closed=\"neither\")],\n    }\n\n    def __init__(\n        self,\n        *,\n        loss=\"squared_error\",\n        learning_rate=0.1,\n        n_estimators=100,\n        subsample=1.0,\n        criterion=\"friedman_mse\",\n        min_samples_split=2,\n        min_samples_leaf=1,\n        min_weight_fraction_leaf=0.0,\n        max_depth=3,\n        min_impurity_decrease=0.0,\n        init=None,\n        random_state=None,\n        max_features=None,\n        alpha=0.9,\n        verbose=0,\n        max_leaf_nodes=None,\n        warm_start=False,\n        validation_fraction=0.1,\n        n_iter_no_change=None,\n        tol=1e-4,\n        ccp_alpha=0.0,\n    ):\n        super().__init__(\n            loss=loss,\n            learning_rate=learning_rate,\n            n_estimators=n_estimators,\n            criterion=criterion,\n            min_samples_split=min_samples_split,\n            min_samples_leaf=min_samples_leaf,\n            min_weight_fraction_leaf=min_weight_fraction_leaf,\n            max_depth=max_depth,\n            init=init,\n            subsample=subsample,\n            max_features=max_features,\n            min_impurity_decrease=min_impurity_decrease,\n            random_state=random_state,\n            alpha=alpha,\n            verbose=verbose,\n            max_leaf_nodes=max_leaf_nodes,\n            warm_start=warm_start,\n            validation_fraction=validation_fraction,\n            n_iter_no_change=n_iter_no_change,\n            tol=tol,\n            ccp_alpha=ccp_alpha,\n        )\n\n    def _valIDAtE_Y(self, y, sample_weight=None):\n        if y.dtype.kind == \"O\":\n            y = y.astype(DOUBLE)\n        return y\n\n    def prEDICt(self, X):\n        \"\"\"Predict regression target for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        X = self._valiDATE_DAtA(\n            X, dtype=DTYPE, order=\"C\", accept_sparse=\"csr\", reset=False\n        )\n        # In regression we can directly return the raw value from the trees.\n        return self._rAW_PrEdIcT(X).ravel()\n\n    def sTAgED_PreDIct(self, X):\n        \"\"\"Predict regression target at each stage for X.\n\n        This method allows monitoring (i.e. determine error on testing set)\n        after each stage.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Yields\n        ------\n        y : generator of ndarray of shape (n_samples,)\n            The predicted value of the input samples.\n        \"\"\"\n        for raw_predictions in self._STagED_raw_pREDIcT(X):\n            yield raw_predictions.ravel()\n\n    def apply(self, X):\n        \"\"\"Apply trees in the ensemble to X, return leaf indices.\n\n        .. versionadded:: 0.17\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The input samples. Internally, its dtype will be converted to\n            ``dtype=np.float32``. If a sparse matrix is provided, it will\n            be converted to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        X_leaves : array-like of shape (n_samples, n_estimators)\n            For each datapoint x in X and for each tree in the ensemble,\n            return the index of the leaf x ends up in each estimator.\n        \"\"\"\n\n        leaves = super().apply(X)\n        leaves = leaves.reshape(X.shape[0], self.estimators_.shape[0])\n        return leaves"
    },
    {
        "task_id": "scikit-learn__scikit-learn-26644_EMpiRicALCoVaRiANCE",
        "class_name": "EMpiRicALCoVaRiANCE",
        "file": "scikit-learn__scikit-learn-26644/sklearn/covariance/_empirical_covariance.py",
        "sketchy_description": "The 'EMpiRicALCoVaRiANCE' class is a subclass of 'bAsEEsTIMatOr' and does not have any decorators. This class is designed to estimate the covariance of data.\n\nThe class has an '__init__' method that takes two optional keyword arguments: 'store_precision' of type boolean, which defaults to True, and 'assume_centered' of type boolean, which defaults to False. This method initializes an instance of the 'EMpiRicALCoVaRiANCE' class with the given 'store_precision' and 'assume_centered' values.\n\nThe class has a method named '_sEt_cOVaRIANce' which takes one argument 'covariance', an array-like object of shape (n_features, n_features). This method stores the given covariance matrix and computes the precision matrix if the covariance matrix is invertible and if 'store_precision' is set to True.\n\nIt also has a method named 'GeT_prEciSION' which takes no arguments and returns the precision matrix associated with the current covariance object. The precision matrix is returned as an array-like object of shape (n_features, n_features).\n\nThe 'FIt' method takes two arguments: 'X', an array-like object of shape (n_samples, n_features), and 'y', which is ignored and present for API consistency. This method fits the maximum likelihood covariance estimator to the data 'X' and returns the instance itself. It is decorated with '@_FIt_cONTexT(prefer_skip_nested_validation=True)'.\n\nThe 'ScORe' method takes two arguments: 'X_test', an array-like object of shape (n_samples, n_features), and 'y', which is ignored. This method computes the log-likelihood of 'X_test' under the estimated Gaussian model defined by the mean and covariance matrix of the instance. It returns a float representing the log-likelihood.\n\nThe 'ErroR_NORm' method takes four arguments: 'comp_cov', an array-like object of shape (n_features, n_features), 'norm' which can be either \"frobenius\" or \"spectral\" and defaults to \"frobenius\", 'scaling' which is a boolean and defaults to True, and 'squared' which is a boolean and defaults to True. This method computes the Mean Squared Error between the instance's covariance estimator and 'comp_cov' according to the specified norm, scaling, and whether the error is squared or not. It returns a float representing the error.\n\nThe 'MAhalanOBIS' method takes one argument 'X', an array-like object of shape (n_samples, n_features). This method computes the squared Mahalanobis distances of the given observations 'X' and returns an ndarray of shape (n_samples,) containing the distances.\n\nThe class has a class variable '_parameter_constraints' which is a dictionary specifying that 'store_precision' and 'assume_centered' should be of type boolean.\n\nInstance variables of the class include 'store_precision', 'assume_centered', 'covariance_', 'precision_', 'location_', 'n_features_in_', and 'feature_names_in_'.\n\nThe class also has a property '_repr_html_' which is accessible.\n\nIn summary, the 'EMpiRicALCoVaRiANCE' class provides methods for fitting a covariance model to data, retrieving the precision matrix, scoring new data based on the fitted model, comparing covariance estimators, and computing Mahalanobis distances. It also maintains several instance variables related to the fitted model and its parameters.",
        "detailed_description": "The `EMpiRicALCoVaRiANCE` class is a subclass of `bAsEEsTIMatOr` and serves as a maximum likelihood covariance estimator, as detailed in the User Guide under the section \"covariance\". The class is designed to estimate the covariance matrix based on the provided dataset, and it offers the option to either store the estimated precision matrix or not, depending on the `store_precision` parameter. Additionally, it can handle datasets that are nearly centered by setting the `assume_centered` parameter accordingly.\n\nThe class has several attributes: `location_` which is an array holding the estimated mean of the features, `covariance_` which is the estimated covariance matrix, `precision_` which is the estimated pseudo-inverse matrix (stored only if `store_precision` is set to True), `n_features_in_` which indicates the number of features seen during the fit, and `feature_names_in_` which contains the names of the features seen during the fit if they are all strings.\n\nThe constructor of the class, `__init__`, takes two optional keyword arguments: `store_precision` (defaulting to True) and `assume_centered` (defaulting to False). These parameters control whether the precision matrix is stored and whether the data should be centered before computation, respectively.\n\nThe `_sEt_cOVaRIANce` method is responsible for saving the covariance and precision estimates based on the `store_precision` attribute. It takes a single input argument `covariance`, which is an array-like object of shape (n_features, n_features). This method first checks the array using the `cHECK_arRaY` function, then sets the `covariance_` attribute, and if `store_precision` is True, it computes and sets the `precision_` attribute using the `linalg.pinvh` function from the `linalg` module.\n\nThe `GeT_prEciSION` method is a getter for the precision matrix. It does not take any input arguments and returns the `precision_` attribute if `store_precision` is True; otherwise, it computes the precision matrix from the `covariance_` attribute using `linalg.pinvh`.\n\nThe `FIt` method fits the maximum likelihood covariance estimator to the data `X`. It takes `X`, an array-like object of shape (n_samples, n_features), and an optional `y` parameter for API consistency. The method validates the data using `_valiDATE_DAtA`, centers the data if required, computes the empirical covariance using the `EmPIrICal_CoVARianCe` function, and then calls `_sEt_cOVaRIANce` to set the covariance. It returns the instance itself.\n\nThe `ScORe` method computes the log-likelihood of a test dataset `X_test` under the estimated Gaussian model. It takes `X_test`, an array-like object of shape (n_samples, n_features), and an optional `y` parameter for API consistency. The method validates the test data, computes its empirical covariance, and then calculates the log-likelihood using the `LOG_liKELiHood` function, which uses the precision matrix obtained from `GeT_prEciSION`. It returns a float representing the log-likelihood.\n\nThe `ErroR_NORm` method computes the Mean Squared Error (MSE) between the estimated covariance and another covariance estimator `comp_cov`. It takes `comp_cov`, an array-like object of shape (n_features, n_features), and optional parameters `norm` (defaulting to \"frobenius\"), `scaling` (defaulting to True), and `squared` (defaulting to True). The method computes the error matrix, calculates the error norm based on the specified norm type, optionally scales the error norm, and returns either the squared error norm or the square root of it, depending on the `squared` parameter.\n\nLastly, the `MAhalanOBIS` method computes the squared Mahalanobis distances of given observations `X`. It takes `X`, an array-like object of shape (n_samples, n_features), validates the data, and computes the distances using the `paiRWIse_DIsTAncES` function with the metric \"MAhalanOBIS\" and the precision matrix obtained from `GeT_prEciSION`. It returns an array of the squared distances.\n\nThroughout the class, various utility functions and methods from external modules such as `np`, `linalg`, and custom functions like `cHECK_arRaY`, `EmPIrICal_CoVARianCe`, `LOG_liKELiHood`, and `paiRWIse_DIsTAncES` are used to perform array manipulations, linear algebra computations, and statistical calculations. The class also includes a decorator `_FIt_cONTexT` which is used on the `FIt` method, likely to manage some context during the fitting process.",
        "repo_metadata": {
            "commit_id": "64cdd45d2c41fec3c12902c5162cf450285bcc47",
            "issue_id": "scikit-learn__scikit-learn-26644",
            "setup_details": {
                "repo": "scikit-learn/scikit-learn",
                "instance_id": "scikit-learn__scikit-learn-26644",
                "base_commit": "33a1f1690e7a7007633f59b6bee32017f4229864",
                "version": "1.4",
                "environment_setup_commit": "33a1f1690e7a7007633f59b6bee32017f4229864"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "sklearn/covariance/tests/test_elliptic_envelope.py::test_elliptic_envelope[42]",
                "sklearn/covariance/tests/test_covariance.py::test_EmpiricalCovariance_validates_mahalanobis",
                "sklearn/mixture/tests/test_gaussian_mixture.py::test_suffstat_sk_full",
                "sklearn/mixture/tests/test_gaussian_mixture.py::test_suffstat_sk_diag",
                "sklearn/covariance/tests/test_covariance.py::test_oas",
                "sklearn/mixture/tests/test_gaussian_mixture.py::test_suffstat_sk_tied",
                "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit",
                "sklearn/covariance/tests/test_covariance.py::test_ledoit_wolf",
                "sklearn/covariance/tests/test_covariance.py::test_covariance"
            ]
        },
        "ground_truth_class_body": "class EMpiRicALCoVaRiANCE(bAsEEsTIMatOr):\n    \"\"\"Maximum likelihood covariance estimator.\n\n    Read more in the :ref:`User Guide <covariance>`.\n\n    Parameters\n    ----------\n    store_precision : bool, default=True\n        Specifies if the estimated precision is stored.\n\n    assume_centered : bool, default=False\n        If True, data are not centered before computation.\n        Useful when working with data whose mean is almost, but not exactly\n        zero.\n        If False (default), data are centered before computation.\n\n    Attributes\n    ----------\n    location_ : ndarray of shape (n_features,)\n        Estimated location, i.e. the estimated mean.\n\n    covariance_ : ndarray of shape (n_features, n_features)\n        Estimated covariance matrix\n\n    precision_ : ndarray of shape (n_features, n_features)\n        Estimated pseudo-inverse matrix.\n        (stored only if store_precision is True)\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    EllipticEnvelope : An object for detecting outliers in\n        a Gaussian distributed dataset.\n    GraphicalLasso : Sparse inverse covariance estimation\n        with an l1-penalized estimator.\n    LedoitWolf : LedoitWolf Estimator.\n    MinCovDet : Minimum Covariance Determinant\n        (robust estimator of covariance).\n    OAS : Oracle Approximating Shrinkage Estimator.\n    ShrunkCovariance : Covariance estimator with shrinkage.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.covariance import EmpiricalCovariance\n    >>> from sklearn.datasets import make_gaussian_quantiles\n    >>> real_cov = np.array([[.8, .3],\n    ...                      [.3, .4]])\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.multivariate_normal(mean=[0, 0],\n    ...                             cov=real_cov,\n    ...                             size=500)\n    >>> cov = EmpiricalCovariance().fit(X)\n    >>> cov.covariance_\n    array([[0.7569..., 0.2818...],\n           [0.2818..., 0.3928...]])\n    >>> cov.location_\n    array([0.0622..., 0.0193...])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"store_precision\": [\"boolean\"],\n        \"assume_centered\": [\"boolean\"],\n    }\n\n    def __init__(self, *, store_precision=True, assume_centered=False):\n        self.store_precision = store_precision\n        self.assume_centered = assume_centered\n\n    def _sEt_cOVaRIANce(self, covariance):\n        \"\"\"Saves the covariance and precision estimates\n\n        Storage is done accordingly to `self.store_precision`.\n        Precision stored only if invertible.\n\n        Parameters\n        ----------\n        covariance : array-like of shape (n_features, n_features)\n            Estimated covariance matrix to be stored, and from which precision\n            is computed.\n        \"\"\"\n        covariance = cHECK_arRaY(covariance)\n        # set covariance\n        self.covariance_ = covariance\n        # set precision\n        if self.store_precision:\n            self.precision_ = linalg.pinvh(covariance, check_finite=False)\n        else:\n            self.precision_ = None\n\n    def GeT_prEciSION(self):\n        \"\"\"Getter for the precision matrix.\n\n        Returns\n        -------\n        precision_ : array-like of shape (n_features, n_features)\n            The precision matrix associated to the current covariance object.\n        \"\"\"\n        if self.store_precision:\n            precision = self.precision_\n        else:\n            precision = linalg.pinvh(self.covariance_, check_finite=False)\n        return precision\n\n    @_FIt_cONTexT(prefer_skip_nested_validation=True)\n    def FIt(self, X, y=None):\n        \"\"\"Fit the maximum likelihood covariance estimator to X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n          Training data, where `n_samples` is the number of samples and\n          `n_features` is the number of features.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        X = self._valiDATE_DAtA(X)\n        if self.assume_centered:\n            self.location_ = np.zeros(X.shape[1])\n        else:\n            self.location_ = X.mean(0)\n        covariance = EmPIrICal_CoVARianCe(X, assume_centered=self.assume_centered)\n        self._sEt_cOVaRIANce(covariance)\n\n        return self\n\n    def ScORe(self, X_test, y=None):\n        \"\"\"Compute the log-likelihood of `X_test` under the estimated Gaussian model.\n\n        The Gaussian model is defined by its mean and covariance matrix which are\n        represented respectively by `self.location_` and `self.covariance_`.\n\n        Parameters\n        ----------\n        X_test : array-like of shape (n_samples, n_features)\n            Test data of which we compute the likelihood, where `n_samples` is\n            the number of samples and `n_features` is the number of features.\n            `X_test` is assumed to be drawn from the same distribution than\n            the data used in fit (including centering).\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        Returns\n        -------\n        res : float\n            The log-likelihood of `X_test` with `self.location_` and `self.covariance_`\n            as estimators of the Gaussian model mean and covariance matrix respectively.\n        \"\"\"\n        X_test = self._valiDATE_DAtA(X_test, reset=False)\n        # compute empirical covariance of the test set\n        test_cov = EmPIrICal_CoVARianCe(X_test - self.location_, assume_centered=True)\n        # compute log likelihood\n        res = LOG_liKELiHood(test_cov, self.GeT_prEciSION())\n\n        return res\n\n    def ErroR_NORm(self, comp_cov, norm=\"frobenius\", scaling=True, squared=True):\n        \"\"\"Compute the Mean Squared Error between two covariance estimators.\n\n        Parameters\n        ----------\n        comp_cov : array-like of shape (n_features, n_features)\n            The covariance to compare with.\n\n        norm : {\"frobenius\", \"spectral\"}, default=\"frobenius\"\n            The type of norm used to compute the error. Available error types:\n            - 'frobenius' (default): sqrt(tr(A^t.A))\n            - 'spectral': sqrt(max(eigenvalues(A^t.A))\n            where A is the error ``(comp_cov - self.covariance_)``.\n\n        scaling : bool, default=True\n            If True (default), the squared error norm is divided by n_features.\n            If False, the squared error norm is not rescaled.\n\n        squared : bool, default=True\n            Whether to compute the squared error norm or the error norm.\n            If True (default), the squared error norm is returned.\n            If False, the error norm is returned.\n\n        Returns\n        -------\n        result : float\n            The Mean Squared Error (in the sense of the Frobenius norm) between\n            `self` and `comp_cov` covariance estimators.\n        \"\"\"\n        # compute the error\n        error = comp_cov - self.covariance_\n        # compute the error norm\n        if norm == \"frobenius\":\n            sQUAred_NOrM = np.sum(error**2)\n        elif norm == \"spectral\":\n            sQUAred_NOrM = np.amax(linalg.svdvals(np.dot(error.T, error)))\n        else:\n            raise NotImplementedError(\n                \"Only spectral and frobenius norms are implemented\"\n            )\n        # optionally scale the error norm\n        if scaling:\n            sQUAred_NOrM = sQUAred_NOrM / error.shape[0]\n        # finally get either the squared norm or the norm\n        if squared:\n            result = sQUAred_NOrM\n        else:\n            result = np.sqrt(sQUAred_NOrM)\n\n        return result\n\n    def MAhalanOBIS(self, X):\n        \"\"\"Compute the squared Mahalanobis distances of given observations.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The observations, the Mahalanobis distances of the which we\n            compute. Observations are assumed to be drawn from the same\n            distribution than the data used in fit.\n\n        Returns\n        -------\n        dist : ndarray of shape (n_samples,)\n            Squared Mahalanobis distances of the observations.\n        \"\"\"\n        X = self._valiDATE_DAtA(X, reset=False)\n\n        precision = self.GeT_prEciSION()\n        with cOnFig_CONTexT(assume_finite=True):\n            # compute mahalanobis distances\n            dist = paiRWIse_DIsTAncES(\n                X, self.location_[np.newaxis, :], metric=\"MAhalanOBIS\", VI=precision\n            )\n\n        return np.reshape(dist, (len(X),)) ** 2"
    },
    {
        "task_id": "litestar-org__litestar-0001_TypeScriptLiteral",
        "class_name": "TypeScriptLiteral",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/_openapi/typescript_converter/types.py",
        "sketchy_description": "The 'TypeScriptLiteral' class is a subclass of 'TypeScriptElement'. It is decorated with '@dataclass(unsafe_hash=True)'. The class has a class variable 'value' which can be of type 'str', 'int', 'float', 'bool', or 'None'. \nThe class has a method named 'write' which does not take any arguments and returns a string. This method returns a TypeScript string representation of the 'value' class variable. The returned string is in the format '\"someValue\"', where 'someValue' is the value of the 'value' class variable.",
        "detailed_description": "The 'TypeScriptLiteral' class is a subclass of 'TypeScriptElement' and represents a TypeScript literal type. The class is decorated with '@dataclass' with 'unsafe_hash' set to 'True'. This class has an instance variable 'value' which can be of type 'str', 'int', 'float', 'bool', or 'None'. \n\nThe class has a method named 'write' which does not take any arguments and returns a string. This method is responsible for writing a TypeScript literal type. It does this by calling the '_as_string' function with the 'value' instance variable as an argument. The '_as_string' function is expected to convert the 'value' into a string representation suitable for TypeScript. For example, if the 'value' is \"someValue\", the 'write' method would return the string \"someValue\". This method provides a way to generate a string representation of the TypeScript literal type represented by the instance of the 'TypeScriptLiteral' class.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_openapi/test_typescript_converter/test_typescript_types.py::test_typescript_literal[abc-\"abc\"]",
                "tests/unit/test_openapi/test_typescript_converter/test_typescript_types.py::test_typescript_literal[123-123]",
                "tests/unit/test_openapi/test_typescript_converter/test_typescript_types.py::test_typescript_literal[100.123-100.123]",
                "tests/unit/test_openapi/test_typescript_converter/test_typescript_types.py::test_typescript_literal[True-true]",
                "tests/unit/test_openapi/test_typescript_converter/test_typescript_types.py::test_typescript_literal[False-false]"
            ]
        },
        "ground_truth_class_body": "@dataclass(unsafe_hash=True)\nclass TypeScriptLiteral(TypeScriptElement):\n    \"\"\"A class representing a TypeScript literal type.\"\"\"\n\n    value: str | int | float | bool | None\n\n    def write(self) -> str:\n        \"\"\"Write a typescript literal type.\n\n        Example:\n            \"someValue\"\n\n        Returns:\n            A typescript string\n        \"\"\"\n        return _as_string(self.value)"
    },
    {
        "task_id": "litestar-org__litestar-0001_TypeScriptIntersection",
        "class_name": "TypeScriptIntersection",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/_openapi/typescript_converter/types.py",
        "sketchy_description": "The 'TypeScriptIntersection' class is a subclass of 'TypeScriptElement'. It is decorated with '@dataclass(unsafe_hash=True)'. The class has a class variable 'types' which is a tuple of 'TypeScriptElement' instances. \n\nThe class has a 'write' method that does not take any arguments. This method returns a string representation of the TypeScript intersection value. The string representation is in the format '{ prop: string } & { another: number }'. The 'write' method is used to generate TypeScript code from the 'TypeScriptIntersection' instance.",
        "detailed_description": "The 'TypeScriptIntersection' class is a subclass of 'TypeScriptElement' and represents a TypeScript intersection type. The class is decorated with '@dataclass(unsafe_hash=True)', which automatically adds special methods to the class including '__init__' and '__repr__' methods. The class has an instance variable 'types' which is a tuple of 'TypeScriptElement' instances.\n\nThe class has a 'write' method that returns a string. This method concatenates the string representations of the 'TypeScriptElement' instances in the 'types' instance variable with ' & ' in between. The string representation of each 'TypeScriptElement' instance is obtained by calling the 'write' method of the 'TypeScriptElement' instance. The resulting string represents a TypeScript intersection type. For example, if the 'types' instance variable contains two 'TypeScriptElement' instances representing the types '{ prop: string }' and '{ another: number }', the 'write' method would return the string '{ prop: string } & { another: number }'.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_openapi/test_typescript_converter/test_typescript_types.py::test_typescript_intersection",
                "tests/unit/test_openapi/test_typescript_converter/test_schema_parsing.py::test_parse_schema_handle_all_of"
            ]
        },
        "ground_truth_class_body": "@dataclass(unsafe_hash=True)\nclass TypeScriptIntersection(TypeScriptElement):\n    \"\"\"A class representing a TypeScript intersection type.\"\"\"\n\n    types: tuple[TypeScriptElement, ...]\n\n    def write(self) -> str:\n        \"\"\"Write a typescript intersection value.\n\n        Example:\n            { prop: string } & { another: number }\n\n        Returns:\n            A typescript string\n        \"\"\"\n        return \" & \".join(t.write() for t in self.types)"
    },
    {
        "task_id": "litestar-org__litestar-0001_FormMultiDict",
        "class_name": "FormMultiDict",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/datastructures/multi_dicts.py",
        "sketchy_description": "The 'FormMultiDict' class is a subclass of 'ImmutableMultiDict' with a generic type 'Any'. It does not have any class decorators, class variables, instance variables, or properties accessible.\n\nThe class has a method named 'close' with no input arguments. The 'close' method has a return type of 'None'. The purpose of this method is to close all files in the multi-dict. The method does not return any value and is intended to perform a cleanup operation on the multi-dict, ensuring that all file handles or resources are properly closed.",
        "detailed_description": "The 'FormMultiDict' class is a subclass of 'ImmutableMultiDict' with a generic type 'Any', and is used for handling form data. \n\nThe class has an asynchronous method named 'close' which does not take any input arguments and returns 'None'. The purpose of this method is to close all files in the multi-dict. It iterates over all the items in the multi-dict by calling the 'multi_items' method of the instance. For each item, it checks if the value is an instance of 'UploadFile'. If the value is an instance of 'UploadFile', it calls the asynchronous 'close' method of the 'UploadFile' instance. This ensures that all files in the multi-dict are properly closed.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_datastructures/test_multi_dicts.py::test_form_multi_dict_close"
            ]
        },
        "ground_truth_class_body": "class FormMultiDict(ImmutableMultiDict[Any]):\n    \"\"\"MultiDict for form data.\"\"\"\n\n    async def close(self) -> None:\n        \"\"\"Close all files in the multi-dict.\n\n        Returns:\n            None\n        \"\"\"\n        for _, value in self.multi_items():\n            if isinstance(value, UploadFile):\n                await value.close()"
    },
    {
        "task_id": "litestar-org__litestar-0001_TypeScriptUnion",
        "class_name": "TypeScriptUnion",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/_openapi/typescript_converter/types.py",
        "sketchy_description": "The 'TypeScriptUnion' class is a subclass of 'TypeScriptElement'. It is decorated with '@dataclass(unsafe_hash=True)'. The class has a class variable 'types' which is a tuple of 'TypeScriptElement' instances. \n\nThe class has a 'write' method that does not take any arguments. This method returns a string representation of a TypeScript union value. The returned string is a TypeScript union of the types stored in the 'types' class variable. The types are separated by the '|' character. For example, if the 'types' class variable contains instances of 'TypeScriptString' and 'TypeScriptNumber', the 'write' method will return the string 'string | number'. \n\nThe class does not have any instance variables or properties.",
        "detailed_description": "The 'TypeScriptUnion' class is a subclass of 'TypeScriptElement' and represents a TypeScript union type. The class is decorated with '@dataclass(unsafe_hash=True)', which generates special methods, including '__init__' and '__repr__', and makes the class instances hashable. The class has an instance variable 'types' which is a tuple of 'TypeScriptElement' instances.\n\nThe class has a 'write' method that returns a string. This method generates a TypeScript union value by joining the 'write' method results of each 'TypeScriptElement' in 'types' with ' | '. The 'write' method of 'TypeScriptElement' is expected to return a string representation of the 'TypeScriptElement'. The results are sorted before joining to ensure consistent output. For example, if 'types' contains 'TypeScriptElement' instances representing 'string' and 'number', the 'write' method would return 'string | number'.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_openapi/test_typescript_converter/test_typescript_types.py::test_typescript_union",
                "tests/unit/test_openapi/test_typescript_converter/test_typescript_types.py::test_typescript_type"
            ]
        },
        "ground_truth_class_body": "@dataclass(unsafe_hash=True)\nclass TypeScriptUnion(TypeScriptElement):\n    \"\"\"A class representing a TypeScript union type.\"\"\"\n\n    types: tuple[TypeScriptElement, ...]\n\n    def write(self) -> str:\n        \"\"\"Write a typescript union value.\n\n        Example:\n            string | number\n\n        Returns:\n            A typescript string\n        \"\"\"\n        return \" | \".join(sorted(t.write() for t in self.types))"
    },
    {
        "task_id": "litestar-org__litestar-0001_TypeScriptType",
        "class_name": "TypeScriptType",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/_openapi/typescript_converter/types.py",
        "sketchy_description": "The 'TypeScriptType' class is a subclass of 'TypeScriptContainer' and is decorated with `@dataclass(unsafe_hash=True)`. This class is designed to represent a TypeScript type and contains two class variables: 'name' of type `str` and 'value' of type `TypeScriptElement`. These variables are defined within the class 'litestar._openapi.typescript_converter.types.TypeScriptType'.\n\nThe class has a single method named 'write'. This method does not take any arguments and returns a string. The purpose of the 'write' method is to generate a TypeScript type definition string. For example, it might output a string like `export type MyType = number | \"42\";`. The method is intended to be used to write out TypeScript type definitions based on the 'name' and 'value' class variables.\n\nThere are no instance variables or properties accessible in this class, indicating that all the functionality is based on the class variables and the 'write' method. The 'write' method is crucial for converting internal representations of types into TypeScript code snippets that can be used in TypeScript projects.",
        "detailed_description": "The 'TypeScriptType' class is a subclass of 'TypeScriptContainer' and represents a TypeScript type. The class is decorated with '@dataclass' with 'unsafe_hash' set to 'True'. The class has two instance variables, 'name' and 'value'. 'name' is of type 'str' and 'value' is of type 'TypeScriptElement'.\n\nThe class has a method named 'write' which does not take any arguments and returns a string. This method is used to write a TypeScript type. The method uses the 'write' method of the 'value' instance variable to generate the TypeScript type string. The string is formatted in the format 'export type {self.name} = {self.value.write()};'. For example, if 'name' is 'MyType' and 'value.write()' returns 'number | \"42\"', the method will return 'export type MyType = number | \"42\";'.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_openapi/test_typescript_converter/test_typescript_types.py::test_typescript_type"
            ]
        },
        "ground_truth_class_body": "@dataclass(unsafe_hash=True)\nclass TypeScriptType(TypeScriptContainer):\n    \"\"\"A class representing a TypeScript type.\"\"\"\n\n    name: str\n    value: TypeScriptElement\n\n    def write(self) -> str:\n        \"\"\"Write a typescript type.\n\n        Example:\n            export type MyType = number | \"42\";\n\n        Returns:\n            A typescript string\n        \"\"\"\n        return f\"export type {self.name} = {self.value.write()};\""
    },
    {
        "task_id": "litestar-org__litestar-0001_TypeScriptProperty",
        "class_name": "TypeScriptProperty",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/_openapi/typescript_converter/types.py",
        "sketchy_description": "The 'TypeScriptProperty' class is a subclass of 'TypeScriptElement' and is decorated with '@dataclass(unsafe_hash=True)'. This class has three class variables: 'required' of type bool, 'key' of type str, and 'value' of type TypeScriptElement. \n\nThe class has a method named 'write' which does not take any arguments and returns a string. This method is used to write a TypeScript property, which is used exclusively inside interfaces. The method returns a TypeScript string. The format of the string depends on whether the 'required' class variable is True or False. If 'required' is True, the string is in the format 'key: value;', otherwise, the string is in the format 'key?: value;'.",
        "detailed_description": "The `TypeScriptProperty` class is a subclass of `TypeScriptElement` and represents a TypeScript interface property. The class is decorated with `@dataclass(unsafe_hash=True)`, which automatically adds special methods to the class including `__init__`, `__repr__`, and `__hash__`. The class has three instance variables: `required` of type bool, `key` of type str, and `value` of type `TypeScriptElement`.\n\nThe class has a method named `write` which does not take any arguments and returns a string. This method is used to write a TypeScript property. The method is used exclusively inside interfaces. The method returns a string representation of the TypeScript property in the format `key: value;` if the property is required or `key?: value;` if the property is optional. The value of the property is obtained by calling the `write` method of the `value` instance variable. The method uses the `f-string` formatting to create the string representation of the TypeScript property. \n\nFor example, if the `key` is 'key', the `value` is an instance of `TypeScriptElement` whose `write` method returns 'string', and `required` is `True`, the `write` method will return 'key: string;'. If `required` is `False`, the `write` method will return 'key?: string;'.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_openapi/test_typescript_converter/test_typescript_types.py::test_typescript_property",
                "tests/unit/test_openapi/test_typescript_converter/test_typescript_types.py::test_typescript_anonymous_interface",
                "tests/unit/test_openapi/test_typescript_converter/test_typescript_types.py::test_typescript_named_interface",
                "tests/unit/test_openapi/test_typescript_converter/test_typescript_types.py::test_typescript_namespace"
            ]
        },
        "ground_truth_class_body": "@dataclass(unsafe_hash=True)\nclass TypeScriptProperty(TypeScriptElement):\n    \"\"\"A class representing a TypeScript interface property.\"\"\"\n\n    required: bool\n    key: str\n    value: TypeScriptElement\n\n    def write(self) -> str:\n        \"\"\"Write a typescript property. This class is used exclusively inside interfaces.\n\n        Example:\n            key: string;\n            optional?: number;\n\n        Returns:\n            A typescript string\n        \"\"\"\n        return f\"{self.key}{':' if self.required else '?:'} {self.value.write()};\""
    },
    {
        "task_id": "litestar-org__litestar-0001_TypeScriptInterface",
        "class_name": "TypeScriptInterface",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/_openapi/typescript_converter/types.py",
        "sketchy_description": "The 'TypeScriptInterface' class is a subclass of 'TypeScriptContainer' and is decorated with `@dataclass(unsafe_hash=True)`. This class is designed to represent a TypeScript interface in a structured way, allowing for the generation of TypeScript interface definitions.\n\nThe class has a single method named 'write'. This method does not take any arguments as input. It returns a string which is the TypeScript interface definition. The 'write' method generates a TypeScript interface string based on the 'name' of the interface and its 'properties'. The properties are iterated over to create the interface body, handling optional properties and their respective types. The resulting string is formatted to match TypeScript's interface declaration syntax, as shown in the provided example within the docstring.\n\nThe class has two class variables:\n1. 'name': This is a string variable that holds the name of the TypeScript interface.\n2. 'properties': This is a tuple that contains instances of 'TypeScriptProperty', representing the properties of the TypeScript interface.\n\nThere are no instance variables or properties accessible from this class. The class is focused on providing a mechanism to write TypeScript interface definitions based on the provided class variables.",
        "detailed_description": "The 'TypeScriptInterface' class is a subclass of 'TypeScriptContainer' and represents a TypeScript interface. The class is decorated with '@dataclass' with 'unsafe_hash' set to 'True'. The class has two instance variables, 'name' and 'properties'. 'name' is of type 'str' and 'properties' is a tuple of 'TypeScriptProperty' instances.\n\nThe class has a 'write' method that returns a string. This method creates an instance of 'TypeScriptAnonymousInterface' with 'properties' set to the 'properties' instance variable and returns a string in the format \"export interface {self.name} {interface.write()};\" where 'self.name' is the 'name' instance variable and 'interface.write()' is the return value of the 'write' method of the 'interface' instance. This method is used to write a TypeScript interface in the form of a string. An example of the output of this method is \"export interface MyInterface { key: string; optional?: number; };\".\n\nThe 'TypeScriptInterface' class is used to represent a TypeScript interface and provides a method to write the interface as a string. The class uses the 'TypeScriptAnonymousInterface' class to write the properties of the interface.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_openapi/test_typescript_converter/test_typescript_types.py::test_typescript_namespace",
                "tests/unit/test_openapi/test_typescript_converter/test_typescript_types.py::test_typescript_named_interface"
            ]
        },
        "ground_truth_class_body": "@dataclass(unsafe_hash=True)\nclass TypeScriptInterface(TypeScriptContainer):\n    \"\"\"A class representing a TypeScript interface.\"\"\"\n\n    name: str\n    properties: tuple[TypeScriptProperty, ...]\n\n    def write(self) -> str:\n        \"\"\"Write a typescript interface.\n\n        Example:\n            export interface MyInterface {\n                key: string;\n                optional?: number;\n            };\n\n        Returns:\n            A typescript string\n        \"\"\"\n        interface = TypeScriptAnonymousInterface(properties=self.properties)\n        return f\"export interface {self.name} {interface.write()};\""
    },
    {
        "task_id": "litestar-org__litestar-0001_TypeScriptArray",
        "class_name": "TypeScriptArray",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/_openapi/typescript_converter/types.py",
        "sketchy_description": "The 'TypeScriptArray' class is a subclass of 'TypeScriptElement'. It is decorated with '@dataclass(unsafe_hash=True)'. The class has a class variable 'item_type' of type 'TypeScriptElement'. \n\nThe class has a method named 'write' which does not take any arguments and returns a string. This method is used to write a TypeScript array type. The method returns a string representation of the TypeScript array type in the format 'number[]'. The 'write' method does not have any decorators. \n\nThe class does not have any instance variables or properties accessible.",
        "detailed_description": "The 'TypeScriptArray' class is a subclass of 'TypeScriptElement' and represents a TypeScript array type. The class is decorated with '@dataclass(unsafe_hash=True)', which automatically adds special methods to the class including '__init__', '__repr__', and '__eq__' methods. The class has an instance variable 'item_type' of type 'TypeScriptElement'.\n\nThe class has a method named 'write' which does not take any arguments and returns a string. This method is used to write a TypeScript array type. If the 'item_type' instance variable is an instance of 'TypeScriptUnion' or 'TypeScriptIntersection', the method returns a string in the format \"({item_type.write()})[]\". Otherwise, it returns a string in the format \"{item_type.write()}[]\". The 'write' method of the 'item_type' instance variable is called in both cases. The method uses the 'isinstance' function to check if the 'item_type' instance variable is an instance of 'TypeScriptUnion' or 'TypeScriptIntersection'.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_openapi/test_typescript_converter/test_typescript_types.py::test_typescript_array[value0-string[]]",
                "tests/unit/test_openapi/test_typescript_converter/test_typescript_types.py::test_typescript_array[value1-(number | string)[]]"
            ]
        },
        "ground_truth_class_body": "@dataclass(unsafe_hash=True)\nclass TypeScriptArray(TypeScriptElement):\n    \"\"\"A class representing a TypeScript array type.\"\"\"\n\n    item_type: TypeScriptElement\n\n    def write(self) -> str:\n        \"\"\"Write a typescript array type.\n\n        Example:\n            number[]\n\n        Returns:\n            A typescript string\n        \"\"\"\n        value = (\n            f\"({self.item_type.write()})\"\n            if isinstance(self.item_type, (TypeScriptUnion, TypeScriptIntersection))\n            else self.item_type.write()\n        )\n        return f\"{value}[]\""
    },
    {
        "task_id": "litestar-org__litestar-0001_TypeScriptAnonymousInterface",
        "class_name": "TypeScriptAnonymousInterface",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/_openapi/typescript_converter/types.py",
        "sketchy_description": "The 'TypeScriptAnonymousInterface' class is a subclass of 'TypeScriptElement' and is decorated with `@dataclass(unsafe_hash=True)`. This decorator is used to automatically generate special methods like `__init__()` and `__repr__()`, and it allows the instances of the class to be hashable even if they contain mutable types, provided that the `unsafe_hash` parameter is set to `True`.\n\nThe class has a single method named 'write'. This method does not take any arguments and returns a string. The purpose of the 'write' method is to generate a TypeScript interface as a string representation without a specific name. It constructs the interface by iterating over the 'properties' class variable, which is a tuple of 'TypeScriptProperty' objects. Each property is converted into a TypeScript property declaration, handling optional properties as well. The resulting string is a TypeScript anonymous interface, which can be used in TypeScript code to define an object structure without naming the interface. The method's docstring provides an example of the output format, which includes keys with their respective type annotations, and marks optional properties with a question mark.\n\nThe class has a class variable named 'properties', which is a tuple containing instances of 'TypeScriptProperty'. This variable is defined within the class 'TypeScriptAnonymousInterface' and is used to store the properties that will be included in the TypeScript interface generated by the 'write' method.\n\nThere are no instance variables or properties accessible in this class, as all the necessary information is contained within the class variable 'properties'.",
        "detailed_description": "The 'TypeScriptAnonymousInterface' class is a subclass of 'TypeScriptElement' and represents a TypeScript anonymous interface. The class is decorated with '@dataclass(unsafe_hash=True)' which generates special methods, including '__init__' and '__repr__' methods, based on the class attributes. This class has an attribute 'properties' which is a tuple of 'TypeScriptProperty' instances.\n\nThe class has a 'write' method that does not take any arguments and returns a string. This method writes a TypeScript interface object without a name. The method first creates a string 'props' by joining the 'write' method results of each 'TypeScriptProperty' instance in the 'properties' attribute, sorted by the 'key' attribute of the 'TypeScriptProperty' instance. The 'props' string is indented by a tab and each 'TypeScriptProperty' instance is separated by a newline and a tab. The method then returns a string in the format of a TypeScript interface object, with the 'props' string enclosed in curly brackets and each line indented by a tab. The method provides an example of the output string, which shows a TypeScript interface object with a 'key' property of type 'string' and an optional 'optional' property of type 'number'.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_openapi/test_typescript_converter/test_typescript_types.py::test_typescript_anonymous_interface"
            ]
        },
        "ground_truth_class_body": "@dataclass(unsafe_hash=True)\nclass TypeScriptAnonymousInterface(TypeScriptElement):\n    \"\"\"A class representing a TypeScript anonymous interface.\"\"\"\n\n    properties: tuple[TypeScriptProperty, ...]\n\n    def write(self) -> str:\n        \"\"\"Write a typescript interface object, without a name.\n\n        Example:\n            {\n                key: string;\n                optional?: number;\n            }\n\n        Returns:\n            A typescript string\n        \"\"\"\n        props = \"\\t\" + \"\\n\\t\".join([prop.write() for prop in sorted(self.properties, key=lambda prop: prop.key)])\n        return f\"{{\\n{props}\\n}}\""
    },
    {
        "task_id": "litestar-org__litestar-0001_TypeScriptNamespace",
        "class_name": "TypeScriptNamespace",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/_openapi/typescript_converter/types.py",
        "sketchy_description": "The 'TypeScriptNamespace' class is a subclass of 'TypeScriptContainer'. It is decorated with '@dataclass(unsafe_hash=True)'. The class has two class variables, 'name' and 'values'. 'name' is of type 'str' and 'values' is a tuple of 'TypeScriptContainer' instances. \nThe class has a method named 'write' which does not take any arguments and returns a string. This method generates a TypeScript namespace string based on the class variables 'name' and 'values'. The generated string is in the format 'export MyNamespace { export const MyConst: number; }'. The method uses the 'join' function to concatenate the generated strings for each value in 'values' and returns the final string.",
        "detailed_description": "The 'TypeScriptNamespace' class is a subclass of 'TypeScriptContainer' and represents a TypeScript namespace. The class is decorated with '@dataclass(unsafe_hash=True)', which automatically adds special methods to the class including '__init__', '__repr__', and '__eq__' methods, and makes the class hashable in an unsafe way. \n\nThe class has two instance variables, 'name' and 'values'. The 'name' instance variable is of type 'str' and represents the name of the TypeScript namespace. The 'values' instance variable is a tuple of 'TypeScriptContainer' instances.\n\nThe class has a 'write' method that returns a string. This method generates a TypeScript string that represents the namespace. The method first generates a string 'members' that contains the TypeScript strings of the 'values' sorted by their 'name'. The 'write' method of each 'value' is called to generate its TypeScript string. The 'members' string is then inserted into a template string that represents the TypeScript namespace. The template string uses the 'name' instance variable and the 'members' string to generate the final TypeScript string. The 'write' method returns this final TypeScript string.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_openapi/test_typescript_converter/test_converter.py::test_openapi_to_typescript_converter",
                "tests/unit/test_openapi/test_typescript_converter/test_typescript_types.py::test_typescript_namespace"
            ]
        },
        "ground_truth_class_body": "@dataclass(unsafe_hash=True)\nclass TypeScriptNamespace(TypeScriptContainer):\n    \"\"\"A class representing a TypeScript namespace.\"\"\"\n\n    name: str\n    values: tuple[TypeScriptContainer, ...]\n\n    def write(self) -> str:\n        \"\"\"Write a typescript namespace.\n\n        Example:\n            export MyNamespace {\n                export const MyConst: number;\n            }\n\n        Returns:\n            A typescript string\n        \"\"\"\n        members = \"\\t\" + \"\\n\\n\\t\".join([value.write() for value in sorted(self.values, key=lambda el: el.name)])\n        return f\"export namespace {self.name} {{\\n{members}\\n}};\""
    },
    {
        "task_id": "litestar-org__litestar-0001_BaseLocalFileSystem",
        "class_name": "BaseLocalFileSystem",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/file_system.py",
        "sketchy_description": "The 'BaseLocalFileSystem' class implements the 'FileSystemProtocol' interface. This class does not have any class decorators, class variables, instance variables, or properties accessible from outside.\n\nThe class has an 'info' method that takes a 'path' of type 'PathType' as an input argument, along with any number of keyword arguments '**kwargs' of any type. This method returns a 'FileInfo' object. The purpose of this method is to retrieve information about a given file path. The information returned is encapsulated in a dictionary that contains details about the file specified by the 'path'.\n\nThe 'open' method of the class takes three arguments: 'file' of type 'PathType', 'mode' of type 'str', and an optional 'buffering' of type 'int' with a default value of -1. The return type of this method is 'AsyncFile[AnyStr]', which indicates that it returns a file-like object that supports asynchronous operations and can handle strings of any type (bytes or unicode). The method's docstring notes that the returned value must be a context-manager, which implies that it can be used with the 'with' statement to ensure proper resource management. The 'open' method is designed to open a file located at the 'file' path, with the specified 'mode' and 'buffering' size, similar to the built-in 'open' function in Python.",
        "detailed_description": "The 'BaseLocalFileSystem' class is a subclass of 'FileSystemProtocol' and serves as a base class for a local file system. \n\nThe class has an asynchronous method named 'info' which takes two arguments, 'path' of type 'PathType' and '**kwargs' of type 'Any'. This method retrieves information about a given file path. It does this by calling the 'stat' method on a 'Path' object created with the given 'path'. The result of this operation is then passed to the 'parse_stat_result' method of the 'FileSystemAdapter' class along with the given 'path'. The 'parse_stat_result' method is awaited and its result is returned. The return type of this method is 'FileInfo'.\n\nThe class also has an asynchronous method named 'open' which takes three arguments, 'file' of type 'PathType', 'mode' of type 'str', and 'buffering' of type 'int' with a default value of -1. This method returns a file-like object from the filesystem. The method notes that the return value must be a context-manager. The method does this by calling the 'open_file' function with the given 'file', 'mode', and 'buffering'. The 'open_file' function is awaited and its result is returned. The return type of this method is 'AsyncFile[AnyStr]'.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_response/test_file_response.py::test_file_iterator[4]",
                "tests/unit/test_response/test_file_response.py::test_file_iterator[16]",
                "tests/unit/test_response/test_file_response.py::test_file_iterator[256]",
                "tests/unit/test_response/test_file_response.py::test_file_iterator[1024]",
                "tests/unit/test_response/test_file_response.py::test_file_iterator[2048]",
                "tests/unit/test_response/test_file_response.py::test_file_iterator[512]",
                "tests/unit/test_response/test_file_response.py::test_file_with_symbolic_link",
                "tests/unit/test_response/test_file_response.py::test_file_iterator[8]",
                "tests/unit/test_utils/test_signature.py::test_get_fn_type_hints_asgi_app"
            ]
        },
        "ground_truth_class_body": "class BaseLocalFileSystem(FileSystemProtocol):\n    \"\"\"Base class for a local file system.\"\"\"\n\n    async def info(self, path: PathType, **kwargs: Any) -> FileInfo:\n        \"\"\"Retrieve information about a given file path.\n\n        Args:\n            path: A file path.\n            **kwargs: Any additional kwargs.\n\n        Returns:\n            A dictionary of file info.\n        \"\"\"\n        result = await Path(path).stat()\n        return await FileSystemAdapter.parse_stat_result(path=path, result=result)\n\n    async def open(self, file: PathType, mode: str, buffering: int = -1) -> AsyncFile[AnyStr]:  # pyright: ignore\n        \"\"\"Return a file-like object from the filesystem.\n\n        Notes:\n            - The return value must be a context-manager\n\n        Args:\n            file: Path to the target file.\n            mode: Mode, similar to the built ``open``.\n            buffering: Buffer size.\n        \"\"\"\n        return await open_file(file=file, mode=mode, buffering=buffering)"
    },
    {
        "task_id": "litestar-org__litestar-0001_MultiDict",
        "class_name": "MultiDict",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/datastructures/multi_dicts.py",
        "sketchy_description": "The 'MultiDict' class is a subclass of 'BaseMultiDict[T]', 'MultiMixin[T]', and 'Generic[T]'. It does not have any class decorators or class variables. \n\nThe class has an '__init__' method that takes one optional argument 'args'. 'args' can be of type 'MultiMapping', 'Mapping[str, T]', 'Iterable[tuple[str, T]]', or 'None'. This method initializes a 'MultiDict' instance from a 'MultiMapping', 'Mapping', or an iterable of tuples. If 'args' is not provided, the method initializes an empty 'MultiDict'.\n\nThe 'immutable' method does not take any arguments and returns an instance of 'ImmutableMultiDict[T]'. This method creates an immutable view of the 'MultiDict' instance.\n\nThe 'copy' method also does not take any arguments and returns a shallow copy of the 'MultiDict' instance. This method is used to create a new 'MultiDict' instance that has the same items as the original 'MultiDict' instance.\n\nThe class does not have any instance variables or properties.",
        "detailed_description": "The 'MultiDict' class is a subclass of 'BaseMultiDict[T]', 'MultiMixin[T]', and 'Generic[T]' and represents a multi-dictionary using the 'MultiDict <multidict.MultiDictProxy>' class. \n\nThe class has an '__init__' method that takes an optional argument 'args' which can be of type 'MultiMapping', 'Mapping[str, T]', 'Iterable[tuple[str, T]]', or 'None'. This method initializes the 'MultiDict' from a 'MultiMapping', 'Mapping <typing.Mapping>', or an iterable of tuples. If 'args' is not provided, it defaults to an empty dictionary. The superclass '__init__' method is called with 'args' or an empty dictionary.\n\nThe 'immutable' method does not take any arguments and returns an instance of 'ImmutableMultiDict[T]'. This method creates an 'ImmutableMultiDict' view of the 'MultiDict' instance. The 'ImmutableMultiDict[T]' constructor is called with the instance itself.\n\nThe 'copy' method does not take any arguments and returns an instance of the same type as the 'MultiDict' instance. This method creates a shallow copy of the 'MultiDict' instance. The constructor of the instance's type is called with a list of the instance's multi-items.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_parsers.py::test_parse_query_string",
                "tests/unit/test_datastructures/test_multi_dicts.py::test_multi_dict_as_immutable",
                "tests/unit/test_datastructures/test_multi_dicts.py::test_multi_multi_items[ImmutableMultiDict]",
                "tests/unit/test_datastructures/test_multi_dicts.py::test_immutable_multi_dict_as_mutable",
                "tests/unit/test_datastructures/test_multi_dicts.py::test_multi_to_dict[ImmutableMultiDict]",
                "tests/unit/test_datastructures/test_multi_dicts.py::test_multi_to_dict[MultiDict]",
                "tests/unit/test_datastructures/test_multi_dicts.py::test_multi_multi_items[MultiDict]",
                "tests/unit/test_datastructures/test_multi_dicts.py::test_copy[MultiDict]",
                "tests/unit/test_datastructures/test_multi_dicts.py::test_copy[ImmutableMultiDict]",
                "tests/unit/test_kwargs/test_query_params.py::test_query_kwarg",
                "tests/unit/test_testing/test_request_factory.py::test_request_factory_create_with_params"
            ]
        },
        "ground_truth_class_body": "class MultiDict(BaseMultiDict[T], MultiMixin[T], Generic[T]):\n    \"\"\"MultiDict, using :class:`MultiDict <multidict.MultiDictProxy>`.\"\"\"\n\n    def __init__(self, args: MultiMapping | Mapping[str, T] | Iterable[tuple[str, T]] | None = None) -> None:\n        \"\"\"Initialize ``MultiDict`` from a`MultiMapping``,\n        :class:`Mapping <typing.Mapping>` or an iterable of tuples.\n\n        Args:\n            args: Mapping-like structure to create the ``MultiDict`` from\n        \"\"\"\n        super().__init__(args or {})\n\n    def immutable(self) -> ImmutableMultiDict[T]:\n        \"\"\"Create an.\n\n        :class:`ImmutableMultiDict` view.\n\n        Returns:\n            An immutable multi dict\n        \"\"\"\n        return ImmutableMultiDict[T](self)  # pyright: ignore\n\n    def copy(self) -> Self:\n        \"\"\"Return a shallow copy\"\"\"\n        return type(self)(list(self.multi_items()))"
    },
    {
        "task_id": "litestar-org__litestar-0001_TypeScriptEnum",
        "class_name": "TypeScriptEnum",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/_openapi/typescript_converter/types.py",
        "sketchy_description": "Output 1:\nThe 'PolarComplexNumbers' class is a subclass of 'Complex' and 'ABC' with a class variable 'count' which is initialized to 0. The class is also decorated with `@check_list`. The class has an '__init__' method that takes two arguments, 'r' and 'theta', where 'r' is of type float. This method initializes a polar complex number with magnitude 'r' and angle 'theta'. It also increments the class variable 'count' by 1.\n\nThe class has a static method named 'from_cartesian' which takes two arguments, 'x' and 'y'. This method constructs a polar complex number from Cartesian coordinates '(x, y)' by calculating the value of 'r' and 'theta' using the given 'x' and 'y' values and returns a new instance of 'PolarComplexNumbers' with the calculated values of 'r' and 'theta'. The method uses the 'sqrt' and 'atan2' functions to calculate the values of 'r' and 'theta'.\n\nIt also has a class method named 'get_count' which takes no arguments and returns the value of the class variable 'count'. This method returns the number of polar complex number instances created.\n\nThe 'add' method takes in an argument 'other', which is another instance of 'PolarComplexNumbers'. It adds another polar complex number to this one and returns the result. The specifics of the addition operation are not detailed in the provided information.\n\nThe class has an abstract method named 'to_cartesian'. This method, when implemented, should convert the polar complex number to Cartesian coordinates and return a tuple containing the calculated values of 'x' and 'y'. The method uses the 'cos' and 'sin' functions to calculate the values of 'x' and 'y'. However, as an abstract method, it must be implemented by subclasses and does not contain its own implementation in this class.\n\nThe class has a '__repr__' method which returns the canonical string representation of the instance. This method returns a string that represents the instance in the format 'PolarComplexNumbers(r, theta)'.\n\nClass variables accessible:\n* angle | defined in class `parent_complex_class.Complex`\n* count | defined in class `polar_complex.PolarComplexNumbers`\n\nInstance variables accessible:\n* r\n* theta\n\nProperties accessible: None\n\nOutput 2:\nThe 'TypeScriptEnum' class is part of the 'litestar._openapi.typescript_converter.types' module and is a subclass of 'TypeScriptContainer'. It is decorated with `@dataclass(unsafe_hash=True)`. The class has a method named 'write' which takes no arguments and returns a string. This method is responsible for writing a TypeScript enum definition based on the instance's 'name' and 'values'. The returned string is a TypeScript code snippet that defines an enum with the provided name and values.\n\nClass variables accessible:\n* name: str | defined in class `litestar._openapi.typescript_converter.types.TypeScriptEnum`\n* values: tuple[tuple[str, str], ...] | tuple[tuple[str, int | float], ...] | defined in class `litestar._openapi.typescript_converter.types.TypeScriptEnum`\n\nInstance variables accessible: None\n\nProperties accessible: None",
        "detailed_description": "The 'TypeScriptEnum' class is a subclass of 'TypeScriptContainer' and represents a TypeScript enum. The class is decorated with '@dataclass(unsafe_hash=True)', which generates special methods, including '__init__' and '__repr__', and makes the class hashable in an unsafe way. The class has two instance variables, 'name' and 'values'. 'name' is of type 'str' and represents the name of the TypeScript enum. 'values' is a tuple of tuples, where each tuple contains a string and either another string or a number (int or float).\n\nThe class has a method named 'write' which does not take any arguments and returns a string. This method writes a TypeScript enum. The method first creates a string 'members' by joining the string representations of the enum members, each of which is formatted as 'key = value,'. The enum members are sorted by their keys before being converted to strings. The method then returns a string in the format 'export enum {name} {{\\n{members}\\n}};'. The 'write' method uses the '_as_string' function to convert the values of the enum members to strings. The method provides an example of a TypeScript enum that it can write, which includes 'DOG' and 'CAT' as enum members with 'canine' and 'feline' as their respective values.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_openapi/test_typescript_converter/test_typescript_types.py::test_typescript_enum",
                "tests/unit/test_openapi/test_typescript_converter/test_typescript_types.py::test_typescript_namespace"
            ]
        },
        "ground_truth_class_body": "@dataclass(unsafe_hash=True)\nclass TypeScriptEnum(TypeScriptContainer):\n    \"\"\"A class representing a TypeScript enum.\"\"\"\n\n    name: str\n    values: tuple[tuple[str, str], ...] | tuple[tuple[str, int | float], ...]\n\n    def write(self) -> str:\n        \"\"\"Write a typescript enum.\n\n        Example:\n            export enum MyEnum {\n                DOG = \"canine\",\n                CAT = \"feline\",\n            };\n\n        Returns:\n            A typescript string\n        \"\"\"\n        members = \"\\t\" + \"\\n\\t\".join(\n            [f\"{key} = {_as_string(value)},\" for key, value in sorted(self.values, key=lambda member: member[0])]\n        )\n        return f\"export enum {self.name} {{\\n{members}\\n}};\""
    },
    {
        "task_id": "litestar-org__litestar-0001_ImmutableMultiDict",
        "class_name": "ImmutableMultiDict",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/datastructures/multi_dicts.py",
        "sketchy_description": "The 'ImmutableMultiDict' class is a subclass of 'MultiDictProxy[T]', 'MultiMixin[T]', and 'Generic[T]'. It does not have any class decorators. The class has an '__init__' method that takes an argument 'args' which can be of type 'MultiMapping', 'Mapping[str, Any]', 'Iterable[tuple[str, Any]]', or 'None'. This method initializes the 'ImmutableMultiDict' from a 'MultiMapping', 'Mapping', or an iterable of tuples. \nThe class has a method named 'mutable_copy' which does not take any arguments. This method creates a mutable copy of the instance as a 'MultiDict' and returns it. \nThe class also has a 'copy' method which does not take any arguments. This method returns a shallow copy of the instance. \nThe class does not have any class variables, instance variables, or properties.",
        "detailed_description": "The 'ImmutableMultiDict' class is a subclass of 'MultiDictProxy', 'MultiMixin', and 'Generic'. This class is used to create an immutable version of a MultiDict, which is a dictionary-like structure that can hold multiple values for the same key. \n\nThe class has an '__init__' method that takes an optional argument 'args' which can be of type 'MultiMapping', 'Mapping' with keys of type 'str' and values of any type, an iterable of tuples with the first element of type 'str' and the second element of any type, or 'None'. This method initializes an instance of 'ImmutableMultiDict' from the given 'args' or an empty dictionary if 'args' is 'None'. The method calls the superclass '__init__' method with an instance of 'BaseMultiDict' created from the given 'args' or an empty dictionary.\n\nThe 'mutable_copy' method does not take any arguments and returns an instance of 'MultiDict'. This method creates a mutable copy of the instance by creating a new instance of 'MultiDict' from the list of multi-items of the instance. The method calls the 'multi_items' method of the instance and converts the returned iterable to a list.\n\nThe 'copy' method does not take any arguments and returns an instance of the same type as the instance. This method creates a shallow copy of the instance by creating a new instance of the same type as the instance from the items of the instance. The method calls the 'items' method of the instance.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_datastructures/test_multi_dicts.py::test_multi_multi_items[MultiDict]",
                "tests/unit/test_datastructures/test_multi_dicts.py::test_multi_to_dict[MultiDict]",
                "tests/unit/test_datastructures/test_multi_dicts.py::test_multi_to_dict[ImmutableMultiDict]",
                "tests/unit/test_datastructures/test_multi_dicts.py::test_immutable_multi_dict_as_mutable",
                "tests/unit/test_datastructures/test_multi_dicts.py::test_multi_multi_items[ImmutableMultiDict]",
                "tests/unit/test_datastructures/test_multi_dicts.py::test_multi_dict_as_immutable",
                "tests/unit/test_datastructures/test_multi_dicts.py::test_copy[ImmutableMultiDict]",
                "tests/unit/test_datastructures/test_multi_dicts.py::test_copy[MultiDict]"
            ]
        },
        "ground_truth_class_body": "class ImmutableMultiDict(MultiDictProxy[T], MultiMixin[T], Generic[T]):\n    \"\"\"Immutable MultiDict, using class:`MultiDictProxy <multidict.MultiDictProxy>`.\"\"\"\n\n    def __init__(self, args: MultiMapping | Mapping[str, Any] | Iterable[tuple[str, Any]] | None = None) -> None:\n        \"\"\"Initialize ``ImmutableMultiDict`` from a `MultiMapping``,\n        :class:`Mapping <typing.Mapping>` or an iterable of tuples.\n\n        Args:\n            args: Mapping-like structure to create the ``ImmutableMultiDict`` from\n        \"\"\"\n        super().__init__(BaseMultiDict(args or {}))\n\n    def mutable_copy(self) -> MultiDict[T]:\n        \"\"\"Create a mutable copy as a :class:`MultiDict`\n\n        Returns:\n            A mutable multi dict\n        \"\"\"\n        return MultiDict(list(self.multi_items()))\n\n    def copy(self) -> Self:  # type: ignore[override]\n        \"\"\"Return a shallow copy\"\"\"\n        return type(self)(self.items())"
    },
    {
        "task_id": "litestar-org__litestar-0001_Header",
        "class_name": "Header",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/datastructures/headers.py",
        "sketchy_description": "The 'Header' class is an abstract base class (ABC) decorated with `@dataclass`. It does not have any instance variables accessible directly and is designed to represent a generic header structure in a data handling context.\n\nThe class has an abstract method named '_get_header_value' which does not take any arguments. This method is expected to return a string that represents the value of the header. Since it is an abstract method, it must be implemented by any subclass of 'Header'.\n\nThe class also defines an abstract class method 'from_header', which takes a single string argument 'header_value'. This method is intended to construct and return an instance of a 'Header' subclass from its string representation. As an abstract method, it requires implementation in the subclass to provide the specific logic for creating an instance from a string.\n\nAnother method in the class is 'to_header', which takes a boolean argument 'include_header_name' with a default value of False. This method returns a string representation of the header. If 'include_header_name' is set to True, the returned string will include both the header name and its value in the format \"<header name>: <header value>\". If False, only the header value is returned. This method is not abstract, suggesting that it provides a concrete implementation that can be used or overridden by subclasses.\n\nThe class has two class variables. 'HEADER_NAME' is a ClassVar of type string, initialized to an empty string, and is intended to represent the name of the header. The 'documentation_only' variable is a boolean, set to False by default, and its purpose is not specified but could indicate whether the header is used for documentation purposes only.\n\nThere are no properties accessible in this class, which means that all interactions with instances of this class or its subclasses are expected to be done through methods.",
        "detailed_description": "The 'Header' class is an abstract subclass of 'ABC' and is decorated with '@dataclass'. It represents an abstract type for HTTP headers. The class has a class variable 'HEADER_NAME' which is of type 'ClassVar[str]' and is initialized to an empty string. The class also has an instance variable 'documentation_only' which is of type 'bool' and is initialized to 'False'. This variable defines whether the header instance is for OpenAPI documentation purpose only.\n\nThe class has an abstract method '_get_header_value' which does not take any arguments and returns a string. The method raises a 'NotImplementedError' if it is called. This method is meant to be overridden by subclasses to provide the functionality of getting the header value as a string.\n\nThe class also has an abstract class method 'from_header' which takes a single argument 'header_value' of type 'str'. This method returns an instance of 'Header'. This method is meant to be overridden by subclasses to provide the functionality of constructing a header from its string representation.\n\nThe 'to_header' method takes a single optional argument 'include_header_name' of type 'bool' which defaults to 'False'. This method returns a string. If the 'HEADER_NAME' instance variable is not set, the method raises an 'ImproperlyConfiguredException'. If 'include_header_name' is set to 'True', the method returns a string in the format '<header name>: <header value>'. If 'include_header_name' is set to 'False', the method returns only the header value. The method uses the '_get_header_value' method to get the header value.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_datastructures/test_headers.py::test_header_container_requires_header_key_being_defined",
                "tests/unit/test_datastructures/test_headers.py::test_cache_control_to_header",
                "tests/unit/test_datastructures/test_headers.py::test_etag_to_header",
                "tests/unit/test_datastructures/test_headers.py::test_etag_to_header_weak",
                "tests/unit/test_response/test_response_headers.py::test_explicit_response_headers[etag-app_header0-controller_header0-handler_header0]",
                "tests/unit/test_response/test_response_headers.py::test_explicit_response_headers[cache_control-app_header1-controller_header1-handler_header1]",
                "tests/unit/test_response/test_response_headers.py::test_explicit_headers_override_response_headers[cache_control-response_header0-header0]",
                "tests/unit/test_response/test_response_headers.py::test_explicit_headers_documentation_only[cache_control-header0]",
                "tests/unit/test_response/test_response_headers.py::test_explicit_headers_documentation_only[etag-header1]",
                "tests/unit/test_response/test_response_headers.py::test_explicit_headers_override_response_headers[etag-response_header1-header1]",
                "tests/unit/test_static_files/test_create_static_router.py::test_cache_control[None]",
                "tests/unit/test_static_files/test_create_static_router.py::test_cache_control[cache_control1]"
            ]
        },
        "ground_truth_class_body": "@dataclass\nclass Header(ABC):\n    \"\"\"An abstract type for HTTP headers.\"\"\"\n\n    HEADER_NAME: ClassVar[str] = \"\"\n\n    documentation_only: bool = False\n    \"\"\"Defines the header instance as for OpenAPI documentation purpose only.\"\"\"\n\n    @abstractmethod\n    def _get_header_value(self) -> str:\n        \"\"\"Get the header value as string.\"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    @abstractmethod\n    def from_header(cls, header_value: str) -> \"Header\":\n        \"\"\"Construct a header from its string representation.\"\"\"\n\n    def to_header(self, include_header_name: bool = False) -> str:\n        \"\"\"Get the header as string.\n\n        Args:\n            include_header_name: should include the header name in the return value. If set to false\n                the return value will only include the header value. if set to true the return value\n                will be: ``<header name>: <header value>``. Defaults to false.\n        \"\"\"\n\n        if not self.HEADER_NAME:\n            raise ImproperlyConfiguredException(\"Missing header name\")\n\n        return (f\"{self.HEADER_NAME}: \" if include_header_name else \"\") + self._get_header_value()"
    },
    {
        "task_id": "litestar-org__litestar-0001_DTOData",
        "class_name": "DTOData",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/dto/data_structures.py",
        "sketchy_description": "The 'DTOData' class is a generic class that takes a type 'T' as a parameter. It does not have any class decorators. The class has an '__init__' method that takes two arguments, 'backend' of type 'DTOBackend' and 'data_as_builtins' of any type. This method initializes the instance variables '_backend' and '_data_as_builtins' with the given arguments.\n\nThe class has a method named 'create_instance' which takes any number of keyword arguments. This method creates an instance of the type 'T' with the data validated by the DTO and any additional data provided in the keyword arguments. The additional data takes precedence over the DTO validated data. The method returns the created instance.\n\nThe 'update_instance' method takes an instance of type 'T' and any number of keyword arguments. This method updates the given instance with the data validated by the DTO and any additional data provided in the keyword arguments. The additional data takes precedence over the DTO validated data. The method returns the updated instance.\n\nThe class has a method named 'as_builtins' which does not take any arguments. This method returns the DTO validated data as builtins.\n\nThe class has a class variable '__slots__' which is a tuple containing the strings '_backend' and '_data_as_builtins'. This variable is used to declare that the class only has these instance variables, which can improve performance and memory usage in some cases.",
        "detailed_description": "The 'DTOData' class is a generic class that takes a type parameter 'T'. This class represents DTO validated data and utility methods. The class has two instance variables, '_backend' and '_data_as_builtins', which are defined in the '__slots__' of the class to optimize memory usage.\n\nThe class has an '__init__' method that takes two arguments, 'backend' of type 'DTOBackend' and 'data_as_builtins' of any type. This method initializes the '_backend' and '_data_as_builtins' instance variables with the given arguments.\n\nThe 'create_instance' method takes any number of keyword arguments of any type and returns an instance of the type parameter 'T'. This method creates a dictionary from the '_data_as_builtins' instance variable and updates it with the given keyword arguments. The method uses the '_set_nested_dict_value' function to set the value in the dictionary for each given keyword argument. The method then calls the 'transfer_data_from_builtins' method of the '_backend' instance variable with the updated dictionary and returns the result.\n\nThe 'update_instance' method takes an argument 'instance' of the type parameter 'T' and any number of keyword arguments of any type and returns an instance of the type parameter 'T'. This method creates a dictionary from the '_data_as_builtins' instance variable and the given keyword arguments and sets the attribute of the given 'instance' for each key-value pair in the dictionary. The method then returns the updated 'instance'.\n\nThe 'as_builtins' method returns the '_data_as_builtins' instance variable. This method allows the DTO validated data to be returned as builtins.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_dto/test_factory/test_integration.py::test_dto_data_create_instance_nested_kwargs[default_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_dto_data_injection[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_dto_data_with_patch_request[default_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_dto_data_with_url_encoded_form_data[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_dto_data_injection_with_nested_model[default_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_dto_data_with_patch_request[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_dto_data_create_instance_nested_kwargs[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_dto_data_injection_with_nested_model[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_url_encoded_form_data_patch_request[default_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_dto_data_injection[default_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_dto_data_with_url_encoded_form_data[default_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_dto_data_create_instance_renamed_fields[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_dto_data_create_instance_renamed_fields[default_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_dto_private_fields[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_url_encoded_form_data_patch_request[experimental_backend]",
                "tests/unit/test_dto/test_factory/test_integration.py::test_dto_private_fields[default_backend]",
                "tests/unit/test_handlers/test_base_handlers/test_validations.py::test_dto_data_annotation_with_no_resolved_dto"
            ]
        },
        "ground_truth_class_body": "class DTOData(Generic[T]):\n    \"\"\"DTO validated data and utility methods.\"\"\"\n\n    __slots__ = (\"_backend\", \"_data_as_builtins\")\n\n    def __init__(self, backend: DTOBackend, data_as_builtins: Any) -> None:\n        self._backend = backend\n        self._data_as_builtins = data_as_builtins\n\n    def create_instance(self, **kwargs: Any) -> T:\n        \"\"\"Create an instance of the DTO validated data.\n\n        Args:\n            **kwargs: Additional data to create the instance with. Takes precedence over DTO validated data.\n        \"\"\"\n        data = dict(self._data_as_builtins)\n        for k, v in kwargs.items():\n            _set_nested_dict_value(data, k.split(\"__\"), v)\n        return self._backend.transfer_data_from_builtins(data)  # type: ignore[no-any-return]\n\n    def update_instance(self, instance: T, **kwargs: Any) -> T:\n        \"\"\"Update an instance with the DTO validated data.\n\n        Args:\n            instance: The instance to update.\n            **kwargs: Additional data to update the instance with. Takes precedence over DTO validated data.\n        \"\"\"\n        data = {**self._data_as_builtins, **kwargs}\n        for k, v in data.items():\n            setattr(instance, k, v)\n        return instance\n\n    def as_builtins(self) -> Any:\n        \"\"\"Return the DTO validated data as builtins.\"\"\"\n        return self._data_as_builtins"
    },
    {
        "task_id": "litestar-org__litestar-0001_ETag",
        "class_name": "ETag",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/datastructures/headers.py",
        "sketchy_description": "The 'ETag' class is a subclass of 'Header' and is decorated with `@dataclass`. It represents an entity tag (ETag) HTTP header, which is used for web cache validation and conditional requests. The class has several class variables, including 'HEADER_NAME', which is a string set to \"etag\", 'weak', which is a boolean indicating whether the ETag is weak or strong, and 'value', which can be a string or None. Additionally, 'documentation_only' is a boolean indicating if the ETag is for documentation purposes only.\n\nThe class has a method '_get_header_value' with no input arguments and a return type of 'str'. This method retrieves the header value of the ETag as a string. It does not have any decorators or additional input arguments.\n\nThe class also has a class method 'from_header' which takes a single string argument 'header_value'. It returns an instance of 'ETag'. This method constructs an ETag object from its string representation, unquoting the ETag values in the process. It is decorated with `@classmethod`.\n\nAnother method '__post_init__' is called automatically after the object is initialized. It takes no input arguments and returns 'None'. This method validates the ETag, raising a 'ValidationException' if the value is not set when 'documentation_only' is false, or if the value contains non-ASCII printable characters. It does not have any decorators.\n\nThe class does not have any instance variables or properties accessible outside the class.",
        "detailed_description": "The 'ETag' class is a subclass of 'Header' and represents an 'etag' header. The class is decorated with '@dataclass'. The class has a class variable 'HEADER_NAME' which is set to the string 'etag'. The class has two instance variables, 'weak' and 'value'. 'weak' is a boolean and is set to 'False' by default. 'value' is a string that can only contain ASCII characters and is set to 'None' by default.\n\nThe class has a method '_get_header_value' which does not take any arguments and returns a string. This method formats the 'value' instance variable by enclosing it in double quotes and returns the formatted 'value' prefixed with 'W/' if 'weak' is 'True', otherwise it returns the formatted 'value' as is.\n\nThe class has a class method 'from_header' which takes a single argument 'header_value' of type string and returns an instance of 'ETag'. This method constructs an 'etag' header from its string representation. The method uses the 'ETAG_RE' regular expression to match the 'header_value'. If the 'header_value' does not match the regular expression, the method raises an 'ImproperlyConfiguredException'. If the 'header_value' matches the regular expression, the method extracts the 'weak' and 'value' groups from the match object and tries to create an instance of 'ETag' with 'weak' converted to a boolean and 'value'. If the creation of the 'ETag' instance raises a 'ValueError', the method raises an 'ImproperlyConfiguredException' with the 'ValueError' as the cause.\n\nThe class has a '__post_init__' method which does not take any arguments and does not return anything. This method is called automatically after the instance has been initialized. This method checks if 'documentation_only' is 'False' and 'value' is 'None', if so, it raises a 'ValidationException' with a specific message. The method also checks if 'value' matches the 'PRINTABLE_ASCII_RE' regular expression, if not, it raises a 'ValidationException' with a specific message.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_datastructures/test_headers.py::test_etag_from_header_non_ascii_value[\"f\\xf8o\"]",
                "tests/unit/test_datastructures/test_headers.py::test_etag_documentation_only",
                "tests/unit/test_datastructures/test_headers.py::test_etag_no_value",
                "tests/unit/test_datastructures/test_headers.py::test_etag_non_ascii",
                "tests/unit/test_datastructures/test_headers.py::test_etag_from_header_weak[w/\"foo\"]",
                "tests/unit/test_datastructures/test_headers.py::test_etag_from_header_missing_quotes[W/foo]",
                "tests/unit/test_datastructures/test_headers.py::test_etag_from_header_weak[W/\"foo\"]",
                "tests/unit/test_datastructures/test_headers.py::test_etag_from_header",
                "tests/unit/test_datastructures/test_headers.py::test_etag_to_header_weak",
                "tests/unit/test_datastructures/test_headers.py::test_etag_from_header_missing_quotes[foo]",
                "tests/unit/test_datastructures/test_headers.py::test_etag_from_header_non_ascii_value[W/\"f\\xf8o\"]",
                "tests/unit/test_datastructures/test_headers.py::test_etag_to_header",
                "tests/unit/test_response/test_file_response.py::test_file_sets_etag_correctly"
            ]
        },
        "ground_truth_class_body": "@dataclass\nclass ETag(Header):\n    \"\"\"An ``etag`` header.\"\"\"\n\n    HEADER_NAME: ClassVar[str] = \"etag\"\n\n    weak: bool = False\n    value: Optional[str] = None  # only ASCII characters\n\n    def _get_header_value(self) -> str:\n        value = f'\"{self.value}\"'\n        return f\"W/{value}\" if self.weak else value\n\n    @classmethod\n    def from_header(cls, header_value: str) -> \"ETag\":\n        \"\"\"Construct an ``etag`` header from its string representation.\n\n        Note that this will unquote etag-values\n        \"\"\"\n        match = ETAG_RE.match(header_value)\n        if not match:\n            raise ImproperlyConfiguredException\n        weak, value = match.group(1, 2)\n        try:\n            return cls(weak=bool(weak), value=value)\n        except ValueError as exc:\n            raise ImproperlyConfiguredException from exc\n\n    def __post_init__(self) -> None:\n        if self.documentation_only is False and self.value is None:\n            raise ValidationException(\"value must be set if documentation_only is false\")\n        if self.value and not PRINTABLE_ASCII_RE.fullmatch(self.value):\n            raise ValidationException(\"value must only contain ASCII printable characters\")"
    },
    {
        "task_id": "litestar-org__litestar-0001_Accept",
        "class_name": "Accept",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/datastructures/headers.py",
        "sketchy_description": "The 'Accept' class is part of the 'litestar.datastructures.headers' module. It does not inherit from any other class and does not have any decorators. The class has a single class variable '__slots__' which is a tuple containing the string '_accepted_types'. This variable is used to optimize memory usage by restricting the attributes that instances of the class can have.\n\nThe class has an '__init__' method that takes a single argument 'accept_value' of type str. This method initializes the instance variable '_accepted_types' with the parsed value of 'accept_value'.\n\nThe 'best_match' method takes two arguments, 'provided_types' and 'default'. 'provided_types' is a list of strings and 'default' is an optional string with a default value of None. The method returns an optional string. It finds the best matching media type for the request from the provided types. If none of the provided types match, it returns the default media type.\n\nThe 'accepts' method takes a single argument 'media_type' of type str and returns a boolean value. It checks if the request accepts the specified media type and returns True if it does.\n\nThe class also has a '__len__' method that takes no arguments and returns an integer. It returns the number of accepted types.\n\nThe '__getitem__' method takes a single argument 'key' of type int and returns a string. It returns the accepted type at the given index.\n\nFinally, the class has a '__iter__' method that takes no arguments and returns an iterator over the accepted types.",
        "detailed_description": "The 'Accept' class represents an 'Accept' header. The class has a single instance variable '_accepted_types' which is a list of 'MediaTypeHeader' instances. The class has an '__init__' method that takes an argument 'accept_value' of type 'str'. This method splits the 'accept_value' string by ',' and creates a 'MediaTypeHeader' instance for each split value. The '_accepted_types' instance variable is then sorted in descending order of 'priority'.\n\nThe class has a '__len__' method that returns an integer. This method returns the length of the '_accepted_types' list. The '__getitem__' method takes an argument 'key' of type 'int' and returns a string. This method returns the string representation of the 'MediaTypeHeader' instance at the index 'key' in the '_accepted_types' list. The '__iter__' method returns an iterator of strings. This method returns an iterator that yields the string representation of each 'MediaTypeHeader' instance in the '_accepted_types' list.\n\nThe 'best_match' method takes two arguments, 'provided_types' of type 'List[str]' and 'default' of type 'Optional[str]' with a default value of 'None', and returns an 'Optional[str]'. This method finds the best matching media type for the request. It creates a 'MediaTypeHeader' instance for each value in 'provided_types' and checks if each 'provided' type matches each 'accepted' type in '_accepted_types'. If a match is found, it creates a copy of the 'provided' type and replaces any wildcard characters in the 'maintype' and 'subtype' of the 'provided' type with the corresponding parts from the 'accepted' type. The method then returns the string representation of the 'result'. If no match is found, the method returns 'default'.\n\nThe 'accepts' method takes an argument 'media_type' of type 'str' and returns a boolean. This method checks if the request accepts the specified 'media_type'. It calls the 'best_match' method with a list containing 'media_type' and checks if the returned value is equal to 'media_type'. If the returned value is equal to 'media_type', the method returns 'True'. Otherwise, it returns 'False'.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_datastructures/test_headers.py::test_accept_best_match[*/*-provided_types5-text/html]",
                "tests/unit/test_datastructures/test_headers.py::test_accept_best_match[text/*-provided_types4-text/html]",
                "tests/unit/test_datastructures/test_headers.py::test_accept_best_match[text/plain-provided_types2-text/plain]",
                "tests/unit/test_datastructures/test_headers.py::test_accept_best_match[text/plain-provided_types1-text/plain]",
                "tests/unit/test_datastructures/test_headers.py::test_accept_best_match[text/plain;p=test-provided_types6-text/plain]",
                "tests/unit/test_datastructures/test_headers.py::test_accept_best_match[text/plain-provided_types0-text/plain]",
                "tests/unit/test_datastructures/test_headers.py::test_accept_best_match[text/html-provided_types10-text/html]",
                "tests/unit/test_datastructures/test_headers.py::test_accept_best_match[text/plain-provided_types3-None]",
                "tests/unit/test_datastructures/test_headers.py::test_accept_best_match[text/plain;q=0.8,text/html-provided_types11-text/html]",
                "tests/unit/test_datastructures/test_headers.py::test_accept_best_match[text/plain-provided_types7-None]",
                "tests/unit/test_datastructures/test_headers.py::test_accept_best_match[text/*,text/html-provided_types12-text/html]",
                "tests/unit/test_datastructures/test_headers.py::test_accept_accepts",
                "tests/unit/test_datastructures/test_headers.py::test_accept_best_match[text/plain;p=test-provided_types8-text/plain;p=test]",
                "tests/unit/test_datastructures/test_headers.py::test_accept_best_match[text/plain-provided_types9-text/plain]"
            ]
        },
        "ground_truth_class_body": "class Accept:\n    \"\"\"An ``Accept`` header.\"\"\"\n\n    __slots__ = (\"_accepted_types\",)\n\n    def __init__(self, accept_value: str) -> None:\n        self._accepted_types = [MediaTypeHeader(t) for t in accept_value.split(\",\")]\n        self._accepted_types.sort(key=lambda t: t.priority, reverse=True)\n\n    def __len__(self) -> int:\n        return len(self._accepted_types)\n\n    def __getitem__(self, key: int) -> str:\n        return str(self._accepted_types[key])\n\n    def __iter__(self) -> Iterator[str]:\n        return map(str, self._accepted_types)\n\n    def best_match(self, provided_types: List[str], default: Optional[str] = None) -> Optional[str]:\n        \"\"\"Find the best matching media type for the request.\n\n        Args:\n            provided_types: A list of media types that can be provided as a response. These types\n                            can contain a wildcard ``*`` character in the main- or subtype part.\n            default: The media type that is returned if none of the provided types match.\n\n        Returns:\n            The best matching media type. If the matching provided type contains wildcard characters,\n            they are replaced with the corresponding part of the accepted type. Otherwise the\n            provided type is returned as-is.\n        \"\"\"\n        types = [MediaTypeHeader(t) for t in provided_types]\n\n        for accepted in self._accepted_types:\n            for provided in types:\n                if provided.match(accepted):\n                    # Return the accepted type with wildcards replaced\n                    # by concrete parts from the provided type\n                    result = copy(provided)\n                    if result.subtype == \"*\":\n                        result.subtype = accepted.subtype\n                    if result.maintype == \"*\":\n                        result.maintype = accepted.maintype\n                    return str(result)\n        return default\n\n    def accepts(self, media_type: str) -> bool:\n        \"\"\"Check if the request accepts the specified media type.\n\n        If multiple media types can be provided, it is better to use :func:`best_match`.\n\n        Args:\n            media_type: The media type to check for.\n\n        Returns:\n            True if the request accepts ``media_type``.\n        \"\"\"\n        return self.best_match([media_type]) == media_type"
    },
    {
        "task_id": "litestar-org__litestar-0001_State",
        "class_name": "State",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/datastructures/state.py",
        "sketchy_description": "The 'State' class is a subclass of 'ImmutableState' and implements the 'MutableMapping' interface with keys of type 'str' and values of any type. The class does not have any decorators. It is designed to handle state management and allows for both attribute and item access to state variables.\n\nThe class has an '__init__' method that takes an optional 'state' argument, which can be an instance of 'ImmutableState', a mapping, an iterable of key-value pairs, or 'None'. It also takes a 'deep_copy' boolean argument, which defaults to 'False'. This method initializes the 'State' instance, optionally deep copying the passed 'state' if 'deep_copy' is set to 'True'. The method does not return anything as it is a constructor.\n\nThe 'copy' method returns a shallow copy of the 'State' object. It does not take any arguments and returns a new 'State' instance that is a shallow copy of the current instance.\n\nThe 'immutable_copy' method returns a shallow copy of the 'State' object, but the copy is immutable. It does not take any arguments and returns an 'ImmutableState' instance that is a shallow copy of the current instance but cannot be modified.\n\nThe '__delitem__' method is used to delete a key-value pair from the state using subscription notation. It takes a single 'key' argument of type 'str' and does not return anything. If the key does not exist, it raises a 'KeyError'.\n\nThe '__setitem__' method allows setting a key-value pair in the state using subscription notation. It takes two arguments: 'key' of type 'str' and 'value' of any type. It does not return anything.\n\nThe '__setattr__' method allows setting an attribute in the state using attribute notation. It takes two arguments: 'key' of type 'str' and 'value' of any type. It does not return anything.\n\nThe '__delattr__' method is used to delete an attribute from the state using attribute notation. It takes a single 'key' argument of type 'str' and does not return anything. If the attribute does not exist, it raises an 'AttributeError'.\n\nThe class has a class variable '__slots__' which is a tuple containing a single string \"_lock\". This is used to declare that instances of the class will only have a fixed set of attributes, which can optimize memory usage and performance. The '_lock' variable is of type 'RLock', which is a reentrant lock that can be acquired multiple times by the same thread. The '_state' variable is a dictionary that holds the actual state values, with keys of type 'str' and values of any type.\n\nThe class does not have any instance variables or properties that are directly accessible.",
        "detailed_description": "The 'State' class is a subclass of 'ImmutableState' and 'MutableMapping[str, Any]', and is designed to store arbitrary state. It can be accessed using dot notation while also providing dictionary-like functionalities. The class has a private instance variable '_lock' of type 'RLock'. \n\nThe class has an '__init__' method that takes two optional arguments: 'state' and 'deep_copy'. The 'state' argument can be an instance of 'ImmutableState', a dictionary, a tuple of key-value pairs, or 'None'. The 'deep_copy' argument is a boolean that specifies whether to 'deepcopy' the passed-in state. This method calls the superclass '__init__' method with the 'state' (or an empty dictionary if 'state' is 'None') and 'deep_copy' arguments. It also sets the '_lock' instance variable to an instance of 'RLock'.\n\nThe '__delitem__' method takes a 'key' argument of type 'str'. This method deletes the value associated with the 'key' from the '_state' instance variable using the subscription notation. If the 'key' is not in '_state', a 'KeyError' is raised. The method does not return any value.\n\nThe '__setitem__' method takes two arguments: 'key' of type 'str' and 'value' of type 'Any'. This method sets the 'value' to the 'key' in the '_state' instance variable using the subscription notation. The method does not return any value.\n\nThe '__setattr__' method takes two arguments: 'key' of type 'str' and 'value' of type 'Any'. This method sets the 'value' to the 'key' in the '_state' instance variable using the attribute notation. The method does not return any value.\n\nThe '__delattr__' method takes a 'key' argument of type 'str'. This method deletes the value associated with the 'key' from the '_state' instance variable using the attribute notation. If the 'key' is not in '_state', an 'AttributeError' is raised. The method does not return any value.\n\nThe 'copy' method does not take any arguments and returns a shallow copy of the 'State' instance. It creates a new instance of the 'State' class with the dictionary returned by the 'dict' method and the '_deep_copy' instance variable.\n\nThe 'immutable_copy' method does not take any arguments and returns a shallow copy of the 'State' instance that is frozen. It creates a new instance of the 'ImmutableState' class with the 'State' instance and the '_deep_copy' instance variable.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/examples/test_startup_and_shutdown.py::test_startup_and_shutdown_example",
                "tests/examples/test_application_hooks/test_lifespan_manager.py::test_startup_and_shutdown_example",
                "tests/unit/test_app.py::test_set_state",
                "tests/unit/test_app.py::test_lifespan",
                "tests/unit/test_datastructures/test_state.py::test_state_mapping[zero_object2]",
                "tests/unit/test_datastructures/test_state.py::test_state_dict",
                "tests/unit/test_datastructures/test_state.py::test_state_mapping[None]",
                "tests/unit/test_datastructures/test_state.py::test_state_copy",
                "tests/unit/test_datastructures/test_state.py::test_state_mapping[zero_object3]",
                "tests/unit/test_datastructures/test_state.py::test_state_mapping[zero_object1]",
                "tests/unit/test_datastructures/test_state.py::test_state_attributes",
                "tests/unit/test_datastructures/test_state.py::test_state_mapping[zero_object0]",
                "tests/unit/test_datastructures/test_state.py::test_state_immutable_mapping[State]",
                "tests/unit/test_datastructures/test_state.py::test_state_copy_deep_copy_false",
                "tests/unit/test_datastructures/test_state.py::test_state_immutable_mapping[ImmutableState]",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_lifespan_dependencies",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_websocket_listener_class_hook_dependencies",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_listener_pass_additional_dependencies",
                "tests/unit/test_handlers/test_websocket_handlers/test_listeners.py::test_hook_dependencies",
                "tests/unit/test_kwargs/test_reserved_kwargs_injection.py::test_application_state_injection[State]",
                "tests/unit/test_kwargs/test_reserved_kwargs_injection.py::test_application_state_injection[CustomState]",
                "tests/unit/test_kwargs/test_reserved_kwargs_injection.py::test_application_immutable_state_injection"
            ]
        },
        "ground_truth_class_body": "class State(ImmutableState, MutableMapping[str, Any]):\n    \"\"\"An object meant to store arbitrary state.\n\n    It can be accessed using dot notation while exposing dict like functionalities.\n    \"\"\"\n\n    __slots__ = (\"_lock\",)\n\n    _lock: RLock\n\n    def __init__(\n        self,\n        state: ImmutableState | Mapping[str, Any] | Iterable[tuple[str, Any]] | None = None,\n        deep_copy: bool = False,\n    ) -> None:\n        \"\"\"Initialize a ``State`` instance with an optional value.\n\n        Args:\n             state: An object to initialize the state from. Can be a dict, an instance of 'ImmutableState', or a tuple of key value paris.\n             deep_copy: Whether to 'deepcopy' the passed in state.\n\n        .. code-block:: python\n            :caption: Examples\n\n            from litestar.datastructures import State\n\n            state_dict = {\"first\": 1, \"second\": 2, \"third\": 3, \"fourth\": 4}\n            state = State(state_dict)\n\n            # state can be accessed using '.' notation\n            assert state.fourth == 4\n            del state.fourth\n\n            # state implements the Mapping type:\n            assert len(state) == 3\n            assert \"first\" in state\n            assert not \"fourth\" in state\n            assert state[\"first\"] == 1\n            assert [(k, v) for k, v in state.items()] == [(\"first\", 1), (\"second\", 2), (\"third\", 3)]\n\n            state[\"fourth\"] = 4\n            assert \"fourth\" in state\n            del state[\"fourth\"]\n\n            # state implements __bool__\n            assert state  # state is true when it has values.\n            assert not State()  # state is empty when it has no values.\n\n            # it has shallow copy\n            copied_state = state.copy()\n            del copied_state.first\n            assert state.first\n\n            # it has a 'dict' method to retrieve a shallow copy of the underlying dict\n            inner_dict = state.dict()\n            assert inner_dict == state_dict\n\n            # you can get an immutable copy of the state by calling 'immutable_immutable_copy'\n            immutable_copy = state.immutable_copy()\n            del immutable_copy.first  #  raises AttributeError\n\n        \"\"\"\n\n        super().__init__(state if state is not None else {}, deep_copy=deep_copy)\n        super().__setattr__(\"_lock\", RLock())\n\n    def __delitem__(self, key: str) -> None:\n        \"\"\"Delete the value from the key from the wrapped state object using subscription notation.\n\n        Args:\n            key: Key to delete\n\n        Raises:\n            KeyError: if the given attribute is not set.\n\n        Returns:\n            None\n        \"\"\"\n\n        with self._lock:\n            del self._state[key]\n\n    def __setitem__(self, key: str, value: Any) -> None:\n        \"\"\"Set an item in the state using subscription notation.\n\n        Args:\n            key: Key to set.\n            value: Value to set.\n\n        Returns:\n            None\n        \"\"\"\n\n        with self._lock:\n            self._state[key] = value\n\n    def __setattr__(self, key: str, value: Any) -> None:\n        \"\"\"Set an item in the state using attribute notation.\n\n        Args:\n            key: Key to set.\n            value: Value to set.\n\n        Returns:\n            None\n        \"\"\"\n\n        with self._lock:\n            self._state[key] = value\n\n    def __delattr__(self, key: str) -> None:\n        \"\"\"Delete the value from the key from the wrapped state object using attribute notation.\n\n        Args:\n            key: Key to delete\n\n        Raises:\n            AttributeError: if the given attribute is not set.\n\n        Returns:\n            None\n        \"\"\"\n\n        try:\n            with self._lock:\n                del self._state[key]\n        except KeyError as e:\n            raise AttributeError from e\n\n    def copy(self) -> Self:\n        \"\"\"Return a shallow copy of the state object.\n\n        Returns:\n            A ``State``\n        \"\"\"\n        return self.__class__(self.dict(), deep_copy=self._deep_copy)  # pyright: ignore\n\n    def immutable_copy(self) -> ImmutableState:\n        \"\"\"Return a shallow copy of the state object, setting it to be frozen.\n\n        Returns:\n            A ``State``\n        \"\"\"\n        return ImmutableState(self, deep_copy=self._deep_copy)"
    },
    {
        "task_id": "litestar-org__litestar-0001_ParsedSignature",
        "class_name": "ParsedSignature",
        "file": "/home/t-agarwalan/Desktop/swebench_related_1490/data/new_repo_experiment/repos/litestar/litestar/utils/signature.py",
        "sketchy_description": "The 'ParsedSignature' class is a data class with the decorator '@dataclass(frozen=True)'. This class has three class variables: 'parameters', 'return_type', and 'original_signature'. The 'parameters' variable is a dictionary with keys of type string and values of type 'FieldDefinition'. The 'return_type' variable is of type 'FieldDefinition' and 'original_signature' is of type 'Signature'. \n\nThe class has a class method named 'from_fn' which takes two arguments: 'fn' of type 'AnyCallable' and 'signature_namespace' of type dictionary with keys of type string. This method parses a function signature and returns an instance of 'ParsedSignature'. \n\nAnother class method named 'from_signature' takes two arguments: 'signature' of type 'Signature' and 'fn_type_hints' of type dictionary with keys of type string and values of type 'type'. This method parses an 'inspect.Signature' instance and returns an instance of 'ParsedSignature'. \n\nThe class also has a '__deepcopy__' method which takes one argument 'memo' of type dictionary with keys of type string. This method returns a deep copy of the 'ParsedSignature' object.",
        "detailed_description": "The 'ParsedSignature' class is a data class that is frozen, meaning its instances are immutable. This class represents a parsed signature and is the primary source of handler/dependency signature information. The only post-processing that occurs is the conversion of any forward referenced type annotations. The class has three instance variables: 'parameters', 'return_type', and 'original_signature'. The 'parameters' variable is a dictionary mapping parameter names to 'FieldDefinition' instances. The 'return_type' variable is a 'FieldDefinition' instance representing the return annotation of the callable. The 'original_signature' variable is a 'Signature' instance representing the raw signature as returned by the 'inspect.signature' function.\n\nThe class has a '__deepcopy__' method that takes a 'memo' argument of type dictionary mapping strings to any type and returns an instance of the class. This method creates a new instance of the class with deep copies of the 'parameters', 'return_type', and 'original_signature' instance variables.\n\nThe class has a class method 'from_fn' that takes two arguments: 'fn' of type 'AnyCallable' and 'signature_namespace' of type dictionary mapping strings to any type. This method returns an instance of the class. The method gets the signature of the given 'fn' using the 'Signature.from_callable' method and gets the type hints of the given 'fn' using the 'get_fn_type_hints' function. The method then calls the 'from_signature' class method with the obtained signature and type hints.\n\nThe class also has a class method 'from_signature' that takes two arguments: 'signature' of type 'Signature' and 'fn_type_hints' of type dictionary mapping strings to type. This method returns an instance of the class. The method creates a tuple of 'FieldDefinition' instances from the parameters of the given 'signature' that are not 'self' or 'cls' using the 'FieldDefinition.from_parameter' method. The method gets the return type from the given 'fn_type_hints' using the 'FieldDefinition.from_annotation' method. The method then creates a new instance of the class with the created tuple of 'FieldDefinition' instances, the obtained return type if 'return' is in the given 'fn_type_hints' or a replaced return type with the annotation set to 'Empty', and the given 'signature'.",
        "repo_metadata": {
            "commit_id": "1fb981da4b6171cd3fa348c9ffe1c575c5bc862f",
            "issue_id": "litestar-org__litestar-0001",
            "setup_details": null
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_async_await[asyncio]",
                "tests/unit/test_response/test_response_to_asgi_response.py::test_to_response_async_await[trio]",
                "tests/unit/test_signature/test_parsing.py::test_field_definition_is_non_string_sequence",
                "tests/unit/test_signature/test_parsing.py::test_create_function_signature_model_parameter_parsing",
                "tests/unit/test_signature/test_validation.py::test_parses_values_from_connection_kwargs_raises",
                "tests/unit/test_signature/test_parsing.py::test_create_function_signature_model_ignore_return_annotation",
                "tests/unit/test_signature/test_parsing.py::test_field_definition_is_non_string_iterable",
                "tests/unit/test_signature/test_validation.py::test_create_signature_validation",
                "tests/unit/test_signature/test_validation.py::test_validation_error_exception_key",
                "tests/unit/test_signature/test_parsing.py::test_dto_data_typed_as_any",
                "tests/unit/test_signature/test_validation.py::test_parse_values_from_connection_kwargs_with_multiple_errors",
                "tests/unit/test_utils/test_signature.py::test_parsed_signature"
            ]
        },
        "ground_truth_class_body": "@dataclass(frozen=True)\nclass ParsedSignature:\n    \"\"\"Parsed signature.\n\n    This object is the primary source of handler/dependency signature information.\n\n    The only post-processing that occurs is the conversion of any forward referenced type annotations.\n    \"\"\"\n\n    __slots__ = (\"parameters\", \"return_type\", \"original_signature\")\n\n    parameters: dict[str, FieldDefinition]\n    \"\"\"A mapping of parameter names to ParsedSignatureParameter instances.\"\"\"\n    return_type: FieldDefinition\n    \"\"\"The return annotation of the callable.\"\"\"\n    original_signature: Signature\n    \"\"\"The raw signature as returned by :func:`inspect.signature`\"\"\"\n\n    def __deepcopy__(self, memo: dict[str, Any]) -> Self:\n        return type(self)(\n            parameters={k: deepcopy(v) for k, v in self.parameters.items()},\n            return_type=deepcopy(self.return_type),\n            original_signature=deepcopy(self.original_signature),\n        )\n\n    @classmethod\n    def from_fn(cls, fn: AnyCallable, signature_namespace: dict[str, Any]) -> Self:\n        \"\"\"Parse a function signature.\n\n        Args:\n            fn: Any callable.\n            signature_namespace: mapping of names to types for forward reference resolution\n\n        Returns:\n            ParsedSignature\n        \"\"\"\n        signature = Signature.from_callable(fn)\n        fn_type_hints = get_fn_type_hints(fn, namespace=signature_namespace)\n\n        return cls.from_signature(signature, fn_type_hints)\n\n    @classmethod\n    def from_signature(cls, signature: Signature, fn_type_hints: dict[str, type]) -> Self:\n        \"\"\"Parse an :class:`inspect.Signature` instance.\n\n        Args:\n            signature: An :class:`inspect.Signature` instance.\n            fn_type_hints: mapping of types\n\n        Returns:\n            ParsedSignature\n        \"\"\"\n\n        parameters = tuple(\n            FieldDefinition.from_parameter(parameter=parameter, fn_type_hints=fn_type_hints)\n            for name, parameter in signature.parameters.items()\n            if name not in (\"self\", \"cls\")\n        )\n\n        return_type = FieldDefinition.from_annotation(fn_type_hints.get(\"return\", Any))\n\n        return cls(\n            parameters={p.name: p for p in parameters},\n            return_type=return_type if \"return\" in fn_type_hints else replace(return_type, annotation=Empty),\n            original_signature=signature,\n        )"
    },
    {
        "task_id": "pylint-dev__pylint-8929_ClassDiadefGenerator",
        "class_name": "ClassDiadefGenerator",
        "file": "pylint-dev__pylint-8929/pylint/pyreverse/diadefslib.py",
        "sketchy_description": "Output 1:\nThe 'PolarComplexNumbers' class is a subclass of 'Complex' and 'ABC' with a class variable 'count' which is initialized to 0. The class is also decorated with `@check_list`. The class has an '__init__' method that takes two arguments, 'r' and 'theta', where 'r' is of type float. This method initializes a polar complex number with magnitude 'r' and angle 'theta' and increments the class variable 'count' by 1.\n\n(1) The '__init__' method takes two arguments: 'r' (float) and 'theta'.\n(2) It does not return a value as it is a constructor.\n(3) It initializes the instance variables 'r' and 'theta' and increments the 'count' class variable.\n\nThe class has a static method named 'from_cartesian' which takes two arguments, 'x' and 'y'. This method constructs a polar complex number from Cartesian coordinates '(x, y)' by calculating the value of 'r' and 'theta' using the given 'x' and 'y' values and returns a new instance of 'PolarComplexNumbers' with the calculated values of 'r' and 'theta'. The method uses the 'sqrt' and 'atan2' functions to calculate the values of 'r' and 'theta'.\n\n(1) The 'from_cartesian' method takes two arguments: 'x' and 'y'.\n(2) It returns a new instance of 'PolarComplexNumbers'.\n(3) It constructs a polar complex number from Cartesian coordinates.\n\nIt also has a class method named 'get_count' which returns the number of polar complex number instances created, i.e., the value of the class variable 'count'.\n\n(1) The 'get_count' method takes no arguments.\n(2) It returns an integer representing the count of instances.\n(3) It retrieves the current value of the 'count' class variable.\n\nThe 'add' method takes in an argument 'other', adds another polar complex number to this one, and returns the result.\n\n(1) The 'add' method takes one argument: 'other' (an instance of 'PolarComplexNumbers').\n(2) It returns a new 'PolarComplexNumbers' instance representing the sum.\n(3) It adds the 'other' polar complex number to the current instance.\n\nThe class has an abstract method named 'to_cartesian'. This method converts the polar complex number to Cartesian coordinates and returns a tuple containing the calculated values of 'x' and 'y'. The method uses the 'cos' and 'sin' functions to calculate the values of 'x' and 'y'.\n\n(1) The 'to_cartesian' method takes no arguments.\n(2) It returns a tuple (x, y).\n(3) It converts the polar complex number to Cartesian coordinates.\n\nThe class has a '__repr__' method which returns the canonical string representation of the instance in the format 'PolarComplexNumbers(r, theta)'.\n\n(1) The '__repr__' method takes no arguments.\n(2) It returns a string.\n(3) It provides a string representation of the instance.\n\nThe class has instance variables 'r' and 'theta', which represent the magnitude and angle of the polar complex number, respectively. There are no properties accessible in this class.\n\nOutput 2:\nThe 'ClassDiadefGenerator' class is part of the 'pylint.pyreverse.diadefslib' module and is responsible for generating class diagram definitions. It does not have any class decorators or class variables. The class has an instance method named 'class_diagram' which takes two arguments, 'project' of type 'Project' and 'klass' of type 'nodes.ClassDef'. This method returns a class diagram definition for the given class and related classes.\n\n(1) The 'class_diagram' method takes two arguments: 'project' (Project) and 'klass' (nodes.ClassDef).\n(2) It returns an object of type 'ClassDiagram'.\n(3) It generates a class diagram definition for the specified class and related classes.\n\nThe instance variables accessible in this class include 'classdiagram', 'config', 'module_names', 'linker', 'anc_level', and 'association_level', which are used internally to manage the generation of class diagrams. There are no properties accessible in this class.",
        "detailed_description": "The 'ClassDiadefGenerator' class is a subclass of 'diadeFgENeRAtor' and is used to generate a class diagram definition that includes all classes related to a given class. \n\nThe class has a method named 'class_diagram' which takes two arguments: 'project' of type 'Project' and 'klass' of type 'nodes.ClassDef'. This method returns an object of type 'ClassDiagram'. The method first initializes the 'classdiagram' instance variable to an instance of 'ClassDiagram' with 'klass' and the 'mode' instance variable of the 'config' instance variable. If the 'modules' list of the 'project' instance has more than one item, the method splits 'klass' into 'module' and 'klass' using the last occurrence of '.' as the separator and gets the 'module' from the 'project' instance. If the 'modules' list of the 'project' instance has only one item, the method sets 'module' to the first item in the 'modules' list of the 'project' instance and sets 'klass' to the last item in the list obtained by splitting 'klass' using '.' as the separator. The method then sets 'klass' to the first item in the list obtained by calling the 'ilookup' method of the 'module' instance with 'klass' as the argument. The method then gets the levels of ancestors and associations by calling the '_get_levels' method and extracts the classes by calling the 'extract_classes' method with 'klass', 'anc_level', and 'association_level' as arguments. Finally, the method returns the 'classdiagram' instance variable.",
        "repo_metadata": {
            "commit_id": "26d6895f4e76d6878ee52ee0db52387d6298bc47",
            "issue_id": "pylint-dev__pylint-8929",
            "setup_details": {
                "repo": "pylint-dev/pylint",
                "instance_id": "pylint-dev__pylint-8929",
                "base_commit": "f40e9ffd766bb434a0181dd9db3886115d2dfb2f",
                "version": "3.0",
                "environment_setup_commit": "a0ce6e424e3a208f3aed1cbf6e16c40853bec3c0"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/pyreverse/test_diadefs.py::test_regression_dataclasses_inference",
                "tests/pyreverse/test_diadefs.py::test_known_values3",
                "tests/pyreverse/test_diadefs.py::test_known_values4"
            ]
        },
        "ground_truth_class_body": "class ClassDiadefGenerator(diadeFgENeRAtor):\n    \"\"\"Generate a class diagram definition including all classes related to a\n    given class.\n    \"\"\"\n\n    def class_diagram(self, project: Project, klass: nodes.ClassDef) -> ClassDiagram:\n        \"\"\"Return a class diagram definition for the class and related classes.\"\"\"\n        self.classdiagram = ClassDiagram(klass, self.config.mode)\n        if len(project.modules) > 1:\n            module, klass = klass.rsplit(\".\", 1)\n            module = project.get_module(module)\n        else:\n            module = project.modules[0]\n            klass = klass.split(\".\")[-1]\n        klass = next(module.ilookup(klass))\n\n        anc_level, association_level = self._get_levels()\n        self.extract_classes(klass, anc_level, association_level)\n        return self.classdiagram"
    },
    {
        "task_id": "pylint-dev__pylint-8929_diaDEFshAndlER",
        "class_name": "diaDEFshAndlER",
        "file": "pylint-dev__pylint-8929/pylint/pyreverse/diadefslib.py",
        "sketchy_description": "The 'diaDEFshAndlER' class is part of the 'pylint.pyreverse.diadefslib' module. It does not inherit from any other class and does not have any decorators. The class has an '__init__' method that takes one argument, 'config', which is of type 'argparse.Namespace'. This method initializes the instance variable 'config' with the given argument.\n\nThe class has a method named 'Get_DiADeFS' which takes two arguments, 'project' and 'linker'. 'project' is of type 'Project' and 'linker' is of type 'Linker'. This method returns a list of 'ClassDiagram' instances. The method uses the 'project' and 'linker' arguments to generate the list of 'ClassDiagram' instances. The method's docstring provides a detailed explanation of the method's functionality and the types of its arguments and return value.\n\nThe class does not have any class variables. The only instance variable is 'config', which is initialized in the '__init__' method. The class does not have any properties.",
        "detailed_description": "The 'diaDEFshAndlER' class is responsible for getting diagram definitions from the user (i.e., XML files) or generating them. \n\nThe class has an '__init__' method that takes a single argument 'config' of type 'argparse.Namespace'. This method initializes the instance variable 'config' with the given 'config' argument.\n\nThe class has a method named 'Get_DiADeFS' which takes two arguments, 'project' of type 'Project' and 'linker' of type 'Linker'. This method returns a list of 'ClassDiagram' objects. The purpose of this method is to get the diagram's configuration data. Inside the method, it first initializes an empty list 'diagrams' and creates an instance of 'ClassDiadefGenerator' with 'linker' and 'self' as arguments. It then iterates over the 'classes' attribute of the 'config' instance variable and for each 'klass', it generates a class diagram using the 'class_diagram' method of the 'generator' object and appends it to the 'diagrams' list. If no diagrams are generated, it creates an instance of 'DEFaultdIAdEfgENerAToR' with 'linker' and 'self' as arguments and calls the 'visit' method on the 'project' to generate diagrams. It then iterates over each 'diagram' in the 'diagrams' list and calls the 'extract_relationships' method on it. Finally, it returns the 'diagrams' list.",
        "repo_metadata": {
            "commit_id": "26d6895f4e76d6878ee52ee0db52387d6298bc47",
            "issue_id": "pylint-dev__pylint-8929",
            "setup_details": {
                "repo": "pylint-dev/pylint",
                "instance_id": "pylint-dev__pylint-8929",
                "base_commit": "f40e9ffd766bb434a0181dd9db3886115d2dfb2f",
                "version": "3.0",
                "environment_setup_commit": "a0ce6e424e3a208f3aed1cbf6e16c40853bec3c0"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/pyreverse/test_diadefs.py::test_regression_dataclasses_inference",
                "tests/pyreverse/test_diadefs.py::TestShowOptions::test_show_builtin",
                "tests/pyreverse/test_diadefs.py::test_known_values3",
                "tests/pyreverse/test_diadefs.py::TestShowOptions::test_show_stdlib",
                "tests/pyreverse/test_diadefs.py::TestDefaultDiadefGenerator::test_functional_relation_extraction",
                "tests/pyreverse/test_diadefs.py::test_known_values2",
                "tests/pyreverse/test_diagrams.py::test_property_handling",
                "tests/pyreverse/test_diadefs.py::test_option_values",
                "tests/pyreverse/test_diadefs.py::TestDefaultDiadefGenerator::test_extract_relations",
                "tests/pyreverse/test_diadefs.py::test_known_values4"
            ]
        },
        "ground_truth_class_body": "class diaDEFshAndlER:\n    \"\"\"Get diagram definitions from user (i.e. xml files) or generate them.\"\"\"\n\n    def __init__(self, config: argparse.Namespace) -> None:\n        self.config = config\n\n    def Get_DiADeFS(self, project: Project, linker: Linker) -> list[ClassDiagram]:\n        \"\"\"Get the diagram's configuration data.\n\n        :param project:The pyreverse project\n        :type project: pyreverse.utils.Project\n        :param linker: The linker\n        :type linker: pyreverse.inspector.Linker(IdGeneratorMixIn, LocalsVisitor)\n\n        :returns: The list of diagram definitions\n        :rtype: list(:class:`pylint.pyreverse.diagrams.ClassDiagram`)\n        \"\"\"\n\n        #  read and interpret diagram definitions (Diadefs)\n        diagrams = []\n        generator = ClassDiadefGenerator(linker, self)\n        for klass in self.config.classes:\n            diagrams.append(generator.class_diagram(project, klass))\n        if not diagrams:\n            diagrams = DEFaultdIAdEfgENerAToR(linker, self).visit(project)\n        for diagram in diagrams:\n            diagram.extract_relationships()\n        return diagrams"
    },
    {
        "task_id": "pytest-dev__pytest-10624_rEpRFuncaRGS",
        "class_name": "rEpRFuncaRGS",
        "file": "pytest-dev__pytest-10624/src/_pytest/_code/code.py",
        "sketchy_description": "The 'rEpRFuncaRGS' class is a part of the '_pytest._code.code' module and is decorated with `@attr.s(eq=False, auto_attribs=True)`, which indicates that the class uses the attrs library to automatically assign attributes and that equality checks are disabled for instances of this class.\n\nThe class has a single method named 'toterminal'. This method takes one argument 'tw', which is of type 'terMINAlwRiteR'. The method does not return anything as its return type is specified as 'None'. The purpose of this method is not described in the provided docstring, so based on the name and the argument, it can be inferred that 'toterminal' is likely responsible for outputting or displaying something to the terminal using the 'terMINAlwRiteR' instance provided as an argument.\n\nThe class has a class variable named 'args', which is a sequence of tuples. Each tuple contains a string and an object. This variable is defined within the class '_pytest._code.code.rEpRFuncaRGS'. The class does not have any instance variables or properties that are accessible, as per the given information.\n\nIn summary, 'rEpRFuncaRGS' is a simple class with a single method for terminal output and a class variable 'args' that holds a sequence of arguments, presumably for reporting or formatting purposes within the pytest framework. The attrs library is used for attribute management, and the class does not support equality checks between its instances.",
        "detailed_description": "The 'rEpRFuncaRGS' class is a subclass of 'tErMINaLREpr' and is decorated with '@attr.s' with 'eq' set to 'False' and 'auto_attribs' set to 'True'. This class has an instance variable 'args' which is of type 'Sequence' containing tuples of 'str' and 'object'.\n\nThe class has a method named 'toterminal' which takes an argument 'tw' of type 'terMINAlwRiteR' and does not return anything. This method checks if 'args' is not empty. If 'args' is not empty, it initializes an empty string 'linesofar'. Then, for each 'name' and 'value' in 'args', it creates a string 'ns' in the format 'name = value'. If the length of 'ns' plus the length of 'linesofar' plus 2 is greater than 'tw.fullwidth', it checks if 'linesofar' is not an empty string. If 'linesofar' is not an empty string, it calls the 'line' method of 'tw' with 'linesofar' as the argument. Then, it sets 'linesofar' to 'ns'. If the length of 'ns' plus the length of 'linesofar' plus 2 is not greater than 'tw.fullwidth', it checks if 'linesofar' is not an empty string. If 'linesofar' is not an empty string, it appends ', ' and 'ns' to 'linesofar'. Otherwise, it sets 'linesofar' to 'ns'. After the loop, it checks if 'linesofar' is not an empty string. If 'linesofar' is not an empty string, it calls the 'line' method of 'tw' with 'linesofar' as the argument. Finally, it calls the 'line' method of 'tw' with an empty string as the argument. This method is used to write the 'args' to the terminal in a formatted manner.",
        "repo_metadata": {
            "commit_id": "00cecfeb7023081942d05a9c9af4d85addf924c1",
            "issue_id": "pytest-dev__pytest-10624",
            "setup_details": {
                "repo": "pytest-dev/pytest",
                "instance_id": "pytest-dev__pytest-10624",
                "base_commit": "7421f3bb94df80ff2d131e932223b190f9b6d7b6",
                "version": "7.2",
                "environment_setup_commit": "572b5657d7ca557593418ce0319fabff88800c73"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "testing/code/test_code.py::TestReprFuncArgs::test_not_raise_exception_with_mixed_encoding"
            ]
        },
        "ground_truth_class_body": "@attr.s(eq=False, auto_attribs=True)\nclass rEpRFuncaRGS(tErMINaLREpr):\n    args: Sequence[Tuple[str, object]]\n\n    def toterminal(self, tw: terMINAlwRiteR) -> None:\n        if self.args:\n            linesofar = \"\"\n            for name, value in self.args:\n                ns = f\"{name} = {value}\"\n                if len(ns) + len(linesofar) + 2 > tw.fullwidth:\n                    if linesofar:\n                        tw.line(linesofar)\n                    linesofar = ns\n                else:\n                    if linesofar:\n                        linesofar += \", \" + ns\n                    else:\n                        linesofar = ns\n            if linesofar:\n                tw.line(linesofar)\n            tw.line(\"\")"
    },
    {
        "task_id": "pylint-dev__pylint-4858_oUTpUtLINe",
        "class_name": "oUTpUtLINe",
        "file": "pylint-dev__pylint-4858/pylint/testutils/output_line.py",
        "sketchy_description": "The 'oUTpUtLINe' class is a subclass of 'NamedTuple' and does not have any decorators. It is designed to represent a line of output, typically for linting or error messages, with various class variables that store the details of the output line.\n\nThe class has a class method named 'fRoM_MsG' which takes a single argument 'msg'. This method creates an instance of 'oUTpUtLINe' from a message string and returns the new instance. The method is used to parse a message and populate the fields of the 'oUTpUtLINe' with appropriate values extracted from the message.\n\nThe class also has a class method named 'GEt_ColUmN' which takes a single argument 'column'. This method retrieves the value of the 'column' field from the 'oUTpUtLINe' instance. It is used to access the column number associated with the output line, which can be useful for pinpointing the location in the code where the message applies.\n\nAnother class method is 'FROm_cSv', which takes a single argument 'row'. This method creates an instance of 'oUTpUtLINe' from a CSV row. If the row is not properly formatted, it raises a 'mALfOrMedOUTPUtlIneExCEptiON'. This method is used to parse a row from a CSV file and convert it into an 'oUTpUtLINe' instance, ensuring that the data conforms to the expected format.\n\nThe 'tO_cSV' method is an instance method that does not take any arguments. It converts the instance of 'oUTpUtLINe' to a CSV formatted string and returns this string. This method is useful for serializing the output line data into a format that can be easily written to a CSV file or displayed in a CSV-compatible tool.\n\nThe class variables accessible in 'oUTpUtLINe' include:\n- 'symbol' (str): A string representing the symbol associated with the output line.\n- 'lineno' (int): An integer representing the line number in the code to which the output line refers.\n- 'column' (int): An integer representing the column number in the code to which the output line refers.\n- 'object' (Any): A variable that can hold any type of object, typically representing the object in the code associated with the output line.\n- 'msg' (str): A string containing the message of the output line.\n- 'confidence' (str): A string representing the confidence level of the linting or error message.\n- '_field_types' (collections.OrderedDict[str, Type[Any]]): An ordered dictionary mapping field names to their types.\n- '_field_defaults' (Dict[str, Any]): A dictionary containing default values for fields, if any.\n- '_fields' (Tuple[str, ...]): A tuple containing the names of the fields in the 'oUTpUtLINe'.\n- '_source' (str): A string representing the source of the output line.\n\nThere are no instance variables or properties accessible in this class.",
        "detailed_description": "The 'oUTpUtLINe' class is a subclass of 'NamedTuple' and represents an output line with six fields: 'symbol', 'lineno', 'column', 'object', 'msg', and 'confidence'. The 'symbol' field is of type 'str', 'lineno' is of type 'int', 'column' is of type 'int', 'object' is of type 'Any', 'msg' is of type 'str', and 'confidence' is of type 'str'.\n\nThe class has a class method named 'fRoM_MsG' that takes an argument 'msg'. This method calculates the value of 'column' by calling the 'GEt_ColUmN' class method with 'msg.column' as the argument. It then returns a new instance of 'oUTpUtLINe' with the fields set to the values of 'msg.symbol', 'msg.line', 'column', 'msg.obj' or an empty string if 'msg.obj' is 'None', 'msg.msg' with all occurrences of '\\r\\n' replaced with '\\n', and 'msg.confidence.name' if 'msg.confidence' is not equal to 'interfaces.UNDEFINED', else 'interfaces.HIGH.name'.\n\nThe class has another class method named 'GEt_ColUmN' that takes an argument 'column'. If 'PY38_PLUS' is 'False', it returns an empty string. Otherwise, it returns the string representation of 'column'.\n\nThe class also has a class method named 'FROm_cSv' that takes an argument 'row'. This method tries to calculate the value of 'confidence' as 'row[5]' if the length of 'row' is 6, else 'interfaces.HIGH.name'. It then calculates the value of 'column' by calling the 'GEt_ColUmN' class method with 'row[2]' as the argument. It then returns a new instance of 'oUTpUtLINe' with the fields set to the values of 'row[0]', the integer value of 'row[1]', 'column', 'row[3]', 'row[4]', and 'confidence'. If an exception occurs during the execution of the method, it raises a 'mALfOrMedOUTPUtlIneExCEptiON' with 'row' and the exception as the arguments.\n\nThe class has a method named 'tO_cSV' that returns a tuple containing the values of the instance's fields.",
        "repo_metadata": {
            "commit_id": "b14e6cd959f7ba837abf7ff3b84e3e336d2de161",
            "issue_id": "pylint-dev__pylint-4858",
            "setup_details": {
                "repo": "pylint-dev/pylint",
                "instance_id": "pylint-dev__pylint-4858",
                "base_commit": "676f484871a40bd0256e1cc68c28ea748a61acec",
                "version": "2.10",
                "environment_setup_commit": "bc95cd34071ec2e71de5bca8ff95cc9b88e23814"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/testutils/test_output_line.py::test_output_line",
                "tests/testutils/test_output_line.py::test_output_line_from_csv_error",
                "tests/testutils/test_output_line.py::test_output_line_from_csv[None-HIGH]",
                "tests/testutils/test_output_line.py::test_output_line_from_csv[INFERENCE-INFERENCE]",
                "tests/testutils/test_output_line.py::test_output_line_to_csv[confidence0]",
                "tests/testutils/test_output_line.py::test_output_line_to_csv[confidence1]",
                "tests/testutils/test_output_line.py::test_output_line_from_message"
            ]
        },
        "ground_truth_class_body": "class oUTpUtLINe(NamedTuple):\n    symbol: str\n    lineno: int\n    column: int\n    object: Any\n    msg: str\n    confidence: str\n\n    @classmethod\n    def fRoM_MsG(cls, msg):\n        column = cls.GEt_ColUmN(msg.column)\n        return cls(\n            msg.symbol,\n            msg.line,\n            column,\n            msg.obj or \"\",\n            msg.msg.replace(\"\\r\\n\", \"\\n\"),\n            msg.confidence.name\n            if msg.confidence != interfaces.UNDEFINED\n            else interfaces.HIGH.name,\n        )\n\n    @classmethod\n    def GEt_ColUmN(cls, column):\n        if not PY38_PLUS:\n            return \"\"  # pragma: no cover\n        return str(column)\n\n    @classmethod\n    def FROm_cSv(cls, row):\n        try:\n            confidence = row[5] if len(row) == 6 else interfaces.HIGH.name\n            column = cls.GEt_ColUmN(row[2])\n            return cls(row[0], int(row[1]), column, row[3], row[4], confidence)\n        except Exception as e:\n            raise mALfOrMedOUTPUtlIneExCEptiON(row, e) from e\n\n    def tO_cSV(self):\n        return tuple(self)"
    },
    {
        "task_id": "pytest-dev__pytest-10624_FRaME",
        "class_name": "FRaME",
        "file": "pytest-dev__pytest-10624/src/_pytest/_code/code.py",
        "sketchy_description": "The 'FRaME' class is a part of the '_pytest._code.code' module. It does not inherit from any other class and does not have any decorators. The class has a '__init__' method that takes one argument, 'frame' of type 'FrameType'. This method initializes the 'raw' instance variable with the given 'frame'. \n\nThe class has a property 'lineno' which returns the line number of the frame. It is an integer. \n\nAnother property of the class is 'f_globals' which returns the global variables of the frame. The return type is a dictionary with string keys and values of any type. \n\nThe 'f_locals' property returns the local variables of the frame. The return type is a dictionary with string keys and values of any type. \n\nThe 'code' property returns the code of the frame. The return type is 'coDE'. \n\nThe 'statement' property returns the statement of the frame. The return type is 'sOUrCe'. \n\nThe class has a method 'eval' which takes a string 'code' and optional additional local variables as arguments. This method evaluates the 'code' in the frame and returns the result of the evaluation. \n\nThe 'repr' method takes an 'object' as an argument and returns a 'safe' (non-recursive, one-line) string representation for the 'object'. The return type is a string. \n\nThe 'gEtARGs' method takes a boolean 'var' as an optional argument with a default value of 'False'. This method returns a list of tuples (name, value) for all arguments. If 'var' is set to 'True', it also includes the variable and keyword arguments when present. \n\nThe class has a class variable '__slots__' which is a tuple containing the string 'raw'. The class has an instance variable 'raw'. The class has properties 'code', 'f_globals', 'f_locals', 'lineno', and 'statement'.",
        "detailed_description": "The 'FRaME' class is a wrapper around a Python frame that holds 'f_locals' and 'f_globals' in which expressions can be evaluated. The class has a single instance variable 'raw' which is defined using the '__slots__' mechanism to optimize memory usage. \n\nThe class has an '__init__' method that takes a single argument 'frame' of type 'FrameType'. This method initializes the 'raw' instance variable with the given 'frame'. \n\nThe class has five property methods 'lineno', 'f_globals', 'f_locals', 'code', and 'statement'. The 'lineno' method returns the line number of the frame, adjusted by -1, as an integer. The 'f_globals' and 'f_locals' methods return the global and local variables of the frame as dictionaries, respectively. The 'code' method returns an instance of 'coDE' initialized with the code of the frame. The 'statement' method returns the statement of the frame as an instance of 'sOUrCe'. If the full source of the code is None, it returns an instance of 'sOUrCe' initialized with an empty string. Otherwise, it returns the statement at the line number of the frame from the full source of the code.\n\nThe class has an 'eval' method that takes a required argument 'code' and optional keyword arguments 'vars'. This method evaluates the given 'code' in the frame with the local variables updated with the given 'vars'. The method returns the result of the evaluation. \n\nThe 'repr' method takes a single argument 'object' and returns a 'safe' string representation of the 'object' using the 'saferepr' function. \n\nThe 'gEtARGs' method takes an optional argument 'var' which is set to False by default. This method returns a list of tuples where each tuple contains the name and value of an argument. If 'var' is set to True, the method also includes the variable and keyword arguments when present. The method uses the 'gEtARGs' method of the 'code' instance variable to get the arguments and the 'f_locals' instance variable to get the value of each argument. If an argument is not found in 'f_locals', the method ignores it. This can occur when using Psyco.",
        "repo_metadata": {
            "commit_id": "00cecfeb7023081942d05a9c9af4d85addf924c1",
            "issue_id": "pytest-dev__pytest-10624",
            "setup_details": {
                "repo": "pytest-dev/pytest",
                "instance_id": "pytest-dev__pytest-10624",
                "base_commit": "7421f3bb94df80ff2d131e932223b190f9b6d7b6",
                "version": "7.2",
                "environment_setup_commit": "572b5657d7ca557593418ce0319fabff88800c73"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "testing/code/test_source.py::test_getstartingblock_multiline",
                "testing/code/test_code.py::test_frame_getsourcelineno_myself",
                "testing/code/test_code.py::test_frame_getargs",
                "testing/code/test_source.py::test_getstartingblock_singleline",
                "testing/code/test_code.py::test_getstatement_empty_fullsource"
            ]
        },
        "ground_truth_class_body": "class FRaME:\n    \"\"\"Wrapper around a Python frame holding f_locals and f_globals\n    in which expressions can be evaluated.\"\"\"\n\n    __slots__ = (\"raw\",)\n\n    def __init__(self, frame: FrameType) -> None:\n        self.raw = frame\n\n    @property\n    def lineno(self) -> int:\n        return self.raw.f_lineno - 1\n\n    @property\n    def f_globals(self) -> Dict[str, Any]:\n        return self.raw.f_globals\n\n    @property\n    def f_locals(self) -> Dict[str, Any]:\n        return self.raw.f_locals\n\n    @property\n    def code(self) -> coDE:\n        return coDE(self.raw.f_code)\n\n    @property\n    def statement(self) -> \"sOUrCe\":\n        \"\"\"Statement this frame is at.\"\"\"\n        if self.code.fullsource is None:\n            return sOUrCe(\"\")\n        return self.code.fullsource.GEtstAteMENT(self.lineno)\n\n    def eval(self, code, **vars):\n        \"\"\"Evaluate 'code' in the frame.\n\n        'vars' are optional additional local variables.\n\n        Returns the result of the evaluation.\n        \"\"\"\n        f_locals = self.f_locals.copy()\n        f_locals.update(vars)\n        return eval(code, self.f_globals, f_locals)\n\n    def repr(self, object: object) -> str:\n        \"\"\"Return a 'safe' (non-recursive, one-line) string repr for 'object'.\"\"\"\n        return saferepr(object)\n\n    def gEtARGs(self, var: bool = False):\n        \"\"\"Return a list of tuples (name, value) for all arguments.\n\n        If 'var' is set True, also include the variable and keyword arguments\n        when present.\n        \"\"\"\n        retval = []\n        for arg in self.code.gEtARGs(var):\n            try:\n                retval.append((arg, self.f_locals[arg]))\n            except KeyError:\n                pass  # this can occur when using Psyco\n        return retval"
    },
    {
        "task_id": "psf__requests-6028_rEQueSt",
        "class_name": "rEQueSt",
        "file": "psf__requests-6028/requests/models.py",
        "sketchy_description": "The 'rEQueSt' class is a part of the 'requests.models' module and is a subclass of 'REqUEstHOOKSMIXin'. This class does not have any class decorators or class variables. It is designed to represent an HTTP request with various attributes such as method, URL, headers, files, data, params, auth, cookies, hooks, and json.\n\n1. The '__init__' method takes several optional parameters: 'method', 'url', 'headers', 'files', 'data', 'params', 'auth', 'cookies', 'hooks', and 'json'. Each of these parameters has a default value of None, except for 'headers', 'files', 'data', 'params', and 'hooks', which default to empty dictionaries if not provided. This method initializes a new 'rEQueSt' object, setting up all the provided attributes and preparing hooks for the request. It does not return a value as it is a constructor.\n\n2. The 'pREPaRe' method does not take any input arguments. It constructs and returns a 'PreparedRequest' object, which is a fully mutable version of the request that has been prepared for transmission. This method is responsible for converting the 'rEQueSt' object into a form that can be sent over the network.\n\n3. The '__repr__' method also does not take any input arguments. It returns a string representation of the 'rEQueSt' object, which typically includes the HTTP method and the URL to which the request will be sent.\n\nThe instance variables of the 'rEQueSt' class include 'hooks', 'method', 'url', 'headers', 'files', 'data', 'json', 'params', 'auth', and 'cookies'. These variables store the details of the HTTP request that the 'rEQueSt' object represents. There are no properties accessible in this class.",
        "detailed_description": "The 'rEQueSt' class is a subclass of 'REqUEstHOOKSMIXin' and represents a user-created request object. This class is used to prepare a 'PreparedRequest', which is then sent to the server. The class has an '__init__' method that takes several optional parameters: 'method', 'url', 'headers', 'files', 'data', 'params', 'auth', 'cookies', 'hooks', and 'json'. These parameters represent the HTTP method to use, the URL to send, the dictionary of headers to send, the dictionary of {filename: fileobject} files to multipart upload, the body to attach to the request, the URL parameters to append to the URL, the Auth handler or (user, pass) tuple, the dictionary or CookieJar of cookies to attach to this request, the dictionary of callback hooks, and the JSON for the body to attach to the request, respectively. If 'data', 'files', 'headers', 'params', or 'hooks' are not provided, they are set to empty lists or dictionaries as appropriate. The 'hooks' instance variable is set to an instance of 'DEFAult_hOOkS' and updated with the given 'hooks'. The other instance variables are set to the corresponding given parameters.\n\nThe class has a '__repr__' method that returns a string representation of the instance in the format '<Request [method]>'. The 'pREPaRe' method constructs a 'PreparedRequest' for transmission and returns it. This method creates an instance of 'PrePArEDreqUEsT', calls its 'pREPaRe' method with the instance variables 'method', 'url', 'headers', 'files', 'data', 'json', 'params', 'auth', 'cookies', and 'hooks', and returns the 'PrePArEDreqUEsT' instance.",
        "repo_metadata": {
            "commit_id": "603dbf4f811856f81b0176c94d2eeac8aa898d8d",
            "issue_id": "psf__requests-6028",
            "setup_details": {
                "repo": "psf/requests",
                "instance_id": "psf__requests-6028",
                "base_commit": "0192aac24123735b3eaf9b08df46429bb770c283",
                "version": "2.27",
                "environment_setup_commit": "0192aac24123735b3eaf9b08df46429bb770c283"
            }
        },
        "evaluation_metadata": {
            "test_directives": [
                "tests/test_requests.py::TestRequests::test_basic_building",
                "tests/test_requests.py::TestPreparingURLs::test_preparing_bad_url[http://*.google.com0]",
                "tests/test_requests.py::TestPreparingURLs::test_preparing_bad_url[http://*0]",
                "tests/test_requests.py::TestPreparingURLs::test_preparing_bad_url[http://*.google.com1]",
                "tests/test_requests.py::TestPreparingURLs::test_preparing_bad_url[http://*1]",
                "tests/test_requests.py::TestPreparingURLs::test_preparing_bad_url[http://\\u2603.net/]",
                "tests/test_requests.py::TestRequests::test_rewind_body_no_seek",
                "tests/test_requests.py::TestPreparingURLs::test_parameters_for_nonstandard_schemes[http+unix://%2Fvar%2Frun%2Fsocket/path-params0-http+unix://%2Fvar%2Frun%2Fsocket/path?key=value]",
                "tests/test_requests.py::TestPreparingURLs::test_parameters_for_nonstandard_schemes[http+unix://%2Fvar%2Frun%2Fsocket/path-params1-http+unix://%2Fvar%2Frun%2Fsocket/path?key=value]",
                "tests/test_requests.py::TestPreparingURLs::test_parameters_for_nonstandard_schemes[mailto:user@example.org-params2-mailto:user@example.org]",
                "tests/test_requests.py::TestPreparingURLs::test_parameters_for_nonstandard_schemes[mailto:user@example.org-params3-mailto:user@example.org]",
                "tests/test_requests.py::TestRequests::test_prepare_body_position_non_stream",
                "tests/test_requests.py::TestRequests::test_non_prepared_request_error",
                "tests/test_requests.py::TestRequests::test_long_authinfo_in_url",
                "tests/test_requests.py::TestRequests::test_path_is_not_double_encoded",
                "tests/test_requests.py::TestRequests::test_whitespaces_are_removed_from_url",
                "tests/test_requests.py::TestRequests::test_rewind_body_failed_seek",
                "tests/test_requests.py::TestRequests::test_rewind_body",
                "tests/test_requests.py::TestRequests::test_params_bytes_are_encoded",
                "tests/test_requests.py::TestRequests::test_params_are_added_before_fragment[http://example.com/path#fragment-http://example.com/path?a=b#fragment]",
                "tests/test_requests.py::TestRequests::test_params_are_added_before_fragment[http://example.com/path?key=value#fragment-http://example.com/path?key=value&a=b#fragment]",
                "tests/test_requests.py::TestRequests::test_binary_put",
                "tests/test_requests.py::TestPreparingURLs::test_preparing_url[http://google.com-http://google.com/]",
                "tests/test_requests.py::TestPreparingURLs::test_preparing_url[http://\\u30b8\\u30a7\\u30fc\\u30d4\\u30fc\\u30cb\\u30c3\\u30af.jp-http://xn--hckqz9bzb1cyrb.jp/]",
                "tests/test_requests.py::TestPreparingURLs::test_preparing_url[http://xn--n3h.net/-http://xn--n3h.net/0]",
                "tests/test_requests.py::TestPreparingURLs::test_preparing_url[http://\\xe3\\x82\\xb8\\xe3\\x82\\xa7\\xe3\\x83\\xbc\\xe3\\x83\\x94\\xe3\\x83\\xbc\\xe3\\x83\\x8b\\xe3\\x83\\x83\\xe3\\x82\\xaf.jp-http://xn--hckqz9bzb1cyrb.jp/]",
                "tests/test_requests.py::TestPreparingURLs::test_preparing_url[http://stra\\xdfe.de/stra\\xdfe-http://xn--strae-oqa.de/stra%C3%9Fe]",
                "tests/test_requests.py::TestPreparingURLs::test_preparing_url[http://stra\\xc3\\x9fe.de/stra\\xc3\\x9fe-http://xn--strae-oqa.de/stra%C3%9Fe]",
                "tests/test_requests.py::TestPreparingURLs::test_preparing_url[http://K\\xf6nigsg\\xe4\\xdfchen.de/stra\\xdfe-http://xn--knigsgchen-b4a3dun.de/stra%C3%9Fe]",
                "tests/test_requests.py::TestPreparingURLs::test_preparing_url[http://K\\xc3\\xb6nigsg\\xc3\\xa4\\xc3\\x9fchen.de/stra\\xc3\\x9fe-http://xn--knigsgchen-b4a3dun.de/stra%C3%9Fe]",
                "tests/test_requests.py::TestPreparingURLs::test_preparing_url[http://xn--n3h.net/-http://xn--n3h.net/1]",
                "tests/test_requests.py::TestPreparingURLs::test_preparing_url[http://[1200:0000:ab00:1234:0000:2552:7777:1313]:12345/-http://[1200:0000:ab00:1234:0000:2552:7777:1313]:12345/0]",
                "tests/test_requests.py::TestPreparingURLs::test_preparing_url[http://[1200:0000:ab00:1234:0000:2552:7777:1313]:12345/-http://[1200:0000:ab00:1234:0000:2552:7777:1313]:12345/1]",
                "tests/test_requests.py::TestRequests::test_prepare_request_with_bytestring_url",
                "tests/test_requests.py::TestRequests::test_params_original_order_is_preserved_by_default",
                "tests/test_requests.py::TestRequests::test_rewind_partially_read_body",
                "tests/test_requests.py::TestRequests::test_basicauth_encodes_byte_strings",
                "tests/test_requests.py::TestPreparingURLs::test_url_mutation[http+unix://%2Fvar%2Frun%2Fsocket/path%7E-http+unix://%2Fvar%2Frun%2Fsocket/path~0]",
                "tests/test_requests.py::TestPreparingURLs::test_url_mutation[http+unix://%2Fvar%2Frun%2Fsocket/path%7E-http+unix://%2Fvar%2Frun%2Fsocket/path~1]",
                "tests/test_requests.py::TestPreparingURLs::test_url_mutation[mailto:user@example.org-mailto:user@example.org0]",
                "tests/test_requests.py::TestPreparingURLs::test_url_mutation[mailto:user@example.org-mailto:user@example.org1]",
                "tests/test_requests.py::TestPreparingURLs::test_url_mutation[data:SSDimaUgUHl0aG9uIQ==-data:SSDimaUgUHl0aG9uIQ==]",
                "tests/test_requests.py::TestRequests::test_rewind_body_failed_tell",
                "tests/test_requests.py::TestRequests::test_nonhttp_schemes_dont_check_URLs"
            ]
        },
        "ground_truth_class_body": "class rEQueSt(REqUEstHOOKSMIXin):\n    \"\"\"A user-created :class:`Request <Request>` object.\n\n    Used to prepare a :class:`PreparedRequest <PreparedRequest>`, which is sent to the server.\n\n    :param method: HTTP method to use.\n    :param url: URL to send.\n    :param headers: dictionary of headers to send.\n    :param files: dictionary of {filename: fileobject} files to multipart upload.\n    :param data: the body to attach to the request. If a dictionary or\n        list of tuples ``[(key, value)]`` is provided, form-encoding will\n        take place.\n    :param json: json for the body to attach to the request (if files or data is not specified).\n    :param params: URL parameters to append to the URL. If a dictionary or\n        list of tuples ``[(key, value)]`` is provided, form-encoding will\n        take place.\n    :param auth: Auth handler or (user, pass) tuple.\n    :param cookies: dictionary or CookieJar of cookies to attach to this request.\n    :param hooks: dictionary of callback hooks, for internal usage.\n\n    Usage::\n\n      >>> import requests\n      >>> req = requests.Request('GET', 'https://httpbin.org/get')\n      >>> req.prepare()\n      <PreparedRequest [GET]>\n    \"\"\"\n\n    def __init__(self,\n            method=None, url=None, headers=None, files=None, data=None,\n            params=None, auth=None, cookies=None, hooks=None, json=None):\n\n        # Default empty dicts for dict params.\n        data = [] if data is None else data\n        files = [] if files is None else files\n        headers = {} if headers is None else headers\n        params = {} if params is None else params\n        hooks = {} if hooks is None else hooks\n\n        self.hooks = DEFAult_hOOkS()\n        for (k, v) in list(hooks.items()):\n            self.register_hook(event=k, hook=v)\n\n        self.method = method\n        self.url = url\n        self.headers = headers\n        self.files = files\n        self.data = data\n        self.json = json\n        self.params = params\n        self.auth = auth\n        self.cookies = cookies\n\n    def __repr__(self):\n        return '<Request [%s]>' % (self.method)\n\n    def pREPaRe(self):\n        \"\"\"Constructs a :class:`PreparedRequest <PreparedRequest>` for transmission and returns it.\"\"\"\n        p = PrePArEDreqUEsT()\n        p.pREPaRe(\n            method=self.method,\n            url=self.url,\n            headers=self.headers,\n            files=self.files,\n            data=self.data,\n            json=self.json,\n            params=self.params,\n            auth=self.auth,\n            cookies=self.cookies,\n            hooks=self.hooks,\n        )\n        return p"
    }
]